{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d498a499-f688-4233-bdc1-454c1a8ab2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /workspace\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c57f890-955e-4174-9c3f-ab64ef8eca1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Change to the CryptoCorpusBuilder directory\n",
    "os.chdir('/workspace/CryptoCorpusBuilder')\n",
    "print(f\"Changed directory to: {os.getcwd()}\")\n",
    "\n",
    "# Add the current directory to the Python path\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "# Create test directories\n",
    "base_dir = Path(\"/workspace/data/test_corpus\")\n",
    "for subdir in [\"arxiv\", \"github\", \"quantopian\", \"fred\", \"bitmex\"]:\n",
    "    os.makedirs(base_dir / subdir, exist_ok=True)\n",
    "\n",
    "# List what's in the sources/specific_collectors directory to see exact file names\n",
    "collectors_dir = os.path.join(os.getcwd(), 'sources', 'specific_collectors')\n",
    "print(f\"Files in {collectors_dir}:\")\n",
    "if os.path.exists(collectors_dir):\n",
    "    for file in os.listdir(collectors_dir):\n",
    "        print(f\"  {file}\")\n",
    "else:\n",
    "    print(f\"Directory {collectors_dir} does not exist\")\n",
    "\n",
    "# Now try to import\n",
    "try:\n",
    "    from sources.specific_collectors.arvix_collector import ArxivCollector\n",
    "    print(\"Successfully imported ArxivCollector\")\n",
    "except ModuleNotFoundError as e:\n",
    "    print(f\"Failed to import ArxivCollector: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76653a0a-3750-4e68-9e9f-114a71530a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing arvix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcc0a16-0b7d-444b-b44e-9c769f86cb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Configure logging to see more details\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Change to the CryptoCorpusBuilder directory\n",
    "os.chdir('/workspace/CryptoCorpusBuilder')\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Add the current directory to the Python path\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "# Create test directory\n",
    "output_dir = \"/workspace/data/test_corpus/arxiv\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Clear existing files to start fresh\n",
    "import shutil\n",
    "shutil.rmtree(output_dir)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Cleared and recreated directory: {output_dir}\")\n",
    "\n",
    "# Import the collector\n",
    "from sources.specific_collectors.arvix_collector import ArxivCollector\n",
    "\n",
    "# Initialize collector with more verbose debug logging\n",
    "collector = ArxivCollector(output_dir)\n",
    "\n",
    "# Modify the logger to show DEBUG level messages\n",
    "collector.logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Run a test with 6 papers\n",
    "try:\n",
    "    print(\"\\nStarting ArxivCollector test with 6 papers...\")\n",
    "    \n",
    "    # Try 3 papers for each search term to get 6 total\n",
    "    search_terms = [\"cryptocurrency trading\", \"bitcoin price prediction\"]\n",
    "    max_results_per_term = 3\n",
    "    \n",
    "    # Override the collect method to add more logging\n",
    "    original_collect = collector.collect\n",
    "    \n",
    "    def collect_with_logging(*args, **kwargs):\n",
    "        print(f\"\\nCollecting with args: {args}, kwargs: {kwargs}\")\n",
    "        results = original_collect(*args, **kwargs)\n",
    "        print(f\"Collect returned {len(results)} papers\")\n",
    "        return results\n",
    "    \n",
    "    collector.collect = collect_with_logging\n",
    "    \n",
    "    # Run the collection\n",
    "    results = collector.collect(\n",
    "        search_terms=search_terms,\n",
    "        max_results=max_results_per_term\n",
    "    )\n",
    "    \n",
    "    # Detailed results analysis\n",
    "    print(\"\\n===== ArxivCollector Results =====\")\n",
    "    print(f\"Downloaded {len(results)} papers\")\n",
    "    \n",
    "    if results:\n",
    "        valid_papers = 0\n",
    "        for i, paper in enumerate(results):\n",
    "            print(f\"\\nPaper #{i+1}:\")\n",
    "            print(f\"  Title: {paper.get('title')}\")\n",
    "            print(f\"  Authors: {', '.join(paper.get('authors', []))}\")\n",
    "            print(f\"  ArXiv ID: {paper.get('arxiv_id')}\")\n",
    "            print(f\"  Category: {paper.get('primary_category')}\")\n",
    "            \n",
    "            # Check PDF link before download\n",
    "            pdf_link = paper.get('pdf_link')\n",
    "            print(f\"  PDF Link: {pdf_link}\")\n",
    "            \n",
    "            # Check if file exists\n",
    "            filepath = paper.get('filepath')\n",
    "            if filepath and os.path.exists(filepath):\n",
    "                size_kb = os.path.getsize(filepath) / 1024\n",
    "                print(f\"  File: {filepath}\")\n",
    "                print(f\"  File size: {size_kb:.2f} KB\")\n",
    "                print(f\"  File exists: Yes\")\n",
    "                valid_papers += 1\n",
    "            else:\n",
    "                print(f\"  File: {filepath}\")\n",
    "                print(f\"  File exists: No\")\n",
    "        \n",
    "        print(f\"\\nSummary: {valid_papers} out of {len(results)} papers have valid files\")\n",
    "    else:\n",
    "        print(\"No papers were downloaded.\")\n",
    "    \n",
    "    print(\"\\n===== Test Directory Contents =====\")\n",
    "    # List files in the output directory\n",
    "    for root, dirs, files in os.walk(output_dir):\n",
    "        level = root.replace(output_dir, '').count(os.sep)\n",
    "        indent = ' ' * 4 * level\n",
    "        print(f\"{indent}{os.path.basename(root)}/\")\n",
    "        sub_indent = ' ' * 4 * (level + 1)\n",
    "        for file in files:\n",
    "            filepath = os.path.join(root, file)\n",
    "            size_kb = os.path.getsize(filepath) / 1024\n",
    "            print(f\"{sub_indent}{file} ({size_kb:.2f} KB)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error running ArxivCollector: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2252d4aa-a292-4bf3-85b2-7f7ddc983357",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd7f5de-a4bb-47c0-86b9-a66cd1538dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Change to the CryptoCorpusBuilder directory\n",
    "os.chdir('/workspace/CryptoCorpusBuilder')\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Add the current directory to the Python path\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "# Create test directory\n",
    "output_dir = \"/workspace/data/test_corpus/github\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Clear existing files to start fresh\n",
    "import shutil\n",
    "shutil.rmtree(output_dir)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Cleared and recreated directory: {output_dir}\")\n",
    "\n",
    "# Create a modified function to download GitHub repo as ZIP\n",
    "def download_github_repo(repo_name, output_path):\n",
    "    \"\"\"Download a GitHub repository as a ZIP file and extract it\"\"\"\n",
    "    try:\n",
    "        # Construct GitHub ZIP URL\n",
    "        zip_url = f\"https://github.com/{repo_name}/archive/refs/heads/main.zip\"\n",
    "        print(f\"Downloading repo ZIP from: {zip_url}\")\n",
    "        \n",
    "        # Make request\n",
    "        response = requests.get(zip_url, stream=True)\n",
    "        \n",
    "        # Check if main branch doesn't exist, try master\n",
    "        if response.status_code == 404:\n",
    "            zip_url = f\"https://github.com/{repo_name}/archive/refs/heads/master.zip\"\n",
    "            print(f\"Main branch not found, trying master: {zip_url}\")\n",
    "            response = requests.get(zip_url, stream=True)\n",
    "        \n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Extract directly from the in-memory ZIP file\n",
    "        z = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "        z.extractall(output_path)\n",
    "        \n",
    "        # Get the extracted directory name (should be the only directory)\n",
    "        extracted_dirs = [d for d in os.listdir(output_path) if os.path.isdir(os.path.join(output_path, d))]\n",
    "        if extracted_dirs:\n",
    "            extracted_dir = os.path.join(output_path, extracted_dirs[0])\n",
    "            print(f\"Extracted to: {extracted_dir}\")\n",
    "            return extracted_dir\n",
    "        else:\n",
    "            print(\"No directory was extracted\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading repository: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test with a single repository\n",
    "try:\n",
    "    print(\"\\nTesting GitHub repository download...\")\n",
    "    \n",
    "    # Choose a popular crypto trading bot repo\n",
    "    repo_name = \"freqtrade/freqtrade\"\n",
    "    \n",
    "    # Download the repository\n",
    "    local_path = download_github_repo(repo_name, output_dir)\n",
    "    \n",
    "    if local_path:\n",
    "        print(f\"\\n===== GitHub Repository Download Results =====\")\n",
    "        print(f\"Downloaded repository: {repo_name}\")\n",
    "        print(f\"Local path: {local_path}\")\n",
    "        \n",
    "        # Check directory contents\n",
    "        if os.path.exists(local_path):\n",
    "            # Count files in the repository\n",
    "            file_count = sum(1 for _ in Path(local_path).glob('**/*') if _.is_file())\n",
    "            print(f\"File count: {file_count}\")\n",
    "            \n",
    "            # Look for README\n",
    "            readme_files = list(Path(local_path).glob('README*'))\n",
    "            if readme_files:\n",
    "                print(f\"README: {readme_files[0].name}\")\n",
    "                \n",
    "                # Show README contents\n",
    "                with open(readme_files[0], 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    readme_content = f.read(500)  # Read first 500 chars\n",
    "                    print(\"\\nREADME excerpt:\")\n",
    "                    print(readme_content + \"...\")\n",
    "            \n",
    "            # Check for Python files\n",
    "            py_files = list(Path(local_path).glob('**/*.py'))\n",
    "            print(f\"Python files: {len(py_files)}\")\n",
    "            if py_files and len(py_files) > 0:\n",
    "                print(f\"Example Python file: {py_files[0].name}\")\n",
    "    else:\n",
    "        print(\"Failed to download repository\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in test: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77df137-c115-4f6d-b7c9-4e8343dd3abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Quantopian collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41eed305-2ea8-428a-93de-ee179f150ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /workspace/CryptoCorpusBuilder\n",
      "Cleared and recreated directory: /workspace/data/test_corpus/quantopian\n",
      "Downloading repo ZIP from: https://github.com/quantopian/research_public/archive/refs/heads/main.zip\n",
      "Main branch not found, trying master: https://github.com/quantopian/research_public/archive/refs/heads/master.zip\n",
      "Extracted to: /workspace/data/test_corpus/quantopian/research_public-master\n",
      "\n",
      "Found 204 notebook files\n",
      "  1. advanced_sample_analyses/Employee-to-Earnings-Efficiency.ipynb\n",
      "  2. advanced_sample_analyses/Fed-Sentiment-Volatility.ipynb\n",
      "  3. advanced_sample_analyses/Macro-ETFs-and-Fed-Sentiment.ipynb\n",
      "  4. advanced_sample_analyses/Stoploss-Moving-Window.ipynb\n",
      "  5. advanced_sample_analyses/Tesla-and-Oil-(Short).ipynb\n",
      "  ... and 199 more notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Change to the CryptoCorpusBuilder directory\n",
    "os.chdir('/workspace/CryptoCorpusBuilder')\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Add the current directory to the Python path\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "# Create test directory\n",
    "output_dir = \"/workspace/data/test_corpus/quantopian\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Clear existing files to start fresh\n",
    "import shutil\n",
    "shutil.rmtree(output_dir)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Cleared and recreated directory: {output_dir}\")\n",
    "\n",
    "# Download function\n",
    "def download_github_repo(repo_name, output_path):\n",
    "    \"\"\"Download a GitHub repository as a ZIP file and extract it\"\"\"\n",
    "    try:\n",
    "        import requests\n",
    "        import zipfile\n",
    "        import io\n",
    "        \n",
    "        # Construct GitHub ZIP URL\n",
    "        zip_url = f\"https://github.com/{repo_name}/archive/refs/heads/main.zip\"\n",
    "        print(f\"Downloading repo ZIP from: {zip_url}\")\n",
    "        \n",
    "        # Make request\n",
    "        response = requests.get(zip_url, stream=True)\n",
    "        \n",
    "        # Check if main branch doesn't exist, try master\n",
    "        if response.status_code == 404:\n",
    "            zip_url = f\"https://github.com/{repo_name}/archive/refs/heads/master.zip\"\n",
    "            print(f\"Main branch not found, trying master: {zip_url}\")\n",
    "            response = requests.get(zip_url, stream=True)\n",
    "        \n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Extract directly from the in-memory ZIP file\n",
    "        z = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "        z.extractall(output_path)\n",
    "        \n",
    "        # Get the extracted directory name (should be the only directory)\n",
    "        extracted_dirs = [d for d in os.listdir(output_path) if os.path.isdir(os.path.join(output_path, d))]\n",
    "        if extracted_dirs:\n",
    "            extracted_dir = os.path.join(output_path, extracted_dirs[0])\n",
    "            print(f\"Extracted to: {extracted_dir}\")\n",
    "            # Return as Path object\n",
    "            return Path(extracted_dir)\n",
    "        else:\n",
    "            print(\"No directory was extracted\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading repository: {e}\")\n",
    "        return None\n",
    "\n",
    "# Download the repository\n",
    "repo_name = \"quantopian/research_public\"\n",
    "repo_dir = download_github_repo(repo_name, output_dir)\n",
    "\n",
    "# Check the repository contents\n",
    "if repo_dir and os.path.exists(repo_dir):\n",
    "    # Find all notebook files\n",
    "    notebook_files = list(repo_dir.glob(\"**/*.ipynb\"))\n",
    "    print(f\"\\nFound {len(notebook_files)} notebook files\")\n",
    "    \n",
    "    # Print first 5 notebook paths\n",
    "    for i, nb_path in enumerate(notebook_files[:5]):\n",
    "        rel_path = nb_path.relative_to(repo_dir)\n",
    "        print(f\"  {i+1}. {rel_path}\")\n",
    "        \n",
    "    if len(notebook_files) > 5:\n",
    "        print(f\"  ... and {len(notebook_files) - 5} more notebooks\")\n",
    "else:\n",
    "    print(\"Failed to download repository\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c294d5d-0e59-4482-a05a-4e6dfcb702b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /workspace\n",
      "Repository directory: /workspace/data/test_corpus/quantopian/research_public-master\n",
      "Found 204 notebook files\n",
      "\n",
      "Processing notebook 1: advanced_sample_analyses/Employee-to-Earnings-Efficiency.ipynb\n",
      "Processed successfully\n",
      "\n",
      "Processing notebook 2: advanced_sample_analyses/Fed-Sentiment-Volatility.ipynb\n",
      "Processed successfully\n",
      "\n",
      "Processing notebook 3: advanced_sample_analyses/Macro-ETFs-and-Fed-Sentiment.ipynb\n",
      "Processed successfully\n",
      "\n",
      "Processing notebook 4: advanced_sample_analyses/Stoploss-Moving-Window.ipynb\n",
      "Processed successfully\n",
      "\n",
      "Processing notebook 5: advanced_sample_analyses/Tesla-and-Oil-(Short).ipynb\n",
      "Processed successfully\n",
      "\n",
      "===== Notebook Processing Results =====\n",
      "Processed 5 notebooks\n",
      "\n",
      "Notebook #1:\n",
      "  Title: Employee-to-Earnings-Efficiency\n",
      "  Path: advanced_sample_analyses/Employee-to-Earnings-Efficiency.ipynb\n",
      "  Imports: d, math, m, scipy, numpy, zipline\n",
      "\n",
      "# initi\n",
      "  Code cells: 13\n",
      "  Markdown cells: 12\n",
      "  Total cells: 25\n",
      "  File size: 627.05 KB\n",
      "\n",
      "Notebook #2:\n",
      "  Title: Fed-Sentiment-Volatility\n",
      "  Path: advanced_sample_analyses/Fed-Sentiment-Volatility.ipynb\n",
      "  Imports: matplotlib, d, time\n",
      "from pyk, numpy\n",
      "import, p, m\n",
      "  Code cells: 33\n",
      "  Markdown cells: 23\n",
      "  Total cells: 56\n",
      "  File size: 1355.21 KB\n",
      "\n",
      "Notebook #3:\n",
      "  Title: Macro-ETFs-and-Fed-Sentiment\n",
      "  Path: advanced_sample_analyses/Macro-ETFs-and-Fed-Sentiment.ipynb\n",
      "  Imports: matplotlib, d, time\n",
      "from pyk, numpy\n",
      "import, p\n",
      "  Code cells: 21\n",
      "  Markdown cells: 21\n",
      "  Total cells: 42\n",
      "  File size: 1047.83 KB\n",
      "\n",
      "Notebook #4:\n",
      "  Title: Stoploss-Moving-Window\n",
      "  Path: advanced_sample_analyses/Stoploss-Moving-Window.ipynb\n",
      "  Imports: d, numpy\n",
      "import m, p, matplotlib, scipy\n",
      "  Code cells: 9\n",
      "  Markdown cells: 11\n",
      "  Total cells: 20\n",
      "  File size: 121.82 KB\n",
      "\n",
      "Notebook #5:\n",
      "  Title: Tesla-and-Oil-(Short)\n",
      "  Path: advanced_sample_analyses/Tesla-and-Oil-(Short).ipynb\n",
      "  Imports: matplotlib, pykalman, numpy\n",
      "import, time\n",
      "import d\n",
      "  Code cells: 12\n",
      "  Markdown cells: 10\n",
      "  Total cells: 22\n",
      "  File size: 366.35 KB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# We're already in the right directory, so we'll just continue from there\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Path to the downloaded repository\n",
    "repo_dir = Path(\"/workspace/data/test_corpus/quantopian/research_public-master\")\n",
    "print(f\"Repository directory: {repo_dir}\")\n",
    "\n",
    "# Find all notebook files\n",
    "notebook_files = list(repo_dir.glob(\"**/*.ipynb\"))\n",
    "print(f\"Found {len(notebook_files)} notebook files\")\n",
    "\n",
    "# Process a few notebooks\n",
    "processed_notebooks = []\n",
    "\n",
    "for i, notebook_path in enumerate(notebook_files[:5]):  # Process first 5 notebooks\n",
    "    try:\n",
    "        print(f\"\\nProcessing notebook {i+1}: {notebook_path.relative_to(repo_dir)}\")\n",
    "        \n",
    "        # Extract notebook info\n",
    "        with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "            notebook = json.load(f)\n",
    "        \n",
    "        # Get metadata\n",
    "        metadata = notebook.get('metadata', {})\n",
    "        \n",
    "        # Get title from first heading cell or filename\n",
    "        title = notebook_path.stem\n",
    "        \n",
    "        # Look for title in markdown cells\n",
    "        for cell in notebook.get('cells', []):\n",
    "            if cell.get('cell_type') == 'markdown':\n",
    "                source = ''.join(cell.get('source', []))\n",
    "                # Look for heading\n",
    "                if source.startswith('# '):\n",
    "                    title = source.strip().lstrip('#').strip()\n",
    "                    print(f\"Found title: {title}\")\n",
    "                    break\n",
    "        \n",
    "        # Extract cell statistics\n",
    "        code_cells = []\n",
    "        markdown_cells = []\n",
    "        \n",
    "        for cell in notebook.get('cells', []):\n",
    "            cell_type = cell.get('cell_type')\n",
    "            source = ''.join(cell.get('source', []))\n",
    "            \n",
    "            if cell_type == 'code':\n",
    "                code_cells.append({\n",
    "                    'source': source,\n",
    "                    'outputs': len(cell.get('outputs', []))\n",
    "                })\n",
    "            elif cell_type == 'markdown':\n",
    "                markdown_cells.append({\n",
    "                    'source': source\n",
    "                })\n",
    "        \n",
    "        # Extract import statements\n",
    "        imports = []\n",
    "        for cell in code_cells:\n",
    "            source = cell['source']\n",
    "            # Match import statements\n",
    "            for match in re.finditer(r'^(?:from\\s+(\\S+)\\s+import|import\\s+([^as]+)(?:\\s+as\\s+\\S+)?)', source, re.MULTILINE):\n",
    "                module = match.group(1) or match.group(2)\n",
    "                if module:\n",
    "                    module = module.strip().split('.')[0]  # Get base module\n",
    "                    if module and module not in imports:\n",
    "                        imports.append(module)\n",
    "        \n",
    "        notebook_info = {\n",
    "            'title': title,\n",
    "            'path': str(notebook_path),\n",
    "            'imports': imports,\n",
    "            'code_cell_count': len(code_cells),\n",
    "            'markdown_cell_count': len(markdown_cells),\n",
    "            'total_cell_count': len(code_cells) + len(markdown_cells)\n",
    "        }\n",
    "        \n",
    "        processed_notebooks.append(notebook_info)\n",
    "        print(f\"Processed successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing notebook {notebook_path}: {e}\")\n",
    "\n",
    "# Print results\n",
    "print(f\"\\n===== Notebook Processing Results =====\")\n",
    "print(f\"Processed {len(processed_notebooks)} notebooks\")\n",
    "\n",
    "for i, notebook in enumerate(processed_notebooks):\n",
    "    print(f\"\\nNotebook #{i+1}:\")\n",
    "    print(f\"  Title: {notebook.get('title')}\")\n",
    "    rel_path = Path(notebook.get('path')).relative_to(repo_dir)\n",
    "    print(f\"  Path: {rel_path}\")\n",
    "    print(f\"  Imports: {', '.join(notebook.get('imports', []))}\")\n",
    "    print(f\"  Code cells: {notebook.get('code_cell_count')}\")\n",
    "    print(f\"  Markdown cells: {notebook.get('markdown_cell_count')}\")\n",
    "    print(f\"  Total cells: {notebook.get('total_cell_count')}\")\n",
    "    \n",
    "    # Check file size\n",
    "    filepath = notebook.get('path')\n",
    "    if filepath and os.path.exists(filepath):\n",
    "        size_kb = os.path.getsize(filepath) / 1024\n",
    "        print(f\"  File size: {size_kb:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc54005-ea74-4382-a0f9-f477fb78c0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bitmex collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af908a1f-352e-40a6-861e-e24a3b815fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "below patches all collectors to absolute path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ee246f-1c79-4954-811d-7516b3454a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Set the working directory\n",
    "os.chdir('/workspace/CryptoCorpusBuilder')\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Get all collector files in specific_collectors directory\n",
    "collectors_dir = os.path.join(os.getcwd(), 'sources', 'specific_collectors')\n",
    "collector_files = list(Path(collectors_dir).glob('*.py'))\n",
    "\n",
    "print(f\"Found {len(collector_files)} collector files to patch\")\n",
    "\n",
    "# Regular expression to match relative imports\n",
    "relative_import_pattern = re.compile(r'from\\s+\\.\\.([\\w\\.]+)\\s+import')\n",
    "\n",
    "# Process each file\n",
    "for file_path in collector_files:\n",
    "    print(f\"\\nProcessing: {file_path.name}\")\n",
    "    \n",
    "    # Read the file\n",
    "    with open(file_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Find all relative imports\n",
    "    matches = relative_import_pattern.findall(content)\n",
    "    if not matches:\n",
    "        print(\"  No relative imports found\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"  Found relative imports: {matches}\")\n",
    "    \n",
    "    # Replace relative imports with absolute ones\n",
    "    updated_content = content\n",
    "    for module in matches:\n",
    "        relative_import = f\"from ..{module} import\"\n",
    "        absolute_import = f\"from sources.{module} import\"\n",
    "        \n",
    "        # Check if this import exists in the content\n",
    "        if relative_import in updated_content:\n",
    "            print(f\"  Replacing: {relative_import} -> {absolute_import}\")\n",
    "            updated_content = updated_content.replace(relative_import, absolute_import)\n",
    "    \n",
    "    # Write the updated content back to the file\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(updated_content)\n",
    "    \n",
    "    print(f\"  Updated file: {file_path.name}\")\n",
    "\n",
    "print(\"\\nAll collector files have been patched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f83f116-8af3-4a87-8bce-b592dc943be3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04acf0b-122e-41d9-8d18-d68a96f162f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the CryptoCorpusBuilder directory to Python path\n",
    "sys.path.append('/workspace/CryptoCorpusBuilder')\n",
    "\n",
    "# Try a clean import of the WebCollector\n",
    "try:\n",
    "    from sources.web_collector import WebCollector\n",
    "    print(\"Successfully imported WebCollector\")\n",
    "    \n",
    "    # Now try importing the BitMEXResearchCollector\n",
    "    from sources.specific_collectors.bitmex_collector import BitMEXResearchCollector\n",
    "    print(\"Successfully imported BitMEXResearchCollector\")\n",
    "except ImportError as e:\n",
    "    print(f\"Import error: {e}\")\n",
    "\n",
    "# If imports work, set up a simple test\n",
    "if 'BitMEXResearchCollector' in locals():\n",
    "    # Create test directory\n",
    "    output_dir = \"/workspace/data/test_corpus/bitmex\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize collector\n",
    "    collector = BitMEXResearchCollector(output_dir)\n",
    "    print(f\"Successfully initialized BitMEXResearchCollector with output_dir: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bdbfb4d-bcbc-4f78-8107-f0a92701f8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 22:05:46,923 - BitMEXResearchCollector - WARNING - ⚠️ BitMEXResearchCollector always bypasses robots.txt! ⚠️\n",
      "2025-05-09 22:05:46,923 - BitMEXResearchCollector - WARNING - ⚠️ FOR PRODUCTION, REVIEW ETHICAL AND LEGAL CONSIDERATIONS ⚠️\n",
      "2025-05-09 22:05:46,924 - BitMEXResearchCollector - WARNING - ⚠️ Created test collector that ignores robots.txt ⚠️\n",
      "2025-05-09 22:05:46,924 - BitMEXResearchCollector - WARNING - ⚠️ FOR TESTING PURPOSES ONLY - DO NOT USE IN PRODUCTION ⚠️\n",
      "2025-05-09 22:05:46,926 - BitMEXResearchCollector - INFO - Collecting BitMEX Research blog posts (max 1 pages)\n",
      "2025-05-09 22:05:46,926 - BitMEXResearchCollector - INFO - Fetching page: https://blog.bitmex.com/research/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared and recreated directory: /workspace/data/test_corpus/bitmex\n",
      "\n",
      "⚠️ TESTING ONLY: Running BitMEX collector with robots.txt check bypassed ⚠️\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 22:05:47,447 - BitMEXResearchCollector - INFO - Found 1 potential post containers\n",
      "2025-05-09 22:05:47,450 - BitMEXResearchCollector - INFO - Found post: Removing Bitcoin’s Guardrails\n",
      "2025-05-09 22:05:47,456 - BitMEXResearchCollector - INFO - Saved metadata for 1 posts to /workspace/data/test_corpus/bitmex/bitmex_research_posts.json\n",
      "2025-05-09 22:05:47,457 - BitMEXResearchCollector - INFO - Fetching post: https://blog.bitmex.com/removing-bitcoins-guardrails/\n",
      "2025-05-09 22:05:47,606 - BitMEXResearchCollector - INFO - Saved HTML to /workspace/data/test_corpus/bitmex/removing-bitcoins-guardrails.html\n",
      "2025-05-09 22:05:47,616 - BitMEXResearchCollector - INFO - Saved metadata for 1 posts to /workspace/data/test_corpus/bitmex/bitmex_research_posts.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== BitMEXResearchCollector Results =====\n",
      "Collected 1 research posts\n",
      "\n",
      "Post #1:\n",
      "  Title: Removing Bitcoin’s Guardrails\n",
      "  Date: 7 May 2025\n",
      "  URL: https://blog.bitmex.com/removing-bitcoins-guardrails/\n",
      "  PDF Count: 0\n",
      "  HTML saved: Yes (2.20 KB)\n",
      "\n",
      "===== Test Directory Contents =====\n",
      "bitmex/\n",
      "    bitmex_research.html (323.90 KB)\n",
      "    bitmex_research_posts.json (2.51 KB)\n",
      "    removing-bitcoins-guardrails.html (2.20 KB)\n",
      "\n",
      "⚠️ IMPORTANT: In production, always respect robots.txt rules ⚠️\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Add CryptoCorpusBuilder to Python path\n",
    "sys.path.append('/workspace/CryptoCorpusBuilder')\n",
    "\n",
    "# Import the collector classes\n",
    "from sources.web_collector import WebCollector\n",
    "from sources.specific_collectors.bitmex_collector import BitMEXResearchCollector\n",
    "\n",
    "# Create a subclass that overrides the robots.txt check\n",
    "class TestWebCollector(WebCollector):\n",
    "    def _can_fetch(self, url):\n",
    "        \"\"\"Override the robots.txt check for testing purposes only\"\"\"\n",
    "        self.logger.warning(\"⚠️ Bypassing robots.txt check for TESTING PURPOSES ONLY ⚠️\")\n",
    "        self.logger.warning(\"⚠️ In production, always respect robots.txt rules ⚠️\")\n",
    "        return True  # Always return True, ignoring robots.txt\n",
    "\n",
    "# Create a subclass of BitMEXResearchCollector that uses our test collector\n",
    "class TestBitMEXCollector(BitMEXResearchCollector):\n",
    "    def __init__(self, output_dir):\n",
    "        super().__init__(output_dir)\n",
    "        # Override the _can_fetch method with our test version\n",
    "        self._can_fetch = lambda url: True\n",
    "        # Set a flag to remind us this is a test version\n",
    "        self.respect_robots_txt = False\n",
    "        self.logger.warning(\"⚠️ Created test collector that ignores robots.txt ⚠️\")\n",
    "        self.logger.warning(\"⚠️ FOR TESTING PURPOSES ONLY - DO NOT USE IN PRODUCTION ⚠️\")\n",
    "\n",
    "# Create test directory\n",
    "output_dir = \"/workspace/data/test_corpus/bitmex\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Clear existing files to start fresh\n",
    "import shutil\n",
    "shutil.rmtree(output_dir)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Cleared and recreated directory: {output_dir}\")\n",
    "\n",
    "# Initialize collector\n",
    "collector = TestBitMEXCollector(output_dir)\n",
    "collector.logger.setLevel(logging.INFO)\n",
    "\n",
    "# Test with a small sample\n",
    "try:\n",
    "    print(\"\\n⚠️ TESTING ONLY: Running BitMEX collector with robots.txt check bypassed ⚠️\")\n",
    "    # Limit to 1 page to keep the test quick and minimize impact\n",
    "    results = collector.collect(max_pages=1)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\n===== BitMEXResearchCollector Results =====\")\n",
    "    print(f\"Collected {len(results)} research posts\")\n",
    "    \n",
    "    if results:\n",
    "        # Show first 3 posts (or all if fewer)\n",
    "        show_count = min(3, len(results))\n",
    "        for i, post in enumerate(results[:show_count]):\n",
    "            print(f\"\\nPost #{i+1}:\")\n",
    "            print(f\"  Title: {post.get('title')}\")\n",
    "            print(f\"  Date: {post.get('date')}\")\n",
    "            print(f\"  URL: {post.get('url')}\")\n",
    "            \n",
    "            # Check for PDFs\n",
    "            pdfs = post.get('pdfs', [])\n",
    "            print(f\"  PDF Count: {len(pdfs)}\")\n",
    "            \n",
    "            for j, pdf in enumerate(pdfs[:2]):  # Show first 2 PDFs\n",
    "                print(f\"    PDF #{j+1}: {pdf.get('filename')}\")\n",
    "                filepath = pdf.get('filepath', '')\n",
    "                if filepath and os.path.exists(filepath):\n",
    "                    size_kb = os.path.getsize(filepath) / 1024\n",
    "                    print(f\"    Size: {size_kb:.2f} KB\")\n",
    "                    print(f\"    Exists: Yes\")\n",
    "                else:\n",
    "                    print(f\"    Exists: No\")\n",
    "            \n",
    "            # Check for saved HTML\n",
    "            html_path = post.get('saved_html_path', '')\n",
    "            if html_path and os.path.exists(html_path):\n",
    "                size_kb = os.path.getsize(html_path) / 1024\n",
    "                print(f\"  HTML saved: Yes ({size_kb:.2f} KB)\")\n",
    "            else:\n",
    "                print(f\"  HTML saved: No\")\n",
    "    \n",
    "    # Print directory contents\n",
    "    print(\"\\n===== Test Directory Contents =====\")\n",
    "    for root, dirs, files in os.walk(output_dir):\n",
    "        level = root.replace(output_dir, '').count(os.sep)\n",
    "        indent = ' ' * 4 * level\n",
    "        print(f\"{indent}{os.path.basename(root)}/\")\n",
    "        sub_indent = ' ' * 4 * (level + 1)\n",
    "        for f in files[:5]:  # Show first 5 files\n",
    "            filepath = os.path.join(root, f)\n",
    "            size_kb = os.path.getsize(filepath) / 1024\n",
    "            print(f\"{sub_indent}{f} ({size_kb:.2f} KB)\")\n",
    "        if len(files) > 5:\n",
    "            print(f\"{sub_indent}... and {len(files)-5} more files\")\n",
    "    \n",
    "    print(\"\\n⚠️ IMPORTANT: In production, always respect robots.txt rules ⚠️\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error running BitMEXCollector: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "625e2be4-3bad-4d84-b3e5-15325b92dcd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /workspace\n",
      "Cleared and recreated directory: /workspace/data/test_corpus/bitmex_debug\n",
      "\n",
      "===== Debugging BitMEX Blog =====\n",
      "1. Fetching https://blog.bitmex.com/research/\n",
      "Status code: 200\n",
      "Saved raw HTML to /workspace/data/test_corpus/bitmex_debug/bitmex_blog.html\n",
      "Saved parsed HTML to /workspace/data/test_corpus/bitmex_debug/bitmex_blog_parsed.html\n",
      "\n",
      "2. Checking for article elements\n",
      "Found 0 article elements\n",
      "No article elements found, checking for alternative structures\n",
      "Found 263 heading elements on page\n",
      "  Heading 1: Coming Soon: SXTUSDT Perpetual Swap Listing With Up to 50x Leverage\n",
      "  Heading 2: Removing Bitcoin’s Guardrails\n",
      "  Heading 3: Heatbiting The Office\n",
      "  Heading 4: The Ripple story\n",
      "  Heading 5: Antminer S19 Pro vs Whatsminer M30S+\n",
      "Found 808 potential post containers\n",
      "Found 0 links containing '/blog/'\n",
      "\n",
      "===== Debugging Summary =====\n",
      "Status: success\n",
      "Articles found: 0\n",
      "HTML saved to: /workspace/data/test_corpus/bitmex_debug/bitmex_blog.html\n",
      "\n",
      "===== Directory Contents =====\n",
      "  bitmex_blog.html (323.90 KB)\n",
      "  bitmex_blog_parsed.html (378.67 KB)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,  # Set to DEBUG for more detailed output\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Add CryptoCorpusBuilder to Python path\n",
    "sys.path.append('/workspace/CryptoCorpusBuilder')\n",
    "\n",
    "# Create a direct debugging script without relying on the collector classes\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Set up test directory\n",
    "output_dir = \"/workspace/data/test_corpus/bitmex_debug\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Clear existing files to start fresh\n",
    "import shutil\n",
    "shutil.rmtree(output_dir)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Cleared and recreated directory: {output_dir}\")\n",
    "\n",
    "# Direct debugging function\n",
    "def debug_bitmex_blog():\n",
    "    \"\"\"Debug the BitMEX blog directly\"\"\"\n",
    "    print(\"\\n===== Debugging BitMEX Blog =====\")\n",
    "    \n",
    "    url = \"https://blog.bitmex.com/research/\"\n",
    "    \n",
    "    print(f\"1. Fetching {url}\")\n",
    "    \n",
    "    # Set up session with browser-like headers\n",
    "    session = requests.Session()\n",
    "    session.headers.update({\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\"\n",
    "    })\n",
    "    \n",
    "    try:\n",
    "        response = session.get(url, timeout=30)\n",
    "        print(f\"Status code: {response.status_code}\")\n",
    "        \n",
    "        # Save the raw HTML response for debugging\n",
    "        html_file = os.path.join(output_dir, \"bitmex_blog.html\")\n",
    "        with open(html_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(response.text)\n",
    "        print(f\"Saved raw HTML to {html_file}\")\n",
    "        \n",
    "        # Parse with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Save the parsed HTML for debugging\n",
    "        parsed_file = os.path.join(output_dir, \"bitmex_blog_parsed.html\")\n",
    "        with open(parsed_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(str(soup.prettify()))\n",
    "        print(f\"Saved parsed HTML to {parsed_file}\")\n",
    "        \n",
    "        # Check for articles\n",
    "        print(\"\\n2. Checking for article elements\")\n",
    "        articles = soup.find_all(\"article\")\n",
    "        print(f\"Found {len(articles)} article elements\")\n",
    "        \n",
    "        if articles:\n",
    "            # Examine the first article\n",
    "            first_article = articles[0]\n",
    "            print(\"\\n3. First article details:\")\n",
    "            print(f\"Article class: {first_article.get('class')}\")\n",
    "            \n",
    "            # Check for title\n",
    "            title_elem = first_article.find(['h1', 'h2'], class_='entry-title')\n",
    "            if title_elem:\n",
    "                print(f\"Title element found: {title_elem.text.strip()}\")\n",
    "                \n",
    "                # Check for link\n",
    "                link_elem = title_elem.find('a')\n",
    "                if link_elem:\n",
    "                    print(f\"Link found: {link_elem['href']}\")\n",
    "                else:\n",
    "                    print(\"No link found in title element\")\n",
    "            else:\n",
    "                print(\"No title element found with class 'entry-title'\")\n",
    "                \n",
    "                # Look for any headings\n",
    "                headings = first_article.find_all(['h1', 'h2', 'h3'])\n",
    "                if headings:\n",
    "                    print(f\"Found {len(headings)} heading elements:\")\n",
    "                    for i, h in enumerate(headings[:3]):\n",
    "                        print(f\"  Heading {i+1}: {h.text.strip()}\")\n",
    "                else:\n",
    "                    print(\"No heading elements found\")\n",
    "            \n",
    "            # Check for date\n",
    "            date_elem = first_article.find(class_='entry-date')\n",
    "            if date_elem:\n",
    "                print(f\"Date element found: {date_elem.text.strip()}\")\n",
    "            else:\n",
    "                print(\"No date element found with class 'entry-date'\")\n",
    "            \n",
    "            # Check for any links\n",
    "            links = first_article.find_all('a')\n",
    "            print(f\"Found {len(links)} links in first article\")\n",
    "            for i, link in enumerate(links[:3]):\n",
    "                print(f\"  Link {i+1}: {link.text.strip()} -> {link.get('href')}\")\n",
    "                \n",
    "            # Save the first article HTML\n",
    "            article_file = os.path.join(output_dir, \"first_article.html\")\n",
    "            with open(article_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(str(first_article.prettify()))\n",
    "            print(f\"Saved first article HTML to {article_file}\")\n",
    "        else:\n",
    "            print(\"No article elements found, checking for alternative structures\")\n",
    "            \n",
    "            # Look for any headings\n",
    "            headings = soup.find_all(['h1', 'h2', 'h3'])\n",
    "            print(f\"Found {len(headings)} heading elements on page\")\n",
    "            for i, h in enumerate(headings[:5]):\n",
    "                print(f\"  Heading {i+1}: {h.text.strip()}\")\n",
    "            \n",
    "            # Look for any blog post containers\n",
    "            post_containers = soup.find_all(class_=lambda c: c and ('post' in c.lower() or 'entry' in c.lower()))\n",
    "            print(f\"Found {len(post_containers)} potential post containers\")\n",
    "            \n",
    "            # Look for links that might be blog posts\n",
    "            blog_links = soup.find_all('a', href=lambda h: h and '/blog/' in h)\n",
    "            print(f\"Found {len(blog_links)} links containing '/blog/'\")\n",
    "            for i, link in enumerate(blog_links[:5]):\n",
    "                print(f\"  Link {i+1}: {link.text.strip()} -> {link.get('href')}\")\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"articles\": len(articles),\n",
    "            \"html_file\": html_file\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "# Run the debugging function\n",
    "debug_result = debug_bitmex_blog()\n",
    "\n",
    "# Summarize results\n",
    "print(\"\\n===== Debugging Summary =====\")\n",
    "print(f\"Status: {debug_result.get('status')}\")\n",
    "if debug_result.get('status') == 'success':\n",
    "    print(f\"Articles found: {debug_result.get('articles')}\")\n",
    "    print(f\"HTML saved to: {debug_result.get('html_file')}\")\n",
    "else:\n",
    "    print(f\"Error: {debug_result.get('error')}\")\n",
    "\n",
    "print(\"\\n===== Directory Contents =====\")\n",
    "for file in os.listdir(output_dir):\n",
    "    filepath = os.path.join(output_dir, file)\n",
    "    size_kb = os.path.getsize(filepath) / 1024\n",
    "    print(f\"  {file} ({size_kb:.2f} KB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "928ebb4f-99e3-4eff-a056-f5de8e2d9382",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 22:05:10,538 - UpdatedBitMEXCollector - INFO - Collecting BitMEX Research blog posts (max 1 pages)\n",
      "2025-05-09 22:05:10,538 - UpdatedBitMEXCollector - INFO - Fetching page: https://blog.bitmex.com/research/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared and recreated directory: /workspace/data/test_corpus/bitmex_updated\n",
      "\n",
      "Starting updated BitMEX collector test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 22:05:13,782 - UpdatedBitMEXCollector - INFO - Found 1 potential post containers\n",
      "2025-05-09 22:05:13,784 - UpdatedBitMEXCollector - INFO - Found post: Removing Bitcoin’s Guardrails\n",
      "2025-05-09 22:05:13,790 - UpdatedBitMEXCollector - INFO - Saved metadata for 1 posts to /workspace/data/test_corpus/bitmex_updated/bitmex_research_posts.json\n",
      "2025-05-09 22:05:13,790 - UpdatedBitMEXCollector - INFO - Fetching post: https://blog.bitmex.com/removing-bitcoins-guardrails/\n",
      "2025-05-09 22:05:13,943 - UpdatedBitMEXCollector - INFO - Saved HTML to /workspace/data/test_corpus/bitmex_updated/removing-bitcoins-guardrails.html\n",
      "2025-05-09 22:05:13,948 - UpdatedBitMEXCollector - INFO - Saved metadata for 1 posts to /workspace/data/test_corpus/bitmex_updated/bitmex_research_posts.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Updated BitMEXCollector Results =====\n",
      "Collected 1 research posts\n",
      "\n",
      "Post #1:\n",
      "  Title: Removing Bitcoin’s Guardrails\n",
      "  URL: https://blog.bitmex.com/removing-bitcoins-guardrails/\n",
      "  PDF Count: 0\n",
      "  HTML saved: Yes (2.20 KB)\n",
      "\n",
      "===== Test Directory Contents =====\n",
      "bitmex_updated/\n",
      "    bitmex_research.html (323.90 KB)\n",
      "    bitmex_research_posts.json (2.52 KB)\n",
      "    removing-bitcoins-guardrails.html (2.20 KB)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create test directory\n",
    "output_dir = \"/workspace/data/test_corpus/bitmex_updated\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Clear existing files to start fresh\n",
    "import shutil\n",
    "shutil.rmtree(output_dir)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Cleared and recreated directory: {output_dir}\")\n",
    "\n",
    "# Create a new version of the BitMEX collector\n",
    "class UpdatedBitMEXCollector:\n",
    "    \"\"\"Updated collector for BitMEX Research blog posts\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir):\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.base_url = 'https://blog.bitmex.com/research/'\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\",\n",
    "            \"Accept-Language\": \"en-US,en;q=0.5\"\n",
    "        })\n",
    "        self.logger = logging.getLogger(\"UpdatedBitMEXCollector\")\n",
    "    \n",
    "    def collect(self, max_pages=1):\n",
    "        \"\"\"Collect research blog posts from BitMEX with updated HTML parsing\"\"\"\n",
    "        self.logger.info(f\"Collecting BitMEX Research blog posts (max {max_pages} pages)\")\n",
    "        \n",
    "        all_posts = []\n",
    "        \n",
    "        # Fetch the main research page\n",
    "        self.logger.info(f\"Fetching page: {self.base_url}\")\n",
    "        response = self.session.get(self.base_url, timeout=30)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            self.logger.error(f\"Failed to fetch page: {response.status_code}\")\n",
    "            return all_posts\n",
    "            \n",
    "        # Save the raw HTML for inspection if needed\n",
    "        raw_html_path = self.output_dir / \"bitmex_research.html\"\n",
    "        with open(raw_html_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(response.text)\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Based on the debug output, look for post containers\n",
    "        # Let's try several approaches\n",
    "        \n",
    "        # Approach 1: Look for headings within containers\n",
    "        posts_found = 0\n",
    "        \n",
    "        # Find all div containers that might be posts\n",
    "        post_containers = soup.find_all(\"div\", class_=lambda c: c and ('post' in c.lower() or 'entry' in c.lower() or 'article' in c.lower()))\n",
    "        \n",
    "        self.logger.info(f\"Found {len(post_containers)} potential post containers\")\n",
    "        \n",
    "        # Process each container\n",
    "        for container in post_containers[:10]:  # Limit to first 10 for testing\n",
    "            # Look for a heading that might be the title\n",
    "            heading = container.find(['h1', 'h2', 'h3', 'h4'])\n",
    "            \n",
    "            if not heading:\n",
    "                continue\n",
    "                \n",
    "            # Found a potential post\n",
    "            posts_found += 1\n",
    "            \n",
    "            # Extract title\n",
    "            title = heading.text.strip()\n",
    "            \n",
    "            # Look for a link to the full post\n",
    "            link = None\n",
    "            if heading.find('a'):\n",
    "                link = heading.find('a')['href']\n",
    "                # Make sure it's an absolute URL\n",
    "                if link and not link.startswith(('http://', 'https://')):\n",
    "                    link = urljoin(self.base_url, link)\n",
    "            \n",
    "            # Look for a date\n",
    "            date = None\n",
    "            date_elem = container.find(class_=lambda c: c and ('date' in c.lower() or 'time' in c.lower()))\n",
    "            if date_elem:\n",
    "                date = date_elem.text.strip()\n",
    "            \n",
    "            # Look for content preview\n",
    "            content_preview = None\n",
    "            content_elem = container.find(['p', 'div'], class_=lambda c: c and ('content' in c.lower() or 'excerpt' in c.lower() or 'summary' in c.lower()))\n",
    "            if content_elem:\n",
    "                content_preview = content_elem.text.strip()\n",
    "            \n",
    "            # Create post object\n",
    "            post = {\n",
    "                'title': title,\n",
    "                'url': link,\n",
    "                'date': date,\n",
    "                'excerpt': content_preview,\n",
    "                'container_classes': container.get('class', [])\n",
    "            }\n",
    "            \n",
    "            all_posts.append(post)\n",
    "            \n",
    "            self.logger.info(f\"Found post: {title}\")\n",
    "        \n",
    "        # If no posts found with the above approach, try a fallback method\n",
    "        if not all_posts:\n",
    "            self.logger.info(\"No posts found with container approach, trying fallback method\")\n",
    "            \n",
    "            # Fallback: Just look for all headings and assume they're posts\n",
    "            headings = soup.find_all(['h1', 'h2', 'h3'])\n",
    "            \n",
    "            for heading in headings[:10]:  # Limit to first 10\n",
    "                # Skip navigation or sidebar headings\n",
    "                parent_classes = ' '.join(parent.get('class', []) for parent in heading.parents if parent.get('class'))\n",
    "                if any(x in parent_classes.lower() for x in ['nav', 'sidebar', 'menu', 'footer', 'header']):\n",
    "                    continue\n",
    "                \n",
    "                title = heading.text.strip()\n",
    "                \n",
    "                # Look for a link\n",
    "                link = None\n",
    "                if heading.find('a'):\n",
    "                    link = heading.find('a')['href']\n",
    "                    # Make sure it's an absolute URL\n",
    "                    if link and not link.startswith(('http://', 'https://')):\n",
    "                        link = urljoin(self.base_url, link)\n",
    "                elif heading.parent and heading.parent.name == 'a':\n",
    "                    link = heading.parent['href']\n",
    "                    if link and not link.startswith(('http://', 'https://')):\n",
    "                        link = urljoin(self.base_url, link)\n",
    "                \n",
    "                # Create a simplified post object\n",
    "                post = {\n",
    "                    'title': title,\n",
    "                    'url': link,\n",
    "                    'source': 'fallback_method'\n",
    "                }\n",
    "                \n",
    "                all_posts.append(post)\n",
    "                self.logger.info(f\"Found post (fallback): {title}\")\n",
    "        \n",
    "        # Save metadata before attempting to download\n",
    "        self._save_metadata(all_posts)\n",
    "        \n",
    "        # Process each post to download PDFs and save HTML\n",
    "        processed_posts = self._process_posts(all_posts)\n",
    "        \n",
    "        return processed_posts\n",
    "    \n",
    "    def _process_posts(self, posts):\n",
    "        \"\"\"Process posts to download PDFs and save HTML\"\"\"\n",
    "        processed_posts = []\n",
    "        \n",
    "        for post in posts:\n",
    "            url = post.get('url')\n",
    "            if not url:\n",
    "                continue\n",
    "                \n",
    "            # Fetch the post page\n",
    "            try:\n",
    "                self.logger.info(f\"Fetching post: {url}\")\n",
    "                response = self.session.get(url, timeout=30)\n",
    "                \n",
    "                if response.status_code != 200:\n",
    "                    self.logger.warning(f\"Failed to fetch post: {response.status_code}\")\n",
    "                    continue\n",
    "                    \n",
    "                # Parse the post content\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                \n",
    "                # Extract the main content\n",
    "                content_elem = soup.find(['div', 'article'], class_=lambda c: c and ('content' in c.lower() or 'entry' in c.lower() or 'article' in c.lower()))\n",
    "                \n",
    "                if content_elem:\n",
    "                    # Get text content\n",
    "                    post['content_text'] = content_elem.get_text('\\n', strip=True)\n",
    "                    \n",
    "                    # Get HTML content\n",
    "                    post['content_html'] = str(content_elem)\n",
    "                    \n",
    "                    # Extract any PDF links\n",
    "                    pdf_links = []\n",
    "                    for link in content_elem.find_all('a', href=True):\n",
    "                        href = link['href']\n",
    "                        if href.lower().endswith('.pdf'):\n",
    "                            # Make sure URL is absolute\n",
    "                            full_url = urljoin(url, href)\n",
    "                            pdf_links.append(full_url)\n",
    "                    \n",
    "                    # Download PDFs\n",
    "                    downloaded_pdfs = []\n",
    "                    for i, pdf_url in enumerate(pdf_links):\n",
    "                        # Create filename\n",
    "                        title = post.get('title', 'unknown')\n",
    "                        safe_title = re.sub(r'[^\\w\\s-]', '', title).strip().lower()\n",
    "                        safe_title = re.sub(r'[-\\s]+', '-', safe_title)\n",
    "                        filename = f\"{safe_title}-{i+1}.pdf\"\n",
    "                        \n",
    "                        # Download the PDF\n",
    "                        filepath = self._download_file(pdf_url, filename)\n",
    "                        \n",
    "                        if filepath:\n",
    "                            downloaded_pdfs.append({\n",
    "                                'url': pdf_url,\n",
    "                                'filepath': str(filepath),\n",
    "                                'filename': filename\n",
    "                            })\n",
    "                    \n",
    "                    post['pdfs'] = downloaded_pdfs\n",
    "                    \n",
    "                    # Save post HTML\n",
    "                    html_path = self._save_post_html(post)\n",
    "                    if html_path:\n",
    "                        post['saved_html_path'] = str(html_path)\n",
    "                \n",
    "                processed_posts.append(post)\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error processing post {url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save updated metadata\n",
    "        self._save_metadata(processed_posts)\n",
    "        \n",
    "        return processed_posts\n",
    "    \n",
    "    def _download_file(self, url, filename):\n",
    "        \"\"\"Download a file with proper naming\"\"\"\n",
    "        filepath = self.output_dir / filename\n",
    "        \n",
    "        try:\n",
    "            self.logger.info(f\"Downloading {url} to {filepath}\")\n",
    "            \n",
    "            # Download with stream to handle large files\n",
    "            response = self.session.get(url, stream=True, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            with open(filepath, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "            \n",
    "            self.logger.info(f\"Successfully downloaded to {filepath}\")\n",
    "            return filepath\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error downloading {url}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _save_post_html(self, post):\n",
    "        \"\"\"Save post content as HTML file\"\"\"\n",
    "        if 'content_html' not in post or not post['content_html']:\n",
    "            return None\n",
    "            \n",
    "        # Create filename from title\n",
    "        title = post.get('title', 'unknown')\n",
    "        safe_title = re.sub(r'[^\\w\\s-]', '', title).strip().lower()\n",
    "        safe_title = re.sub(r'[-\\s]+', '-', safe_title)\n",
    "        \n",
    "        html_path = self.output_dir / f\"{safe_title}.html\"\n",
    "        \n",
    "        # Create a complete HTML document\n",
    "        html_content = f\"\"\"\n",
    "        <!DOCTYPE html>\n",
    "        <html>\n",
    "        <head>\n",
    "            <title>{post.get('title', 'BitMEX Research Post')}</title>\n",
    "            <meta charset=\"utf-8\">\n",
    "            <meta name=\"date\" content=\"{post.get('date', '')}\">\n",
    "        </head>\n",
    "        <body>\n",
    "            <h1>{post.get('title', '')}</h1>\n",
    "            <p class=\"date\">{post.get('date', '')}</p>\n",
    "            <div class=\"content\">\n",
    "                {post.get('content_html', '')}\n",
    "            </div>\n",
    "            <div class=\"metadata\">\n",
    "                <p>Source: <a href=\"{post.get('url', '')}\">{post.get('url', '')}</a></p>\n",
    "            </div>\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            with open(html_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(html_content)\n",
    "                \n",
    "            self.logger.info(f\"Saved HTML to {html_path}\")\n",
    "            return html_path\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error saving HTML: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _save_metadata(self, posts):\n",
    "        \"\"\"Save metadata about collected posts\"\"\"\n",
    "        metadata_path = self.output_dir / \"bitmex_research_posts.json\"\n",
    "        \n",
    "        try:\n",
    "            with open(metadata_path, 'w') as f:\n",
    "                json.dump(posts, f, indent=2)\n",
    "                \n",
    "            self.logger.info(f\"Saved metadata for {len(posts)} posts to {metadata_path}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error saving metadata: {e}\")\n",
    "\n",
    "# Test the updated collector\n",
    "try:\n",
    "    print(\"\\nStarting updated BitMEX collector test...\")\n",
    "    \n",
    "    # Initialize and run the collector\n",
    "    collector = UpdatedBitMEXCollector(output_dir)\n",
    "    results = collector.collect(max_pages=1)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n===== Updated BitMEXCollector Results =====\")\n",
    "    print(f\"Collected {len(results)} research posts\")\n",
    "    \n",
    "    if results:\n",
    "        # Show first 3 posts (or all if fewer)\n",
    "        show_count = min(3, len(results))\n",
    "        for i, post in enumerate(results[:show_count]):\n",
    "            print(f\"\\nPost #{i+1}:\")\n",
    "            print(f\"  Title: {post.get('title')}\")\n",
    "            print(f\"  URL: {post.get('url')}\")\n",
    "            \n",
    "            # Check for PDFs\n",
    "            pdfs = post.get('pdfs', [])\n",
    "            print(f\"  PDF Count: {len(pdfs)}\")\n",
    "            \n",
    "            for j, pdf in enumerate(pdfs[:2]):  # Show first 2 PDFs\n",
    "                print(f\"    PDF #{j+1}: {pdf.get('filename')}\")\n",
    "                filepath = pdf.get('filepath', '')\n",
    "                if filepath and os.path.exists(filepath):\n",
    "                    size_kb = os.path.getsize(filepath) / 1024\n",
    "                    print(f\"    Size: {size_kb:.2f} KB\")\n",
    "                    print(f\"    Exists: Yes\")\n",
    "                else:\n",
    "                    print(f\"    Exists: No\")\n",
    "            \n",
    "            # Check for saved HTML\n",
    "            html_path = post.get('saved_html_path', '')\n",
    "            if html_path and os.path.exists(html_path):\n",
    "                size_kb = os.path.getsize(html_path) / 1024\n",
    "                print(f\"  HTML saved: Yes ({size_kb:.2f} KB)\")\n",
    "            else:\n",
    "                print(f\"  HTML saved: No\")\n",
    "    \n",
    "    # Print directory contents\n",
    "    print(\"\\n===== Test Directory Contents =====\")\n",
    "    for root, dirs, files in os.walk(output_dir):\n",
    "        level = root.replace(output_dir, '').count(os.sep)\n",
    "        indent = ' ' * 4 * level\n",
    "        print(f\"{indent}{os.path.basename(root)}/\")\n",
    "        sub_indent = ' ' * 4 * (level + 1)\n",
    "        for f in files[:5]:  # Show first 5 files\n",
    "            filepath = os.path.join(root, f)\n",
    "            size_kb = os.path.getsize(filepath) / 1024\n",
    "            print(f\"{sub_indent}{f} ({size_kb:.2f} KB)\")\n",
    "        if len(files) > 5:\n",
    "            print(f\"{sub_indent}... and {len(files)-5} more files\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Error running updated BitMEX collector: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3fbe56-e350-4bf4-936a-b7fdeda8a16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "FREDCollector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c942c566-17fc-4f2c-9c7f-58fe1ec59ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"FRED_API_KEY\"] = \"05796b72da56e97a6f7ea908ecf57b59\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94238be4-2422-4612-b1ad-193904841dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared and recreated directory: /workspace/data/test_corpus/fred\n",
      "FRED API Key available: Yes\n",
      "\n",
      "Testing FREDCollector integration...\n",
      "FREDCollector methods:\n",
      "  api_request\n",
      "  collect\n",
      "  download_file\n",
      "\n",
      "Testing actual data collection (with real API key)...\n",
      "\n",
      "===== FREDCollector Results =====\n",
      "Downloaded 8 data files\n",
      "File: /workspace/data/test_corpus/fred/VIXCLS_CBOE Volatility Index_ VIX.json\n",
      "  Size: 1242.10 KB\n",
      "  Exists: Yes\n",
      "File: /workspace/data/test_corpus/fred/VIXCLS_CBOE Volatility Index_ VIX.csv\n",
      "  Size: 349.83 KB\n",
      "  Exists: Yes\n",
      "File: /workspace/data/test_corpus/fred/DTWEXBGS_Nominal Broad U_S_ Dollar Index.json\n",
      "  Size: 691.31 KB\n",
      "  Exists: Yes\n",
      "File: /workspace/data/test_corpus/fred/DTWEXBGS_Nominal Broad U_S_ Dollar Index.csv\n",
      "  Size: 203.04 KB\n",
      "  Exists: Yes\n",
      "File: /workspace/data/test_corpus/fred/VIXCLS_CBOE Volatility Index_ VIX.json\n",
      "  Size: 1242.10 KB\n",
      "  Exists: Yes\n",
      "File: /workspace/data/test_corpus/fred/VIXCLS_CBOE Volatility Index_ VIX.csv\n",
      "  Size: 349.83 KB\n",
      "  Exists: Yes\n",
      "File: /workspace/data/test_corpus/fred/VXVCLS_CBOE S_P 500 3-Month Volatility Index.json\n",
      "  Size: 612.59 KB\n",
      "  Exists: Yes\n",
      "File: /workspace/data/test_corpus/fred/VXVCLS_CBOE S_P 500 3-Month Volatility Index.csv\n",
      "  Size: 172.39 KB\n",
      "  Exists: Yes\n",
      "\n",
      "===== Test Directory Contents =====\n",
      "fred/\n",
      "    VIXCLS_CBOE Volatility Index_ VIX.json (1242.10 KB)\n",
      "    VIXCLS_CBOE Volatility Index_ VIX.csv (349.83 KB)\n",
      "    DTWEXBGS_Nominal Broad U_S_ Dollar Index.json (691.31 KB)\n",
      "    DTWEXBGS_Nominal Broad U_S_ Dollar Index.csv (203.04 KB)\n",
      "    VXVCLS_CBOE S_P 500 3-Month Volatility Index.json (612.59 KB)\n",
      "    ... and 1 more files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Add CryptoCorpusBuilder to Python path\n",
    "sys.path.append('/workspace/CryptoCorpusBuilder')\n",
    "\n",
    "# Import the collector\n",
    "from sources.specific_collectors.fred_collector import FREDCollector\n",
    "\n",
    "# Create test directory\n",
    "output_dir = \"/workspace/data/test_corpus/fred\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Clear existing files to start fresh\n",
    "import shutil\n",
    "shutil.rmtree(output_dir)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Cleared and recreated directory: {output_dir}\")\n",
    "\n",
    "# Check for FRED API key environment variable\n",
    "fred_api_key = os.getenv(\"FRED_API_KEY\")\n",
    "print(f\"FRED API Key available: {'Yes' if fred_api_key else 'No'}\")\n",
    "\n",
    "# If we don't have an API key, let's make a mock one for testing\n",
    "# (This won't work for real API calls, but will help test the code structure)\n",
    "if not fred_api_key:\n",
    "    fred_api_key = \"MOCK_API_KEY_FOR_TESTING\"\n",
    "    print(\"Using mock API key for testing\")\n",
    "\n",
    "# Initialize collector\n",
    "collector = FREDCollector(output_dir, api_key=fred_api_key)\n",
    "collector.logger.setLevel(logging.INFO)\n",
    "\n",
    "# Test with just the integration without making actual API calls\n",
    "try:\n",
    "    print(\"\\nTesting FREDCollector integration...\")\n",
    "    \n",
    "    # Test the structure without making API calls\n",
    "    # We'll just verify that the collector methods are accessible\n",
    "    \n",
    "    print(\"FREDCollector methods:\")\n",
    "    methods = [method for method in dir(collector) if not method.startswith('_') and callable(getattr(collector, method))]\n",
    "    for method in methods:\n",
    "        print(f\"  {method}\")\n",
    "    \n",
    "    # For a more complete test, we'd need a valid FRED API key\n",
    "    if fred_api_key != \"MOCK_API_KEY_FOR_TESTING\":\n",
    "        print(\"\\nTesting actual data collection (with real API key)...\")\n",
    "        \n",
    "        # Test with a small sample\n",
    "        results = collector.collect(\n",
    "            series_ids=[\"VIXCLS\", \"DTWEXBGS\"],  # VIX index and Dollar index\n",
    "            search_terms=[\"volatility\"],\n",
    "            max_results=2  # Limit to 2 results for testing\n",
    "        )\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\n===== FREDCollector Results =====\")\n",
    "        print(f\"Downloaded {len(results)} data files\")\n",
    "        \n",
    "        for file_path in results:\n",
    "            print(f\"File: {file_path}\")\n",
    "            if os.path.exists(file_path):\n",
    "                size_kb = os.path.getsize(file_path) / 1024\n",
    "                print(f\"  Size: {size_kb:.2f} KB\")\n",
    "                print(f\"  Exists: Yes\")\n",
    "            else:\n",
    "                print(f\"  Exists: No\")\n",
    "    else:\n",
    "        print(\"\\nSkipping actual API calls due to mock API key\")\n",
    "        print(\"To fully test, set the FRED_API_KEY environment variable\")\n",
    "    \n",
    "    # Print directory contents\n",
    "    print(\"\\n===== Test Directory Contents =====\")\n",
    "    for root, dirs, files in os.walk(output_dir):\n",
    "        level = root.replace(output_dir, '').count(os.sep)\n",
    "        indent = ' ' * 4 * level\n",
    "        print(f\"{indent}{os.path.basename(root)}/\")\n",
    "        sub_indent = ' ' * 4 * (level + 1)\n",
    "        for f in files[:5]:  # Show first 5 files\n",
    "            filepath = os.path.join(root, f)\n",
    "            size_kb = os.path.getsize(filepath) / 1024\n",
    "            print(f\"{sub_indent}{f} ({size_kb:.2f} KB)\")\n",
    "        if len(files) > 5:\n",
    "            print(f\"{sub_indent}... and {len(files)-5} more files\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error testing FREDCollector: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cc5f3e-d516-4355-8809-58aeffd9ad04",
   "metadata": {},
   "outputs": [],
   "source": [
    "ISDA COllector test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f595598-abe1-45ae-9639-d207c5ab8551",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 20:09:16,481 - ISDACollector - INFO - Collecting ISDA documentation (max 2 pages)\n",
      "2025-05-08 20:09:16,481 - ISDACollector - INFO - Collecting ISDA documentation (max 2 pages)\n",
      "2025-05-08 20:09:16,482 - ISDACollector - INFO - Fetching page 1: https://www.isda.org/category/documentation/\n",
      "2025-05-08 20:09:16,482 - ISDACollector - INFO - Fetching page 1: https://www.isda.org/category/documentation/\n",
      "2025-05-08 20:09:25,914 - ISDACollector - INFO - No more documents found on page 1\n",
      "2025-05-08 20:09:25,914 - ISDACollector - INFO - No more documents found on page 1\n",
      "2025-05-08 20:09:25,925 - ISDACollector - INFO - Saved metadata for 0 documents to /workspace/data/test_isda/isda_documents.json\n",
      "2025-05-08 20:09:25,925 - ISDACollector - INFO - Saved metadata for 0 documents to /workspace/data/test_isda/isda_documents.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully collected 0 documents from ISDA\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb89e52e-57e9-46bb-99d6-22ccde720d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 20:10:07,562 - ISDADebugger - INFO - Directly fetching page: https://www.isda.org/category/documentation/\n",
      "2025-05-08 20:10:09,500 - ISDADebugger - INFO - Saved HTML to /workspace/data/test_isda/isda_page.html\n",
      "2025-05-08 20:10:09,512 - ISDADebugger - INFO - Analyzing HTML structure:\n",
      "2025-05-08 20:10:09,513 - ISDADebugger - INFO - Found 6 article elements\n",
      "2025-05-08 20:10:09,514 - ISDADebugger - INFO - Article 1:\n",
      "2025-05-08 20:10:09,514 - ISDADebugger - INFO -   Found 1 title elements\n",
      "2025-05-08 20:10:09,515 - ISDADebugger - INFO -   Found 2 links\n",
      "2025-05-08 20:10:09,515 - ISDADebugger - INFO -   Article classes: ['news-small', 'post-1139259', 'post', 'type-post', 'status-publish', 'format-standard', 'hentry', 'category-press-releases', 'tag-credit-default-swaps', 'tag-credit-derivatives', 'tag-determinations-committee', 'tag-legal']\n",
      "2025-05-08 20:10:09,515 - ISDADebugger - INFO - Article 2:\n",
      "2025-05-08 20:10:09,516 - ISDADebugger - INFO -   Found 1 title elements\n",
      "2025-05-08 20:10:09,516 - ISDADebugger - INFO -   Found 2 links\n",
      "2025-05-08 20:10:09,517 - ISDADebugger - INFO -   Article classes: ['news-small', 'post-1139255', 'post', 'type-post', 'status-publish', 'format-standard', 'hentry', 'category-press-releases', 'tag-common-domain-model', 'tag-isda-create', 'tag-legal']\n",
      "2025-05-08 20:10:09,517 - ISDADebugger - INFO - Article 3:\n",
      "2025-05-08 20:10:09,517 - ISDADebugger - INFO -   Found 1 title elements\n",
      "2025-05-08 20:10:09,518 - ISDADebugger - INFO -   Found 2 links\n",
      "2025-05-08 20:10:09,518 - ISDADebugger - INFO -   Article classes: ['news-small', 'post-1139251', 'post', 'type-post', 'status-publish', 'format-standard', 'hentry', 'category-press-releases', 'tag-common-domain-model', 'tag-digital-regulatory-reporting', 'tag-mifid', 'tag-mifir', 'tag-reporting']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"ISDADebugger\")\n",
    "\n",
    "# Set up the test directory\n",
    "test_dir = Path(\"/workspace/data/test_isda\")\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# Fetch the ISDA documentation page directly\n",
    "url = \"https://www.isda.org/category/documentation/\"\n",
    "logger.info(f\"Directly fetching page: {url}\")\n",
    "\n",
    "try:\n",
    "    # Use a modern browser user agent\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers, timeout=30)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # Save the HTML content for inspection\n",
    "    html_path = test_dir / \"isda_page.html\"\n",
    "    with open(html_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(response.text)\n",
    "    \n",
    "    logger.info(f\"Saved HTML to {html_path}\")\n",
    "    \n",
    "    # Parse with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Debug HTML structure\n",
    "    logger.info(\"Analyzing HTML structure:\")\n",
    "    \n",
    "    # Check for articles\n",
    "    articles = soup.find_all('article')\n",
    "    logger.info(f\"Found {len(articles)} article elements\")\n",
    "    \n",
    "    # If no articles, look for alternative elements that might contain documents\n",
    "    if not articles:\n",
    "        # Try different container elements\n",
    "        containers = soup.find_all(['div', 'section'], class_=lambda c: c and any(word in c for word in ['post', 'content', 'article', 'entry']))\n",
    "        logger.info(f\"Found {len(containers)} potential container elements\")\n",
    "        \n",
    "        # Check for links that might be document links\n",
    "        pdf_links = soup.find_all('a', href=lambda h: h and h.lower().endswith('.pdf'))\n",
    "        logger.info(f\"Found {len(pdf_links)} direct PDF links\")\n",
    "        \n",
    "        # Look for any title-like elements\n",
    "        titles = soup.find_all(['h1', 'h2', 'h3', 'h4'])\n",
    "        logger.info(f\"Found {len(titles)} heading elements\")\n",
    "        \n",
    "        # Print the first few titles to see what we're working with\n",
    "        for i, title in enumerate(titles[:5]):\n",
    "            logger.info(f\"Title {i+1}: {title.text.strip()}\")\n",
    "    \n",
    "    # If articles were found, check if they have the expected structure\n",
    "    else:\n",
    "        for i, article in enumerate(articles[:3]):\n",
    "            logger.info(f\"Article {i+1}:\")\n",
    "            \n",
    "            # Check for title elements\n",
    "            title_elems = article.find_all(['h1', 'h2', 'h3'])\n",
    "            logger.info(f\"  Found {len(title_elems)} title elements\")\n",
    "            \n",
    "            # Check for links\n",
    "            links = article.find_all('a', href=True)\n",
    "            logger.info(f\"  Found {len(links)} links\")\n",
    "            \n",
    "            # Check for class names to help debug selectors\n",
    "            if 'class' in article.attrs:\n",
    "                logger.info(f\"  Article classes: {article['class']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error debugging ISDA collector: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe1cb9dc-cfc8-4bd5-8e00-9d5a47e34bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 20:13:30,262 - ISDADebugger - INFO - Directly fetching page: https://www.isda.org/category/documentation/\n",
      "2025-05-08 20:13:30,262 - ISDADebugger - INFO - Directly fetching page: https://www.isda.org/category/documentation/\n",
      "2025-05-08 20:13:32,259 - ISDADebugger - INFO - Saved HTML to /workspace/data/test_isda/isda_page.html\n",
      "2025-05-08 20:13:32,259 - ISDADebugger - INFO - Saved HTML to /workspace/data/test_isda/isda_page.html\n",
      "2025-05-08 20:13:32,272 - ISDADebugger - INFO - Analyzing HTML structure:\n",
      "2025-05-08 20:13:32,272 - ISDADebugger - INFO - Analyzing HTML structure:\n",
      "2025-05-08 20:13:32,273 - ISDADebugger - INFO - Found 6 article elements\n",
      "2025-05-08 20:13:32,273 - ISDADebugger - INFO - Found 6 article elements\n",
      "2025-05-08 20:13:32,274 - ISDADebugger - INFO - Article 1:\n",
      "2025-05-08 20:13:32,274 - ISDADebugger - INFO - Article 1:\n",
      "2025-05-08 20:13:32,276 - ISDADebugger - INFO -   Found 2 title elements\n",
      "2025-05-08 20:13:32,276 - ISDADebugger - INFO -   Found 2 title elements\n",
      "2025-05-08 20:13:32,277 - ISDADebugger - INFO -   Found 2 links\n",
      "2025-05-08 20:13:32,277 - ISDADebugger - INFO -   Found 2 links\n",
      "2025-05-08 20:13:32,278 - ISDADebugger - INFO -   Article classes: ['news-small', 'post-1139259', 'post', 'type-post', 'status-publish', 'format-standard', 'hentry', 'category-press-releases', 'tag-credit-default-swaps', 'tag-credit-derivatives', 'tag-determinations-committee', 'tag-legal']\n",
      "2025-05-08 20:13:32,278 - ISDADebugger - INFO -   Article classes: ['news-small', 'post-1139259', 'post', 'type-post', 'status-publish', 'format-standard', 'hentry', 'category-press-releases', 'tag-credit-default-swaps', 'tag-credit-derivatives', 'tag-determinations-committee', 'tag-legal']\n",
      "2025-05-08 20:13:32,278 - ISDADebugger - INFO -   Categories: ['category-press-releases']\n",
      "2025-05-08 20:13:32,278 - ISDADebugger - INFO -   Categories: ['category-press-releases']\n",
      "2025-05-08 20:13:32,279 - ISDADebugger - INFO -   Tags: ['tag-credit-default-swaps', 'tag-credit-derivatives', 'tag-determinations-committee', 'tag-legal']\n",
      "2025-05-08 20:13:32,279 - ISDADebugger - INFO -   Tags: ['tag-credit-default-swaps', 'tag-credit-derivatives', 'tag-determinations-committee', 'tag-legal']\n",
      "2025-05-08 20:13:32,280 - ISDADebugger - INFO - Article 2:\n",
      "2025-05-08 20:13:32,280 - ISDADebugger - INFO - Article 2:\n",
      "2025-05-08 20:13:32,280 - ISDADebugger - INFO -   Found 2 title elements\n",
      "2025-05-08 20:13:32,280 - ISDADebugger - INFO -   Found 2 title elements\n",
      "2025-05-08 20:13:32,283 - ISDADebugger - INFO -   Found 2 links\n",
      "2025-05-08 20:13:32,283 - ISDADebugger - INFO -   Found 2 links\n",
      "2025-05-08 20:13:32,283 - ISDADebugger - INFO -   Article classes: ['news-small', 'post-1139255', 'post', 'type-post', 'status-publish', 'format-standard', 'hentry', 'category-press-releases', 'tag-common-domain-model', 'tag-isda-create', 'tag-legal']\n",
      "2025-05-08 20:13:32,283 - ISDADebugger - INFO -   Article classes: ['news-small', 'post-1139255', 'post', 'type-post', 'status-publish', 'format-standard', 'hentry', 'category-press-releases', 'tag-common-domain-model', 'tag-isda-create', 'tag-legal']\n",
      "2025-05-08 20:13:32,284 - ISDADebugger - INFO -   Categories: ['category-press-releases']\n",
      "2025-05-08 20:13:32,284 - ISDADebugger - INFO -   Categories: ['category-press-releases']\n",
      "2025-05-08 20:13:32,285 - ISDADebugger - INFO -   Tags: ['tag-common-domain-model', 'tag-isda-create', 'tag-legal']\n",
      "2025-05-08 20:13:32,285 - ISDADebugger - INFO -   Tags: ['tag-common-domain-model', 'tag-isda-create', 'tag-legal']\n",
      "2025-05-08 20:13:32,287 - ISDADebugger - INFO - Article 3:\n",
      "2025-05-08 20:13:32,287 - ISDADebugger - INFO - Article 3:\n",
      "2025-05-08 20:13:32,288 - ISDADebugger - INFO -   Found 2 title elements\n",
      "2025-05-08 20:13:32,288 - ISDADebugger - INFO -   Found 2 title elements\n",
      "2025-05-08 20:13:32,289 - ISDADebugger - INFO -   Found 2 links\n",
      "2025-05-08 20:13:32,289 - ISDADebugger - INFO -   Found 2 links\n",
      "2025-05-08 20:13:32,290 - ISDADebugger - INFO -   Article classes: ['news-small', 'post-1139251', 'post', 'type-post', 'status-publish', 'format-standard', 'hentry', 'category-press-releases', 'tag-common-domain-model', 'tag-digital-regulatory-reporting', 'tag-mifid', 'tag-mifir', 'tag-reporting']\n",
      "2025-05-08 20:13:32,290 - ISDADebugger - INFO -   Article classes: ['news-small', 'post-1139251', 'post', 'type-post', 'status-publish', 'format-standard', 'hentry', 'category-press-releases', 'tag-common-domain-model', 'tag-digital-regulatory-reporting', 'tag-mifid', 'tag-mifir', 'tag-reporting']\n",
      "2025-05-08 20:13:32,291 - ISDADebugger - INFO -   Categories: ['category-press-releases']\n",
      "2025-05-08 20:13:32,291 - ISDADebugger - INFO -   Categories: ['category-press-releases']\n",
      "2025-05-08 20:13:32,292 - ISDADebugger - INFO -   Tags: ['tag-common-domain-model', 'tag-digital-regulatory-reporting', 'tag-mifid', 'tag-mifir', 'tag-reporting']\n",
      "2025-05-08 20:13:32,292 - ISDADebugger - INFO -   Tags: ['tag-common-domain-model', 'tag-digital-regulatory-reporting', 'tag-mifid', 'tag-mifir', 'tag-reporting']\n",
      "2025-05-08 20:13:32,292 - ISDADebugger - INFO - Article 4:\n",
      "2025-05-08 20:13:32,292 - ISDADebugger - INFO - Article 4:\n",
      "2025-05-08 20:13:32,293 - ISDADebugger - INFO -   Found 2 title elements\n",
      "2025-05-08 20:13:32,293 - ISDADebugger - INFO -   Found 2 title elements\n",
      "2025-05-08 20:13:32,294 - ISDADebugger - INFO -   Found 2 links\n",
      "2025-05-08 20:13:32,294 - ISDADebugger - INFO -   Found 2 links\n",
      "2025-05-08 20:13:32,295 - ISDADebugger - INFO -   Article classes: ['news-small', 'post-1138110', 'post', 'type-post', 'status-publish', 'format-standard', 'hentry', 'category-the-swap', 'tag-commodity-derivative', 'tag-derivatives', 'tag-emerging-markets', 'tag-equity-derivatives', 'tag-fx-derivatives', 'tag-interest-rate-derivative', 'tag-liquidity', 'tag-risk-management', 'tag-sustainable-finance']\n",
      "2025-05-08 20:13:32,295 - ISDADebugger - INFO -   Article classes: ['news-small', 'post-1138110', 'post', 'type-post', 'status-publish', 'format-standard', 'hentry', 'category-the-swap', 'tag-commodity-derivative', 'tag-derivatives', 'tag-emerging-markets', 'tag-equity-derivatives', 'tag-fx-derivatives', 'tag-interest-rate-derivative', 'tag-liquidity', 'tag-risk-management', 'tag-sustainable-finance']\n",
      "2025-05-08 20:13:32,297 - ISDADebugger - INFO -   Categories: ['category-the-swap']\n",
      "2025-05-08 20:13:32,297 - ISDADebugger - INFO -   Categories: ['category-the-swap']\n",
      "2025-05-08 20:13:32,298 - ISDADebugger - INFO -   Tags: ['tag-commodity-derivative', 'tag-derivatives', 'tag-emerging-markets', 'tag-equity-derivatives', 'tag-fx-derivatives', 'tag-interest-rate-derivative', 'tag-liquidity', 'tag-risk-management', 'tag-sustainable-finance']\n",
      "2025-05-08 20:13:32,298 - ISDADebugger - INFO -   Tags: ['tag-commodity-derivative', 'tag-derivatives', 'tag-emerging-markets', 'tag-equity-derivatives', 'tag-fx-derivatives', 'tag-interest-rate-derivative', 'tag-liquidity', 'tag-risk-management', 'tag-sustainable-finance']\n",
      "2025-05-08 20:13:32,299 - ISDADebugger - INFO - Article 5:\n",
      "2025-05-08 20:13:32,299 - ISDADebugger - INFO - Article 5:\n",
      "2025-05-08 20:13:32,299 - ISDADebugger - INFO -   Found 2 title elements\n",
      "2025-05-08 20:13:32,299 - ISDADebugger - INFO -   Found 2 title elements\n",
      "2025-05-08 20:13:32,300 - ISDADebugger - INFO -   Found 2 links\n",
      "2025-05-08 20:13:32,300 - ISDADebugger - INFO -   Found 2 links\n",
      "2025-05-08 20:13:32,301 - ISDADebugger - INFO -   Article classes: ['news-small', 'post-1134920', 'post', 'type-post', 'status-publish', 'format-standard', 'hentry', 'category-press-releases', 'tag-commodity-derivative', 'tag-credit-derivatives', 'tag-derivatives', 'tag-emerging-markets', 'tag-equity-derivatives', 'tag-fx-derivatives', 'tag-interest-rate-derivative', 'tag-liquidity', 'tag-risk-management', 'tag-sustainable-finance']\n",
      "2025-05-08 20:13:32,301 - ISDADebugger - INFO -   Article classes: ['news-small', 'post-1134920', 'post', 'type-post', 'status-publish', 'format-standard', 'hentry', 'category-press-releases', 'tag-commodity-derivative', 'tag-credit-derivatives', 'tag-derivatives', 'tag-emerging-markets', 'tag-equity-derivatives', 'tag-fx-derivatives', 'tag-interest-rate-derivative', 'tag-liquidity', 'tag-risk-management', 'tag-sustainable-finance']\n",
      "2025-05-08 20:13:32,302 - ISDADebugger - INFO -   Categories: ['category-press-releases']\n",
      "2025-05-08 20:13:32,302 - ISDADebugger - INFO -   Categories: ['category-press-releases']\n",
      "2025-05-08 20:13:32,303 - ISDADebugger - INFO -   Tags: ['tag-commodity-derivative', 'tag-credit-derivatives', 'tag-derivatives', 'tag-emerging-markets', 'tag-equity-derivatives', 'tag-fx-derivatives', 'tag-interest-rate-derivative', 'tag-liquidity', 'tag-risk-management', 'tag-sustainable-finance']\n",
      "2025-05-08 20:13:32,303 - ISDADebugger - INFO -   Tags: ['tag-commodity-derivative', 'tag-credit-derivatives', 'tag-derivatives', 'tag-emerging-markets', 'tag-equity-derivatives', 'tag-fx-derivatives', 'tag-interest-rate-derivative', 'tag-liquidity', 'tag-risk-management', 'tag-sustainable-finance']\n",
      "2025-05-08 20:13:32,304 - ISDADebugger - INFO - Article 6:\n",
      "2025-05-08 20:13:32,304 - ISDADebugger - INFO - Article 6:\n",
      "2025-05-08 20:13:32,305 - ISDADebugger - INFO -   Found 2 title elements\n",
      "2025-05-08 20:13:32,305 - ISDADebugger - INFO -   Found 2 title elements\n",
      "2025-05-08 20:13:32,306 - ISDADebugger - INFO -   Found 2 links\n",
      "2025-05-08 20:13:32,306 - ISDADebugger - INFO -   Found 2 links\n",
      "2025-05-08 20:13:32,307 - ISDADebugger - INFO -   Article classes: ['news-small', 'post-1121346', 'post', 'type-post', 'status-publish', 'format-standard', 'hentry', 'category-iq', 'tag-derivatives', 'tag-documentation', 'tag-fx-definitions', 'tag-master-agreement', 'tag-notices-hub', 'tag-legal', 'tag-protocols', 'tag-voluntary-carbon-markets']\n",
      "2025-05-08 20:13:32,307 - ISDADebugger - INFO -   Article classes: ['news-small', 'post-1121346', 'post', 'type-post', 'status-publish', 'format-standard', 'hentry', 'category-iq', 'tag-derivatives', 'tag-documentation', 'tag-fx-definitions', 'tag-master-agreement', 'tag-notices-hub', 'tag-legal', 'tag-protocols', 'tag-voluntary-carbon-markets']\n",
      "2025-05-08 20:13:32,308 - ISDADebugger - INFO -   Categories: ['category-iq']\n",
      "2025-05-08 20:13:32,308 - ISDADebugger - INFO -   Categories: ['category-iq']\n",
      "2025-05-08 20:13:32,309 - ISDADebugger - INFO -   Tags: ['tag-derivatives', 'tag-documentation', 'tag-fx-definitions', 'tag-master-agreement', 'tag-notices-hub', 'tag-legal', 'tag-protocols', 'tag-voluntary-carbon-markets']\n",
      "2025-05-08 20:13:32,309 - ISDADebugger - INFO -   Tags: ['tag-derivatives', 'tag-documentation', 'tag-fx-definitions', 'tag-master-agreement', 'tag-notices-hub', 'tag-legal', 'tag-protocols', 'tag-voluntary-carbon-markets']\n",
      "2025-05-08 20:13:32,310 - ISDADebugger - INFO - Found 0 pagination elements\n",
      "2025-05-08 20:13:32,310 - ISDADebugger - INFO - Found 0 pagination elements\n",
      "2025-05-08 20:13:32,312 - ISDADebugger - INFO - Found 0 direct PDF links\n",
      "2025-05-08 20:13:32,312 - ISDADebugger - INFO - Found 0 direct PDF links\n",
      "2025-05-08 20:13:32,314 - ISDADebugger - INFO - Found 2 document-related sections\n",
      "2025-05-08 20:13:32,314 - ISDADebugger - INFO - Found 2 document-related sections\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "\n",
    "def debug_isda_website():\n",
    "    \"\"\"Directly debug the ISDA website structure\"\"\"\n",
    "    # Set up logging\n",
    "    logger = logging.getLogger(\"ISDADebugger\")\n",
    "    logger.setLevel(logging.INFO)\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n",
    "    logger.addHandler(handler)\n",
    "    \n",
    "    # Set up output directory\n",
    "    output_dir = Path(\"/workspace/data/test_isda\")\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Directly fetch the page\n",
    "    url = \"https://www.isda.org/category/documentation/\"\n",
    "    logger.info(f\"Directly fetching page: {url}\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers={\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        })\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Save HTML to file for inspection\n",
    "        html_path = output_dir / \"isda_page.html\"\n",
    "        with open(html_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(response.text)\n",
    "        logger.info(f\"Saved HTML to {html_path}\")\n",
    "        \n",
    "        # Parse HTML\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        # Analyze HTML structure\n",
    "        logger.info(\"Analyzing HTML structure:\")\n",
    "        \n",
    "        # Look for articles\n",
    "        articles = soup.find_all(\"article\")\n",
    "        logger.info(f\"Found {len(articles)} article elements\")\n",
    "        \n",
    "        # Analyze first few articles\n",
    "        for i, article in enumerate(articles[:6]):  # Examine up to 6 articles\n",
    "            logger.info(f\"Article {i+1}:\")\n",
    "            \n",
    "            # Look for title elements\n",
    "            titles = article.find_all([\"h1\", \"h2\", \"h3\", \"h4\", \"h5\"])\n",
    "            logger.info(f\"  Found {len(titles)} title elements\")\n",
    "            \n",
    "            # Look for links\n",
    "            links = article.find_all(\"a\", href=True)\n",
    "            logger.info(f\"  Found {len(links)} links\")\n",
    "            \n",
    "            # Get article classes\n",
    "            classes = article.get(\"class\", [])\n",
    "            logger.info(f\"  Article classes: {classes}\")\n",
    "            \n",
    "            # Get category info from classes\n",
    "            categories = [c for c in classes if c.startswith(\"category-\")]\n",
    "            if categories:\n",
    "                logger.info(f\"  Categories: {categories}\")\n",
    "            \n",
    "            # Get tag info from classes\n",
    "            tags = [c for c in classes if c.startswith(\"tag-\")]\n",
    "            if tags:\n",
    "                logger.info(f\"  Tags: {tags}\")\n",
    "        \n",
    "        # Look for pagination\n",
    "        pagination = soup.find_all(class_=lambda c: c and \"pagination\" in c)\n",
    "        logger.info(f\"Found {len(pagination)} pagination elements\")\n",
    "        \n",
    "        # Extract main document patterns\n",
    "        pdf_links = soup.find_all(\"a\", href=lambda h: h and h.lower().endswith(\".pdf\"))\n",
    "        logger.info(f\"Found {len(pdf_links)} direct PDF links\")\n",
    "        \n",
    "        # Check for documentation-specific sections\n",
    "        doc_sections = soup.find_all(class_=lambda c: c and \"document\" in c.lower())\n",
    "        logger.info(f\"Found {len(doc_sections)} document-related sections\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error debugging ISDA website: {e}\")\n",
    "        return False\n",
    "\n",
    "# Now run the debug function\n",
    "debug_isda_website()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "102723a0-15d3-471d-9216-f99af3b9ddc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 20:17:07,713 - ISDADocFinder - INFO - Checking ISDA main site navigation for documentation links\n",
      "2025-05-08 20:17:07,713 - ISDADocFinder - INFO - Checking ISDA main site navigation for documentation links\n",
      "2025-05-08 20:17:08,824 - ISDADocFinder - INFO - Found potential documentation link in nav: legal -> https://www.isda.org/category/legal/\n",
      "2025-05-08 20:17:08,824 - ISDADocFinder - INFO - Found potential documentation link in nav: legal -> https://www.isda.org/category/legal/\n",
      "2025-05-08 20:17:08,826 - ISDADocFinder - INFO - Found potential documentation link in nav: view all books -> https://www.isda.org/books\n",
      "2025-05-08 20:17:08,826 - ISDADocFinder - INFO - Found potential documentation link in nav: view all books -> https://www.isda.org/books\n",
      "2025-05-08 20:17:08,827 - ISDADocFinder - INFO - Checking potential documentation URL: https://www.isda.org/books/\n",
      "2025-05-08 20:17:08,827 - ISDADocFinder - INFO - Checking potential documentation URL: https://www.isda.org/books/\n",
      "2025-05-08 20:17:16,705 - ISDADocFinder - INFO - Found 1 document sections on https://www.isda.org/books/\n",
      "2025-05-08 20:17:16,705 - ISDADocFinder - INFO - Found 1 document sections on https://www.isda.org/books/\n",
      "2025-05-08 20:17:16,708 - ISDADocFinder - INFO - Checking potential documentation URL: https://www.isda.org/documentation/\n",
      "2025-05-08 20:17:16,708 - ISDADocFinder - INFO - Checking potential documentation URL: https://www.isda.org/documentation/\n",
      "2025-05-08 20:17:18,661 - ISDADocFinder - INFO - URL not found: https://www.isda.org/documentation/\n",
      "2025-05-08 20:17:18,661 - ISDADocFinder - INFO - URL not found: https://www.isda.org/documentation/\n",
      "2025-05-08 20:17:18,664 - ISDADocFinder - INFO - Checking potential documentation URL: https://www.isda.org/documents/\n",
      "2025-05-08 20:17:18,664 - ISDADocFinder - INFO - Checking potential documentation URL: https://www.isda.org/documents/\n",
      "2025-05-08 20:17:20,552 - ISDADocFinder - INFO - URL not found: https://www.isda.org/documents/\n",
      "2025-05-08 20:17:20,552 - ISDADocFinder - INFO - URL not found: https://www.isda.org/documents/\n",
      "2025-05-08 20:17:20,553 - ISDADocFinder - INFO - Checking potential documentation URL: https://www.isda.org/protocols/\n",
      "2025-05-08 20:17:20,553 - ISDADocFinder - INFO - Checking potential documentation URL: https://www.isda.org/protocols/\n",
      "2025-05-08 20:17:24,100 - ISDADocFinder - INFO - Found 2 PDF links on https://www.isda.org/protocols/\n",
      "2025-05-08 20:17:24,100 - ISDADocFinder - INFO - Found 2 PDF links on https://www.isda.org/protocols/\n",
      "2025-05-08 20:17:24,102 - ISDADocFinder - INFO - Checking potential documentation URL: https://www.isda.org/guides/\n",
      "2025-05-08 20:17:24,102 - ISDADocFinder - INFO - Checking potential documentation URL: https://www.isda.org/guides/\n",
      "2025-05-08 20:17:26,236 - ISDADocFinder - INFO - URL not found: https://www.isda.org/guides/\n",
      "2025-05-08 20:17:26,236 - ISDADocFinder - INFO - URL not found: https://www.isda.org/guides/\n",
      "2025-05-08 20:17:26,238 - ISDADocFinder - INFO - Checking potential documentation URL: https://www.isda.org/bookstore/\n",
      "2025-05-08 20:17:26,238 - ISDADocFinder - INFO - Checking potential documentation URL: https://www.isda.org/bookstore/\n",
      "2025-05-08 20:17:28,208 - ISDADocFinder - INFO - URL not found: https://www.isda.org/bookstore/\n",
      "2025-05-08 20:17:28,208 - ISDADocFinder - INFO - URL not found: https://www.isda.org/bookstore/\n",
      "2025-05-08 20:17:28,210 - ISDADocFinder - INFO - Checking potential documentation URL: https://www.isda.org/books-and-protocols/\n",
      "2025-05-08 20:17:28,210 - ISDADocFinder - INFO - Checking potential documentation URL: https://www.isda.org/books-and-protocols/\n",
      "2025-05-08 20:17:30,198 - ISDADocFinder - INFO - URL not found: https://www.isda.org/books-and-protocols/\n",
      "2025-05-08 20:17:30,198 - ISDADocFinder - INFO - URL not found: https://www.isda.org/books-and-protocols/\n",
      "2025-05-08 20:17:30,199 - ISDADocFinder - INFO - Checking potential documentation URL: https://www.isda.org/book-and-protocol-list/\n",
      "2025-05-08 20:17:30,199 - ISDADocFinder - INFO - Checking potential documentation URL: https://www.isda.org/book-and-protocol-list/\n",
      "2025-05-08 20:17:32,249 - ISDADocFinder - INFO - URL not found: https://www.isda.org/book-and-protocol-list/\n",
      "2025-05-08 20:17:32,249 - ISDADocFinder - INFO - URL not found: https://www.isda.org/book-and-protocol-list/\n",
      "2025-05-08 20:17:32,251 - ISDADocFinder - INFO - Checking potential documentation URL: https://www.isda.org/legal-documentation/\n",
      "2025-05-08 20:17:32,251 - ISDADocFinder - INFO - Checking potential documentation URL: https://www.isda.org/legal-documentation/\n",
      "2025-05-08 20:17:34,205 - ISDADocFinder - INFO - URL not found: https://www.isda.org/legal-documentation/\n",
      "2025-05-08 20:17:34,205 - ISDADocFinder - INFO - URL not found: https://www.isda.org/legal-documentation/\n",
      "2025-05-08 20:17:34,207 - ISDADocFinder - INFO - Checking potential documentation URL: https://www.isda.org/category/legal/\n",
      "2025-05-08 20:17:34,207 - ISDADocFinder - INFO - Checking potential documentation URL: https://www.isda.org/category/legal/\n",
      "2025-05-08 20:17:35,104 - ISDADocFinder - INFO - Found 1 document sections on https://www.isda.org/category/legal/\n",
      "2025-05-08 20:17:35,104 - ISDADocFinder - INFO - Found 1 document sections on https://www.isda.org/category/legal/\n",
      "2025-05-08 20:17:35,105 - ISDADocFinder - INFO - Checking potential documentation URL: https://www.isda.org/books\n",
      "2025-05-08 20:17:35,105 - ISDADocFinder - INFO - Checking potential documentation URL: https://www.isda.org/books\n",
      "2025-05-08 20:17:41,143 - ISDADocFinder - INFO - Found 1 document sections on https://www.isda.org/books\n",
      "2025-05-08 20:17:41,143 - ISDADocFinder - INFO - Found 1 document sections on https://www.isda.org/books\n",
      "2025-05-08 20:17:41,146 - ISDADocFinder - INFO - Checking search page: https://www.isda.org/?s=documentation+protocol\n",
      "2025-05-08 20:17:41,146 - ISDADocFinder - INFO - Checking search page: https://www.isda.org/?s=documentation+protocol\n",
      "2025-05-08 20:17:44,405 - ISDADocFinder - INFO - Found potential documentation link in search: ISDA 2021 SBS Top-Up Protocol -> https://www.isda.org/protocol/isda-2021-sbs-top-up-protocol/\n",
      "2025-05-08 20:17:44,405 - ISDADocFinder - INFO - Found potential documentation link in search: ISDA 2021 SBS Top-Up Protocol -> https://www.isda.org/protocol/isda-2021-sbs-top-up-protocol/\n",
      "2025-05-08 20:17:44,407 - ISDADocFinder - INFO - Found potential documentation link in search: Adhere to this Protocol -> https://www.isda.org/protocol/isda-2021-sbs-top-up-protocol/submit/\n",
      "2025-05-08 20:17:44,407 - ISDADocFinder - INFO - Found potential documentation link in search: Adhere to this Protocol -> https://www.isda.org/protocol/isda-2021-sbs-top-up-protocol/submit/\n",
      "2025-05-08 20:17:44,409 - ISDADocFinder - INFO - Found potential documentation link in search: View Adhering Parties -> https://www.isda.org/protocol/isda-2021-sbs-top-up-protocol/adhering-parties\n",
      "2025-05-08 20:17:44,409 - ISDADocFinder - INFO - Found potential documentation link in search: View Adhering Parties -> https://www.isda.org/protocol/isda-2021-sbs-top-up-protocol/adhering-parties\n",
      "2025-05-08 20:17:44,410 - ISDADocFinder - INFO - Found potential documentation link in search: ISDA August 2012 DF Protocol -> https://www.isda.org/protocol/isda-august-2012-df-protocol/\n",
      "2025-05-08 20:17:44,410 - ISDADocFinder - INFO - Found potential documentation link in search: ISDA August 2012 DF Protocol -> https://www.isda.org/protocol/isda-august-2012-df-protocol/\n",
      "2025-05-08 20:17:44,411 - ISDADocFinder - INFO - Found potential documentation link in search: Adhere to this Protocol -> https://www.isda.org/protocol/isda-august-2012-df-protocol/submit/\n",
      "2025-05-08 20:17:44,411 - ISDADocFinder - INFO - Found potential documentation link in search: Adhere to this Protocol -> https://www.isda.org/protocol/isda-august-2012-df-protocol/submit/\n",
      "2025-05-08 20:17:44,412 - ISDADocFinder - INFO - Found potential documentation link in search: View Adhering Parties -> https://www.isda.org/protocol/isda-august-2012-df-protocol/adhering-parties\n",
      "2025-05-08 20:17:44,412 - ISDADocFinder - INFO - Found potential documentation link in search: View Adhering Parties -> https://www.isda.org/protocol/isda-august-2012-df-protocol/adhering-parties\n",
      "2025-05-08 20:17:44,413 - ISDADocFinder - INFO - Found potential documentation link in search: ISDA March 2013 DF Protocol -> https://www.isda.org/protocol/isda-march-2013-df-protocol/\n",
      "2025-05-08 20:17:44,413 - ISDADocFinder - INFO - Found potential documentation link in search: ISDA March 2013 DF Protocol -> https://www.isda.org/protocol/isda-march-2013-df-protocol/\n",
      "2025-05-08 20:17:44,414 - ISDADocFinder - INFO - Found potential documentation link in search: Adhere to this Protocol -> https://www.isda.org/protocol/isda-march-2013-df-protocol/submit/\n",
      "2025-05-08 20:17:44,414 - ISDADocFinder - INFO - Found potential documentation link in search: Adhere to this Protocol -> https://www.isda.org/protocol/isda-march-2013-df-protocol/submit/\n",
      "2025-05-08 20:17:44,415 - ISDADocFinder - INFO - Found potential documentation link in search: View Adhering Parties -> https://www.isda.org/protocol/isda-march-2013-df-protocol/adhering-parties\n",
      "2025-05-08 20:17:44,415 - ISDADocFinder - INFO - Found potential documentation link in search: View Adhering Parties -> https://www.isda.org/protocol/isda-march-2013-df-protocol/adhering-parties\n",
      "2025-05-08 20:17:44,415 - ISDADocFinder - INFO - Found potential documentation link in search: April 2025 Benchmark Module of the ISDA 2021 Fallbacks Protocol -> https://www.isda.org/protocol/april-2025-benchmark-module-of-the-isda-2021-fallbacks-protocol/\n",
      "2025-05-08 20:17:44,415 - ISDADocFinder - INFO - Found potential documentation link in search: April 2025 Benchmark Module of the ISDA 2021 Fallbacks Protocol -> https://www.isda.org/protocol/april-2025-benchmark-module-of-the-isda-2021-fallbacks-protocol/\n",
      "2025-05-08 20:17:44,416 - ISDADocFinder - INFO - Found potential documentation link in search: Adhere to this Protocol -> https://www.isda.org/protocol/april-2025-benchmark-module-of-the-isda-2021-fallbacks-protocol/submit/\n",
      "2025-05-08 20:17:44,416 - ISDADocFinder - INFO - Found potential documentation link in search: Adhere to this Protocol -> https://www.isda.org/protocol/april-2025-benchmark-module-of-the-isda-2021-fallbacks-protocol/submit/\n",
      "2025-05-08 20:17:44,419 - ISDADocFinder - INFO - Found potential documentation link in search: View Adhering Parties -> https://www.isda.org/protocol/april-2025-benchmark-module-of-the-isda-2021-fallbacks-protocol/adhering-parties\n",
      "2025-05-08 20:17:44,419 - ISDADocFinder - INFO - Found potential documentation link in search: View Adhering Parties -> https://www.isda.org/protocol/april-2025-benchmark-module-of-the-isda-2021-fallbacks-protocol/adhering-parties\n",
      "2025-05-08 20:17:44,424 - ISDADocFinder - INFO - Found 16 potential documentation pages, saved to /workspace/data/test_isda/isda_documentation_sources.json\n",
      "2025-05-08 20:17:44,424 - ISDADocFinder - INFO - Found 16 potential documentation pages, saved to /workspace/data/test_isda/isda_documentation_sources.json\n",
      "2025-05-08 20:17:44,431 - ISDADocCollector - INFO - Collecting ISDA documentation from 16 sources\n",
      "2025-05-08 20:17:44,431 - ISDADocCollector - INFO - Collecting ISDA documentation from 16 sources\n",
      "2025-05-08 20:17:44,432 - ISDADocCollector - INFO - Processing source 1/5: https://www.isda.org/books/\n",
      "2025-05-08 20:17:44,432 - ISDADocCollector - INFO - Processing source 1/5: https://www.isda.org/books/\n",
      "2025-05-08 20:17:45,570 - ISDADocCollector - INFO - Found 0 PDF links on https://www.isda.org/books/\n",
      "2025-05-08 20:17:45,570 - ISDADocCollector - INFO - Found 0 PDF links on https://www.isda.org/books/\n",
      "2025-05-08 20:17:48,607 - ISDADocCollector - INFO - Processing source 2/5: https://www.isda.org/protocols/\n",
      "2025-05-08 20:17:48,607 - ISDADocCollector - INFO - Processing source 2/5: https://www.isda.org/protocols/\n",
      "2025-05-08 20:17:51,513 - ISDADocCollector - INFO - Found 2 PDF links on https://www.isda.org/protocols/\n",
      "2025-05-08 20:17:51,513 - ISDADocCollector - INFO - Found 2 PDF links on https://www.isda.org/protocols/\n",
      "2025-05-08 20:17:51,514 - ISDADocCollector - INFO - Downloading PDF: https://www.isda.org/a/k9GEE/ISDA-2018-Benchmarks-Supplement-Protocol.pdf -> /workspace/data/test_isda/isda-2018-benchmarks-supplement-protocol-agreement.pdf\n",
      "2025-05-08 20:17:51,514 - ISDADocCollector - INFO - Downloading PDF: https://www.isda.org/a/k9GEE/ISDA-2018-Benchmarks-Supplement-Protocol.pdf -> /workspace/data/test_isda/isda-2018-benchmarks-supplement-protocol-agreement.pdf\n",
      "2025-05-08 20:17:53,175 - ISDADocCollector - INFO - Successfully downloaded: /workspace/data/test_isda/isda-2018-benchmarks-supplement-protocol-agreement.pdf\n",
      "2025-05-08 20:17:53,175 - ISDADocCollector - INFO - Successfully downloaded: /workspace/data/test_isda/isda-2018-benchmarks-supplement-protocol-agreement.pdf\n",
      "2025-05-08 20:17:53,177 - ISDADocCollector - INFO - Downloading PDF: https://www.isda.org/a/09GEE/ISDA-2018-Benchmarks-Supplement-Protocol-Questionnaire.pdf -> /workspace/data/test_isda/isda-2018-benchmarks-supplement-protocol-questionn.pdf\n",
      "2025-05-08 20:17:53,177 - ISDADocCollector - INFO - Downloading PDF: https://www.isda.org/a/09GEE/ISDA-2018-Benchmarks-Supplement-Protocol-Questionnaire.pdf -> /workspace/data/test_isda/isda-2018-benchmarks-supplement-protocol-questionn.pdf\n",
      "2025-05-08 20:17:54,540 - ISDADocCollector - INFO - Successfully downloaded: /workspace/data/test_isda/isda-2018-benchmarks-supplement-protocol-questionn.pdf\n",
      "2025-05-08 20:17:54,540 - ISDADocCollector - INFO - Successfully downloaded: /workspace/data/test_isda/isda-2018-benchmarks-supplement-protocol-questionn.pdf\n",
      "2025-05-08 20:17:57,552 - ISDADocCollector - INFO - Processing source 3/5: https://www.isda.org/category/legal/\n",
      "2025-05-08 20:17:57,552 - ISDADocCollector - INFO - Processing source 3/5: https://www.isda.org/category/legal/\n",
      "2025-05-08 20:17:58,835 - ISDADocCollector - INFO - Found 0 PDF links on https://www.isda.org/category/legal/\n",
      "2025-05-08 20:17:58,835 - ISDADocCollector - INFO - Found 0 PDF links on https://www.isda.org/category/legal/\n",
      "2025-05-08 20:18:01,840 - ISDADocCollector - INFO - Processing source 4/5: https://www.isda.org/books\n",
      "2025-05-08 20:18:01,840 - ISDADocCollector - INFO - Processing source 4/5: https://www.isda.org/books\n",
      "2025-05-08 20:18:06,715 - ISDADocCollector - INFO - Found 0 PDF links on https://www.isda.org/books\n",
      "2025-05-08 20:18:06,715 - ISDADocCollector - INFO - Found 0 PDF links on https://www.isda.org/books\n",
      "2025-05-08 20:18:09,748 - ISDADocCollector - INFO - Processing source 5/5: https://www.isda.org/protocol/isda-2021-sbs-top-up-protocol/\n",
      "2025-05-08 20:18:09,748 - ISDADocCollector - INFO - Processing source 5/5: https://www.isda.org/protocol/isda-2021-sbs-top-up-protocol/\n",
      "2025-05-08 20:18:10,991 - ISDADocCollector - INFO - Found 3 PDF links on https://www.isda.org/protocol/isda-2021-sbs-top-up-protocol/\n",
      "2025-05-08 20:18:10,991 - ISDADocCollector - INFO - Found 3 PDF links on https://www.isda.org/protocol/isda-2021-sbs-top-up-protocol/\n",
      "2025-05-08 20:18:10,994 - ISDADocCollector - INFO - Downloading PDF: https://www.isda.org/a/fotgE/FINAL_SBS-Top-Up-Protocol-Annotated-Version-121421.pdf -> /workspace/data/test_isda/isda-2021-sbs-top-up-protocol-annotated.pdf\n",
      "2025-05-08 20:18:10,994 - ISDADocCollector - INFO - Downloading PDF: https://www.isda.org/a/fotgE/FINAL_SBS-Top-Up-Protocol-Annotated-Version-121421.pdf -> /workspace/data/test_isda/isda-2021-sbs-top-up-protocol-annotated.pdf\n",
      "2025-05-08 20:18:16,071 - ISDADocCollector - INFO - Successfully downloaded: /workspace/data/test_isda/isda-2021-sbs-top-up-protocol-annotated.pdf\n",
      "2025-05-08 20:18:16,071 - ISDADocCollector - INFO - Successfully downloaded: /workspace/data/test_isda/isda-2021-sbs-top-up-protocol-annotated.pdf\n",
      "2025-05-08 20:18:16,074 - ISDADocCollector - INFO - Downloading PDF: https://www.isda.org/a/sfZTE/ISDA-2021-SBS-Top-Up-Protocol_FOR_PUBLICATION_FEB_25_2021_FINAL_FN.pdf -> /workspace/data/test_isda/isda-2021-sbs-top-up-protocol-will-open-in-a-new-t.pdf\n",
      "2025-05-08 20:18:16,074 - ISDADocCollector - INFO - Downloading PDF: https://www.isda.org/a/sfZTE/ISDA-2021-SBS-Top-Up-Protocol_FOR_PUBLICATION_FEB_25_2021_FINAL_FN.pdf -> /workspace/data/test_isda/isda-2021-sbs-top-up-protocol-will-open-in-a-new-t.pdf\n",
      "2025-05-08 20:18:18,786 - ISDADocCollector - INFO - Successfully downloaded: /workspace/data/test_isda/isda-2021-sbs-top-up-protocol-will-open-in-a-new-t.pdf\n",
      "2025-05-08 20:18:18,786 - ISDADocCollector - INFO - Successfully downloaded: /workspace/data/test_isda/isda-2021-sbs-top-up-protocol-will-open-in-a-new-t.pdf\n",
      "2025-05-08 20:18:18,789 - ISDADocCollector - INFO - Downloading PDF: https://www.isda.org/a/fRZTE/How-to-Adhere-to-2021-SBS-Top-Up-Protocol-Final.pdf -> /workspace/data/test_isda/how-to-adhere-instructions-will-open-in-a-new-tab-.pdf\n",
      "2025-05-08 20:18:18,789 - ISDADocCollector - INFO - Downloading PDF: https://www.isda.org/a/fRZTE/How-to-Adhere-to-2021-SBS-Top-Up-Protocol-Final.pdf -> /workspace/data/test_isda/how-to-adhere-instructions-will-open-in-a-new-tab-.pdf\n",
      "2025-05-08 20:18:20,367 - ISDADocCollector - INFO - Successfully downloaded: /workspace/data/test_isda/how-to-adhere-instructions-will-open-in-a-new-tab-.pdf\n",
      "2025-05-08 20:18:20,367 - ISDADocCollector - INFO - Successfully downloaded: /workspace/data/test_isda/how-to-adhere-instructions-will-open-in-a-new-tab-.pdf\n",
      "2025-05-08 20:18:23,400 - ISDADocCollector - INFO - Collected 5 documents, metadata saved to /workspace/data/test_isda/isda_documents_metadata.json\n",
      "2025-05-08 20:18:23,400 - ISDADocCollector - INFO - Collected 5 documents, metadata saved to /workspace/data/test_isda/isda_documents_metadata.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 5 ISDA documentation files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Set up logging\n",
    "import logging\n",
    "logger = logging.getLogger(\"ISDADocFinder\")\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.StreamHandler()\n",
    "handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n",
    "logger.addHandler(handler)\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path(\"/workspace/data/test_isda\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# First, let's explore ISDA's main documentation library pages\n",
    "def find_isda_documentation():\n",
    "    \"\"\"Find where actual ISDA documentation is stored\"\"\"\n",
    "    # List of potential documentation URLs to check\n",
    "    potential_urls = [\n",
    "        \"https://www.isda.org/books/\",\n",
    "        \"https://www.isda.org/documentation/\",\n",
    "        \"https://www.isda.org/documents/\",\n",
    "        \"https://www.isda.org/protocols/\",\n",
    "        \"https://www.isda.org/guides/\",\n",
    "        \"https://www.isda.org/bookstore/\",\n",
    "        \"https://www.isda.org/books-and-protocols/\",\n",
    "        \"https://www.isda.org/book-and-protocol-list/\",\n",
    "        \"https://www.isda.org/legal-documentation/\"\n",
    "    ]\n",
    "    \n",
    "    # Check navigation on the main site first\n",
    "    logger.info(\"Checking ISDA main site navigation for documentation links\")\n",
    "    try:\n",
    "        main_response = requests.get(\"https://www.isda.org\", headers={\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        })\n",
    "        main_response.raise_for_status()\n",
    "        main_soup = BeautifulSoup(main_response.text, \"html.parser\")\n",
    "        \n",
    "        # Look for navigation menu items that might contain \"documentation\"\n",
    "        nav_items = main_soup.find_all(\"a\", href=True)\n",
    "        for item in nav_items:\n",
    "            text = item.text.strip().lower()\n",
    "            href = item[\"href\"]\n",
    "            \n",
    "            if any(keyword in text for keyword in [\"document\", \"legal\", \"protocol\", \"book\", \"publication\"]):\n",
    "                # Add to potential URLs if it looks like a documentation link\n",
    "                full_url = urljoin(\"https://www.isda.org\", href)\n",
    "                if full_url not in potential_urls:\n",
    "                    potential_urls.append(full_url)\n",
    "                    logger.info(f\"Found potential documentation link in nav: {text} -> {full_url}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error checking main site: {e}\")\n",
    "    \n",
    "    # Test each potential URL\n",
    "    documentation_pages = []\n",
    "    \n",
    "    for url in potential_urls:\n",
    "        logger.info(f\"Checking potential documentation URL: {url}\")\n",
    "        try:\n",
    "            response = requests.get(url, headers={\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "            })\n",
    "            \n",
    "            # Skip if not found\n",
    "            if response.status_code == 404:\n",
    "                logger.info(f\"URL not found: {url}\")\n",
    "                continue\n",
    "                \n",
    "            # Save a copy for inspection\n",
    "            url_filename = url.split(\"/\")[-2] if url.endswith(\"/\") else url.split(\"/\")[-1]\n",
    "            if not url_filename:\n",
    "                url_filename = \"index\"\n",
    "            html_path = output_dir / f\"isda_{url_filename}.html\"\n",
    "            with open(html_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(response.text)\n",
    "            \n",
    "            # Check for PDF links\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            pdf_links = soup.find_all(\"a\", href=lambda h: h and h.lower().endswith(\".pdf\"))\n",
    "            \n",
    "            if pdf_links:\n",
    "                logger.info(f\"Found {len(pdf_links)} PDF links on {url}\")\n",
    "                documentation_pages.append({\n",
    "                    \"url\": url,\n",
    "                    \"pdf_links_count\": len(pdf_links),\n",
    "                    \"title\": soup.title.text if soup.title else url\n",
    "                })\n",
    "            else:\n",
    "                # Also check for document sections\n",
    "                doc_sections = soup.find_all(class_=lambda c: c and \"document\" in str(c).lower())\n",
    "                if doc_sections:\n",
    "                    logger.info(f\"Found {len(doc_sections)} document sections on {url}\")\n",
    "                    documentation_pages.append({\n",
    "                        \"url\": url,\n",
    "                        \"document_sections_count\": len(doc_sections),\n",
    "                        \"title\": soup.title.text if soup.title else url\n",
    "                    })\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error checking {url}: {e}\")\n",
    "    \n",
    "    # Check search page for specific documentation\n",
    "    try:\n",
    "        search_url = \"https://www.isda.org/?s=documentation+protocol\"\n",
    "        logger.info(f\"Checking search page: {search_url}\")\n",
    "        \n",
    "        search_response = requests.get(search_url, headers={\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        })\n",
    "        \n",
    "        search_soup = BeautifulSoup(search_response.text, \"html.parser\")\n",
    "        \n",
    "        # Save search results for inspection\n",
    "        search_path = output_dir / \"isda_search_results.html\"\n",
    "        with open(search_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(search_response.text)\n",
    "        \n",
    "        # Look for search results with PDFs or documentation links\n",
    "        search_results = search_soup.find_all(\"article\")\n",
    "        for result in search_results[:10]:  # Check first 10 results\n",
    "            links = result.find_all(\"a\", href=True)\n",
    "            for link in links:\n",
    "                href = link[\"href\"]\n",
    "                if \"/document/\" in href or \"/protocol/\" in href or \"/bookstore/\" in href:\n",
    "                    documentation_pages.append({\n",
    "                        \"url\": href if href.startswith(\"http\") else urljoin(\"https://www.isda.org\", href),\n",
    "                        \"source\": \"search\",\n",
    "                        \"title\": link.text.strip()\n",
    "                    })\n",
    "                    logger.info(f\"Found potential documentation link in search: {link.text.strip()} -> {href}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error checking search page: {e}\")\n",
    "    \n",
    "    # Save results\n",
    "    results_path = output_dir / \"isda_documentation_sources.json\"\n",
    "    with open(results_path, \"w\") as f:\n",
    "        json.dump(documentation_pages, f, indent=2)\n",
    "    \n",
    "    logger.info(f\"Found {len(documentation_pages)} potential documentation pages, saved to {results_path}\")\n",
    "    return documentation_pages\n",
    "\n",
    "# Run the documentation finder\n",
    "documentation_pages = find_isda_documentation()\n",
    "\n",
    "# Now create a specific collector for the documentation pages we found\n",
    "class ISDADocumentationCollector:\n",
    "    \"\"\"Collector for ISDA documentation from identified sources\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir, sources=None):\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True, parents=True)\n",
    "        self.sources = sources or []\n",
    "        self.logger = logging.getLogger(\"ISDADocCollector\")\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        if not self.logger.handlers:\n",
    "            handler = logging.StreamHandler()\n",
    "            handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n",
    "            self.logger.addHandler(handler)\n",
    "    \n",
    "    def collect(self, max_sources=5):\n",
    "        \"\"\"Collect documentation from identified sources\"\"\"\n",
    "        self.logger.info(f\"Collecting ISDA documentation from {len(self.sources)} sources\")\n",
    "        \n",
    "        documents = []\n",
    "        session = requests.Session()\n",
    "        session.headers.update({\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        })\n",
    "        \n",
    "        # Process each source\n",
    "        for i, source in enumerate(self.sources[:max_sources]):\n",
    "            url = source.get(\"url\")\n",
    "            if not url:\n",
    "                continue\n",
    "                \n",
    "            self.logger.info(f\"Processing source {i+1}/{min(max_sources, len(self.sources))}: {url}\")\n",
    "            \n",
    "            try:\n",
    "                response = session.get(url)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                \n",
    "                # Find all PDF links\n",
    "                pdf_links = []\n",
    "                for a in soup.find_all(\"a\", href=True):\n",
    "                    href = a[\"href\"]\n",
    "                    if href.lower().endswith(\".pdf\"):\n",
    "                        full_url = urljoin(url, href)\n",
    "                        text = a.text.strip()\n",
    "                        \n",
    "                        # Skip if no text\n",
    "                        if not text:\n",
    "                            # Try to get text from parent or surrounding elements\n",
    "                            if a.parent:\n",
    "                                text = a.parent.text.strip()\n",
    "                        \n",
    "                        # Default text if still empty\n",
    "                        if not text:\n",
    "                            text = os.path.basename(href)\n",
    "                        \n",
    "                        pdf_links.append({\n",
    "                            \"url\": full_url,\n",
    "                            \"text\": text\n",
    "                        })\n",
    "                \n",
    "                self.logger.info(f\"Found {len(pdf_links)} PDF links on {url}\")\n",
    "                \n",
    "                # Download each PDF\n",
    "                for pdf in pdf_links:\n",
    "                    pdf_url = pdf[\"url\"]\n",
    "                    text = pdf[\"text\"]\n",
    "                    \n",
    "                    # Generate filename\n",
    "                    safe_text = re.sub(r'[^\\w\\s-]', '', text).strip().lower()\n",
    "                    filename = re.sub(r'[-\\s]+', '-', safe_text)\n",
    "                    \n",
    "                    # Ensure filename has .pdf extension and isn't too long\n",
    "                    if len(filename) > 50:\n",
    "                        filename = filename[:50]\n",
    "                    if not filename.endswith(\".pdf\"):\n",
    "                        filename += \".pdf\"\n",
    "                    \n",
    "                    output_path = self.output_dir / filename\n",
    "                    \n",
    "                    # Download the file\n",
    "                    self.logger.info(f\"Downloading PDF: {pdf_url} -> {output_path}\")\n",
    "                    try:\n",
    "                        pdf_response = session.get(pdf_url, stream=True)\n",
    "                        pdf_response.raise_for_status()\n",
    "                        \n",
    "                        with open(output_path, \"wb\") as f:\n",
    "                            for chunk in pdf_response.iter_content(chunk_size=8192):\n",
    "                                if chunk:\n",
    "                                    f.write(chunk)\n",
    "                        \n",
    "                        self.logger.info(f\"Successfully downloaded: {output_path}\")\n",
    "                        \n",
    "                        # Add to documents list\n",
    "                        documents.append({\n",
    "                            \"title\": text,\n",
    "                            \"url\": pdf_url,\n",
    "                            \"source_url\": url,\n",
    "                            \"local_path\": str(output_path),\n",
    "                            \"file_size\": os.path.getsize(output_path)\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        self.logger.error(f\"Error downloading PDF {pdf_url}: {e}\")\n",
    "                \n",
    "                # Wait between sources\n",
    "                if i < len(self.sources) - 1:\n",
    "                    time.sleep(3)\n",
    "            \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error processing source {url}: {e}\")\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata_path = self.output_dir / \"isda_documents_metadata.json\"\n",
    "        with open(metadata_path, \"w\") as f:\n",
    "            json.dump(documents, f, indent=2)\n",
    "            \n",
    "        self.logger.info(f\"Collected {len(documents)} documents, metadata saved to {metadata_path}\")\n",
    "        return documents\n",
    "\n",
    "# Run the documentation collector with our found sources\n",
    "collector = ISDADocumentationCollector(output_dir, documentation_pages)\n",
    "documents = collector.collect(max_sources=5)\n",
    "print(f\"Downloaded {len(documents)} ISDA documentation files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504dcc6b-21d4-451f-bbb4-408d6530eee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text extraction and domain classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbfbb711-0333-44a7-a1a1-45f15f0b2a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 20:20:56,152 - DomainClassifier - INFO - Initialized classifier with 8 domains\n",
      "2025-05-08 20:20:56,152 - DomainClassifier - INFO - Initialized classifier with 8 domains\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: isda-2018-benchmarks-supplement-protocol-agreement.pdf\n",
      "🔍 Classification: crypto_derivatives\n",
      "📊 Confidence: 0.00\n",
      "📄 Text preview:  \n",
      " \n",
      "1 \n",
      "Copyright © 2018 by International Swaps and Derivatives Association, Inc.  \n",
      "  \n",
      " \n",
      "International Swaps and Derivatives Association,  \n",
      "Inc. \n",
      "ISDA ...\n",
      "--------------------------------------------------------------------------------\n",
      "Processing: isda-2018-benchmarks-supplement-protocol-questionn.pdf\n",
      "🔍 Classification: crypto_derivatives\n",
      "📊 Confidence: 0.47\n",
      "📄 Text preview:  \n",
      "1 \n",
      " \n",
      "Copyright © International Swaps and Derivatives Association, Inc. \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "International Swaps and Derivatives Association,  \n",
      "Inc. \n",
      "ISDA 201...\n",
      "--------------------------------------------------------------------------------\n",
      "Processing: isda-2021-sbs-top-up-protocol-annotated.pdf\n",
      "🔍 Classification: crypto_derivatives\n",
      "📊 Confidence: 0.54\n",
      "📄 Text preview: I\n",
      "nternational Swaps and Derivatives Association, Inc.  \n",
      "ISDA 2021 SBS TOP- UP PROTOCOL  \n",
      "(ANNOTATED PROTOCOL AND RELATED ATTACHMENTS)  \n",
      "I\n",
      "SDA 2021 SB...\n",
      "--------------------------------------------------------------------------------\n",
      "Processing: isda-2021-sbs-top-up-protocol-will-open-in-a-new-t.pdf\n",
      "🔍 Classification: crypto_derivatives\n",
      "📊 Confidence: 0.47\n",
      "📄 Text preview: 1 \n",
      " \n",
      "Copyright © 2021 by International Swaps and Derivatives Association, Inc.  \n",
      "  \n",
      "International Swaps and Derivatives Association,  Inc. \n",
      "ISDA 2021 ...\n",
      "--------------------------------------------------------------------------------\n",
      "Processing: how-to-adhere-instructions-will-open-in-a-new-tab-.pdf\n",
      "🔍 Classification: decentralized_finance\n",
      "📊 Confidence: 0.16\n",
      "📄 Text preview: 1 \n",
      " ISDA 2021 SBS Top -Up Protocol  \n",
      "How to Adhere:  Step -by-Step instructions for an adhering party  \n",
      " \n",
      "1)  Visit www.isda.org .  Go to the “ Protoc...\n",
      "--------------------------------------------------------------------------------\n",
      "Classified 5 out of 5 documents\n"
     ]
    }
   ],
   "source": [
    "from processors.text_extractor import TextExtractor\n",
    "from processors.domain_classifier import DomainClassifier\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Set up paths\n",
    "test_dir = Path(\"/workspace/data/test_isda\")\n",
    "pdf_files = list(test_dir.glob(\"*.pdf\"))\n",
    "\n",
    "# Load domain config\n",
    "from config.domain_config import DOMAINS\n",
    "\n",
    "# Initialize processors\n",
    "extractor = TextExtractor()\n",
    "classifier = DomainClassifier(DOMAINS)\n",
    "\n",
    "# Process each PDF file\n",
    "results = []\n",
    "for pdf_file in pdf_files:\n",
    "    print(f\"Processing: {pdf_file.name}\")\n",
    "    try:\n",
    "        # Extract text\n",
    "        extraction_result = extractor.extract(pdf_file)\n",
    "        if not extraction_result or 'text' not in extraction_result:\n",
    "            print(f\"⚠️ Failed to extract text from {pdf_file.name}\")\n",
    "            continue\n",
    "            \n",
    "        # Get text sample for classification\n",
    "        text_sample = extraction_result['text'][:5000]  # First ~5000 chars for classification\n",
    "        \n",
    "        # Classify the document\n",
    "        classification = classifier.classify(text_sample)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"🔍 Classification: {classification['domain']}\")\n",
    "        print(f\"📊 Confidence: {classification['confidence']:.2f}\")\n",
    "        print(f\"📄 Text preview: {text_sample[:150]}...\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Save results\n",
    "        results.append({\n",
    "            \"filename\": pdf_file.name,\n",
    "            \"domain\": classification[\"domain\"],\n",
    "            \"confidence\": classification[\"confidence\"],\n",
    "            \"text_length\": len(extraction_result['text'])\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {pdf_file.name}: {e}\")\n",
    "\n",
    "# Save classification results\n",
    "with open(test_dir / \"classification_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Classified {len(results)} out of {len(pdf_files)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20696520-3aee-45b8-9100-69e75a6e6f57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fef394-8865-4e2e-ad5e-dd26da0757d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Integration with main corpus builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56e9de63-fe6d-4da2-8a1a-d899f7c17f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found command_interface.py at: ['/workspace/data scraper scripts/command_interface.py']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Search for command_interface.py in the workspace\n",
    "command_interface_paths = glob.glob(\"/workspace/**/command_interface.py\", recursive=True)\n",
    "print(\"Found command_interface.py at:\", command_interface_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2952cd40-4bdf-4554-a113-0159ff3c0758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "# First, let's locate the command_interface.py file\n",
    "def find_file(filename, search_path=\"/workspace\"):\n",
    "    \"\"\"Find a file in the given search path\"\"\"\n",
    "    result = []\n",
    "    for root, dirs, files in os.walk(search_path):\n",
    "        if filename in files:\n",
    "            result.append(os.path.join(root, filename))\n",
    "    return result\n",
    "\n",
    "# Find command_interface.py\n",
    "cmd_interface_paths = find_file(\"command_interface.py\")\n",
    "print(f\"Found command_interface.py at: {cmd_interface_paths}\")\n",
    "\n",
    "# Use the first match if found\n",
    "cmd_interface_path = cmd_interface_paths[0] if cmd_interface_paths else None\n",
    "\n",
    "if not cmd_interface_path:\n",
    "    print(\"Could not find command_interface.py. Testing with direct file path.\")\n",
    "    cmd_interface_path = \"/workspace/data scraper scripts/command_interface.py\"\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(cmd_interface_path):\n",
    "    print(f\"Using command_interface.py at: {cmd_interface_path}\")\n",
    "else:\n",
    "    print(f\"Error: File does not exist at {cmd_interface_path}\")\n",
    "\n",
    "# Directory where command_interface.py is located\n",
    "script_dir = os.path.dirname(cmd_interface_path)\n",
    "print(f\"Script directory: {script_dir}\")\n",
    "\n",
    "# Create directories if needed\n",
    "os.makedirs(\"/workspace/data/corpus_1/crypto_derivatives\", exist_ok=True)\n",
    "os.makedirs(\"/workspace/data/corpus_1/risk_management\", exist_ok=True)\n",
    "\n",
    "# Change to the script directory and run from there\n",
    "os.chdir(script_dir)\n",
    "print(f\"Changed working directory to: {os.getcwd()}\")\n",
    "\n",
    "# Run corpus builder for derivatives domain\n",
    "print(\"\\nRunning corpus builder for the derivatives domain...\")\n",
    "try:\n",
    "    subprocess.run(\n",
    "        [\"python\", os.path.basename(cmd_interface_path), \"run_corpus_builder\", \n",
    "         \"--domains\", \"crypto_derivatives\", \n",
    "         \"--downloads\", \"10\"],\n",
    "        check=True\n",
    "    )\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Error running corpus builder: {e}\")\n",
    "\n",
    "# Use the curated list for more focused download\n",
    "print(\"\\nRunning corpus builder with curated list...\")\n",
    "try:\n",
    "    # Check if curated list exists\n",
    "    crypto_finance_list = os.path.join(script_dir, \"crypto_finance_list.json\")\n",
    "    if os.path.exists(crypto_finance_list):\n",
    "        print(f\"Using curated list at: {crypto_finance_list}\")\n",
    "        subprocess.run(\n",
    "            [\"python\", os.path.basename(cmd_interface_path), \"run_corpus_builder\", \n",
    "             \"--curated-list\", crypto_finance_list],\n",
    "            check=True\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Curated list not found at: {crypto_finance_list}\")\n",
    "        # Let's find it\n",
    "        curated_lists = glob.glob(\"/workspace/**/crypto_finance_list.json\", recursive=True)\n",
    "        if curated_lists:\n",
    "            print(f\"Found curated list at: {curated_lists[0]}\")\n",
    "            subprocess.run(\n",
    "                [\"python\", os.path.basename(cmd_interface_path), \"run_corpus_builder\", \n",
    "                 \"--curated-list\", curated_lists[0]],\n",
    "                check=True\n",
    "            )\n",
    "        else:\n",
    "            print(\"Could not find crypto_finance_list.json\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Error running corpus builder with curated list: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a9c265-6ee4-4533-9069-1bcb386aaebe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c18b98-0e4a-4ce2-aed6-73f99c544ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test anna's archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eef03b40-4c01-4e8d-8e87-3dd2f469e787",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'find_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# First, find test_client.py\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m test_client_paths \u001b[38;5;241m=\u001b[39m \u001b[43mfind_file\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_client.py\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound test_client.py at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_client_paths\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Use the first match if found\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'find_file' is not defined"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# First, find test_client.py\n",
    "test_client_paths = find_file(\"test_client.py\")\n",
    "print(f\"Found test_client.py at: {test_client_paths}\")\n",
    "\n",
    "# Use the first match if found\n",
    "test_client_path = test_client_paths[0] if test_client_paths else None\n",
    "\n",
    "if not test_client_path:\n",
    "    print(\"Could not find test_client.py. Testing with direct file path.\")\n",
    "    test_client_path = \"/workspace/test_client.py\"\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(test_client_path):\n",
    "    print(f\"Using test_client.py at: {test_client_path}\")\n",
    "    \n",
    "    # Directory where test_client.py is located\n",
    "    client_dir = os.path.dirname(test_client_path)\n",
    "    print(f\"Client directory: {client_dir}\")\n",
    "    \n",
    "    # Change to the client directory and run from there\n",
    "    os.chdir(client_dir)\n",
    "    print(f\"Changed working directory to: {os.getcwd()}\")\n",
    "    \n",
    "    # Run test_client.py\n",
    "    print(\"\\nTesting Anna's Archive client...\")\n",
    "    try:\n",
    "        subprocess.run([\"python\", os.path.basename(test_client_path)], check=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error running test_client.py: {e}\")\n",
    "else:\n",
    "    print(f\"Error: File does not exist at {test_client_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0829b0-3c45-4211-bc62-d2c9d81dd857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda5b0fa-1c99-4b8f-9762-bed6f7d27cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56da54a7-6d42-4b1f-8747-5222a172c249",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus Statistics:\n",
      "Total Files: 6\n",
      "Total Size: 50.8 MB\n",
      "\n",
      "Domain Coverage:\n",
      "  downloads: 0 files, 0.0 MB\n",
      "  config: 0 files, 0.0 MB\n",
      "  logs: 0 files, 0.0 MB\n",
      "  batch_test: 0 files, 0.0 MB\n",
      "  crypto_derivatives: 5 files, 42.37 MB\n",
      "  high_frequency_trading: 1 files, 8.43 MB\n",
      "  market_microstructure: 0 files, 0.0 MB\n",
      "  risk_management: 0 files, 0.0 MB\n",
      "  decentralized_finance: 0 files, 0.0 MB\n",
      "  portfolio_construction: 0 files, 0.0 MB\n",
      "  valuation_models: 0 files, 0.0 MB\n",
      "  regulation_compliance: 0 files, 0.0 MB\n",
      "  scidb_papers: 0 files, 0.0 MB\n",
      "  temp: 0 files, 0.0 MB\n",
      "  derivatives: 0 files, 0.0 MB\n",
      "  test_batch: 0 files, 0.0 MB\n",
      "\n",
      "Saved statistics to /workspace/data/corpus_1/corpus_stats.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Set up paths\n",
    "corpus_dir = Path(\"/workspace/data/corpus_1\")\n",
    "corpus_stats = {\n",
    "    \"domains\": {},\n",
    "    \"total_files\": 0,\n",
    "    \"total_size_mb\": 0\n",
    "}\n",
    "\n",
    "# Evaluate each domain\n",
    "for domain in os.listdir(corpus_dir):\n",
    "    # Skip non-directory items\n",
    "    domain_dir = corpus_dir / domain\n",
    "    if not domain_dir.is_dir() or domain.endswith(\"_extracted\"):\n",
    "        continue\n",
    "    \n",
    "    # Count files\n",
    "    pdf_files = list(domain_dir.glob(\"*.pdf\"))\n",
    "    meta_files = list(domain_dir.glob(\"*.pdf.meta\"))\n",
    "    \n",
    "    # Calculate size\n",
    "    domain_size_bytes = sum(os.path.getsize(f) for f in pdf_files if os.path.exists(f))\n",
    "    domain_size_mb = domain_size_bytes / (1024 * 1024)\n",
    "    \n",
    "    # Add to stats\n",
    "    corpus_stats[\"domains\"][domain] = {\n",
    "        \"pdf_files\": len(pdf_files),\n",
    "        \"meta_files\": len(meta_files),\n",
    "        \"size_mb\": round(domain_size_mb, 2)\n",
    "    }\n",
    "    \n",
    "    corpus_stats[\"total_files\"] += len(pdf_files)\n",
    "    corpus_stats[\"total_size_mb\"] += domain_size_mb\n",
    "\n",
    "# Round total size\n",
    "corpus_stats[\"total_size_mb\"] = round(corpus_stats[\"total_size_mb\"], 2)\n",
    "\n",
    "# Print stats\n",
    "print(\"Corpus Statistics:\")\n",
    "print(f\"Total Files: {corpus_stats['total_files']}\")\n",
    "print(f\"Total Size: {corpus_stats['total_size_mb']} MB\")\n",
    "print(\"\\nDomain Coverage:\")\n",
    "for domain, stats in corpus_stats[\"domains\"].items():\n",
    "    print(f\"  {domain}: {stats['pdf_files']} files, {stats['size_mb']} MB\")\n",
    "\n",
    "# Save stats\n",
    "with open(corpus_dir / \"corpus_stats.json\", \"w\") as f:\n",
    "    json.dump(corpus_stats, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved statistics to {corpus_dir / 'corpus_stats.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6103fe8-dcf8-4db8-9981-672790b16309",
   "metadata": {},
   "outputs": [],
   "source": [
    "test extraction entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d9120478-87a0-465d-ab6a-175c1c0f4a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing domain: downloads\n",
      "Processing domain: config\n",
      "Processing domain: logs\n",
      "Processing domain: batch_test\n",
      "Processing domain: crypto_derivatives\n",
      "  Extracting text from: English _en__ _pdf_ __lgli_lgrs_nexusstc_scihub_upload_zlib_ 13_2MB_ _ Book _non_598f73ea78c0aa8be43f264ba61e9e37.pdf\n",
      "    Saved extracted text to: /workspace/data/corpus_1/crypto_derivatives_extracted/English _en__ _pdf_ __lgli_lgrs_nexusstc_scihub_upload_zlib_ 13_2MB_ _ Book _non_598f73ea78c0aa8be43f264ba61e9e37.txt\n",
      "  Extracting text from: _pdf_ __upload_ 0_5MB_ _ Book _unknown__ upload_elsevier_elsevier-2023-2024_10_1_9217d59088dfda99ac5eb5332e35847f.pdf\n",
      "    Saved extracted text to: /workspace/data/corpus_1/crypto_derivatives_extracted/_pdf_ __upload_ 0_5MB_ _ Book _unknown__ upload_elsevier_elsevier-2023-2024_10_1_9217d59088dfda99ac5eb5332e35847f.txt\n",
      "  Extracting text from: English _en__ _pdf_ __lgli_lgrs_nexusstc_zlib_ 1_3MB_ _ Book _non-fiction__ nexu_eb35af40b3ea9c2d9558803514846ae3.pdf\n",
      "    Saved extracted text to: /workspace/data/corpus_1/crypto_derivatives_extracted/English _en__ _pdf_ __lgli_lgrs_nexusstc_zlib_ 1_3MB_ _ Book _non-fiction__ nexu_eb35af40b3ea9c2d9558803514846ae3.txt\n",
      "  Extracting text from: English _en__ _pdf_ __ia_ 13_6MB_ _ Book _unknown__ ia_hedgingfinancial0000mcki__6a3943739622bc73bb0ea64843424ab6.pdf\n",
      "    Saved extracted text to: /workspace/data/corpus_1/crypto_derivatives_extracted/English _en__ _pdf_ __ia_ 13_6MB_ _ Book _unknown__ ia_hedgingfinancial0000mcki__6a3943739622bc73bb0ea64843424ab6.txt\n",
      "  Extracting text from: English _en__ _pdf_ __lgli_lgrs_nexusstc_ 15_9MB_ _ Book _non-fiction__ nexusstc_eed6c801b2e71dfb452f732c9b043e40.pdf\n",
      "    Saved extracted text to: /workspace/data/corpus_1/crypto_derivatives_extracted/English _en__ _pdf_ __lgli_lgrs_nexusstc_ 15_9MB_ _ Book _non-fiction__ nexusstc_eed6c801b2e71dfb452f732c9b043e40.txt\n",
      "Processing domain: high_frequency_trading\n",
      "  Extracting text from: English _en__ _pdf_ __lgli_lgrs_nexusstc_zlib_ 8_8MB_ _ Book _non-fiction__ nexu_ba5a95225f922ea47a236b5a9d3950ea.pdf\n",
      "    Saved extracted text to: /workspace/data/corpus_1/high_frequency_trading_extracted/English _en__ _pdf_ __lgli_lgrs_nexusstc_zlib_ 8_8MB_ _ Book _non-fiction__ nexu_ba5a95225f922ea47a236b5a9d3950ea.txt\n",
      "Processing domain: market_microstructure\n",
      "Processing domain: risk_management\n",
      "Processing domain: decentralized_finance\n",
      "Processing domain: portfolio_construction\n",
      "Processing domain: valuation_models\n",
      "Processing domain: regulation_compliance\n",
      "Processing domain: scidb_papers\n",
      "Processing domain: temp\n",
      "Processing domain: derivatives\n",
      "Processing domain: test_batch\n",
      "\n",
      "Text extraction complete\n"
     ]
    }
   ],
   "source": [
    "from processors.text_extractor import TextExtractor\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the project root to path if necessary\n",
    "sys.path.append(\"/workspace\")\n",
    "\n",
    "# Set up paths\n",
    "corpus_dir = Path(\"/workspace/data/corpus_1\")\n",
    "extractor = TextExtractor()\n",
    "\n",
    "# Process each domain directory\n",
    "for domain_dir in corpus_dir.iterdir():\n",
    "    # Skip non-directories and extracted directories\n",
    "    if not domain_dir.is_dir() or domain_dir.name.endswith(\"_extracted\"):\n",
    "        continue\n",
    "    \n",
    "    # Create extracted directory if it doesn't exist\n",
    "    extracted_dir = corpus_dir / f\"{domain_dir.name}_extracted\"\n",
    "    os.makedirs(extracted_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"Processing domain: {domain_dir.name}\")\n",
    "    \n",
    "    # Extract text from each PDF\n",
    "    pdf_files = list(domain_dir.glob(\"*.pdf\"))\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"  Extracting text from: {pdf_file.name}\")\n",
    "        \n",
    "        # Output file path\n",
    "        output_file = extracted_dir / f\"{pdf_file.stem}.txt\"\n",
    "        \n",
    "        # Skip if already processed\n",
    "        if output_file.exists():\n",
    "            print(f\"    Already processed, skipping\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Extract text\n",
    "            result = extractor.extract(pdf_file)\n",
    "            \n",
    "            if result and 'text' in result:\n",
    "                # Write text to file\n",
    "                with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(result[\"text\"])\n",
    "                \n",
    "                print(f\"    Saved extracted text to: {output_file}\")\n",
    "            else:\n",
    "                print(f\"    No text extracted\")\n",
    "        except Exception as e:\n",
    "            print(f\"    Error extracting text: {e}\")\n",
    "\n",
    "print(\"\\nText extraction complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e59bd6-e312-4960-8ad9-d4480091dafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain balance analysis and plan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "93d06e9f-a4a0-4497-b1e7-177fc269abc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain Balance Analysis:\n",
      "Domain                         Current    Target     Status         \n",
      "-----------------------------------------------------------------\n",
      "crypto_derivatives             5          240        Needs work     \n",
      "high_frequency_trading         1          180        Needs work     \n",
      "market_microstructure          0          180        Not started    \n",
      "risk_management                0          180        Not started    \n",
      "decentralized_finance          0          144        Not started    \n",
      "portfolio_construction         0          120        Not started    \n",
      "valuation_models               0          96         Not started    \n",
      "regulation_compliance          0          60         Not started    \n",
      "\n",
      "Recommended Domain Focus (in order of priority):\n",
      "1. market_microstructure - 0.0% complete, need 180 more files\n",
      "   Suggested search terms:\n",
      "   - crypto market microstructure\n",
      "   - order book dynamics\n",
      "   - liquidity provision blockchain\n",
      "1. risk_management - 0.0% complete, need 180 more files\n",
      "   Suggested search terms:\n",
      "   - cryptocurrency risk models\n",
      "   - crypto portfolio hedging\n",
      "   - defi risk management\n",
      "1. decentralized_finance - 0.0% complete, need 144 more files\n",
      "   Suggested search terms:\n",
      "   - defi protocols\n",
      "   - automated market maker design\n",
      "   - yield optimization strategies\n"
     ]
    }
   ],
   "source": [
    "from config.domain_config import DOMAINS\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Load corpus stats\n",
    "corpus_dir = Path(\"/workspace/data/corpus_1\")\n",
    "stats_file = corpus_dir / \"corpus_stats.json\"\n",
    "\n",
    "if stats_file.exists():\n",
    "    with open(stats_file, \"r\") as f:\n",
    "        corpus_stats = json.load(f)\n",
    "else:\n",
    "    # Create empty stats if file doesn't exist\n",
    "    corpus_stats = {\"domains\": {}, \"total_files\": 0, \"total_size_mb\": 0}\n",
    "\n",
    "# Check domain allocations\n",
    "print(\"Domain Balance Analysis:\")\n",
    "print(f\"{'Domain':<30} {'Current':<10} {'Target':<10} {'Status':<15}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "# Calculate current total files\n",
    "total_files = corpus_stats.get(\"total_files\", 0)\n",
    "target_total = 1200  # From the original allocation\n",
    "\n",
    "domain_status = []\n",
    "for domain, config in DOMAINS.items():\n",
    "    # Get allocation percentage\n",
    "    allocation = config.get(\"allocation\", 0)\n",
    "    \n",
    "    # Calculate target files\n",
    "    target_files = int(allocation * target_total)\n",
    "    \n",
    "    # Get current files\n",
    "    current_files = corpus_stats.get(\"domains\", {}).get(domain, {}).get(\"pdf_files\", 0)\n",
    "    \n",
    "    # Determine status\n",
    "    if current_files == 0:\n",
    "        status = \"Not started\"\n",
    "    elif current_files < target_files * 0.25:\n",
    "        status = \"Needs work\"\n",
    "    elif current_files < target_files * 0.75:\n",
    "        status = \"In progress\"\n",
    "    elif current_files < target_files:\n",
    "        status = \"Nearly done\"\n",
    "    else:\n",
    "        status = \"Complete\"\n",
    "    \n",
    "    # Print status\n",
    "    print(f\"{domain:<30} {current_files:<10} {target_files:<10} {status:<15}\")\n",
    "    \n",
    "    # Add to status list\n",
    "    domain_status.append({\n",
    "        \"domain\": domain,\n",
    "        \"current_files\": current_files,\n",
    "        \"target_files\": target_files,\n",
    "        \"status\": status,\n",
    "        \"percentage\": round(current_files / target_files * 100 if target_files > 0 else 0, 2)\n",
    "    })\n",
    "\n",
    "# Sort domains by percentage complete\n",
    "domain_status.sort(key=lambda x: x[\"percentage\"])\n",
    "\n",
    "print(\"\\nRecommended Domain Focus (in order of priority):\")\n",
    "for domain in domain_status[:3]:\n",
    "    print(f\"1. {domain['domain']} - {domain['percentage']}% complete, need {domain['target_files'] - domain['current_files']} more files\")\n",
    "    # List search terms\n",
    "    search_terms = DOMAINS.get(domain[\"domain\"], {}).get(\"search_terms\", [])\n",
    "    if search_terms:\n",
    "        print(\"   Suggested search terms:\")\n",
    "        for term in search_terms[:3]:\n",
    "            print(f\"   - {term}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdc0dfc-e912-43d6-9617-8f55ff3edc69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5fb3acc-b54a-4774-a1ec-8291917dbba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /workspace\n",
      "Exists: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"Exists:\", os.path.exists(\"./data/corpus_1/crypto_derivatives\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2332acad-18eb-4627-ae48-388b790607ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found CookieAuthClient.py at: /workspace/data scraper scripts/CookieAuthClient.py\n",
      "Set aa_account_id2 authentication cookie\n",
      "✅ Successfully authenticated with account cookie\n",
      "Client initialized\n",
      "\n",
      "Searching for: Mastering Bitcoin Unlocking Digital Cryptocurrencies Antonopoulos\n",
      "Searching for: 'Mastering Bitcoin Unlocking Digital Cryptocurrencies Antonopoulos'\n",
      "Searching for PDFs...\n",
      "Found 1 PDF results in initial search\n",
      "Found 1 results\n",
      "1. English [en], .pdf, 🚀/lgli/lgrs/nexusstc/zlib, 11.1MB, 📘 Book (non-fiction), nexusstc/Mastering bitcoin: unlocking digital cryptocurrencies/0315a333ab8df0d9b71dfe56f50dd431.pdf\n",
      "Mastering Bitcoin : unlocking digital crypto-currencies\n",
      "O'Reilly Media, Incorporated, 1, 2015\n",
      "Andreas M. Antonopoulos\n",
      "base score: 11065.0, final score: 1.6761823 - 11.1MB\n",
      "Selecting best result from 1 options...\n",
      "\n",
      "Top results by quality score:\n",
      "Result #1 (Score: 35):\n",
      "  Title: English [en], .pdf, 🚀/lgli/lgrs/nexusstc/zlib, 11.1MB, 📘 Book (non-fiction), nexusstc/Mastering bitcoin: unlocking digital cryptocurrencies/0315a333ab8df0d9b71dfe56f50dd431.pdf\n",
      "Mastering Bitcoin : unlocking digital crypto-currencies\n",
      "O'Reilly Media, Incorporated, 1, 2015\n",
      "Andreas M. Antonopoulos\n",
      "base score: 11065.0, final score: 1.6761823\n",
      "  Size: 11.1 MB\n",
      "  Info: English [en], .pdf, 🚀/lgli/lgrs/nexusstc/zlib, 11.1MB, 📘 Book (non-fiction), nexusstc/Mastering bitcoin: unlocking digital cryptocurrencies/0315a333ab8df0d9b71dfe56f50dd431.pdf\n",
      "\n",
      "Selected best result: English [en], .pdf, 🚀/lgli/lgrs/nexusstc/zlib, 11.1MB, 📘 Book (non-fiction), nexusstc/Mastering bitcoin: unlocking digital cryptocurrencies/0315a333ab8df0d9b71dfe56f50dd431.pdf\n",
      "Mastering Bitcoin : unlocking digital crypto-currencies\n",
      "O'Reilly Media, Incorporated, 1, 2015\n",
      "Andreas M. Antonopoulos\n",
      "base score: 11065.0, final score: 1.6761823\n",
      "Selected for download: English [en], .pdf, 🚀/lgli/lgrs/nexusstc/zlib, 11.1MB, 📘 Book (non-fiction), nexusstc/Mastering bitcoin: unlocking digital cryptocurrencies/0315a333ab8df0d9b71dfe56f50dd431.pdf\n",
      "Mastering Bitcoin : unlocking digital crypto-currencies\n",
      "O'Reilly Media, Incorporated, 1, 2015\n",
      "Andreas M. Antonopoulos\n",
      "base score: 11065.0, final score: 1.6761823 (11.1MB)\n",
      "Visiting details page: https://annas-archive.org/md5/0315a333ab8df0d9b71dfe56f50dd431\n",
      "Downloading PDF from: https://annas-archive.org/fast_download/0315a333ab8df0d9b71dfe56f50dd431/0/0\n",
      "❌ Hit membership wall - authentication may have failed\n",
      "Download failed\n",
      "\n",
      "Searching for: Mastering Bitcoin Antonopoulos\n",
      "Searching for: 'Mastering Bitcoin Antonopoulos'\n",
      "Searching for PDFs...\n",
      "Found 1 PDF results in initial search\n",
      "Found 1 results\n",
      "1. English [en], Hungarian [hu], Japanese [ja], .pdf, 🚀/lgli/lgrs/nexusstc/zlib, 11.5MB, 📘 Book (non-fiction), nexusstc/Mastering Bitcoin Open Edition/54624a9ae3a0d0673fe2e92bdb0bca29.pdf\n",
      "Mastering Bitcoin Open Edition\n",
      "Transifex, O'Reilly Media, Sebastopol, 2014\n",
      "Andreas M. Antonopoulos\n",
      "base score: 11065.0, final score: 1.675955 - 11.5MB\n",
      "Selecting best result from 1 options...\n",
      "\n",
      "Top results by quality score:\n",
      "Result #1 (Score: 35):\n",
      "  Title: English [en], Hungarian [hu], Japanese [ja], .pdf, 🚀/lgli/lgrs/nexusstc/zlib, 11.5MB, 📘 Book (non-fiction), nexusstc/Mastering Bitcoin Open Edition/54624a9ae3a0d0673fe2e92bdb0bca29.pdf\n",
      "Mastering Bitcoin Open Edition\n",
      "Transifex, O'Reilly Media, Sebastopol, 2014\n",
      "Andreas M. Antonopoulos\n",
      "base score: 11065.0, final score: 1.675955\n",
      "  Size: 11.5 MB\n",
      "  Info: English [en], Hungarian [hu], Japanese [ja], .pdf, 🚀/lgli/lgrs/nexusstc/zlib, 11.5MB, 📘 Book (non-fiction), nexusstc/Mastering Bitcoin Open Edition/54624a9ae3a0d0673fe2e92bdb0bca29.pdf\n",
      "\n",
      "Selected best result: English [en], Hungarian [hu], Japanese [ja], .pdf, 🚀/lgli/lgrs/nexusstc/zlib, 11.5MB, 📘 Book (non-fiction), nexusstc/Mastering Bitcoin Open Edition/54624a9ae3a0d0673fe2e92bdb0bca29.pdf\n",
      "Mastering Bitcoin Open Edition\n",
      "Transifex, O'Reilly Media, Sebastopol, 2014\n",
      "Andreas M. Antonopoulos\n",
      "base score: 11065.0, final score: 1.675955\n",
      "Selected for download: English [en], Hungarian [hu], Japanese [ja], .pdf, 🚀/lgli/lgrs/nexusstc/zlib, 11.5MB, 📘 Book (non-fiction), nexusstc/Mastering Bitcoin Open Edition/54624a9ae3a0d0673fe2e92bdb0bca29.pdf\n",
      "Mastering Bitcoin Open Edition\n",
      "Transifex, O'Reilly Media, Sebastopol, 2014\n",
      "Andreas M. Antonopoulos\n",
      "base score: 11065.0, final score: 1.675955 (11.5MB)\n",
      "Visiting details page: https://annas-archive.org/md5/54624a9ae3a0d0673fe2e92bdb0bca29\n",
      "Downloading PDF from: https://annas-archive.org/fast_download/54624a9ae3a0d0673fe2e92bdb0bca29/0/0\n",
      "❌ Hit membership wall - authentication may have failed\n",
      "Download failed\n",
      "\n",
      "Reminder: Update your metadata to reference the new PDF and .txt files, and restore any priority tags if needed.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project directories to path\n",
    "current_dir = os.getcwd()\n",
    "sys.path.append(current_dir)\n",
    "\n",
    "# Locate CookieAuthClient.py\n",
    "client_paths = []\n",
    "for root, dirs, files in os.walk(current_dir):\n",
    "    if 'CookieAuthClient.py' in files:\n",
    "        client_paths.append(os.path.join(root, 'CookieAuthClient.py'))\n",
    "        sys.path.append(root)\n",
    "\n",
    "if client_paths:\n",
    "    print(f\"Found CookieAuthClient.py at: {client_paths[0]}\")\n",
    "else:\n",
    "    print(\"Could not find CookieAuthClient.py. Please check your project structure.\")\n",
    "    exit(1)\n",
    "\n",
    "from CookieAuthClient import CookieAuthClient\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables for Anna's Archive cookie\n",
    "load_dotenv('/workspace/notebooks/.env')  # Adjust path if needed\n",
    "account_cookie = os.getenv(\"AA_ACCOUNT_COOKIE\")\n",
    "\n",
    "if not account_cookie:\n",
    "    print(\"Error: AA_ACCOUNT_COOKIE not found in .env file\")\n",
    "    exit(1)\n",
    "\n",
    "client = CookieAuthClient(download_dir=\"./data/corpus_1/crypto_derivatives\", account_cookie=account_cookie)\n",
    "print(\"Client initialized\")\n",
    "\n",
    "# Canonical file names\n",
    "canonical_files = [\n",
    "    {\n",
    "        \"query\": \"Mastering Bitcoin Unlocking Digital Cryptocurrencies Antonopoulos\",\n",
    "        \"pdf_name\": \"crypto_derivatives_Mastering_Bitcoin_-_Unlocking_Digital_Cryptocurrencies_by_Andreas_Antonopoulos_2017.pdf\",\n",
    "        \"txt_name\": \"crypto_derivatives_Mastering_Bitcoin_-_Unlocking_Digital_Cryptocurrencies_by_Andreas_Antonopoulos_2017.txt\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Mastering Bitcoin Antonopoulos\",\n",
    "        \"pdf_name\": \"crypto_derivatives_Mastering_Bitcoin_antonopoulos.pdf\",\n",
    "        \"txt_name\": \"crypto_derivatives_Mastering_Bitcoin_antonopoulos.txt\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for book in canonical_files:\n",
    "    print(f\"\\nSearching for: {book['query']}\")\n",
    "    results = client.search(book['query'])\n",
    "    print(f\"Found {len(results)} results\")\n",
    "    for i, result in enumerate(results[:5]):\n",
    "        print(f\"{i+1}. {result.get('title', 'Unknown Title')} - {result.get('file_size', 'Unknown Size')}\")\n",
    "\n",
    "    if results:\n",
    "        best_result = client.select_best_result(results)\n",
    "        if best_result:\n",
    "            print(f\"Selected for download: {best_result.get('title')} ({best_result.get('file_size')})\")\n",
    "            # Download to a temp file, then move/rename to canonical name\n",
    "            temp_pdf = client.download_file(best_result.get('md5'))\n",
    "            if temp_pdf:\n",
    "                canonical_pdf_path = Path(\"./data/corpus_1/crypto_derivatives\") / book['pdf_name']\n",
    "                os.replace(temp_pdf, canonical_pdf_path)\n",
    "                print(f\"PDF saved to: {canonical_pdf_path}\")\n",
    "\n",
    "                # Extract text\n",
    "                try:\n",
    "                    from processors.text_extractor import TextExtractor\n",
    "                    extractor = TextExtractor()\n",
    "                    extracted = extractor.extract(str(canonical_pdf_path))\n",
    "                    if extracted and 'text' in extracted:\n",
    "                        extracted_dir = Path(\"./data/corpus_1/crypto_derivatives_extracted\")\n",
    "                        extracted_dir.mkdir(parents=True, exist_ok=True)\n",
    "                        extracted_path = extracted_dir / book['txt_name']\n",
    "                        with open(extracted_path, 'w', encoding='utf-8') as f:\n",
    "                            f.write(extracted['text'])\n",
    "                        print(f\"Extracted text saved to: {extracted_path}\")\n",
    "                        print(f\"Tokens: {len(extracted['text'].split())}\")\n",
    "                    else:\n",
    "                        print(\"Text extraction failed\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in extraction: {e}\")\n",
    "            else:\n",
    "                print(\"Download failed\")\n",
    "        else:\n",
    "            print(\"Could not select best result\")\n",
    "    else:\n",
    "        print(\"No results found for this book\")\n",
    "\n",
    "print(\"\\nReminder: Update your metadata to reference the new PDF and .txt files, and restore any priority tags if needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4c9cce-3330-4be0-b88e-b59372313b2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48516998-b691-460b-b5a7-3fc0246b678b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
