T H E T E C H N I C A L I N C E R T O

STATISTICAL
CONSEQUENCES OF FAT TAILS

Real World Preasymptotics, Epistemology, and Applications

Nassim Nicholas Taleb

ii

This format is based on André Miede’s ClassicThesis, with adaptation from Ars Classica.

The Statistical Consequences of Fat Tails (Technical Incerto Collection) c⃝ Nassim Nicholas
Taleb, 2019

iii

COAUTHORS 1

Pasquale Cirillo (Chapters 10, 12, and 13 )
Raphael Douady (Chapter 11)
Andrea Fontanari (Chapter 10)
Hélyette Geman ( Chapter 22)
Donald Geman (Chapter 22)
Espen Haug (Chapter 20 )

1 Papers relied upon here are [33, 34, 35, 69, 76, 89, 111, 154, 155, 158, 159, 160, 162, 163, 164, 165, 171, 172]

C O N T E N T S

Nontechnical chapters are indicated with a star *; Discussion chapters are indicated with a y; adaptation from published
("peer-reviewed") papers with a z.

1 prologue (cid:3),†

1

i
2

5

fat tails and their effects, an introduction
a non-technical overview - the darwin college lecture (cid:3),‡
2.1 On the Difference Between Thin and Fat Tails
2.2 A (More Advanced) Categorization and Its Consequences
2.3 The Main Consequences
16

Forecasting

2.3.1
2.3.2 Ebola cannot be compared to falls from ladders
2.3.3 The Law of Large Numbers
2.3.4 Epistemology and Inferential Asymmetry

12

18

19

7

16

2.4 Primer on Power Laws (without mathematics)
2.5 Where are the hidden properties?
2.6 Bayesian Schmayesian
27
2.7 X vs F(X), exposures to X confused with knowledge about X
2.8 Ruin and Path Dependence
2.9 What to do?

32

29

24

20

3 overview of fat tails, part i, the univariate case †

3.1 Level 1: Fat Tails, but Finite Moments

35

3.1.1 A Simple Heuristic to Create Mildly Fat Tails
3.1.2 A Variance-preserving heuristic
3.1.3

Fattening of Tails With Skewed Variance
3.2 Does Stochastic Volatility Generate Power Laws?
3.3 The Body, The Shoulders, and The Tails

40

37

3.3.1 The Crossovers and Tunnel Effect.
41
3.4 Fat Tails, Mean Deviation and the Rising Norms

37

39

44

44

Some Analytics

3.4.1 The common errors
3.4.2
3.4.3 Effect of Fatter Tails on the "efﬁciency" of STD vs MD
3.4.4 Moments and The Power Mean Inequality
3.4.5 Comment: Why We Should Retire Standard Deviation
50

45

47

3.5 Level 2: Subexponentiality

35

35

3.5.1 Revisiting the Rankings
3.5.2 What is a probability distribution?
53
3.5.3 Let us invent a distribution

51

3.6 Level 3: Scalability and Power Laws

54

52

3.6.1

Scalable and Nonscalable, A Deeper View of Fat Tails

7

9

27

47

49

54

v

vi

Contents

3.6.2 Grey Swans

55

3.7 Bell shaped vs non Bell shaped power laws

57

4 overview of fat tails, part 2 (higher dimensions) †
4.1 Fat Tails in Higher Dimension, Finite Moments
59
4.2

Joint Fat-Tailedness and Ellipticality of Distributions
4.2.1 Ellipticality and Independence for Fat Tails

59

60

63

4.3 Fat tails and random matrices, a rapid interlude
4.4 Multivariate scale
4.5 Correlation and Undeﬁned Variance

65

65

65

Finiteness of Correlation

4.5.1
4.5.2 Example of ﬁniteness of Correlation
5
the empirical distribution is not empirical
a econometrics imagines functions in l2 space †

65

66
69

73

a.0.1 Performance of Standard Parametric Risk Estimators
a.0.2 Performance of Standard NonParametric Risk Estimators

73

76

b

79

special cases of fat tails
b.1 Multimodality and Fat Tails, or the War and Peace Model
b.2 Transition probabilites: what can break will break
82
85

c pseudo-stochastic volatility: a case study
d case study: how the myopic loss aversion is misspecified
e
f machine learning considerations

the large deviation principle, in brief
93

91

80

87

f.0.1 Calibration via angles

95

ii
6

the law of medium numbers
limit distributions, a consolidation (cid:3),†
6.1 Central limit in action

99

97

99

6.1.1
Fast convergence: the uniform dist.
6.1.2
Semi-slow convergence: the exponential
6.1.3 The slow Pareto
102
6.1.4 The half-cubic Pareto and its basin of convergence

101

101

103

6.2 Cumulants and convergence
6.3 The law of large numbers
6.4 The Law of Large Numbers for higher moments
109
6.5 Mean deviation for a Stable Distributions

104

106

106

7 how much data do you need? an operational metric for fat-tailedness ‡
113

Introduction and Deﬁnitions

7.1
7.2 The Metric
7.3 Stable Basin of Convergence as Benchmark
116
7.3.1 Equivalence for stable distributions
117
7.3.2 Practical signiﬁcance for sample sufﬁciency

114

7.4 Technical Consequences

118

117

Some oddities with asymmetric distributions

7.4.1
7.4.2 Rate of convergence of a student T distribution to the Gaussian Basin
7.4.3 The lognormal is neither thin nor fat tailed
7.4.4 Can kappa be negative?
7.5 Conclusion and Consequences

118

119

119
119
120
7.5.1 Portfolio pseudo-stabilization
7.5.2 Other aspects of statistical inference
7.5.3

Final comment

121

121

111

119

7.5.4 Cubic Student T (Gaussian Basin)
7.5.5 Lognormal Sums
7.5.6 Exponential
7.5.7 Negative kappa, negative kurtosis

123

125

Contents

vii

121

126

8 diagnostic tools for fat tails. with application to the sp500 †

127

127

Introduction

8.1
8.2 Methods 1 through 3
8.3 The law of large numbers under Paretianity
8.4 Distribution of the tail exponent
8.5 Dependence and Asymmetries

128

134

135

8.5.1 Records and Extrema

135

8.6 Some properties and tests

136
8.6.1 Asymmetry right-left tail
8.6.2 Paretianity and moments

8.7 Convergence Tests

137

136
136

132

8.7.1 Test 1: Kurtosis under Aggregation
8.7.2 Test 2: Excess Conditional Expectation
8.7.3 Test 3- Instability of 4th moment
142
8.7.4 Test 4: MS Plot

142

137

138

8.8 Conclusion

144

iii
9

issues with fat tails
expert calibration under fat tails

145

147

9.0.1 Background: the robustness of metrics
9.1 The Brier Score follows a thin tailed distribution
9.2 The P/L follows a different distribution

151

148

148

iv inequality estimators
10 gini estimation under infinite variance ‡
155

153

155

10.1 Introduction
10.2 Asymptotics of the nonparametric estimator under inﬁnite variance

158

10.2.1 A quick recap on a-stable random variables
10.2.2 The a-stable asymptotic limit of the Gini index

159

160

10.3 The maximum likelihood estimator
10.4 A Paretian illustration
10.5 Small sample correction
10.6 Conclusions

164

162

166

161

11 on the super-additivity and estimation biases of quantile contributions

175

‡
11.1 Introduction
11.2 Estimation For Unmixed Pareto-Tailed Distributions

175

11.2.1 Bias and Convergence

177

11.3 An Inequality About Aggregating Inequality
11.4 Mixed Distributions For The Tail Exponent
11.5 A Larger Total Sum is Accompanied by Increases in bkq
11.6 Conclusion and Proper Estimation of Concentration
11.6.1 Robust methods and use of exhaustive data
11.6.2 How Should We Measure Concentration?

180

182

185

184
185

177

184

viii

Contents

shadow moments papers

v
12 on the shadow moments of apparently infinite-mean phenomena ( with

187

189

189

p. cirillo) ‡
12.1 Introduction
12.2 The dual distribution
12.3 Back to Y: the shadow mean (or population mean)
12.4 Comparison to other methods
12.5 Applications

190

193

195

191

13 on the tail risk of violent conflict and its underestimation (with p. cirillo) ‡

197

197
13.1 Introduction/Summary
13.2 Summary statistical discussion

200

13.2.1 Results
13.2.2 Conclusion

200

200

13.3 Methodological Discussion

201

202

13.3.1 Rescaling method
13.3.2 Expectation by Conditioning (less rigorous)
13.3.3 Reliability of data and effect on tail estimates
13.3.4 Deﬁnition of an "event"
13.3.5 Missing events
13.3.6 Survivorship Bias

205

205

204

13.4 Data analysis

205

206

13.4.1 Peaks over Threshold
13.4.2 Gaps in Series and Autocorrelation
13.4.3 Tail analysis
13.4.4 An alternative view on maxima
13.4.5 Full Data Analysis

208

210

209

207

13.5 Additional robustness and reliability tests
13.5.1 Bootstrap for the GPD 211
13.5.2 Perturbation across bounds of estimates
13.6 Conclusion: is the world more unsafe than it seems?

211

211

g what are the chances of a third world war? (cid:3),†

215

203

203

212

vi metaprobability papers
219
14 how fat tails emerge from recursive epistemic uncertainty †

221

14.1 Methods and Derivations

221

14.1.1 Layering Uncertainties
14.1.2 Higher order integrals in the Standard Gaussian Case
14.1.3 Effect on Small Probabilities

227

221

222

14.2 Regime 2: Cases of decaying parameters a( n)

228

14.2.1 Regime 2-a;“bleed” of higher order error
14.2.2 Regime 2-b; Second Method, a Non Multiplicative Error Rate

228

229

15 stochastic tail exponent for asymmetric power laws †

231

15.1 Background
15.2 One Tailed Distributions with Stochastic Alpha

231

232

15.2.1 General Cases
15.2.2 Stochastic Alpha Inequality
15.2.3 Approximations for the Class P 234

232

232

15.3 Sums of Power Laws
15.4 Asymmetric Stable Distributions
15.5 Pareto Distribution with lognormally distributed a
15.6 Pareto Distribution with Gamma distributed Alpha

234

235

236
237

15.7 The bounded Power Law in Cirillo and Taleb (2016)
15.8 Additional Comments
15.9 Acknowledgments

238

238

Contents

ix

237

vii tails for bounded random variables
16 the meta-distribution of standard p-values ‡

239

241

16.1 Proofs and derivations
16.2 Inverse Power of Test
247
16.3 Application and Conclusion

242

247

17 election predictions as martingales: an arbitrage approach ‡

249

17.0.1 Main results
17.0.2 Organization
17.0.3 A Discussion on Risk Neutrality

251
252

253

17.1 The Bachelier-Style valuation
17.2 Bounded Dual Martingale Process
17.3 Relation to De Finetti’s Probability Assessor
17.4 Conclusion and Comments
259
17.5 Acknowledgements

258

255

253

257

viii option trading and pricing under fat tails
18 financial theory’s failures in option pricing
265
18.0.1 The actual replication process:
18.0.2 Failure:How hedging errors can be prohibitive.

261
263

265

19 unique option pricing measure with neither dynamic hedging nor com-

plete markets ‡
19.1 Background
268
19.2 Proof

267
267

19.2.1 Case 1: Forward as risk-neutral measure
270
19.2.2 Derivations

268

19.3 Case where the Forward is not risk neutral
19.4 comment

272

272

20 option traders never use the black-scholes-merton formula (cid:3),‡

275

20.1 Breaking the Chain of Transmission
20.2 Black-Scholes was an argument
20.3 Myth 1: Traders did not "price" options before Black-Scholes

276

275

278

21 four lessons from jeff holman’s mistakes in the discussion of antifragile (cid:3),‡

281

21.1 Conﬂation of Second and Fourth Moments
21.2 Missing Jensen’s Inequality in Analyzing Option Returns
21.3 The Inseparability of Insurance and Insured
21.4 The Necessity of a Numéraire in Finance
21.5 Appendix (Betting on Tails of Distribution)

283

281

283

285

282

22 tail risk constraints and maximum entropy (with d. geman and h. geman) ‡

287

22.1 Left Tail Risk as the Central Portfolio Constraint
289
22.1.1 The Barbell as seen by E.T. Jaynes

22.2 Revisiting the Mean Variance Setting
22.2.1 Analyzing the Constraints

22.3 Revisiting the Gaussian Case

291
22.3.1 A Mixture of Two Normals

290

292

290

22.4 Maximum Entropy

293
22.4.1 Case A: Constraining the Global Mean
22.4.2 Case B: Constraining the Absolute Mean

287

293

295

x

Contents

22.4.3 Case C: Power Laws for the Right Tail
22.4.4 Extension to a Multi-Period Setting: A Comment

295

22.5 Comments and Conclusion

298

297

ix

bibliography and index

301

1 P R O L O G U E (cid:3),†

The less you understand the world, the easier it is
to make a decision.

Figure 1.1: The problem is not awareness of "fat tails", but the lack of understanding of their consequences.
Saying "it is fat tailed" implies much more than changing the name of the distribution, but a general overhaul
of the statistical tools and types of decisions made.

The main idea behind the Incerto project is that while there is a lot of uncertainty and
opacity about the world, and an incompleteness of information and understanding,
there is little, if any, uncertainty about what actions should be taken based on such
an incompleteness, in any given situation.

This book consists in 1) published papers and 2) (uncensored) commentary,

about classes of statistical distributions that deliver extreme events, and how
we should deal with them for both statistical inference and decision making.
Most "standard" statistics come from theorems designed for thin tails: they
need to be adapted preasymptotically to fat tails, which is not trivial –or

abandoned altogether.

So many times this author has been told "of course we know this" or the beastly "nothing
new" about fat tails by a professor or practitioner who just produced an analysis using
"variance", "GARCH", "kurtosis" , "Sharpe ratio", or "value at risk", or produced some
"statistical signiﬁcance" that is clearly not signiﬁcant.

1

2

prologue (cid:3),†

Figure 1.2:
Complication
without insight: the clarity of
mind of many professionals us-
ing statistics and data science
without an understanding of
the core concepts, what it is
fundamentally about.

Credit: Wikimedia.

More generally, this book draws on the author’s multi-volume series, Incerto [157] and
associated technical research program, which is about how to live in the real world, a
world with a structure of uncertainty that is too complicated for us.

The Incerto tries to connects ﬁve different ﬁelds related to tail probabilities and extremes:
mathematics, philosophy, social science, contract theory, decision theory, and the real
world. If you wonder why contract theory, the answer is: option theory is based on the
notion of contingent and probabilistic contracts designed to modify and share classes of
exposures in the tails of the distribution; in a way option theory is mathematical contract
theory. Decision theory is not about understanding the world, but getting out of trouble
and ensuring survival. This point is the subject of the next volume of the Technical Incerto,
with the temporary working title Convexity, Risk, and Fragility.

I presented this book and the main points at the Bloomberg Quant Conference in
New York in September 2018. After the lecture, a prominent mathematical ﬁnance
professor came to see me. "This is very typical Taleb", he said. "You show what’s
wrong but don’t offer too many substitutes".

Clearly, in business or in anything subjected to the rigors of the real world, he
would have been terminated. People who never had any skin in the game [167] can-
not ﬁgure out the necessity of suspension of belief: don’t give a pilot a faulty metric, learn
to only provide reliable information. Nor can they get the outperformance of via negativa
–Popperian science works by removal.The late David Freedman had tried unsuc-
cessfully to tame vapid and misleading statistical modeling outperformed by "nothing".

But it is the case that the various chapters and papers here do offer solutions
and alternatives, except that these aren’t the most comfortable for some.

prologue (cid:3),†

3

a word on terminology

"Fat tails" is often used in many contexts. In ours, here, it is simply "higher kurtosis than
the Gaussian" –to conform to the ﬁnance practitioner’s lingo. For many it is meant to be a
narrower deﬁnition, limited to "power laws" or "regular variations" –what we call "fat tails"
may be "thick tails" for many. The next chapter will clarify.

acknowledgments

In additions to coauthors mentioned earlier: Jean-Philippe Bouchaud, Robert Frey, Spyros
Makridakis, etc.

Part I

FAT TA I L S A N D T H E I R E F F E C T S , A N I N T R O D U C T I O N

2 A N O N -T E C H N I C A L O V E R V I E W - T H E

D A R W I N C O L L E G E L E C T U R E (cid:3),‡

1

This chapter

presents a nontechnical yet comprehensive presentation of of the
entire statistical consequences of fat tails project. It compresses the main ideas
in one place. Mostly, it provides a list of more than a dozen consequences
of fat tails on statistical inference.

2.1 on the difference between thin and fat tails

We begin with the notion of fat tails and how it relates to extremes using the two imaginary
domains of Mediocristan (thin tails) and Extremistan (fat tails).

(cid:15) In Mediocristan, no observation can really change the statistical properties.
(cid:15) In Extremistan, the tails (the rare events) play a disproportionately large role in

determining the properties.

Let us randomly select two people in Mediocristan with a (very unlikely) combined
height of 4.1 meters a tail event. According to the Gaussian distribution (or, rather its
one-tailed siblings), the most likely combination of the two heights is 2.05 metres and 2.05
metres.
Simply, the probability of exceeding 3 sigmas is 0.00135. The probability of exceeding 6
(cid:0)10. The probability of two 3-sigma events occurring
sigmas, twice as much, is 9.86 (cid:2) 10
(cid:0)6. Therefore the probability of two 3-sigma events occurring is considerably
is 1.8 (cid:2) 10
higher than the probability of one single 6-sigma event. This is using a class of distribu-
tion that is not fat-tailed. Figure 2.1 below shows that as we extend the ratio from the
probability of two 3-sigma events divided by the probability of a 6-sigma event, to the
probability of two 4-sigma events divided by the probability of an 8-sigma event, i.e., the
further we go into the tail, we see that a large deviation can only occur via a combination
(a sum) of a large number of intermediate deviations: the right side of Figure 2.1. In other
words, for something bad to happen, it needs to come from a series of very unlikely events,
not a single one. This is the logic of Mediocristan.

1 A shorter version of this chapter was given at Darwin College, Cambridge (UK) on January 27 2017, as part of
the Darwin College Lecture Series on Extremes. The author extends the warmest thanks to D.J. Needham and
Julius Weitzdorfer, as well as their invisible assistants who patiently and accurately transcribed the ideas into a
coherent text. The author is also grateful towards Ole Peters who corrected some mistakes.

7

8

a non-technical overview - the darwin college lecture (cid:3),‡

Let us now move to Extremistan and randomly select two people with combined wealth
of $ 36 million. The most likely combination is not $18 million and $ 18 million.
It is
approximately $ 35,999,000 and $ 1,000.
This highlights the crisp distinction between the two domains; for the class of subexpo-
nential distributions, ruin is more likely to come from a single extreme event than from a
series of bad episodes. This logic underpins classical risk theory as outlined by Lundberg
early in the 20th Century [105] and formalized by Cramer[38], but forgotten by economists
in recent times. This indicates that insurance can only work in Mediocristan; you should
never write an uncapped insurance contract if there is a risk of catastrophe. The point is
called the catastrophe principle.

As I said earlier, with fat tail distributions, extreme events away from the centre of the
distribution play a very large role. Black Swans are not more frequent, they are more
consequential. The fattest tail distribution has just one very large extreme deviation, rather
than many departures form the norm. Figure 3.4 shows that if we take a distribution like
the Gaussian and start fattening it, then the number of departures away from one standard
deviation drops. The probability of an event staying within one standard deviation of the
mean is 68 per cent. As the tails fatten, to mimic what happens in ﬁnancial markets for
example, the probability of an event staying within one standard deviation of the mean
rises to between 75 and 95 per cent. So note that as we fatten the tails we get higher peaks,
smaller shoulders, and a higher incidence of a very large deviation.

Iso-densities

Figure 2.1:
for
two independent Gaussian dis-
tributions. The line shows x +
y = 4.1. Visibly the maximal
probability is for x = y = 2.05.

-2024-20242.2 a (more advanced) categorization and its consequences

9

Iso-densities

Figure 2.2:
for
two independent Fat tailed dis-
tributions (in the power law
class). The line shows x + y =
36. Visibly the maximal proba-
bility is for either x = 36 (cid:0) ϵ or
y = 36 (cid:0) ϵ, with ϵ going to 0
as the sum x + y becomes larger,
with the iso-densities looking
more and more like a cross.

Figure 2.3: Ratio of two oc-
currences of size K by one of
2K for a Gaussian distribution.
The larger the K, that is, the
more we are in the tails, the
more likely the event is likely
to come from 2 independent re-
alizations of K (hence P(K)2,
and the less from a single event
of magnitude 2K. (Note: This
is fudging for pedagogical sim-
plicity. The correct approach
would be to assume the most
likely,
say S(K0) deviations,
times S(K (cid:0) K0), but no wor-
ries since S(K0) is a constant.)

2.2 a (more advanced) categorization and its consequences

First there are entry level fat tails. This is any distribution with fatter tails than the Gaus-
sian i.e. with more observations within one sigma and with kurtosis (a function of the
fourth central moment) higher than three.

Second, there are subexponential distributions satisfying our thought experiment earlier.
Unless they enter the class of power laws, these are not really fat tails because they do not
have monstrous impacts from rare events. In other words, they have all the moments.

Level three, what is called by a variety of names, the power law, or slowly varying class,

or "Pareto tails" class correspond to real fat tails.

x+y=36-2002040-20020401234K(inσ)500010000150002000025000S(K)2S(2K)10

a non-technical overview - the darwin college lecture (cid:3),‡

that

Figure 2.4: The law of large
is how long
numbers,
it takes for the sample mean
to stabilize, works much more
slowly in Extremistan (here a
Pareto distribution with 1.13
tail exponent , corresponding
to the "Pareto 80-20". Both
have the same mean absolute
deviation. Note that the same
applies to other forms of sam-
pling, such as portfolio theory.

Working from the bottom left of Figure 2.5, we have the degenerate distribution where
there is only one possible outcome i.e. no randomness and no variation. Then, above it,
there is the Bernoulli distribution which has two possible outcomes. Then above it there
are the two Gaussians. There is the natural Gaussian (with support on minus and plus
inﬁnity), and Gaussians that are reached by adding random walks (with compact support,
sort of). These are completely different animals since one can deliver inﬁnity and the other
cannot (except asymptotically). Then above the Gaussians there is the subexponential class.
Its members all have moments, but the subexponential class includes the lognormal, which
is one of the strangest things on earth because sometimes it cheats and moves up to the
top of the diagram. At low variance, it is thin-tailed, at high variance, it behaves like the
very fat tailed.

Membership in the subexponential class satisﬁes the Cramer condition of possibility of
insurance (losses are more likely to come from many events than a single one), as illus-

2.2 a (more advanced) categorization and its consequences

11

Figure 2.5: The tableau of Fat tails, along the various classiﬁcations for convergence purposes (i.e., convergence
to the law of large numbers, etc.) and gravity of inferential problems. Power Laws are in white, the rest in
yellow. See Embrechts et al [60].

trated in Figure 2.1. More technically it means that the expectation of the exponential of
the random variable exists.

2

Once we leave the yellow zone, where the law of large numbers largely works, then
we encounter convergence problems. Here we have what are called power laws, such as
Pareto laws. And then there is one called Supercubic, then there is Levy-Stable. From here
there is no variance. Further up, there is no mean. Then there is a distribution right at the
top, which I call the Fuhgetaboudit. If you see something in that category, you go home
and you dont talk about it. In the category before last, below the top (using the parameter
a, which indicates the "shape" of the tails, for a < 2 but not a (cid:20) 1), there is no variance,
but there is the mean absolute deviation as indicator of dispersion. And recall the Cramer
condition: it applies up to the second Gaussian which means you can do insurance.

The traditional statisticians approach to fat tails has been to assume a different distri-
bution but keep doing business as usual, using same metrics, tests, and statements of
signiﬁcance. But this is not how it really works and they fall into logical inconsistencies.
Once we leave the yellow zone, for which statistical techniques were designed, things no
longer work as planned. The next section presents a dozen issues, almost all terminal.

2 Technical point: Let X be a random variable. The Cramer condition: for all r > 0,

E(erX) < +¥.

DegenerateBernoulliThin-TailedfromConvergencetoGaussianCOMPACT SUPPORTSubexponential Supercubicα≤3Lévy-Stable α<2 α≤1CRAMERCONDITIONℒ1LAW OF LARGE NUMBERS (WEAK)CONVERGENCE ISSUESGaussian from Lattice ApproximationFuhgetabouditCENTRAL LIMIT — BERRY-ESSEEN12

a non-technical overview - the darwin college lecture (cid:3),‡

The problem with overstandardized statistics

Statistical estimation is based on two elements: the central limit the-

orem (which is assumed to work for "large" sums, thus making about
everything conveniently normal) and that of the law of large numbers,
which reduces the variance of the estimation as one increases the sam-
ple size. However, there are now caveats as we can see throughout this
text. In Chapter x, we show a tableau of "what large means" for the central limit the-
orem: convergence can be very, very slow –it is distribution dependent. As shown by
Bouchaud and Potters in [19] and Sornette in [149], the tails for some ﬁnite variance
but inﬁnite higher moments can converge to the Gaussian within (cid:6)
n log n, meaning
the center of the distribution inside such band becomes Gaussian, but remote parts
don’t –and the remote parts determine so much of the properties. My paper (this
Chapter 7) examines the LLN for various distributions.
Life happens in the preasymptotics.
Sadly, in the entry on estimators in the monumental Encyclopedia of Statistical Science
[101], W. Hoeffding writes:

√

"The exact distribution of a statistic is usually highly complicated and
difﬁcult to work with. Hence the need to approximate the exact distribu-
tion by a distribution of a simpler form whose properties are more trans-
parent. The limit theorems of probability theory provide an important tool
for such approximations. In particular, the classical central limit theorems
state that the sum of a large number of independent random variables is
approximately normally distributed under general conditions. In fact, the
normal distribution plays a dominating role among the possible limit dis-
tributions. To quote from Gnedenko and Kolmogorov’s text [[80], Chap.
5]:

"Whereas for the convergence of distribution functions of sums of
independent variables to the normal law only restrictions of a very
general kind, apart from that of being inﬁnitesimal (or asymptotically
constant), have to be imposed on the summands, for the convergence
to another limit law some very special properties are required of the
summands".

Moreover, many statistics behave asymptotically like sums of independent
random variables. All of this helps to explain the importance of the normal
distribution as an asymptotic distribution."

Now what if we do not reach the normal distribution, as life happens before the
asymptote? This is what this book is about.a

a The reader is invited to consult a "statistical estimation" entry in any textbook or online encyclopedia. Odds
are that the notion of "what happens if we do not reach the asymptote" will never be discussed –as in the
9500 pages of the monumental Encyclopedia of Statistics. Further, ask a regular user of statistics about how
much data one needs for such and such distributions, and don’t be surprised at the answer. The problem
is that people have too many prepackaged statistical tools in their heads, ones they never had to rederive
themselves. The motto here is: "statistics is never standard".

2.3 the main consequences

Here are some consequences of moving out of the yellow zone:

1. The law of large numbers, when it works, works too slowly in the real world.

2.3 the main consequences

13

Figure 2.6: In the presence of fat tails, we can ﬁt markedly different regression lines to the same story (the
Gauss-Markov theorem doesn’t apply anymore). Left: a regular (naïve) regression. Right: a regression line
that tries to accommodate the large deviation —a "hedge ratio" so to speak, one that protects the agent from
a large deviation, but mistracks small ones. Missing the largest deviation can be fatal. Note that the sample
doesn’t include the critical observation, but it has been guessed using "shadow mean" methods.

Figure 2.7: Inequality measures such as the Gini co-
efﬁcient require completely different methods of esti-
mation under fat tails, as we will see in Part III. Sci-
ence is hard.

(This is more shocking than you think as it cancels most statistical estimators). See
Figure 2.4 in this chapter for an illustration. The subject is treated in Chapter 7
and distributions are classiﬁed according to how fast they preasymptotically converge
according to the law of large numbers.

2. The mean of the distribution will not correspond to the sample mean, particularly

if the distribution is skewed (or one-tailed).

20406080100x-551015yHxL20406080100x-10-551015yHxL14

a non-technical overview - the darwin college lecture (cid:3),‡

In fact, there is no fat tailed distribution in which the mean can be properly estimated
directly from the sample mean, unless we have orders of magnitude more data than
we do (people in ﬁnance still do not understand this). The point is discussed in the
"shadow mean" chapters, such as Chapter 12 and Chapter 13. And we will introduce
the notion of hidden properties are in 2.5.

3. Standard deviations and variance are not useable.

They fail out of sample –even when they exist; even when all moments exist. Dis-
cussed in Chapter 3.1.

4. Beta, Sharpe Ratio and other common ﬁnancial metrics are uninformative.

(This is a simple consequence of the previous point). Either they require much more
data, many more orders of magnitude, or some different model than the one being
used, of which we are not yet aware. Further, stochastic correlations or covariances
also represent a form of fat tails, discussed in Chapter 3.1 as well.

5. Robust statistics is not robust at all

The story of my life. The so-called "empirical distribution" is not empirical (as it
misrepresents the expected payoffs in the tails). Discussed in Chapter 8.

6. Linear least-square regression doesn’t work (failure of the Gauss-Markov theo-

rem).
See ﬁgure 2.6. Either we need a lot, a lot of data to minimize the squared deviations
(in other words, the Gauss-Markov theorem applies, but not for our preasymptotic
situation as the real world has no inﬁnite date), or we can’t because the second mo-
ment does not exist. In the latter case, if we minimize absolute deviations (MAD,
mean absolute deviations), as we seen in 3.1, not only we may still be facing insufﬁ-
cient data but the deviation slope may not be unique.

7. Maximum likelihood methods work for parameters of the shape of the distribu-

tion (good news).

Take a power law. We may estimate a parameter for its shape, the tail exponent
(written a in this book), which, adding some other parameter (the scale) connects us
back to its mean.

So we can produce more reliable (or at least less unreliable) plug-in estimators for,
say, the tail exponent in some situations. But, of course, not all. Now what do we do
when we do not have a reliable estimator? We do not expose ourselves to harm in
the presence of fragility, but can still take decisions if we are bounded for maximum
losses.

8. The gap between disconﬁrmatory and conﬁrmatory empiricism is wider than in
the difference between absence of

situations covered by common statistics i.e.
evidence and evidence of absence becomes larger.

From a controversy the author had with the cognitive linguist and science writer
Steven Pinker: making pronouncement (and generating theories) from recent vari-
ations in data is not easily possible. Stating "violence has dropped" because the
number of people killed in wars has dropped is not a scientiﬁc statement: a scien-
tiﬁc claim distinguishes itself from an anecdote as it aims at affecting what happens
out of sample, by focusing on statistically signiﬁcance: non statistically signiﬁcant
statements are not the realm of science. However, saying violence has risen upon a
single observation may be a rigorously scientiﬁc claim. The practice of reading into
descriptive statistics may be acceptable under thin tails (as sample sizes do not have

2.3 the main consequences

15

to be large), but never so under fat tails, except, to repeat, in the presence of a large
deviation.

9. The principal component analysis is likely to produce false factors.

This point is a bit technical; it adapts the notion of sample insufﬁciency to large ran-
dom vectors seen via the dimension reduction technique called principal component
analysis (PCA) . The issue a higher dimensional version of our law of large number
complications. The story is best explained in Figure 2.15, which shows the accentu-
ation of what is called the "Wigner Effect", from insufﬁciency of data for the PCA.
Also, to be technical, note that the Marchenko-Pastur distribution is not applicable in
the absence of a ﬁnite fourth moment (or, has been shown in [15], for tail exponent
3
in excess of 4 using our terminology here)
.

10. The method of moments (MoM) fails to work. Higher moments are uninformative

or do not exist.

The same applies to the GMM, the generalized method of moment. This is a long
story, but take for now that the estimation of a given distribution by moment match-
ing fails if higher moments are not ﬁnite, so every sample delivers a different moment
–as we will soon see with the 4th moment of the SP500.

11. There is no such thing as a typical large deviation

Conditional on having a "large" move, the magnitude of such a move is not deﬁned,
especially under serious Fat Tails (the Power Law tails class). This is associated with
the catastrophe principle we saw earlier. In the Gaussian world, the expectation of a
movement, conditional that the movement exceeds 4 standard deviations, is about 4
standard deviations. For a Power Law it will be a multiple of that.

12. The Gini coefﬁcient ceases to be additive.

Methods of measuring sample data for Gini are interpolative –they in effect have
the same problem we saw earlier with the sample mean underestimating or overes-
timating the true mean. Here, an additional complexity arises as the Gini becomes
super-additive under fat tails. As the sampling space grows, the conventional Gini
measurements give an illusion of large concentrations of wealth.
(In other words,
inequality in a continent, say Europe, can be higher than the average inequality of its
members). The same applies to other measures of concentration such as the top 1%
has x percent of the total wealth, etc. The derivations are in Chapters 10 and 11.
13. Large Deviation Theory (Varadan [185] , Dembo and Zeituni [45], etc.) fails to apply

to fat tails. I mean, it really doesn’t apply.

14. Option risks are never mitigated by dynamic hedging. This might be technical for
nonﬁnance people but the entire basis of ﬁnance rests on the possibility and necessity
of dynamic hedging, both of which will be shown to be erroneous in Chapters 18 19
and 20. The required exponential decline require the probability distribution to be
outside the sub-exponential class. Again, we are talking about the Cramer condition.

15. Forecasting in frequency space diverges from expected payoff.

This point is explored in the next section: the foolish notion of focus on frequency
rather than expectation can carry a mild effect under thin tails; not under fat tails.

Let us discuss the major points.

3 To clear up the terminology: in this book, the tail exponent, commonly written a is the limit of quotient of the
log of the survival function in excess of K over log K, which would be 1 for Cauchy. Some researchers use a (cid:0) 1
from the corresponding density function.

16

a non-technical overview - the darwin college lecture (cid:3),‡

2.3.1 Forecasting

In Fooled by Randomness (2001/2005), the character is asked which was more probable
that a given market would go higher or lower by the end of the month. Higher, he said,
much more probable. But then it was revealed that he was making trades that beneﬁt
if that particular market goes down. This of course, appears to be paradoxical for the
nonprobabilist but very ordinary for traders, particularly under nonstandard distributions
(yes, the market is more likely to go up, but should it go down it will fall much much more).
This illustrates the common confusion between a forecast and an exposure (a forecast is a
binary outcome, an exposure has more nuanced results and depends on full distribution).
This example shows one of the extremely elementary mistakes of talking about probability
presented as single numbers not distributions of outcomes, but when we go deeper into
the subject, many less obvious, or less known paradox-style problems occur. Simply, it is
of the opinion of the author, that it is not rigorous to talk about “probability” as a ﬁnal
product, or even as a “foundation” of decisions.

In the real world one is not paid in probability, but in dollars (or in survival, etc.). The
fatter the tails, the more one needs to worry about payoff space – the saying goes: "payoff
swamps probability" (see box). One can be wrong very frequently if the cost is low, so
long as one is convex to payoff (i.e. make large gains when one is right). Further, one
can be forecasting with 99.99% accuracy and still go bust (in fact, more likely to go bust:
funds with impeccable track records were those that went bust during the 2008-2009 rout
4
). A point that may be technical for those outside quantitative ﬁnance: it is the difference
between a vanilla option and a corresponding binary of the same strike, as discussed in
Dynamic Hedging [156]: counterintuitively, fat tailedness lowers the value of the binary and
raise that of the vanilla. This is expressed by the author’s adage: "I’ve never seem a rich
forecaster." We will examine in depth in 3.3.1 where we show that fattening the tails cause
the probability of events higher than 1 standard deviations to drop –but the consequences
to rise (in term of contribution to moments, say effect on the mean or other metrics).

Remark 2.1
Probabilistic forecast errors ("calibration") are in a different probability class from that true
real-world P/L variations (or true payoffs).

"Calibration", which is a measure of how accurate one’s predictions, lies in probability space
–between 0 and 1. Any measure of such calibration will necessarily be thin-tailed (and, if
anything, extra-thin tailed since it is bounded) –whether the random variable under such
prediction is fat tailed or not. On the other hand, payoffs in the real world can be fat tailed,
hence the distribution of such "calibration" will follow the property of the random variable.

We show full derivations and proofs in Appendix 9.

2.3.2 Ebola cannot be compared to falls from ladders

Let us illustrate one of the problem of thin-tailed thinking with a real world example.
People quote so-called "empirical" data to tell us we are foolish to worry about ebola when
only two Americans died of ebola in 2016. We are told that we should worry more about
deaths from diabetes or people tangled in their bedsheets. Let us think about it in terms
of tails. But, if we were to read in the newspaper that 2 billion people have died suddenly,

4 R. Douady, data from Risk Data about funds that collapsed in the 2008 crisis, personal communication

2.3 the main consequences

17

Payoff swamps probability in Extremistan: To see the main difference be-

tween Mediocristan and Extremistan, consider the event of a plane crash.
A lot of people will lose their lives, something very sad, say between 100
and 400 people, so the event is counted as a bad episode, a single one.
For forecasting and risk management, we work on minimizing such a

probability to make it negligible.
Now, consider a type of plane crashes that will kill all the people who ever rode
the plane, even all passengers who ever rode planes in the past. All. Is it the same
type of event? The latter event is in Extremistan and, for these, we don’t talk about
probability but focus instead on the magnitude of the event.

(cid:15) For the ﬁrst type, management consists in reducing the probability –the fre-
quency – of such events. Remember that we count events and aim at reducing
their counts.

(cid:15) For the second type, it consists in reducing the effect should such an event take

place. We do not count events, we measure impact.

If you think the thought experiment is a bit weird, consider that the money center
banks lost in 1982 more money than they ever made in their history, the Savings
and Loans industry (now gone) did so in 1991, and the entire banking system lost
every penny ever made in 2008-9. One can routinely witness people lose everything
they earned cumulatively in a single market event. The same applies to many, many
industries (e.g. automakers and airlines).
But banks are only about money; consider that for wars we cannot afford the naive
focus on event frequency without taking into account the magnitude, as done by the
science writer Steven Pinker in [134], discussed in Chapter 13. This is without even
examining the ruin problems (and nonergodicity) in the end of this section. More
technically, one needs to meet the Cramer condition of non-subexponentiality for a
tally of events (taken at face value) for raw probability to have any meaning at all.
The plane analogy was proposed by the insightful Russ Robert during one of his
econtalk podcasts with the author.

Figure 2.8: Life is about payoffs, not forecasting, and the difference increases in Extremistan

1

it is far more likely that they died of ebola than smoking or diabetes or tangled in their
bedsheets? This is rule number one. "Thou shalt not compare a multiplicative fat-tailed
process in Extremistan in the subexponential class to a thin-tailed process that has Chernoff
bounds from Mediocristan". This is simply because of the catastrophe principle we saw

18

a non-technical overview - the darwin college lecture (cid:3),‡

earlier, illustrated in Figure 2.1.
It is naïve empiricism to compare these processes, to
suggest that we worry too much about ebola and too little about diabetes. In fact it is the
other way round. We worry too much about diabetes and too little about ebola and other
multiplicative effects. This is an error of reasoning that comes from not understanding
fat tails –sadly it is more and more common. What is worse, such errors of reasoning are
promoted by empirical psychology which does not appear to be empirical.

2.3.3 The Law of Large Numbers

Let us now discuss the law of large numbers which is the basis of much of statistics. The
law of large numbers tells us that as we add observations the mean becomes more stable,
n. Figure 2.4 shows that it takes many more observations under a
the rate being around
fat-tailed distribution (on the right hand side) for the mean to stabilize.

p

Table 2.1: Corresponding na, or how many observations to get a drop in the error around the mean for an
equivalent a-stable distribution (the measure is discussed in more details in Chapter 7). The Gaussian case is
the a = 2. For the case with equivalent tails to the 80/20 one needs at least 1011 more data than the Gaussian.

a

1

9
8

5
4

11
8

3
2

13
8

7
4

15
8
2

na

b=(cid:6) 1
2
a

n

b=(cid:6)1
a

n

Symmetric

Skewed

One-tailed

Fughedaboudit

-

-

6.09 (cid:2) 1012

2.8 (cid:2) 1013

1.86 (cid:2) 1014

574,634

5,027

567

165

75

44

30.

895,952

1.88 (cid:2) 106

6,002

613

171

77

44

30

8,632

737

186

79

44

30

The "equivalence" is not straightforward.
One of the best known statistical phenomena is Paretos 80/20 e.g.
twenty per cent of
Italians own 80 per cent of the land. Table 2.1 shows that while it takes 30 observations
in the Gaussian to stabilize the mean up to a given level, it takes 1011 observations in the
Pareto to bring the sample error down by the same amount (assuming the mean exists).

Despite this being trivial to compute, few people compute it. You cannot make claims
about the stability of the sample mean with a fat tailed distribution. There are other ways
to do this, but not from observations on the sample mean.

2.3 the main consequences

19

Figure 2.9: The Masquerade Problem (or Central Asymmetry in Inference). To the left, a degenerate
random variable taking seemingly constant values, with a histogram producing a Dirac stick. One cannot rule
out nondegeneracy. But the right plot exhibits more than one realization. Here one can rule out degeneracy.
This central asymmetry can be generalized and put some rigor into statements like "failure to reject" as the
notion of what is rejected needs to be reﬁned. We produce rules in Chapter ??.

2.3.4 Epistemology and Inferential Asymmetry

Let us now examine the epistemological consequences. Figure 2.9 illustrates the Masquer-
ade Problem (or Central Asymmetry in Inference). On the left is a degenerate random
variable taking seemingly constant values with a histogram producing a Dirac stick.

We have known at least since Sextus Empiricus that we cannot rule out degeneracy but
there are situations in which we can rule out non-degeneracy. If I see a distribution that
has no randomness, I cannot say it is not random. That is, we cannot say there are no
Black Swans . Let us now add one observation. I can now see it is random, and I can rule
out degeneracy. I can say it is not not random. On the right hand side we have seen a
Black Swan , therefore the statement that there are no Black Swans is wrong. This is the
negative empiricism that underpins Western science. As we gather information, we can
rule things out. The distribution on the right can hide as the distribution on the left, but
the distribution on the right cannot hide as the distribution on the left (check). This gives
us a very easy way to deal with randomness. Figure 2.10 generalizes the problem to how
we can eliminate distributions.

If we see a 20 sigma event, we can rule out that the distribution is thin-tailed. If we see
no large deviation, we can not rule out that it is not fat tailed unless we understand the
process very well. This is how we can rank distributions. If we reconsider Figure 2.5 we
can start seeing deviation and ruling out progressively from the bottom. These are based
on how they can deliver tail events. Ranking distributions becomes very simple because
if someone tells you there is a ten-sigma event, it is much more likely that they have the
wrong distribution than it is that you really have ten-sigma event. Likewise, as we saw, fat
tailed distributions do not deliver a lot of deviation from the mean. But once in a while
you get a big deviation. So we can now rule out what is not mediocristan. We can rule out
where we are not we can rule out mediocristan. I can say this distribution is fat tailed by
elimination. But I can not certify that it is thin tailed. This is the Black Swan problem.

1234xf!x"10203040xf!x"Additional VariationApparently degenerate caseMore data shows nondegeneracy20

a non-technical overview - the darwin college lecture (cid:3),‡

Figure 2.10: "The probabilistic veil". Taleb and Pilpel [172] cover the point from an epistemological stand-
point with the "veil" thought experiment by which an observer is supplied with data (generated by someone
with "perfect statistical information", that is, producing it from a generator of time series). The observer, not
knowing the generating process, and basing his information on data and data only, would have to come up with
an estimate of the statistical properties (probabilities, mean, variance, value-at-risk, etc.). Clearly, the observer
having incomplete information about the generator, and no reliable theory about what the data corresponds
to, will always make mistakes, but these mistakes have a certain pattern. This is the central problem of risk
management.

2.4 primer on power laws (without mathematics)

Let us now discuss the intuition behind the Pareto Law. It is simply deﬁned as: say X is
a random variable. For x sufﬁcently large, the probability of exceeding 2x divided by the
probability of exceeding x is no different from the probability of exceeding 4x divided by
the probability of exceeding 2x, and so forth. This property is called "scalability".
So if we have a Pareto (or Pareto-style) distribution, the ratio of people with $ 16 million
compared to $ 8 million is the same as the ratio of people with $ 2 million and $ 1 million.
There is a constant inequality. This distribution has no characteristic scale which makes it
very easy to understand. Although this distribution often has no mean and no standard
deviation we can still understand it –in fact we can understand it much better than we do
with more standard statistical distributions. But because it has no mean we have to ditch
the statistical textbooks and do something more solid, more rigorous, even if it seems less
mathematical.

2

A Pareto distribution has no higher moments: moments either do not exist or become
statistically more and more unstable. So next we move on to a problem with economics
and econometrics. In 2009 I took 55 years of data and looked at how much of the kurtosis

dist1dist2dist3dist4dist5dist6dist7dist8dist9dist10dist11dist12dist13dist14ObservedDistributionGeneratingDistributionsTHE VEILDistributionsruled outNonobservableObservableDistributionsthat cannot beruled out"True"distribution2.4 primer on power laws (without mathematics)

21

Figure 2.11: Popper’s solution of the prob-
lem of induction is in asymmetry: relying
on conﬁrmatory empiricism, that is focus
on "ruling out" what fails to work, via
negativa style. We extend this approach
to statistical inference with the probabilis-
tic veil by progressively ruling out entire
classes of distributions.

Scientiﬁc Rigor and Asymmetries by The Russian School of Probability

One can believe in the rigor of mathematical statements about probabil-

ity without falling into the trap of providing naive computations sub-
jected to model error. There is a wonderful awareness of the asymmetry
throughout the works of the Russian school of probability –and asym-
metry here is the analog of Popper’s idea in mathematical space.

Members across three generations: P.L. Chebyshev, A.A. Markov, A.M. Lyapunov, S.N.
Bernshtein (ie. Bernstein), E.E. Slutskii, N.V. Smirnov, L.N. Bol’shev, V.I. Romanovskii,
A.N. Kolmogorov, Yu.V. Linnik, and the new generation: V. Petrov, A.N. Nagaev, A.
Shyrayev, and a few more.
They had something rather potent in the history of scientiﬁc thought: they thought in
inequalities, not equalities (most famous: Markov, Chebyshev, Bernstein, Lyapunov).
They used bounds, not estimates. Even their central limit version was a matter of
bounds, which we exploit later by seeing what takes place outside the bounds. They
were world apart from the new generation of users who think in terms of precise
probability –or worse, mechanistic social scientists. Their method accommodates
skepticism, one-sided thinking: "A is > x, AO(x) [Big-O: "of order" x], rather than
A = x.
For those working on integrating the mathematical rigor in risk bearing they provide
a great source. We always know one-side, not the other. We know the lowest value
we are willing to pay for insurance, not necessarily the upper bound (or vice versa).a

a The way this connects asymmetry to robustness is as follows. Is robust what does not produce variability
across perturbation of parameters of the probability distribution. If there is change, but with an asymmetry,
i.e. a concave or convex response to such perturbations, the classiﬁcation is fragility and antifragility,
respectively, see [153].

(a function of the fourth moment) came from the largest observation –see Table 2.3. For
a Gaussian the maximum contribution over the same time span should be around .008 (cid:6)
.0028. For the S&P 500 it was about 80 per cent. This tells us that we dont know anything
about kurtosis. Its sample error is huge; or it may not exist so the measurement is heavily

22

a non-technical overview - the darwin college lecture (cid:3),‡

Principle 2.1 (Epistemology: the invisibility of the generator.)

(cid:15) We do not observe probability distributions, just realizations.

(cid:15) A probability distribution cannot tell you if the realization belongs to it.

(cid:15) You need a meta-probability distribution to discuss tail events.

Figure 2.12: The Problem of Induction.
The philosophical problem of enumerative
induction, expressed in the question:
"How many white swans do you need
to count before ruling out the future
occurrence of a black one?"
maps surprisingly perfectly to our prob-
lem of the working of the law of large num-
bers:
"How much data do you need before
making a certain claim with an ac-
ceptable error rate?"
It turns out that the very nature of statis-
tical inference reposes on a clear deﬁnition
and quantitative measure of the inductive
It happens that, under fat
mechanism.
tails, we need considerably more data; as
we will see in Chapters 6 and 7 there is a
way to gauge the relative speed of the in-
ductive mechanism, even if ultimately the
problem of induction cannot be perfectly
solved.
The problem said of induction is generally
misattributed to Hume, [158] .

sample dependent. If we dont know anything about the fourth moment, we know nothing
about the stability of the second moment. It means we are not in a class of distribution
that allows us to work with the variance, even if it exists. This is ﬁnance.
For silver futures, in 46 years 94 per cent of the kurtosis came from one observation. We
cannot use standard statistical methods with ﬁnancial data. GARCH (a method popular in
academia) does not work because we are dealing with squares. The variance of the squares
is analogous to the fourth moment. We do not know the variance. But we can work very
easily with Pareto distributions. They give us less information, but nevertheless, it is more
rigorous if the data are uncapped or if there are any open variables.
Table 2.3, for ﬁnancial data, debunks all the college textbooks we are currently using. A
lot of econometrics that deals with squares goes out of the window. This explains why
economists cannot forecast what is going on they are using the wrong methods. It will
work within the sample, but it will not work outside the sample. If we say that variance (or
kurtosis) is inﬁnite, we are not going to observe anything that is inﬁnite within a sample.

2.4 primer on power laws (without mathematics)

23

Figure 2.13: A Discourse to Show that
Skeptical Philosophy is of Great Use in
Science by François de La Mothe Le
Vayer (1588-1672), apparently Bishop
Huet’s source. Every time I ﬁnd a orig-
inal thinker who ﬁgured out the skepti-
cal solution to the Black Swan problem,
it turns out that he may just be crib-
bing a predecessor –not maliciously, but
we forget to dig to the roots. As we in-
sist, "Hume’s problem" has little to do
with Hume, who carried the heavy multi-
volume Dictionary of Pierre Bayle (his
predecessors) across Europe.
I thought
it was Huet who was as one digs, new
predecessors crop up

.

Figure 2.14: It is not possible to "accept" thin tails, very easy to reject thintailedness. One distribution can
produce jumps and quiet days will not help rule out their occurrence.

Principal component analysis (Figure 2.15) is a dimension reduction method for big data
and it works beautifully with thin tails (at least sometimes). But if there is not enough data
there is an illusion of a structure. As we increase the data (the n variables), the structure
becomes ﬂat (something called in some circles the "Wigner effect" for random matrices,
after Eugene Wigner —do not confuse with Wigner’s discoveries about the dislocation
of atoms under radiation).
In the simulation, the data that has absolutely no structure:
principal components (PCs) should be all equal (asymptotically, as data becomes large);
but the small sample effect causes the ordered PCs to show a declining slope. We have
zero correlation on the matrix. For a fat tailed distribution (the lower section), we need a
lot more data for the spurious correlation to wash out i.e. dimension reduction does not
work with fat tails.

����������������������������������������������������������������������������24

a non-technical overview - the darwin college lecture (cid:3),‡

Table 2.2: An example of a power law

Richer than 1 million
Richer than 2 million
Richer than 4 million
Richer than 8 million
Richer than 16 million
Richer than 32 million

1 in 62.5
1 in 250
1 in 1,000
1 in 4,000
1 in 16,000
1 in ?

(

Table 2.3: Kurtosis from a single observation for ﬁnancial data

Max
(cid:229)n

X4
i=0 X4

t(cid:0)∆ti
t(cid:0)∆ti

)n
i=0

Max Q Years.

Security
0.94
Silver
0.79
SP500
0.79
CrudeOil
0.75
Short Sterling
0.74
Heating Oil
0.72
Nikkei
0.54
FTSE
0.48
JGB
Eurodollar Depo 1M 0.31
0.3
Sugar
0.27
Yen
0.27
Bovespa
Eurodollar Depo 3M 0.25
0.25
CT
0.2
DAX

46.
56.
26.
17.
31.
23.
25.
24.
19.
48.
38.
16.
28.
48.
18.

2.5 where are the hidden properties?

The following summarizes everything that I wrote in The Black Swan (a message that some-
how took more than a decade to go through without distortion. Distributions can be
one-tailed (left or right) or two-tailed. If the distribution has a fat tail, it can be fat tailed
one tail or it can be fat tailed two tails. And if is fat tailed one tail, it can be fat tailed left
tail or fat tailed right tail.
See Figure 2.17 for the intuition: if it is fat tailed and we look at the sample mean, we
observe fewer tail events. The common mistake is to think that we can naively derive the
mean in the presence of one-tailed distributions. But there are unseen rare events and
with time these will ﬁll in. But by deﬁnition, they are low probability events. The trick is
to estimate the distribution and then derive the mean. This is called plug-in estimation,
see Table 2.4. It is not done by measuring the directly observable sample mean which is
biased with fat-tailed distributions. This is why, outside a crisis, banks seem to make large
proﬁts. Then once in a while they lose everything and more and have to be bailed out
by the taxpayer. The way we handle this is by differentiating the true mean (which I call
"shadow") from the realized mean, as in the Tableau in Table 2.4.
We can also do that for the Gini coefﬁcient to estimate the "shadow" one rather than the

naively observed one.

2.5 where are the hidden properties?

25

Figure 2.15: Spurious PCAs Under Fat Tails: A Monte Carlo experiment that shows how spurious correla-
tions and covariances are more acute under fat tails. Principal Components ranked by variance for 30 Gaussian
uncorrelated variables, n=100 (above) and 1000 data points, and principal components ranked by variance for
30 Stable Distributed ( with tail a = 3
2 , symmetry b = 1, centrality m = 0, scale s = 1) (below). Both are
"uncorrelated" identically distributed variables.

This is what we mean when we say that the "empirical" distribution is not "empirical". In
other words: 1) there is a wedge between population and sample attributes and, 2) even
exhaustive historical data can be seen as mere sampling from a broader phenomenon.

Once we have ﬁgured out the distribution, we can estimate the statistical mean. This
works much better than directly measuring the sample mean. For a Pareto distribution,
for instance, 98% of observations are below the mean. There is a bias in the observed mean.
But once we know that we have a Pareto distribution, we should ignore the sample mean
and look elsewhere.
Note that the ﬁeld of Extreme Value Theory [84] [60] [85] focuses on tail properties, not
the mean or statistical inference.

0.000.050.100.150.20020004000600080001000026

a non-technical overview - the darwin college lecture (cid:3),‡

Figure 2.16: A central asymmetry: the difference between absence of evidence and evidence of absence is
compounded by fat tails. It requires a more elaborate understanding of random events —or a more naturalistic
one. Courtesy Stefan Gasic.

Figure 2.17: Shadow Mean at work: Below: Inverse Turkey Problem The unseen rare event is positive. When
you look at a positively skewed (antifragile) time series and make (nonparametric) inferences about the unseen,
you miss the good stuff an underestimate the beneﬁts. Above: The opposite problem. The ﬁlled area corresponds
to what we do not tend to see in small samples, from insufﬁciency of data points. Interestingly the shaded area
increases with model error.

-���-���-���-��-��-��-�����������������������������������������������������������������������������������������Table 2.4: Shadow mean, sample mean and their ratio for different minimum thresholds. In bold the values for
the 145k threshold. Rescaled data. From Cirillo and Taleb [34]

2.6 bayesian schmayesian

27

Thresh.(cid:2)103
50
100
145
300
500
600

Shadow(cid:2)107
1.9511
2.3709
3.0735
3.6766
4.7659
5.5573

Sample(cid:2)107 Ratio
1.5299
1.5628
1.7354
1.6240
1.6561
1.7348

1.2753
1.5171
1.7710
2.2639
2.8776
3.2034

2.6 bayesian schmayesian

In the absence of reliable information, Bayesian methods can be of little help. This author
has faced since the publication of The Black Swan numerous questions concerning the use of
something vaguely Bayesian to solve problems about the unknown under fat tails. Unless
one cannot manufacture information beyond what’s available, no technique, Bayesian nor
Schmayesian can help. The key is that one needs a reliable prior, something not readily
observable (see Diaconis and Friedman [48] for the difﬁculty for an agent in formulating a
prior).
A problem is the speed of updating, as we will cover in Chapter 6, which is highly
distribution dependent. The mistake in the rational expectation literature is to believe that
two observers supplied with the same information would necessarily converge to the same
view. Unfortunately, the conditions for that to happen in real time or to happen at all are
quite speciﬁc.

One of course can use Bayesian methods (under adequate priors) for the estimation of
parameters if 1) one has a clear idea about the range of values (say from universality classes
or other stable basins) and 2) these parameters follow a tractable distribution with low
variance such as, say, the tail exponent of a Pareto distribution (which is inverse-gamma
distributed), [9].

Moral Hazard in Financial Education: The most depressing experience I’ve had was
when I taught a course on Fat Tails at the University of Massachussetts Amherst, at
the business school, during my very brief stint there. One PhD student in ﬁnance told
me bluntly that he liked my ideas but that a ﬁnancial education career commanded
"the highest salary in the land" (that is, among all other specialties in education). He
preferred to use Markowitz methods as these were used by other professors, hence al-
lowed him to get his papers published, and get a high paying job. I was disgusted, but
predicted he would subsequently have a very successful career writing non-papers.
He did.

2.7 x vs f (x), exposures to x confused with knowledge about x

Take X a random or nonrandom variable, and F(X) the exposure, payoff, the effect of X
on you, the end bottom line. (X is often is higher dimensions but let’s assume to simplify
that it is a simple one-dimensional variable).

28

a non-technical overview - the darwin college lecture (cid:3),‡

Practitioners and risk takers often observe the following disconnect: people (nonpracti-
tioners) talking X (with the implication that practitioners should care about X in running
their affairs) while practitioners think about F(X), nothing but F(X). And the straight
confusion since Aristotle between X and F(X) has been chronic as discussed in Antifragile
[161] which is written around that theme. Sometimes people mention F(X) as utility but
miss the full payoff. And the confusion is at two level: one, simple confusion; second, in
the decision-science literature, seeing the difference and not realizing that action on F(X)
is easier than action on X.

(cid:15) The variable X can be unemployment in Senegal, F1(X) is the effect on the bottom
line of the IMF, and F2(X) is the effect on your grandmother (which I assume is
minimal).

(cid:15) X can be a stock price, but you own an option on it, so F(X) is your exposure an
option value for X, or, even more complicated the utility of the exposure to the
option value.

(cid:15) X can be changes in wealth, F(X) the convex-concave way it affects your well-being.
One can see that F(X) is vastly more stable or robust than X (it has thinner tails).

Convex vs. linear functions of a variable X Consider Fig. 2.19; confusing F(X) (on the
vertical) and X (the horizontal) is more and more signiﬁcant when F(X) is nonlinear. The
more convex F(X), the more the statistical and other properties of F(X) will be divorced
from those of X. For instance, the mean of F(X) will be different from F(Mean ofX), by
Jensen’s inequality. But beyond Jensen’s inequality, the difference in risks between the two
will be more and more considerable. When it comes to probability, the more nonlinear F,
the less the probabilities of X matters compared to that of F. Moral of the story: focus on
F, which we can alter, rather than on the measurement of the elusive properties of X.

Figure 2.18: The Conﬂation Problem X (random variable) and F(X) a function of it (or payoff). If F(X) is
convex we don’t need to know much about it –it becomes an academic problem. And it is safer to focus on
transforming F(X) than X.

Limitations of knowledge What is crucial, our limitations of knowledge apply to X not
necessarily to F(X). We have no control over X, some control over F(X). In some cases a
very, very large control over F(X).

The danger with the treatment of the Black Swan problem is as follows: people focus on
X ("predicting X"). My point is that, although we do not understand X, we can deal with
it by working on F which we can understand, while others work on predicting X which
we can’t because small probabilities are incomputable, particularly in "fat tailed" domains.
F(x) is how the end result affects you.

ProbabilityDistributionofXProbabilityDistributionofF(X)2.8 ruin and path dependence

29

Figure 2.19: The Conﬂation Problem: a convex-concave transformation of a fat tailed X produces a thin
tailed distribution (above). A sigmoidal transformation (below) that is bounded on a distribution in ((cid:0)¥, ¥)
produces an ArcSine distribution, with compact support.

Figure 2.20: A concave-convex transformation (of the style of a probit –an inverse CDF for the Gaussian– or
of a logit) makes the tails of the distribution of f (x) thicker

The probability distribution of F(X) is markedly different from that of X, particularly
when F(X) is nonlinear. We need a nonlinear transformation of the distribution of X to
get F(X). We had to wait until 1964 to start a discussion on “convex transformations of
random variables”, Van Zwet (1964)[184] –as the topic didn’t seem important before.

Ubiquity of S curves
F is almost always nonlinear (actually I know of no exception to
nonlinearity), often “S curved”, that is convex-concave (for an increasing function). See the
longer discussion in F.

2.8 ruin and path dependence

Let us ﬁnish with path dependence and time probability. Our greatgrandmothers did
understand fat tails. These are not so scary; we ﬁgured out how to survive by making
rational decisions based on deep statistical properties.

Path dependence is as follows.

If I iron my shirts and then wash them, I get vastly
different results compared to when I wash my shirts and then iron them. My ﬁrst work,

Concave-ConvexTransformationDistributionofxDistributionoff(x)30

a non-technical overview - the darwin college lecture (cid:3),‡

Fragility and Antifragility When F(X)is concave (fragile), errors about X can trans-
late into extreme negative values for F(X). When F(X) is convex, one is largely im-
mune from severe negative variations.
In situations of trial and error, or with an
option, we do not need to understand X as much as our exposure to the risks. Simply
the statistical properties of X are swamped by those of H. The point of Antifragile
is that exposure is more important than the naive notion of “knowledge”, that is,
understanding X.
The more nonlinear F the less the probabilities of X matters in the probability distri-
bution of the ﬁnal package F.
Many people confuse the probabilites of X with those of F. I am serious: the entire
literature reposes largely on this mistake. For Baal’s sake, focus on F, not X.

Dynamic Hedging [156], was about how traders avoid the "absorbing barrier" since once
you are bust, you can no longer continue: anything that will eventually go bust will lose
all past proﬁts.
The physicists Ole Peters and Murray Gell-Mann [126] shed new light on this point,
and revolutionized decision theory showing that a key belief since the development of
applied probability theory in economics was wrong. They pointed out that all economics
textbooks make this mistake; the only exception are by information theorists such as Kelly
and Thorp.

Let us explain ensemble probabilities.
Assume that 100 of us, randomly selected, go to a casino and gamble. If the 28th person
is ruined, this has no impact on the 29th gambler. So we can compute the casinos return
using the law of large numbers by taking the returns of the 100 people who gambled. If
we do this two or three times, then we get a good estimate of what the casinos edge is.
It does
The problem comes when ensemble probability is applied to us as individuals.
not work because if one of us goes to the casino and on day 28 is ruined, there is no
day 29. This is why Cramer showed insurance could not work outside what he called
the Cramer condition, which excludes possible ruin from single shocks. Likewise, no
individual investor will achieve the alpha return on the market because no single investor
has inﬁnite pockets (or, as Ole Peters has observed, is running his life across branching
parallel universes). We can only get the return on the market under strict conditions.

Time probability and ensemble probability are not the same. This only works if the risk
takers has an allocation policy compatible with the Kelly criterion[97],[176] using logs.
Peters wrote three papers on time probability (one with Murray Gell-Mann) and showed
that a lot of paradoxes disappeared.

Let us see how we can work with these and what is wrong with the literature.

If we
visibly incur a tiny risk of ruin, but have a frequent exposure, it will go to probability
one over time. If we ride a motorcycle we have a small risk of ruin, but if we ride that
motorcycle a lot then we will reduce our life expectancy. The way to measure this is:

Behavioral ﬁnance so far makes conclusions from statics not dynamics, hence misses
the picture. It applies trade-offs out of context and develops the consensus that people
irrationally overestimate tail risk (hence need to be "nudged" into taking more of these
exposures). But the catastrophic event is an absorbing barrier. No risky exposure can
be analyzed in isolation: risks accumulate. If we ride a motorcycle, smoke, ﬂy our own
propeller plane, and join the maﬁa, these risks add up to a near-certain premature death.
Tail risks are not a renewable resource.

2.8 ruin and path dependence

31

Better be convex than right:

In the fall of 2017, a ﬁrm went bust bet-
ting against volatility –they were predicting lower real market volatility
(rather, variance) than "expected" by the market. They were correct in the
prediction, but went bust nevertheless. They were just very concave in the
payoff function. Recall that x is not f (x) and that in the real world there

are almost no linear f (x).
The following example can show us how.
off
The payoff
meaning if x moves by up to 1 unit
a proﬁt,

the following pay-
f (x) = 1 (cid:0) x2 daily,
there is
This is a typical contract called "variance swap".

function is
(say,

in the ﬁgure below.

standard deviation),

losses beyond.

Consider

Now consider the following two types successions of deviations of x for 7 days (ex-
pressed in standard deviations).
Succession 1 (thin tails): f1, 1, 1, 1, 1, 0, 0g. Mean variation= 0.71. P/L= 2.
Succession 2 (fat tails): f0, 0, 0, 0, 0, 0, 5g. Mean variation= 0.71 (same). P/L=(cid:0)18
(bust, really bust).

In both cases they forecast right, but the lumping of the volatility –the fatness of tails–
made a huge difference.

This in a nutshell explains why, in the real world, "bad" forecasters can make great
traders and decision makers and vice versa –something every operator knows but that
the mathematically and practically unsophisticated "forecasting" literature, centuries
behind practice, misses.

Every risk taker who survived understands this. Warren Buffett understands this. Gold-
man Sachs understands this. They do not want small risks, they want zero risk because
that is the difference between the ﬁrm surviving and not surviving over twenty, thirty, one
hundred years. This attitude to tail risk can explain that Goldman Sachs is 149 years old
–it ran as partnership with unlimited liability for approximately the ﬁrst 130 years, but
was bailed out once in 2009, after it became a bank. This is not in the decision theory
literature but we (people with skin in the game) practice it every day. We take a unit, look
at how long a life we wish it to have and see by how much the life expectancy is reduced
by repeated exposure.

-3-2-1123x-4-224f(x)=1-x232

a non-technical overview - the darwin college lecture (cid:3),‡

Remark 2.2 (Psychology of decision making)
The psychological literature focuses on one-single episode exposures and narrowly deﬁned cost-
beneﬁt analyses. Some analyses label people as paranoid for overestimating small risks, but
don’t get that if we had the smallest tolerance for collective tail risks, we would not have made
it for the past several million years.

Next let us consider layering, why systemic risks are in a different category from individ-
ual, idiosyncratic ones. Look at the (inverted) pyramid in Fig. 2.22: the worst-case scenario
is not that an individual dies. It is worse if your family, friends and pets die. It is worse
if you die and your arch enemy survives. They collectively have more life expectancy lost
from a terminal tail event.

So there are layers. The biggest risk is that the entire ecosystem dies. The precautionary

principle puts structure around the idea of risk for units expected to survive.

Ergodicity in this context means that your analysis for ensemble probability translates

into time probability. If it doesn’t, ignore ensemble probability altogether.

2.9 what to do?

To summarize, we ﬁrst need to make a distinction between mediocristan and Extremistan,
two separate domains that about never overlap with one another. If we dont make that
distinction, we dont have any valid analysis. Second, if we dont make the distinction

Figure 2.21: Ensemble probability vs.
time probability. The treatment by option
traders is done via the absorbing barrier. I
have traditionally treated this in Dynamic
Hedging [156] and Antifragile[153] as the
conﬂation between X (a random variable)
and f (X) a function of said r.v., which
may include an absorbing state.

2.9 what to do?

33

Figure 2.22: A hierarchy for
survival. Higher entities have
a longer life expectancy, hence
tail risk matters more for these.
Lower entities such as you and
I are renewable.

Principle 2.2 (Repetition of exposures)
Focus only on the reduction of life expectancy of the unit assuming repeated exposure at a
certain density or frequency.

between time probability (path dependent) and ensemble probability (path independent)
we dont have a valid analysis.

The next phase of the Incerto project is to gain understanding of fragility, robustness, and,
eventually, anti-fragility. Once we know something is fat-tailed, we can use heuristics to
see how an exposure there reacts to random events: how much is a given unit harmed
by them. It is vastly more effective to focus on being insulated from the harm of random
events than try to ﬁgure them out in the required details (as we saw the inferential errors
under fat tails are huge). So it is more solid, much wiser, more ethical, and more effective
to focus on detection heuristics and policies rather than fabricate statistical properties.

The beautiful thing we discovered is that everything that is fragile has to present a con-
cave exposure [153] similar –if not identical –to the payoff of a short option, that is, a
negative exposure to volatility. It is nonlinear, necessarily. It has to have harm that accel-
erates with intensity, up to the point of breaking. If I jump 10m I am harmed more than
10 times than if I jump one metre. That is a necessary property of fragility. We just need
to look at acceleration in the tails. We have built effective stress testing heuristics based on
such an option-like property [169].
In the real world we want simple things that work [78]; we want to impress our accoun-
tant and not our peers. (My argument in the latest instalment of the Incerto, Skin in the
Game is that systems judged by peers and not evolution rot from overcomplication). To
survive we need to have clear techniques that map to our procedural intuitions.

The new focus is on how to detect and measure convexity and concavity. This is much,

much simpler than probability.

34 NOTES

notes

1

"Gabish" rather than "capisce"? Gabish is the recreated pronunciation of Siculo-Galabrez (Calabrese); the "p"

used to sound like a "b" and the "g" like a Semitic kof, a hard K, from Punic. Much like capicoli is "gabagool".

More formally: let X be a random variable belonging to the class of distributions with a "power law" right

2

tail:

where L : [xmin, +¥) ! (0, +¥) is a slowly varying function, deﬁned as limx!+¥
apply the same to the negative domain.

P(X > x) (cid:24) L(x) x

(cid:0)a

(2.1)
L(kx)
L(x) = 1 for any k > 0. We can

3 O V E R V I E W O F FAT TA I L S , PA R T I , T H E

U N I V A R I AT E C A S E †

This Chapter is organized as follows. We look at three levels of fat-tails with

more emphasis on the intuitions and heuristics than formal mathematical
differences, which will be pointed out later in the discussions of limit
theorems. The three levels are:

(cid:15) Fat tails, entry level (sort of), i.e., ﬁnite moments
(cid:15) Subexponential class
(cid:15) Power Law class

Level one will be the longest as we will use it to build intuitions. While, mathemat-
ically, it is the least used (fat tails are usually associated with power laws and limit
behavior); analytically and practically it is relied upon the most (we can get the im-
mediate consequences of fat-tailedness with little effort, the equivalent of a functional
derivative that provides a good grasp of local sensitivities). For instance, as a trader,
the author was able to get most of the effect of fattailedness with a simple heuristic
of averaging option prices across two volatilities.

3.1 level 1: fat tails, but finite moments

In this section, we link fatness of tails to higher moments, but stay in the situation that no
moment is inﬁnite.

3.1.1 A Simple Heuristic to Create Mildly Fat Tails

Remark 3.1 (Fat Tails and Jensen’s inequality )
For a Gaussian distribution (and, possibly, members of the location-scale family of distribu-
tions), tail probabilities are convex to the scale of the distribution, here the variance s2. This
allows us to fatten the tails by "stochasticizing" the variance, checking the effect of Jensen’s
inequality on the total.

Heteroscedasticity is the general technical term often used in time series analysis to char-
acterize a process with ﬂuctuating scale. Our method "stochasticizes", that is perturbates
the variance of the distribution under the constraint of conservation of the mean.

35

36

overview of fat tails, part i, the univariate case †

Figure 3.1: How random
volatility creates fatter tails
owing to the convexity of some
parts of the density to the scale
of the distribution.

But note that any fat tailed process, even a power law, can be in sample (that is ﬁnite
number of observations necessarily discretized) described by a simple Gaussian process
with changing variance, a regime switching process, or a combination of Gaussian plus a
series of variable jumps (though not one where jumps are of equal size, see the summary
in [118]).
This method will also allow us to answer the great question: "where do the tails start?"
in 3.3.
Let f (

a, x) be the density of the normal distribution (with mean 0) as a function of the
))

p

(

variance for a given point x of the distribution. Let us compare f

(

(p

)

(p

))

f

+ f

1 (cid:0) a, x

by Jensen’s inequality. We assume the average s2 con-
to 1
2
stant, but the discussion works just as well if we just assumed s constant —it is a long
debate whether one should constraint on the mean of the variance or that of the standard
deviation.

a + 1, x

(p

1
2

p

1 (cid:0) a +

a + 1, x

Since higher moments increase under fat tails, as compared to lower ones, it should be
possible to simply increase fat tailedness while keeping lower moments (the ﬁrst two or
three) invariant.

3.1 level 1: fat tails, but finite moments

37

3.1.2 A Variance-preserving heuristic

(

)

X2

Keep E
bution, since E
E

(

)

constant and increase E

(

)

X4

is itself analog to the variance of E
))
2

(

, by "stochasticizing" the variance of the distri-
measured across samples –

)

(
X2
)

X4

is the noncentral equivalent of E

X2 (cid:0) E
so we will focus on the simpler
version outside of situations where it matters. Further, we will do the "stochasticizing" in
a more involved way in later sections of the chapter.

X2

(

)

X4
((

An effective heuristic to get some intuition about the effect of the fattening of tails consists
in simulating a random variable set to be at mean 0, but with the following variance-
)
preserving tail fattening trick: the random variable follows a distribution N
with probability p = 1
1.

1 (cid:0) a
2 , with 0 ⩽ a <

with the remaining probability 1

2 and N

1 + a

0, s

0, s

p

p

(

)

(

The characteristic function is

ϕ(t, a) =

1
2

e

(cid:0) 1

2 (1+a)t2s2

(

1 + eat2s2

)

Odd moments are nil. The second moment is preserved since

M(2) = ((cid:0)i)2¶t,2ϕ(t)j

0 = s2

and the fourth moment

M(4) = ((cid:0)i)4¶t,4ϕj
0= 3
)

(

(

)

a2 + 1

s4

(3.1)

(3.2)

(3.3)

which puts the traditional kurtosis at 3
(assuming we do not remove 3 to compare
to the Gaussian). This means we can get an "implied a from kurtosis. The value of a is
roughly the mean deviation of the stochastic volatility parameter "volatility of volatility"
or Vvol in a more fully parametrized form.

a2 + 1

Limitations of the simple heuristic
This heuristic, while useful for intuition building,
is of limited powers as it can only raise kurtosis to twice that of a Gaussian, so it should
be used only to getting some intuition about the effects of the convexity. Section 3.1.3 will
present a more involved technique.

Remark 3.2 (Peaks)
As Figure 3.4 shows:
observations around the center of the distribution.

fat tails manifests themselves with higher peaks, a concentration of

3.1.3 Fattening of Tails With Skewed Variance

We can improve on the fat-tail heuristic in 3.1.1, (which limited the kurtosis to twice the
Gaussian) as follows. We Switch between Gaussians with variance:

{

s2(1 + a), with probability p
s2(1 + b), with probability 1 (cid:0) p

(3.4)

38

overview of fat tails, part i, the univariate case †

with p 2 [0,1), both a, b 2 (-1,1) and b= (cid:0)a p

1(cid:0)p , giving a characteristic function:

ϕ(t, a) = p e

(cid:0) 1

2 (a+1)s2t2 (cid:0) (p (cid:0) 1) e

(cid:0) s2t2(ap+p(cid:0)1)
2(p(cid:0)1)

3((1(cid:0)a2)p(cid:0)1)
p(cid:0)1

with Kurtosis
preserving, conditioned on, when a > (<)0, a < (>) 1(cid:0)p
p .
Thus with, say, p = 1=1000, and the corresponding maximum possible a = 999, kurtosis

thus allowing polarized states and high kurtosis, all variance

can reach as high a level as 3000.

This heuristic approximates quite well the effect on probabilities of a lognormal weighting

for the characteristic function

ϕ(t, V) =

∫ ¥

(cid:0) t2v
2

e

(cid:0)
p

0

(

log(v)(cid:0)v0+ Vv2
2
2Vv2
2pvVv

)

2

dv

(3.5)

where v is the variance and Vv is the second order variance, often called volatility of
volatility. Thanks to integration by parts we can use the Fourier transform to obtain all
varieties of payoffs (see Gatheral [73]). But the absence of a closed-form distribution can
be remedied as follows, with the use of distributions for the variance that are analytically
more tractable.

Figure 3.2: Stochastic Variance: Gamma distribution and Lognormal of same mean and variance.

Gamma Variance
The gamma distribution applied to the variance of a Gaussian is is a
useful shortcut for a full distribution of the variance, which allows us to go beyond the
narrow scope of heuristics [25]. It is easier to manipulate analytically than the Lognormal.
Assume that the variance of the Gaussian follows a gamma distribution.

va(cid:0)1

(

)(cid:0)a

V
a
G(a)

(cid:0) av
V

e

Ga(v) =

with mean V and standard deviation Vp
a

. Figure 3.2 shows the matching to a lognormal

(

)

with same ﬁrst two moments where we calibrate the lognormal to mean 1

2 log

aV3
aV+1

and

12345V0.20.40.60.81.0PrGamma(cid:72)1,1(cid:76)vs.LognormalStochasticVariance123450.20.40.60.81.0Gamma(cid:72)4,14(cid:76)vs.LognormalStochasticVariance,Α(cid:61)43.2 does stochastic volatility generate power laws?

39

Figure 3.3:
Stochastic
Variance using Gamma
distribution by pertur-
bating a in equation
3.7.

standard deviation

√

(

)

(cid:0) log

aV
aV+1

. The ﬁnal distribution becomes (once again, assuming

the same mean as a ﬁxed volatility situation:

fa,V(x) =

∫ ¥

0

(cid:0) (x(cid:0)m)2
e
2vp
p
2p

v

Ga(v)dv

(3.6)

allora:

3
4

2

(cid:0) a

2 a

a
2 + 1

4 V

(cid:0) a
2

fa,V(x) =

4 jx (cid:0) mja(cid:0) 1
(cid:0) 1
p
pG(a)

2 Ka(cid:0) 1

2

( p

)

2

p
ajx(cid:0)mj
p
V

.

where Kn(z) is the Bessel K function, which satisﬁes the differential equation (cid:0)y
z2y

+ zy

= 0.

′′

′

(

(3.7)
)

n2 + z2

+

Let us now get deeper into the different forms of stochastic volatility.

3.2 does stochastic volatility generate power laws?

Assume the base distribution is normally distributed, N (m, s). Now there are different
ways to make s, the scale, stochastic. Note that we need sigma to follow some one-tailed
distribution.

(cid:15) We can make s2 (or, possibly s) follow a Lognormal distribution. It does not yield
closed form solutions, but we can get the moments and verify it is not a power law.
(cid:15) We can make s2 (or s) follow a gamma distribution. It does yield closed form solu-

tions, as we saw in the example above, in Eq. 3.7.

(cid:15) We can make 1
(cid:15) We can make 1

s2 –the precision parameter –follow a gamma distribution.

s2 follow a lognormal distribution.

(cid:45)4(cid:45)2024GaussianWithGammaVariance40

overview of fat tails, part i, the univariate case †

Let X be any random variable with pdf f (.) in the location-scale family, and l any random
variable with pdf g(.); X and l are assumed to be independent. Since by standard theorem,
the moments of order p for the product and the ratio X

l are:

(

E

(XY)p

)

= E (X p) E (Y p)

)

)

p

((

E

X
l

= E

((

1
l

)

)

p

E (X p) .

and

(via the Mellin transform).

Note that as proprety of location-scale family, 1
N (0, 1) (that is, normally distributed), then x

l ( x
l f x
s (cid:24) N (0, s).

l ) = fx( x

l ) so, for instance, if x (cid:24)

3.3 the body, the shoulders, and the tails

We assume tails start at the level of convexity of the segment of the probability distribution
to the scale of the distribution –in other words, affected by the stochastic volatility effect.

Figure 3.4: Where do the tails start? Fatter and fatter fails through perturbation of the scale parameter s for
a Gaussian, made more stochastic (instead of being ﬁxed). Some parts of the probability distribution gain in
density, others lose. Intermediate events are less likely, tails events and moderate deviations are more likely.
We can spot the crossovers a1 through a4. The "tails" proper start at a4 on the right and a1on the left.

a4aa3a2a1“Shoulders”!a1,a2",!a3,a4"“Peak”(a2,a3"Right tailLeft tail!4!2240.10.20.30.40.50.63.3 the body, the shoulders, and the tails

41

Table 3.1: Transformations for Stochastic Vol. We can see from the density of the transformations 1
if
we have a power law on hand. LN , N ,G and P are the Lognormal, Normal, Gamma, and Pareto distributions,
respectively.

x or

1p
x

distr

p(x)

LN (m, s)

e

(cid:0) (m(cid:0)log(x))2
2s2p
2psx

(

)

p

1
x
(cid:0) (m+log(x))2
2s2p
2psx

e

N (m, s)

G(a, b)
P(1, a)

b

e

(cid:0) (m(cid:0)x)2
2s2p
2ps
(cid:0)a xa(cid:0)1e
G(a)
(cid:0)a(cid:0)1

ax

(cid:0) x
b

b

x )2

(cid:0) (m(cid:0) 1
2s2p

e

(cid:0)a x

2psx2
(cid:0)a(cid:0)1e
G(a)
a(cid:0)1
ax

(cid:0) 1
bx

(

)

1p
p
x
(cid:0) (m+2 log(x))2
2s2

2
p e

√

sx
(

√

(cid:0)

2
p e

m(cid:0) 1
x2
2s2

)

2

2b

(cid:0) 1
bx2

sx3
(cid:0)a x
(cid:0)2a(cid:0)1e
G(a)
2ax2a(cid:0)1

Table 3.2: The p-moments of possible distributions for variance
)

(

)

(

distr

LN (m, s)

G(a, b)

P(1, a)

2

E (X p)
emp+ p2s2
bp(a)p
a(cid:0)p , p < a

a

E

( 1
X )p
2 p(ps2(cid:0)2m)

1

e
(cid:0)p
((cid:0)1)pb
(1(cid:0)a)p

, p < a

fughedaboudit

a
a+p

2a
2a+p

E

)p

( 1p
X
8 p(ps2(cid:0)4m)

1

e

The Black Swan Problem: As we saw, it is not merely that events in the tails of the
distributions matter, happen, play a large role, etc. The point is that these events play
the major role and their probabilities are not computable, not reliable for any effective
use. The implication is that Black Swans do not necessarily come from fat tails; le
problem can result from an incomplete assessment of tail events.

3.3.1 The Crossovers and Tunnel Effect.

Notice in Figure 3.4 a series of crossover zones, invariant to a. Distributions called "bell
shape" have a convex-concave-convex shape (or quasi-concave shape).

Let X be a random variable with distribution with PDF p(x) from a general class of
all unimodal one-parameter continuous pdfs ps with support D (cid:18) R and scale parameter
s. Let p(.) be quasi-concave on the domain, but neither convex nor concave. The density
function p(x) satisﬁes: p(x) (cid:21) p(x + ϵ) for all ϵ > 0, and x > x
and p(x) (cid:21) p(x (cid:0) ϵ) for all
) = maxx p(x)g. The class of quasiconcave functions is deﬁned as
x < x
follows: for all x and y in the domain and w 2 [0, 1],

with fx

: p(x

(cid:3)

(cid:3)

(cid:3)

(cid:3)

p (w x + (1 (cid:0) w) y) (cid:21) min (p(x), p(y)) .

A- If the variable is "two-tailed", that is, its domain of support D= (-¥,¥), and where
(x) ≜ p(x,s+d)+p(x,s(cid:0)d)
d

,

2

p

42

overview of fat tails, part i, the univariate case †

1. There exist a "high peak" inner tunnel, AT= ( a2, a3) for which the d-perturbed s of
(x)(cid:21)p(x) if x 2 ( a2, a3)

the probability distribution p

d

2. There exists outer tunnels, the "tails", for which p

(a4, ¥)

d

(x)(cid:21)p(x) if x 2 ((cid:0)¥, a1) or x 2

3. There exist intermediate tunnels, the "shoulders", where p

d

(x)(cid:20) p(x) if x 2 (a1, a2 )

or x 2 (a3, a4 )
Let A = fai

g the set of solutions

¶2 p(x)
¶s 2
For the Gaussian (m, s), the solutions obtained by setting the second derivative with respect
to s to 0 are:

ja= 0

x :

.

{

}

(

(cid:0) (x(cid:0)m)2
2s2

e

2s4 (cid:0) 5s2(x (cid:0) m)2 + (x (cid:0) m)4

p

2ps7

)

= 0,

which produces the following crossovers:

{

√

fa1, a2, a3, a4

g =

m (cid:0)

(

p

)

5 +

17

s, m (cid:0)

1
2

√

1
2

(

p

)

17

s,

5 (cid:0)

√

p

(

5 (cid:0)

1
2

)

17

s, m +

√

1
2

m +

(

p

5 +

17

}

)

s

(3.8)

In ﬁgure 3.4, the crossovers for the intervals are numerically f(cid:0)2.13s, (cid:0).66s, .66s, 2.13sg.
As to a symmetric power law(as we will see further down), the Student T Distribution

with scale s and tail exponent a:

(

) a+1
2

a
a+ x2
s2
(
asB

)

a
2 , 1
2

p(x) ≜

p

fa1, a2, a3, a4

g =

√

p

5a(cid:0)

{

(cid:0)

(a+1)(17a+1)+1

a(cid:0)1
p
2

s

,

(cid:0)

√

p

5a(cid:0)

√

p

5a+

(a+1)(17a+1)+1
a(cid:0)1
p
2

s

,

(a+1)(17a+1)+1

a(cid:0)1
p
2

s

,

√

p

5a+

(a+1)(17a+1)+1

a(cid:0)1
p
2

}

s

where B(.) is the Beta function B(a, b) =

∫

G(a)G(b)
G(a+b) =

1

0 dtta(cid:0)1(1 (cid:0) t)b(cid:0)1.

When the Student is "cubic", that is, a = 3:

{

√

fa1, a2, a3, a4

g =

(cid:0)

4 (cid:0)

p

13s, (cid:0)

√

p

4 +

13s,

√

p

4 (cid:0)

√

p

}

13s,

4 +

13s

We can verify that when a ! ¥, the crossovers become those of a Gaussian. For instance,
for a1:

√

p

5a(cid:0)

(a+1)(17a+1)+1

(cid:0)

lima!¥

a(cid:0)1
p
2

√

s

= (cid:0)

(

p

)

17

s

5 (cid:0)

1
2

3.3 the body, the shoulders, and the tails

43

In Summary, Where Does the Tail Start?

√

p

5a+

(a+1)(17a+1)+1

s

For a general class of symmetric distributions with power laws, the tail starts at:

a(cid:0)1
p
2

(cid:6)
, with a inﬁnite in the stochastic volatility Gaussian case and s the
standard deviation. The "tail" is located between around 2 and 3 standard deviations.
This ﬂows from our deﬁnition: which part of the distribution is convex to errors in
the estimation of the scale.
But in practice, because historical measurements of STD will be biased lower because
of small sample effects (as we repeat fat tails accentuate small sample effects), the
deviations will be > 2-3 STDs.

p

Figure 3.5: We compare the
K + x2 and K +
behavior of
jxj.
The difference between
the two weighting functions in-
creases for large values of the
random variable x, which ex-
plains the divergence of the two
(and, more generally, higher
moments) under fat tails.

B- For some one-tailed distribution that have a "bell shape" of convex-concave-convex
shape, under some conditions, the same 4 crossover points hold. The Lognormal is a
special case.

fa1, a2, a3, a4

g =

p

(

p

2

2m(cid:0)

{

1
2

e

(

p

2

2m(cid:0)

1
2

e

5s2(cid:0)
pp

)

p

17s2

,
)

17s2+5s2

(

p

p

2

2m+

p

17s2

5s2(cid:0)

)

(

2m+

1
2

, e

pp

p

2

)}

17s2+5s2

1
2

, e

Stochastic Parameters
The problem of elliptical distributions is that they do not map
the return of securities, owing to the absence of a single variance at any point in time,
see Bouchaud and Chicheportiche (2010) [30]. When the scales of the distributions of
the individuals move but not in tandem, the distribution ceases to be elliptical. Figure
4.2 shows the effect of applying the equivalent of stochastic volatility methods: the more
annoying stochastic correlation. Instead of perturbating the correlation matrix S as a unit
as in section 4.1, we perturbate the correlations with surprising effect.

1+122πx1+x2-3-2-1123x1.52.02.53.044

overview of fat tails, part i, the univariate case †

3.4 fat tails, mean deviation and the rising norms

Next we discuss the beastly use of standard deviation and its interpretation.

3.4.1 The common errors

We start by looking at standard deviation and variance as the properties of higher moments.
Now, What is standard deviation? It appears that the same confusion about fat tails has
polluted our understanding of standard deviation.

The difference between standard deviation (assuming a mean of 0 to simplify) s =
√
j increases under fat tails, as

i and mean absolute deviation MAD = 1
n

one can see in Figure 3.5 . This can provide a conceptual approach to the notion.

(cid:229)jxi

(cid:229) x2

1
n

Dan Goldstein and the author [82] put the following question to investment professionals
and graduate students in ﬁnancial engineering –people who work with risk and deviations
all day long.

A stock (or a fund) has an average return of 0%.
It moves on average 1% a day in
absolute value; the average up move is 1% and the average down move is 1%. It does
not mean that all up moves are 1% –some are .6%, others 1.45%, and so forth.

Assume that we live in the Gaussian world in which the returns (or daily percentage
moves) can be safely modeled using a Normal Distribution. Assume that a year has
256 business days. What is its standard deviation of returns (that is, of the percentage
moves), the sigma that is used for volatility in ﬁnancial applications?

What is the daily standard deviation?

What is the yearly standard deviation?

As the reader can see, the question described mean deviation. And the answers were
overwhelmingly wrong. For the daily question, almost all answered 1%. Yet a Gaussian
random variable that has a daily percentage move in absolute terms of 1% has a standard
deviation that is higher than that, about 1.25%.
It should be up to 1.7% in empirical
distributions. The most common answer for the yearly question was about 16%, which
is about 80% of what would be the true answer. The professionals were scaling daily
256 which is correct provided one had
volatility to yearly volatility by multiplying by
the correct daily volatility.

p

the

The

Figure 3.6:
Ratio
daily
STD/MAD for
returns of the SP500 over the
past 47 years, seen with a
monthly rolling window. We
can see 1.25 as approximately
the value for Gaussian devia-
tions, as the cut point for fat
tailedness.

Time1.11.21.31.41.51.61.7STD(cid:144)MAD3.4 fat tails, mean deviation and the rising norms

45

So subjects tended to provide MAD as their intuition for STD. When professionals in-
volved in ﬁnancial markets and continuously exposed to notions of volatility˘a talk about
standard deviation, they use the wrong measure, mean absolute deviation (MAD) instead
of standard deviation (STD), causing an average underestimation of between 20 and 40%.
In some markets it can be up to 90%.
Further, responders rarely seemed to immediately understand the error when˘ait was
pointed out to them. However when asked to present the equation for standard deviation
they effectively expressed it as the mean root mean square deviation. Some were puzzled
as they were not aware of the existence of MAD.

Why this is relevant: Here you have decision-makers walking around talking about
"volatility" and not quite knowing what it means. We note some clips in the ﬁnancial press
to that effect in which the journalist, while attempting to explain the "VIX", i.e., volatility
index, makes the same mistake. Even the website of the department of commerce misde-
ﬁned volatility.

Further, there is an underestimation as MAD is by Jensen’s inequality lower (or equal)

than STD.

For a Gaussian the ratio (cid:24) 1.25, and it rises from there with fat tails.
How the ratio rises
Example: Take an extremely fat tailed distribution with n=106, observations are all -1
except for a single one of 106,

{

}

X =

(cid:0)1, (cid:0)1, ..., (cid:0)1, 106

.

The mean absolute deviation, MAD (X) = 2. The standard deviation STD (X)=1000. The
ratio standard deviation over mean deviation is 500.

3.4.2 Some Analytics

The ratio for thin tails As a useful heuristic, consider the ratio h

√

E (X2)
E(jXj)

h =

where E is the expectation operator (under the probability measure of concern and X
is a centered variable such E(x) = 0); the ratio increases with the fat tailedness of the

1
p

(Ep(xp))
En(jxj)

distribution; (The general case corresponds to
1
the distribution has ﬁnite moments up to n, and the special case here n = 2).
Simply, xn is a weighting operator that assigns a weight, xn(cid:0)1, which is large for large
values of X, and small for smaller values.
The effect is due to the convexity differential between both functions, jXj is piecewise
linear and loses the convexity effect except for a zone around the origin.

, p > 1, under the condition that

1 The word "inﬁnite" moment is a big ambiguous, it is better to present the problem as "undeﬁned" moment in the
sense that it depends on the sample, and does not replicate outside. Say, for a two-tailed distribution (i.e. with
support on the real line), the designation"inﬁnite" variance might apply for the fourth moment, but not to the
third.

46

overview of fat tails, part i, the univariate case †

Mean Deviation vs Standard Deviation, more technical Why the [REDACTED] did
statistical science pick STD over Mean Deviation? Here is the story, with analytical deriva-
tions not seemingly available in the literature. In Huber [90]:

There had been a dispute between Eddington and Fisher, around 1920, about
the relative merits of dn (mean deviation) and Sn (standard deviation). Fisher
then pointed out that for exactly normal observations, Sn is 12% more efﬁcient
than dn, and this seemed to settle the matter. (My emphasis)

Let us rederive and see what Fisher meant.

Let n be the number of summands:

Asymptotic Relative Efﬁciency (ARE) = lim
n!¥

(

/

)

V(Std)
E(Std)2

V(Mad)
E(Mad)2

Assume we are certain that Xi, the components of sample follow a Gaussian distribution,
normalized to mean=0 and a standard deviation of 1.

Relative Standard Deviation Error

The characteristic function Y

1(t) of the distribution

dx =

1p

1(cid:0)2it

. With the squared deviation z = x2, f , the pdf for n

of x2: Y
1(t) =
summands becomes:

e

∫ ¥
(cid:0)¥

(cid:0) x2
2 +itx2
p
2p

fZ(z) =

∫ ¥

(cid:0)¥

1
2p

exp((cid:0)itz)

)

n

(

1p

1 (cid:0) 2it

dt =

(cid:0)1

n
2

2

(cid:0) n

2 e
G

(cid:0) z
(

2 z
)
n
2

, z > 0.

Now take y =

p

z, fY(y) = 21(cid:0) n

(cid:0) z2
2 zn(cid:0)1
2 e
2 )
G( n

, z > 0, which corresponds to the Chi Distribution

with n degrees of freedom. Integrating to get the variance: V

with the mean equalling

p

2 )
2G( n+1
G( n
2 )

, we get

V(Std)
E(Std)2 =

2 )2
nG( n
2 )2
2G( n+1

(cid:0) 1.

2 )2
std(n) = n (cid:0) 2G( n+1
2 )2
G( n

. And,

Relative Mean Deviation Error Characteristic function again for jxj is that of a folded
Normal distribution, but let us redo it:
(cid:0) x2

))

√

(

(

1 + i erﬁ

, where erﬁ is the imaginary error function

2
p e

2 +itx = e

(cid:0) t2
2

tp

2

∫ ¥
0

Y

2(t) =
er f (iz)=i.

The ﬁrst moment: M1 = (cid:0)i

The second moment, M2 = ((cid:0)i)2 ¶2
¶t2
V(Mad)
p(cid:0)2
2n .
E(Mad)2 =

M2

=

(cid:0)M2
1
M2
1

e

(

(cid:0) t2
2n2

e

¶
¶t1

(

(

)))

√

1 + i erﬁ
(

(

(cid:0) t2
2n2

n (cid:12)
(cid:12)
(cid:12)

tp

2n

(

=
t=0
)))

1 + i erﬁ

tp

2n

2
p .

t=0

n (cid:12)
(cid:12)
(cid:12)

= 2n+p(cid:0)2
pn

. Hence,

Finalmente, the Asymptotic Relative Efﬁciency For a Gaussian

)

(cid:0) 2

(

n

nG( n
G( n+1

2 )2
2 )2
p (cid:0) 2

ARE = lim
n!¥

=

1
p (cid:0) 2

(cid:25) .875

3.4 fat tails, mean deviation and the rising norms

47

which means that the standard deviation is 12.5% more "efﬁcient" than the mean deviation
conditional on the data being Gaussian and these blokes bought the argument. Except that
the slightest contamination blows up the ratio. We will show later why Norm ℓ2 is not
appropriate for about anything; but for now let us get a glimpse on how fragile the STD
is.

3.4.3 Effect of Fatter Tails on the "efﬁciency" of STD vs MD

Consider a standard mixing model for volatility with an occasional jump with a probability
p. We switch between Gaussians (keeping the mean constant and central at 0) with:

V(x) =

{

s2(1 + a)
s2

with probability p
with probability (1 (cid:0) p)

For ease, a simple Monte Carlo simulation would do. Using p = .01 and n = 1000... Figure
3.7 shows how a=2 causes degradation. A minute presence of outliers makes MAD more
"efﬁcient" than STD. Small "outliers" of 5 standard deviations cause MAD to be ﬁve times
more efﬁcient.

Figure 3.7: A simulation of
the Relative Efﬁciency ratio of
Standard deviation over Mean
deviation when injecting a
(1 + a) (cid:2) s, as a
jump size
multiple of s the standard de-
viation.

p

3.4.4 Moments and The Power Mean Inequality

recheck derivations/proofs
Let X ≜ (xi)n

i=1,

∥X∥p≜

)

1=p

jp

(

(cid:229)n

jxi
i=1
n

For any 0 < p < q the following inequality holds:

√

p

n
(cid:229)
i=1

√

wi

jxi

jp (cid:20) q

n
(cid:229)
i=1

wi

jq

jxi

(3.9)

where the positive weights wi sum to unity.

5101520a2468RE48

overview of fat tails, part i, the univariate case †

Proof. The proof for positive p and q is as follows: Deﬁne the following function:
R+ ! R+; f (x) = x

q
p . f is a power function, so it does have a second derivative:

f

:

′′

f

(x) =

(

) (

)

q
p

(cid:0) 1

(cid:0)2

q
p

x

q
p

which is strictly positive within the domain of f , since q > p – f is convex. Hence, by

(

)

Jensen’s inequality : f

(cid:229)n

i=1 wix

p
i

(cid:20) (cid:229)n

i=1 wi f (x

p
i ), so

p
q

wix

p
i

(cid:20) (cid:229)n

i=1 wix

q
i after raising

√

n
(cid:229)
i=1

both side to the power of 1=q (an increasing function, since 1=q is positive) we get the
inequality.

What is critical for our exercise and the study of the effects of fat tails is that, for a
given norm, dispersion of results increases values. For example, take a ﬂat distribution,
}
X= f1, 1g. ∥X∥
produces rising higher norms:

2 =... =∥X∥n = 1. Perturbating while preserving ∥X∥

1 =∥X∥

1 , X =

2 , 3
1
2

{

{

p

f∥X∥n g5

n=1 =

1,

5
2

,

p
3

7
22=3

,

p
4

41
2

p
5

61
24=5

,

}

.

Trying again, with a wider spread, we get even higher values of the norms, X =

fjjXjjng5

n=1 =

8
>><

>>:

1,

5
4

,

√

3

43
2

2

p
4

,

p
5

2101
2 (cid:2) 23=5

1201
4

,

9
>>=

>>;

.

So we can see it becomes rapidly explosive.

One property quite useful with power laws with inﬁnite moment:

∥X∥¥ = sup

(

)

n

i=1

1
n

j

jxi

(3.10)

{

}

,

4 , 7
1
4

(3.11)

(3.12)

For a Gaussian, where x (cid:24) N(0, s), as we assume the mean is 0 without

Gaussian Case
loss of generality,
Let E(X) be the expectation operator for X,

E (X)1=p
E(jXj)

=

(

p(cid:0)1
2p

p

p
2

2

(cid:0)1 (((cid:0)1)p + 1) G
p

2

)) 1
p

(

p+1
2

or, alternatively

E (X p)
E(jXj)

= 2

1

2 (p(cid:0)3) (1 + ((cid:0)1)p)

) 1
2

(cid:0) p
2

(

G

(

1
s2

)

p + 1
2

(3.13)

3.4 fat tails, mean deviation and the rising norms

49

where G(z) is the Euler gamma function; G(z) =
is 0. For even moments:

)

(

∫ ¥
0 tz(cid:0)1e
p

√

s

2

E
X2
E (jXj)

=

(cid:0)tdt. For odd moments, the ratio

hence

√

E (X2)
E (jXj)

=

Standard Deviation
Mean Absolute Deviation

=

√

p

2

As to the fourth moment, it equals 3
For a Power Law distribution with tail exponent a=3, say a Student T

√

p
2

s3 .

√

E (X2)
E (jXj)

=

Standard Deviation
Mean Absolute Deviation

=

p

2

We will return to other metrics and deﬁnitions of fat tails with Power Law distributions
when the moments are said to be "inﬁnite", that is, do not exist. Our heuristic of using the
ratio of moments to mean deviation works only in sample, not outside.

"Inﬁnite" moments
Inﬁnite moments, say inﬁnite variance, always manifest themselves
as computable numbers in observed sample, yielding an estimator M, simply because the
sample is ﬁnite. A distribution, say, Cauchy, with undeﬁned means will always deliver a
measurable mean in ﬁnite samples; but different samples will deliver completely different
means. Figures ?? and ?? illustrate the "drifting" effect of M a with increasing information.

3.4.5 Comment: Why We Should Retire Standard Deviation

The notion of standard deviation has confused hordes of scientists; it is time to retire it
from common use and replace it with the more effective one of mean deviation. Standard
deviation, STD, should be left to mathematicians, physicists and mathematical statisticians
deriving limit theorems. There is no scientiﬁc reason to use it in statistical investigations
in the age of the computer, as it does more harm than good-particularly with the grow-
ing class of people in social science mechanistically applying statistical tools to scientiﬁc
problems.

Say someone just asked you to measure the "average daily variations" for the temperature
of your town (or for the stock price of a company, or the blood pressure of your uncle) over
the past ﬁve days. The ﬁve changes are: (-23, 7, -3, 20, -1). How do you do it?
Do you take every observation: square it, average the total, then take the square root?
Or do you remove the sign and calculate the average? For there are serious differences
between the two methods. The ﬁrst produces an average of 15.7, the second 10.8. The
ﬁrst is technically called the root mean square deviation. The second is the mean absolute
deviation, MAD. It corresponds to "real life" much better than the ﬁrst-and to reality. In
fact, whenever people make decisions after being supplied with the standard deviation
number, they act as if it were the expected mean deviation.
It is all due to a historical accident: in 1893, the great Karl Pearson introduced the term
"standard deviation" for what had been known as "root mean square error". The confusion
started then: people thought it meant mean deviation. The idea stuck: every time a

50

overview of fat tails, part i, the univariate case †

newspaper has attempted to clarify the concept of market "volatility", it deﬁned it verbally
as mean deviation yet produced the numerical measure of the (higher) standard deviation.

But it is not just journalists who fall for the mistake: I recall seeing ofﬁcial documents
from the department of commerce and the Federal Reserve partaking of the conﬂation,
even regulators in statements on market volatility. What is worse, Goldstein and I found
that a high number of data scientists (many with PhDs) also get confused in real life.

It all comes from bad terminology for something non-intuitive. By a psychological phe-
nomenon called attribute substitution, some people mistake MAD for STD because the
former is easier to come to mind – this is "Lindy" as it is well known by cheaters and
illusionists.
1) MAD is more accurate in sample measurements, and less volatile than STD since it is
a natural weight whereas standard deviation uses the observation itself as its own weight,
imparting large weights to large observations, thus overweighing tail events.
2) We often use STD in equations but really end up reconverting it within the process
into MAD (say in ﬁnance, for option pricing). In the Gaussian world, STD is about 1.25
p
2 . But we adjust with stochastic volatility where STD is often as high
time MAD, that is,
as 1.6 times MAD.
3) Many statistical phenomena and processes have "inﬁnite variance" (such as the popular
Pareto 80/20 rule) but have ﬁnite, and sometimes very well behaved, mean deviations.
Whenever the mean exists, MAD exists. The reverse (inﬁnite MAD and ﬁnite STD) is
never true.
4) Many economists have dismissed "inﬁnite variance" models thinking these meant "in-
ﬁnite mean deviation". Sad, but true. When the great Benoit Mandelbrot proposed his
inﬁnite variance models ﬁfty years ago, economists freaked out because of the conﬂation.

√

It is sad that such a minor point can lead to so much confusion: our scientiﬁc tools
are way too far ahead of our casual intuitions, which starts to be a problem with science.
’The statistician cannot evade the
So I close with a statement by Sir Ronald A. Fisher:
responsibility for understanding the process he applies or recommends.’

Note

The usual theory is that if random variables X1, . . . , Xn are independent, then

var(X1 + (cid:1) (cid:1) (cid:1) + Xn) = var(X1) + (cid:1) (cid:1) (cid:1) + var(Xn).

by the linearity of the variance. But then it assumes that one cannot use another metric
2
. As we will see, for the Gaussian md(X) =
then by simple transformation make it additive
√

p s —for the Student T with 3 degrees of freedom, the factor is 2
2

p , etc.

3.5 level 2: subexponentiality

3.5 level 2: subexponentiality

51

Table 3.3: Ranking distributions

Class

Description

D

1

D

2

D

3a

D

3b

D
D

5

6

D

7

D

8

True Thin Tails

Thin tails

Conventional Thin
tails
Starter Fat Tails

Subexponential
Supercubic a

Inﬁnite Variance

Undeﬁned
Moment

First

:

(e.g.

support

Compact
Bernouilli, Binomial)
Gaussian reached organically
through summation of true thin
tails, by Central Limit; compact
support except at the limit n !
¥
Gaussian approximation of a
natural phenomenon
Higher kurtosis than the Gaus-
sian but rapid convergence to
Gaussian under summation
(e.g. lognormal)
Cramer conditions do not hold
for t > 3,
Levy
∫
e

e
Stable
(cid:0)txdF(x) = ¥
Fuhgetaboutdit

(cid:0)tx d(Fx) = ¥
a

<

∫

2

,

3.5.1 Revisiting the Rankings

Probability distributions range between extreme thin-tailed (Bernoulli) and extreme fat
tailed. Among the categories of distributions that are often distinguished due to the con-
vergence properties of moments are:

1. Having a support that is compact but not degenerate
2. Subgaussian
3. Subexponential
4. Power Law with exponent greater than 2
5. Power Law with exponent less than or equal to 2. In particular, Power Law distri-
butions have a ﬁnite mean only if the exponent is greater than 1, and have a ﬁnite
variance only if the exponent exceeds 2.

6. Power Law with exponent less than 1.

Our interest is in distinguishing between cases where tail events dominate impacts, as a
formal deﬁnition of the boundary between the categories of distributions to be considered
as mediocristan and Extremistan.

Centrally, a subexponential distribution is the cutoff between "thin" and "fat" tails. It is

deﬁned as follows.

The mathematics is crisp: the exceedance probability or survival function needs to be

exponential in one not the other. Where is the border?

2 For instance option pricing in the Black Scholes formula is done using variance, but the price maps directly
to MAD; an at-the-money straddle is just a conditional mean deviation. So we translate MAD into standard
deviation, then back to MAD

52

overview of fat tails, part i, the univariate case †

The natural boundary between mediocristan and Extremistan occurs at the subexponen-

tial class which has the following property:
Let X = (Xi)1(cid:20)i(cid:20)n be a sequence of independent and identically distributed random vari-
ables with support in (R+), with cumulative distribution function F. The subexponential
class of distributions is deﬁned by [174],[136].

(cid:3)2(x)
1 (cid:0) F
1 (cid:0) F(x)

= 2

lim
x!+¥

(3.14)

(cid:3)2 = F

′ (cid:3) F is the cumulative distribution of X1 + X2, the sum of two independent
where F
copies of X. This implies that the probability that the sum X1 + X2 exceeds a value x is twice
the probability that either one separately exceeds x. Thus, every time the sum exceeds x,
for large enough values of x, the value of the sum is due to either one or the other exceeding
x—the maximum over the two variables—and the other of them contributes negligibly.

More generally, it can be shown that the sum of n variables is dominated by the maximum
of the values over those variables in the same way. Formally, the following two properties
are equivalent to the subexponential condition [31],[62]. For a given n (cid:21) 2, let Sn = Sn
i=1xi
and Mn = max1(cid:20)i(cid:20)n xi

a) limx!¥

b) limx!¥

P(Sn>x)
P(X>x) = n,
P(Sn>x)
P(Mn>x) = 1.

Thus the sum Sn has the same magnitude as the largest sample Mn, which is another way

of saying that tails play the most important role.

Intuitively, tail events in subexponential distributions should decline more slowly than
an exponential distribution for which large tail events should be irrelevant. Indeed, one
can show that subexponential distributions have no exponential moments:

∫ ¥

0

ϵx dF(x) = +¥

e

(3.15)

for all values of # greater than zero. However,the converse isn’t true, since distributions
can have no exponential moments, yet not satisfy the subexponential condition.

We note that if we choose to indicate deviations as negative values of the variable x,
the same result holds by symmetry for extreme negative values, replacing x ! +¥ with
x ! (cid:0)¥. For two-tailed variables, we can separately consider positive and negative
domains.

3.5.2 What is a probability distribution?

The best way to ﬁgure out a probability distribution is to... invent one. In fact in the next
section, 3.5.3, we will build one that is the exact borderline between thin and fat tails by
construction.
Let s be the survival function. We have s : R ! [0, 1] that satisﬁes

lim
x!+¥

s(x)n
s(nx)

= 1,

(3.16)

and

3.5 level 2: subexponentiality

53

x!+¥ s(x) = 0
lim

x!(cid:0)¥ s(x) = 1
lim

Note : another property of the demarcation is the absence of Lucretius problem from The

Black Swan:

limx!+¥ E(x (cid:0) Kjx > K) = kK, K, k > 0 for fat tails

= k for borderline subexponential
= 0 for thin tails

(3.17)

3.5.3 Let us invent a distribution

Find functions f : R ! [0, 1] that satisfy:

lim
x!+¥

f (x)2
f (x(cid:3)) f (2x)

= 1,

′

f

(x) (cid:20) 0 8x,

and

x!+¥ f (x) = 0.
lim
x!(cid:0)¥ f (x) = 1.
lim
(when the variable is time, this describes the condition for the Lindy Effectexplain the Lindy
effect to take place). Let us assume a candidate function a sigmoid, using the hyperbolic
2 , with k 2 (0,¥). We use this as a kernel distribution (we mix
+ 1
(x):= 1
tangent F
later to modify the kurtosis).
For k >0

2 tanh

kx
p

(

)

k

lim x ! +¥ 1
2

1
2

+

tanh

)

( kx
p

= 1

and

All functions

lim x ! (cid:0)¥ 1
2

+

1
2

Tanh

]

[ kx
p

f (x) = D(

1
2

tanh

)

( kx
p

+

1
2

, x) =

]

2

xk
p

[

kSech
2p

fa(x) =

1
2

(1 (cid:0) tanh(ax)) , a > 0

(3.18)

solve our requirements. Since a is just a rescaling, we need only show this for a = 1. Let
tanh(x)2 (cid:0) 1
f (x) = f1(x). The derivative is f

(cid:20) 0 and

)

(

′

(x) = 1
2

f (x)2
f (2x)

=

2 cosh(x)2 (cid:0) 1
cosh(x)2

1
2

= 1 (cid:0)

1
2 cosh(x)2

< 1

with a limit 1 for x ! ¥. The limits of f (x) for x ! (cid:6)¥ are trivial.

54

overview of fat tails, part i, the univariate case †

Figure 3.8: Three Types of Distributions. As we hit the tails, the Student remains scalable while the Standard
Lognormal shows an intermediate position before eventually ending up getting an inﬁnite slope on a log-log
plot. But beware the lognormal as it may have some surprises (Chapter 7)
.

3.6 level 3: scalability and power laws

Now we get into the serious business.

Why power laws?
There are a lot of theories on why things should be power laws, as sort
of exceptions to the way things work probabilistically. But it seems that the opposite idea
is never presented: power should can be the norm, and the Gaussian a special case as we
will see in Chapter x (effectively the topic of Antifragile and Vol 2 of the Technical Incerto),
of concave-convex responses (sort of dampening of fragility and antifragility, bringing
robustness, hence thinning tails).

3.6.1 Scalable and Nonscalable, A Deeper View of Fat Tails

So far for the discussion on fat tails we stayed in the ﬁnite moments case. For a certain
class of distributions, those with ﬁnite moments, PX>nK
depends on n and K. For a scale-
PX>K
free distribution, with K "in the tails", that is, large enough, PX>nK
depends on n not K.
PX>K
These latter distributions lack in characteristic scale and will end up having a Paretian tail,
i.e., for x large enough, PX>x = Cx

where a is the tail and C is a scaling constant.

(cid:0)a

Note: We can see from the scaling difference between the Student and the Pareto the
conventional deﬁnition of a Power Law tailed distribution is expressed more formally as
P(X > x) = L(x)x
where L(x) is a "slow varying function", which satisﬁes the following:

(cid:0)a

for all constants t > 0.

lim
x!¥

L(t x)
L(x)

= 1

GaussianLogNormal-2Student (3)251020logx10-1310-1010-710-40.1logP>x3.6 level 3: scalability and power laws

55

Table 3.4: Scalability, comparing slowly varying functions/powerlaws to other distributions

k

2

4

6

8

10

12

14

16

18

20

P(X > k)

(cid:0)1

P(X>k)
P(X>2 k)

(Gaussian)

(Gaussian)

P(X > k)

(cid:0)1

Student(3)

P(X>k)
P(X>2 k)
Student (3)

P(X > k)

(cid:0)1

Pareto(2)

P(X>k)
P(X>2 k)
Pareto (2)

44

720

31600.
1.01 (cid:2) 109
1.61 (cid:2) 1015
1.31 (cid:2) 1023
5.63 (cid:2) 1032
1.28 (cid:2) 1044
1.57 (cid:2) 1057
1.03 (cid:2) 1072
3.63 (cid:2) 1088

5.1 (cid:2) 1010
5.5 (cid:2) 1023
9 (cid:2) 1041
9 (cid:2) 1065

fughedaboudit

fughedaboudit

fughedaboudit

fughedaboudit

fughedaboudit

14.4

71.4

216

491

940

1610

2530

3770

5350

7320

4.9

6.8

7.4

7.6

7.7

7.8

7.8

7.9

7.9

7.9

8

64

216

512

1000

1730

2740

4100

5830

8000

4

4

4

4

4

4

4

4

4

4

logP>x
logx

(

)

For x large enough,

converges to a constant, namely the tail exponent -a. A
scalable should produce the slope a in the tails on a log-log plot, as x ! ¥. Compare
to the Gaussian (with STD s and mean m) , by taking the PDF this time instead of the
2s2 x2 which goes to (cid:0)¥

f (x)
exceedance probability log
faster than (cid:0) log(x) for (cid:6)x ! ¥.
So far this gives us the intuition of the difference between classes of distributions. Only
scalable have "true" fat tails, as others turn into a Gaussian under summation. And the tail
exponent is asymptotic; we may never get there and what we may see is an intermediate
version of it. The ﬁgure above drew from Platonic off-the-shelf distributions; in reality
processes are vastly more messy, with switches between exponents.

= (x(cid:0)m)2
2s2

2p) (cid:25) (cid:0) 1

(cid:0) log(s

p

3.6.2 Grey Swans

Why do we use Student T to simulate symmetric power laws?
For convenience, only
for convenience. It is not that we believe that the generating process is Student T. Simply,
the center of the distribution does not matter much for the properties involved in certain
classes of decision making.

The lower the exponent, the less the center plays a role. The higher the exponent, the more

the student T resembles the Gaussian, and the more justiﬁed its use will be accordingly.

More advanced methods involving the use of Levy laws may help in the event of asym-
metry, but the use of two different Pareto distributions with two different exponents, one
for the left tail and the other for the right one would do the job (without unnecessary
complications).

Estimation issues Note that there are many methods to estimate the tail exponent a
from data, what is called a "calibration. However, we will see, the tail exponent is rather

56

overview of fat tails, part i, the univariate case †

Figure 3.9: The Grey Swan
of Brexit when seen using a
power law.

Figure 3.10: Book Sales:
the
near tail can be robust for es-
timation of sales from rank
and vice versa –it works well
and shows robustness so long
as one doesn’t compute gen-
eral expectations or higher non-
truncated moments.

Figure 3.11: The Turkey Prob-
lem, where nothing in the past
properties seems to indicate the
possibility of the jump.

hard to guess, and its calibration marred with errors, owing to the insufﬁciency of data in
the tails. In general, the data will show thinner tail than it should.

We will return to the issue in Chapter ??.

Brexit,entirely consistent with s statistical properties0.010.020.030.040.050.060.070.08|X}0.0010.0050.0100.0500.1000.5001P>Xα=����������������-���������������>�2004006008001000-50-40-30-20-10103.7 bell shaped vs non bell shaped power laws

57

3.7 bell shaped vs non bell shaped power laws

The slowly varying function effect, a case study
The fatter the tails, the less the "body"
matters for the moments (which become inﬁnite, eventually). But for power laws with
thinner tails, the zone that is not power law (the slowly moving part) plays a role –"slowly
varying" is more formally deﬁned in 15.2.2 and 3.6.1. This section will show how appar-
ently equal distributions can have different shapes.

Let us compare a double Pareto distribution with the following PDF:

8
><

>:

fP(x) =

a(1 + x)

(cid:0)a(cid:0)1

x (cid:21) 0

a(1 (cid:0) x)

(cid:0)a(cid:0)1

x < 0

to a Student T with same centrality parameter 0, scale parameter s and PDF fS(x) =
aa=2

2 ((cid:0)a(cid:0)1)

) 1

∫

(

where B(.) is the Euler beta function, B(a, b) = (G(a))(G(b))
G(a+b)

=

1

0 ta(cid:0)1(1 (cid:0)

a+ x2
s2
sB( a
2 )
2 , 1
t)b(cid:0)1 dt.
We have two ways to compare distributions.
(cid:15) Equalizing by tail ratio: setting limx!¥
(

equivalent "tail" distribution with s =

f p(x)
fs(x) = 1 to get the same tail ratio, we get the
a1(cid:0) a

))

1=a

(

2 B

a
2 , 1
2

.

(cid:15) Equalizing by standard deviations (when ﬁnite): we have, with a > 2, E(X2

P) =

(

a

a1(cid:0) a

2 )
2 , 1

2 B( a
a(cid:0)2

)

2=a

.

2

a2(cid:0)3a+2 and E(X2
√

S) =

So we could set

√

p

k

E(X2

P) =

E(X2

S) k ! 2a(cid:0)2=a

B( a
2 )
2 , 1
a(cid:0)1

(cid:0)2=a

}

.

Finally, we have the comparison "bell shape" semi-concave vs the angular double-convex
one as seen in Fig. 3.12.

Figure 3.12: Comparing two
symmetric power laws of same
exponent, one with a brief
slowly varying function, the
other with an extended one.
All moments eventually be-
come the same in spite of
the central differences in their
shape for small deviations.

-4-224x0.51.01.52.02.53.0PDFfp(.)fs(.)4 O V E R V I E W O F FAT TA I L S , PA R T 2

( H I G H E R D I M E N S I O N S ) †

This discussion is about as simpliﬁed as possible handling of higher dimen-

sions. We will look at 1) the simple effect of fat-tailedness for multiple
random variables, 2) Ellipticality and distributions, 3) random matrices and
the associated distribution of eigenvalues, 4) How we can look at covariance
and correlations when moments don’t exist (say, as in the Cauchy case).

Figure 4.1: Fat tails in higher dimensions: For a 3 dimentional vector, thin tails (left) and fat tails (right) of
the same variance. In place of a bell curve with higher peak (the "tunnel") of the univariate case, we see an
increased density of points towards the center.

4.1 fat tails in higher dimension, finite moments

⇀
X = (X1, X2, . . . , Xm) be a p (cid:2) 1 random vector with the variables assumed to be drawn
Let
from a multivariate Gaussian. Consider the joint probability distribution f (x1, . . . , xm) .

59

-202-202-202-4-2024-4-202-4-202460

overview of fat tails, part 2 (higher dimensions) †

We denote the m-variate multivariate Normal distribution by N (0, S), with mean vector
⇀m , variance-covariance matrix S, and joint pdf,
(

(

)

⇀
x

f

= (2p)

(cid:0)m=2jSj(cid:0)1=2exp

(cid:0) 1
2

(

)

(

⇀

x (cid:0) ⇀m

T

S(cid:0)1

⇀

x (cid:0) ⇀m

))

(4.1)

where

⇀
x = (x1, . . . , xm) 2 Rm, and S is a symmetric, positive deﬁnite (m (cid:2) m) matrix.
We can apply the same simplied variance preserving heuristic as in 3.1.1 to fatten the
tails:

)

(

⇀
x

fa

=

1
2

(2p)

(cid:0)m=2jS

j(cid:0)1=2exp

1

(

(cid:0) 1
2

(

⇀

x (cid:0) ⇀m

)

T

S
1
(

(

(cid:0)1

+

1
2

(2p)

(cid:0)m=2jS

j(cid:0)1=2exp

2

(cid:0) 1
2

))

⇀

x (cid:0) ⇀m
)

⇀

x (cid:0) ⇀m

(

T

(cid:0)1

S
2

(

⇀

x (cid:0) ⇀m

))

(4.2)

where a is a scalar that determines the intensity of stochastic volatility, S
2 = S(1 (cid:0) a).
S

1

1 = S(1 + a) and

Figure 4.2: Elliptically Contoured
Joint Returns of Powerlaw (Student
T)

Notice in Figure 4.1, as with the one-dimensional case, a concentration in the middle part
of the distribution

4.2 joint fat-tailedness and ellipticality of distributions

There is another aspect, beyond our earlier deﬁnition(s) of fat-tailedness, once we increase
the dimensionality into random vectors:

From the deﬁnition in [66], X, a p (cid:2) 1 random vector
What is an Elliptical Distribution?
is said to have an elliptical (or elliptical contoured) distribution with location parameters

1 We can simplify by assuming as we did in the single dimension case, without any loss of generality, that

⇀m =

(0, . . . , 0).

4.2 joint fat-tailedness and ellipticality of distributions

61

Figure 4.3: NonElliptical Joint Re-
turns, from stochastic correlations

Figure 4.4: Elliptically Contoured
Joint Returns for for a multivariate
distribution (x, y, z) solving to the
same density.

m, a non-negative matrix S and some scalar function Y if its characteristic functionis of the
form exp(it

′m)Y(tSt

).

′

Intuitively an elliptical distribution should show an ellipse for iso-density plots when
represented in 2-D (for a bivariate) and 3-D (for a trivariate) as in Figures 4.2 and 4.4; a
noneliptical would violate the shape as in Figures 4.3 and 4.5.
The main property of the class of elliptical distribution is that it is closed under linear
transformation. This leads to attractive properties in the building of portfolios, and in the
results of portfolio theory (in fact one cannot have portfolio theory without ellitipcality of
distributions). Under ellipticality, all portfolios can be characterized completely by their

62

overview of fat tails, part 2 (higher dimensions) †

Figure 4.5: NonElliptical
Joint
Returns,
from stochastic correla-
tions, for a multivariate distribution
(x, y, z), solving to the same density.

Figure 4.6: History moves by jumps:
A fat tailed historical process, in which
events are distributed according to a
power
the
"80/20", with a ≃ 1.13, the equivalent
of a 3-D Brownian motion.

corresponds

law that

to

location and scale and any two portfolios with identical location and scale (in return space)
have identical distributions returns.

Note that (ironically) Lévy-Stable distributions are elliptical –but only in the way they are

deﬁned.

4.2 joint fat-tailedness and ellipticality of distributions

63

Figure 4.7: What the proponents of "great modera-
tion" or "long peace" have in mind: history as a thin-
tailed process.

4.2.1 Ellipticality and Independence for Fat Tails

Take the product of two Cauchy densities for x and y (what we used in Fig. 2.1):

f (x) f (y) =

1
p2 (x2 + 1) (y2 + 1)

(4.3)

which, patently, as we saw in Chapter 2 (with the example of the two randomly selected
persons with a total net worth of $36 million), is not elliptical. Compare to the joint
distribution fr(x, y):

fr(x, y) =

√

2p

(

(

1 (cid:0) r2

y

y
1(cid:0)r2

1
)

(cid:0) rx
1(cid:0)r2

(

+ x

x
1(cid:0)r2

(cid:0) ry
1(cid:0)r2

)

3=2

,

)

+ 1

and setting r = 0 to get no correlation,

f0(x, y) =

1
2p (x2 + y2 + 1)3=2

(4.4)

(4.5)

which is elliptical. This shows that absence of correlation is not independence as:

64

overview of fat tails, part 2 (higher dimensions) †

(a) Gaussian

(b) Stoch Vol

(c) Student 3/2

(d) Cauchy

Figure 4.8: The various shapes of the distribution of the eigenvalues for random matrices, which in the Gaus-
sian case follow the Wigner semi-circle distribution. The Cauchy case corresponds to the Student parametrized
to have 1 degrees of freedom

Independence between two variables X and Y is deﬁned by:

f (x, y)
f (x)

= 1,

regardless of the correlation coefﬁcient.
bivariate Gaussian with coefﬁcient 0 is both independent and uncorrelated.

In the class of elliptical distributions, the

The reason the multivariate stable distribution with correlation coefﬁcient set to 0 is not

independent is the following.

′

A random vector X = (X1, . . . , Xk)
is said to have the multivariate stable distribution if
every linear combination of its components Y = a1X1 + (cid:1) (cid:1) (cid:1) + akXk has a stable distribution.
That is, for any constant vector a 2 Rk, the random variable Y = aTX should have a
univariate stable distribution. And to have a linear combination remain within the same
class requires ellipticality. Hence by construction, f0(x, y) is not necessarily equal to f (x) f (y).
Consider the Cauchy case that has an explicit density function. The denominator of the
product of densities includes an additional term, x2y2, which pushes the iso-densities in
one direction or another.

-1000100200Gaussian-2000200400p=10-4a=9998-100000-50000050000100000StudentTDistribution32-6×108-4×108-2×10802×1084×1086×108StudentTDistribution[1]4.3 fat tails and random matrices, a rapid interlude

65

4.3 fat tails and random matrices, a rapid interlude

The eigenvalues of matrices themselves have an analog to Gaussian convergence: the semi-
circle distribution.
Let M be a (n, n) symmetric matrix. We have the eivenvalues l
M.Vi = l
The Wigner semicircle distribution with support [(cid:0)R, R] has for PDF f presenting a
semicircle of radius R centered at (0, 0) and then suitably normalized :

iVi where Vi is the ith eigenvector.

i, 1 (cid:20) i, (cid:20) n such that

f (l) =

√

2
pR2

R2 (cid:0) l2 for (cid:0) R (cid:20) l (cid:20) R.

(4.6)

This distribution arises as the limiting distribution of eigenvalues of (n, n) symmetric

matrices with ﬁnite moments as the size n of the matrix approaches inﬁnity.

We will tour the "fat-tailedness" of the random matrix in what follows as well as the

convergence.

This is the equivalent of fat tails for matrices. Consider for now that the 4th moment
reaching Gaussian levels (i.e. 3) for an univariate situation is equivalent to the eigenvalues
reaching Wigner’s semicircle.

4.4 multivariate scale

Let X be a (p (cid:2) 1) vector following a multivariate Student T distribution, X (cid:24) St (M, S, a),
where S is a (p (cid:2) p) matrix, M a p length vector and a a Paretian tail exponent with PDF

(

f (X) =

(X (cid:0) M).S(cid:0)1.(X (cid:0) M)
n

+ 1

)(cid:0) 1

2 (n+p)

.

In the most simpliﬁed case, with p = 2, M = (0, 0), and S = =

(

√

n

1 (cid:0) r2

(cid:0)nr2+n(cid:0)2rx1x2+x2

1+x2
2

n(cid:0)nr2

f (x1, x2) =

2p (n (cid:0) nr2)

)

(

)

1
r

r

1

(cid:0) n
2

(cid:0)1

.

(4.7)

(4.8)

4.5 correlation and undefined variance

insert section on Student/Cauchy, also show proof of ﬁniteness of correlation

4.5.1 Finiteness of Correlation

Question: Why it is that a fat tailed distribution in the power law class P with inﬁnite or
undeﬁned mean (and higher moments) would have, in higher dimensions, undeﬁned (or
inﬁnite) covariance but ﬁnite correlation?

66

overview of fat tails, part 2 (higher dimensions) †

Consider a distribution with support in ((cid:0)¥, ¥). It has no moments: E(X) is indetermi-
nate, E(X2) = ¥, no covariance, E(XY) is indeterminate. But the (noncentral) correlation
for n variables is bounded by (cid:0)1 and 1.

r ≜

√

(cid:229)n

i=1 xiyi
√
(cid:229)n

(cid:229)n

i=1 x2
i

i=1 y2
i

, n = 2, 3, ...

By the subexponentiality property, we have P (X1 + . . . + Xn) > x (cid:24) P (max (X1, . . . Xn) > x)
as x ! ¥. We note that the power law class is included in the subexponential class S.
Order the variables in absolute values such that jx1
2 = (cid:229)n(cid:0)1
Let k
i=1 y2
i .

j (cid:20) jx2j (cid:20) . . . (cid:20) jxnj

i=1 xiyi, k

1 = (cid:229)n(cid:0)1

3 = (cid:229)n(cid:0)1

i , and k

i=1 x2

√

lim
xn!¥

√

lim
yn!¥

yn√
k

3 + y2
n

xn√
k

2 + x2
n

3

=

xnyn + k
1
√
n + k
n + k
y2
x2
2
xnyn + k
1
√
n + k
n + k
y2
x2
xn!+¥,yn!+¥ = 1
xn!+¥,yn!(cid:0)¥ = (cid:0)1

lim

lim

=

2

3

and

for all values of n (cid:21) 2.

xn!(cid:0)¥,yn!+¥ = (cid:0)1

lim

4.5.2 Example of ﬁniteness of Correlation

,

.

Figure 4.9: Sample distribu-
tion of correlation for a sam-
ple of 103. The correlation ex-
ists for a bivariate T distribu-
tion (exponent 2
3 , correlation
3
4 ) but... not useable

-0.50.00.51.0ρ0246810frequency4.5 correlation and undefined variance

67

further reading

Pitman [136], Embrechts and Goldie (1982) [61]Embrechts (1979
For subexponentiality:
which seems to be close to his doctoral thesis), [62], Chistyakov (1964) [31], Goldie (1978)
[81], Pitman [136], Teugels [174], and, more general, [63].

5 T H E E M P I R I C A L D I S T R I B U T I O N I S

N O T E M P I R I C A L

remove and merge

Figure 5.1: The base rate fal-
lacy,
rather
revisited —or,
The
in the other direction.
"base rate" is an empirical
evaluation that bases itself on
the worst past observations,
an error identiﬁed in [158]
as the fallacy identiﬁed by
the Roman poet Lucrecius in
De rerum natura of
thinking
the
future mountain
equals the tallest on has pre-
viously seen. Quoted without
permission after warning the
author.

tallest

There is a prevalent confusion about the nonparametric empirical distribution

based on the following powerful property: as n grows, the errors around the
empirical histogram for cumulative frequencies are Gaussian regardless of the
base distribution, even if the true distribution is fat-tailed (assuming inﬁnite
support). For the CDF (or survival functions) are both uniform on [0, 1], and,
n (Fn(x) (cid:0) F(x)) (Fn is the observed CDF
further, by the Donsker theorem, the sequence
or survival function for n summands, F the true CDF or survival function) converges in
distribution to a Normal Distribution with mean 0 and variance F(x) (1 (cid:0) F(x)) (one may
ﬁnd even stronger forms of convergence via the Glivenko– Cantelli theorem).

p

Owing to this remarkable property, one may mistakenly assume that the effect of tails of
the distribution converge in the same manner independently of the distribution. Further,
and what contributes to the confusion, the variance, F(x) (1 (cid:0) F(x)) for both empirical CDF
and survival function, drops at the extremes.

69

70

the empirical distribution is not empirical

Figure 5.2: The high watermark: the level of ﬂooding
in Paris in 1910 as a maxima. Clearly one has to
consider that such record will be topped some day in
the future and proper risk management consists in
"how much" more than such a level one should seek
protection.

In truth, and that is a property of extremes, the error effectively increases in the tails if

one multiplies by the deviation that corresponds to the probability.
Let cn be the difference between the empirical and the distributional conditional mean,
deﬁned as:

cn =

n
(cid:229)
i=1

xi

1xi

(cid:21)K

(cid:0)

∫ ¥

K

xdF(x)

= K(Fn(K) (cid:0) F(K)) +

(

xmax
d

(cid:229)
i

¯Fn(K + (i + 1)d) (cid:0) ¯Fn(K + idK) (cid:0)

∫

K+(i+1)d

K+id

)

d ¯F(K)

(cid:0)

∫ ¥

xmax

dF(x),

(5.1)

(cid:0)1(0), that is where the distribution is truncated. cn recovers the dispersion
where xmax = ¯Fn
of the distribution of x which remains fat tailed. Another way to see it is that for fat tailed
variables, probabilities are more stable than their realizations and, more generally, the
lowest moment will always disproportionately be the most stable one.

the empirical distribution is not empirical

71

Biases of the empirical method under Fat Tails

We note that, owing of the convergence to the Gaussian, by Donsker’s theorem:

cn =

∫ ¥

xmax

dF(x) + O

(

)

F(x) (1 (cid:0) F(x))
p
n

so, for sufﬁciently large (but not too large) n,
∫ ¥

cn (cid:25)

dF(x)

xmax

(5.2)

(5.3)

yet, under a Paretian regime, xmax is distributed according to a Fréchet, as we will see
in Section TK.

Theorem 5.1
For an empirical distribution with a sample size n, the underestimation of the conditional tail
expectation cn for a Paretian with scale L and tail index a is:

φ(c, n) =

) 1
a(cid:0)1

(

a (cid:0) 1
a

(

((

a2
1(cid:0)a +1c 1
a(cid:0)1

nL

exp

) a
a(cid:0)1

a (cid:0) 1
a

(

)

))

a2
1(cid:0)a +1

(cid:0)L

n

a
a(cid:0)1

c

(5.4)

and its expectation

E(cn) = G

(

)

a (cid:0) 1
a

a+ 1

a (cid:0)1n

L

a (cid:0)1
1

Proof. The maximum of n variables is in the MDA (Maximum domain of attraction) of
Fréchet with scale b = (Ln)1=a
. We have the conditional expectation > c: E(x)jx>cP(x >
. Randomizing c and doing a probability transformation we get the density
c) =
φ(.).

ac1(cid:0)a
a(cid:0)1

aL

A E C O N O M E T R I C S I M A G I N E S

F U N C T I O N S I N L2 S PA C E †

There is something wrong with econometrics, as almost all papers don’t

replicate in the real world. Two reliability tests in Chapter 8, one about
parametric methods the other about robust statistics, show that there is
something rotten in econometric methods, fundamentally wrong, and
that the methods are not dependable enough to be of use in anything
remotely related to risky decisions. Practitioners keep spinning inconsistent ad hoc
statements to explain failures.
This is a brief nontechnical exposition from the results in [159].

With economic variables one single observation in 10,000, that is, one single day in 40
years, can explain the bulk of the "kurtosis", the ﬁnite-moment standard measure of "fat
tails", that is, both a measure how much the distribution under consideration departs from
the standard Gaussian, or the role of remote events in determining the total properties.
For the U.S. stock market, a single day, the crash of 1987, determined 80% of the kurtosis
for the period between 1952 and 2008. The same problem is found with interest and
exchange rates, commodities, and other variables. Redoing the study at different periods
with different variables shows a total instability to the kurtosis. The problem is not just
that the data had "fat tails", something people knew but sort of wanted to forget; it was
that we would never be able to determine "how fat" the tails were within standard methods.
Never.

The implication is that those tools used in economics that are based on squaring variables
(more technically, the L2 norm), such as standard deviation, variance, correlation, regres-
sion, the kind of stuff you ﬁnd in textbooks, are not valid scientiﬁcally (except in some rare
cases where the variable is bounded). The so-called "p values" you ﬁnd in studies have no
meaning with economic and ﬁnancial variables. Even the more sophisticated techniques
of stochastic calculus used in mathematical ﬁnance do not work in economics except in
selected pockets.

A.0.1 Performance of Standard Parametric Risk Estimators

The implication is that those tools used in economics that are based on squaring variables
(more technically, the Euclidian, or ℓ2 norm), such as standard deviation, variance, correla-
tion, regression, the kind of stuff you ﬁnd in textbooks, are not valid scientiﬁcally(except in
some rare cases where the variable is bounded). The so-called "p values" you ﬁnd in stud-
ies have no meaning with economic and ﬁnancial variables. Even the more sophisticated

73

74

econometrics imagines functions in l2 space †

Table A.1: Maximum contribution to the fourth moment from a single daily observation

Security

Max Q Years.

0.94
Silver
0.79
SP500
0.79
CrudeOil
0.75
Short Sterling
0.74
Heating Oil
0.72
Nikkei
0.54
FTSE
0.48
JGB
Eurodollar Depo 1M 0.31
0.3
Sugar #11
0.27
Yen
0.27
Bovespa
Eurodollar Depo 3M 0.25
0.25
CT
0.2
DAX

46.
56.
26.
17.
31.
23.
25.
24.
19.
48.
38.
16.
28.
48.
18.

techniques of stochastic calculus used in mathematical ﬁnance do not work in economics
except in selected pockets.

The results of most papers in economics based on these standard statistical methods are
thus not expected to replicate, and they effectively don’t. Further, these tools invite foolish
risk taking. Neither do alternative techniques yield reliable measures of rare events, except
that we can tell if a remote event is underpriced, without assigning an exact value.
From [159]), using log returns, Xt := log
quartic observation Max(Xt(cid:0)i∆t
variations over n samples and frequency ∆t.

i=0. Let Q(n) be the contribution of the maximum quartic

. Consider the n-sample maximum

P(t)
P(t(cid:0)i∆t)

4)n

(

)

Q(n) :=

(

Max
(cid:229)n

X4
i=0 X4

t(cid:0)i∆t)n
i=0
t(cid:0)i∆t

Note that for our purposes, where we use central or noncentral kurtosis makes no differ-

ence –results are nearly identical.

)

(

For a Gaussian (i.e., the distribution of the square of a Chi-square distributed variable)
the maximum contribution should be around .008 (cid:6) .0028. Visibly we can

show Q
see that the observed distribution of the 4th moment has the property

104

(

(

)

P

X > max(x4

i )i(cid:20)2(cid:20)n

(cid:25) P

X >

)

n
(cid:229)
i=1

x4
i

Recall that, naively, the fourth moment expresses the stability of the second moment.

And the second moment expresses the stability of the measure across samples.

Note that taking the snapshot at a different period would show extremes coming from
other variables while these variables showing high maximma for the kurtosis, would drop,
a mere result of the instability of the measure across series and time.

econometrics imagines functions in l2 space †

75

Description of the dataset All tradable macro markets data available as of August 2008,
with "tradable" meaning actual closing prices corresponding to transactions (stemming
from markets not bureaucratic evaluations, includes interest rates, currencies, equity in-
dices).

Figure A.1: Max
quartic
across securities in Table A.1.

Figure A.2: Kurtosis across
nonoverlapping periods for Eu-
rodeposits.

Figure A.3: Monthly deliv-
ered volatility in the SP500
(as measured by standard de-
viations). The only structure
it seems to have comes from
the fact that it is bounded at 0.
This is standard.

0.00.20.40.60.8ShareofMaxQuartic010203040EuroDepo3M:AnnualKurt1981-20080.20.40.60.8MonthlyVol76

econometrics imagines functions in l2 space †

Figure A.4: Montly volatility
of volatility from the same
dataset
pre-
dictably unstable.

in Table A.1,

A.0.2 Performance of Standard NonParametric Risk Estimators

Does the past resemble the future in the tails? The following tests are nonparametric, that
is entirely based on empirical probability distributions.

Figure A.5: Comparing one
absolute deviation M[t] and
the subsequent one M[t+1]
over a certain threshold (here
4% in stocks); illustrated how
large deviations have no (or
few) predecessors, and no (or
few) successors– over the past
50 years of data.

Figure A.6: The "regular" is
predictive of the regular, that
is mean deviation.
Com-
paring one
absolute devia-
tion M[t] and the subsequent
one M[t+1] for macroeconomic
data.

0.000.050.100.150.20VolofVolConcentration of tail events without predecessorsConcentration of tail events without successors0.00010.00020.00030.00040.0005M@tD0.00010.00020.00030.0004M@t+1D0.0050.0100.0150.0200.0250.030M@tD0.0050.0100.0150.0200.0250.030M@t+1Deconometrics imagines functions in l2 space †

77

So far we stayed in dimension 1. When we look at higher dimensional properties, such
as covariance matrices, things get worse. We will return to the point with the treatment of
model error in mean-variance optimization.
When xt are now in RN, the problems of sensitivity to changes in the covariance matrix
makes the estimator M extremely unstable. Tail events for a vector are vastly more difﬁcult
to calibrate, and increase in dimensions.

Figure A.7: Correlations are also problematic, which ﬂows from the instability of single variances and the
effect of multiplication of the values of random variables.

The Responses so far by members of the economics/econometrics establishment No
answer as to why they still use STD, regressions, GARCH , value-at-risk and similar meth-
ods.

Peso problem Benoit Mandelbrot used to insist that one can ﬁt anything with Poisson
jumps.

1
Many researchers invoke "outliers" or "peso problem"

as acknowledging fat tails (or the
role of the tails for the distribution), yet ignore them analytically (outside of Poisson mod-
els that are not possible to calibrate except after the fact: conventional Poisson jumps are
thin-tailed). Our approach here is exactly the opposite: do not push outliers under the rug,
rather build everything around them. In other words, just like the FAA and the FDA who
deal with safety by focusing on catastrophe avoidance, we will throw away the ordinary
under the rug and retain extremes as the sole sound approach to risk management. And
this extends beyond safety since much of the analytics and policies that can be destroyed
by tail events are inapplicable.

Peso problem confusion about the Black Swan problem :

"Black Swans" (Taleb, 2007). These cultural icons refer to disasters that occur
"(...)
so infrequently that they are virtually impossible to analyze using standard statistical

1 The peso problem is a discovery of an outlier in money supply, became a name for outliers and unexplained

behavior in econometrics.

78

econometrics imagines functions in l2 space †

inference. However, we ﬁnd this perspective less than helpful because it suggests a state
of hopeless ignorance in which we resign ourselves to being buffeted and battered by
the unknowable."

Andrew Lo, who obviously did not bother to read the book he was citing.

Lack of skin in the game.
Indeed one wonders why econometric methods keep being
used while being wrong, so shockingly wrong, how "University" researchers (adults) can
partake of such acts of artistry. Basically these capture the ordinary and mask higher order
effects. Since blowups are not frequent, these events do not show in data and the researcher
looks smart most of the time while being fundamentally wrong. At the source, researchers,
"quant" risk manager, and academic economist do not have skin in the game so they are
not hurt by wrong risk measures: other people are hurt by them. And the artistry should
continue perpetually so long as people are allowed to harm others with impunity. (More
in Taleb and Sandis [173], Taleb [167] ).

B S P E C I A L C A S E S O F FAT TA I L S

Figure B.1: A coffee cup is
less likely to incur "small"
than large harm.
It shat-
ters, hence is exposed to (al-
most) everything or nothing.
is
The same type of payoff
prevalent
in markets with,
say, (reval)devaluations, where
small movements beyond a bar-
rier are less likely than larger
ones.

For monomodal distributions, fat tails are the norm: one can look at tens of thousands
of time series of the socio-economic variables without encountering a single episode
of "platykurtic" distributions. But for multimodal distributions, some surprises can
occur.

Figure B.2: Negative (relative)
kurtosis and bimodality (3 is
the Gaussian).

79

timeLow Probability       Region-100-80-60-40-200condition-10-5510μ1-μ21.52.02.53.0Kurtosis80

special cases of fat tails

b.1 multimodality and fat tails, or the war and peace model

We noted earlier in 3.1.1 that stochasticizing, ever so mildly, variances, the distribution
gains in fat tailedness (as expressed by kurtosis). But we maintained the same mean.

But should we stochasticize the mean as well (while preserving the initial average), and
separate the potential outcomes wide enough, so that we get many modes, the "kurtosis"
(as measured by the fourth moment) would drop. And if we associate different variances
with different means, we get a variety of "regimes", each with its set of probabilities.

Either the very meaning of "fat tails" loses its signiﬁcance under multimodality, or takes
on a new one where the "middle", around the expectation ceases to matter.[5, 106].
Now, there are plenty of situations in real life in which we are confronted to many pos-
sible regimes, or states. Assuming ﬁnite moments for all states, consider the following
structure: s1 a calm regime, with expected mean m1 and standard deviation s
1, s2 a violent
regime, with expected mean m2 and standard deviation s
2, or more such states. Each state
has its probability pi.

Now take the simple case of a Gaussian with switching means and variance: with proba-
bility 1

1) and with probability 1

2). The kurtosis will be

1, s

2 , X (cid:24) N (m

2 , X (cid:24) N (m
2, s
(

(

(

2

(m

1

(cid:0) m

2) 4 (cid:0) 6
(

s2
1

(cid:0) s2
2
))

(m

1

(cid:0) m

2)2 + 2

1 + s2
s2

2

)

)

2

Kurtosis = 3 (cid:0)

2

(B.1)

As we see the kurtosis is a function of d = m
2 ,
1
the kurtosis will be below that of the regular Gaussian, and our measure will naturally be
negative. In fact for the kurtosis to remain at 3,

2. For situations where s

1 = s

2, m

1

̸= m

(cid:0) m

√

p
jdj= 4

6

max(s

1, s

2)2 (cid:0) min(s

1, s

2)2,

the stochasticity of the mean offsets the stochasticity of volatility.

Assume, to simplify a one-period model, as if one was standing in front of a discrete
slice of history, looking forward at outcomes. (Adding complications (transition matrices
between different regimes) doesn’t change the main result.)
The characteristic function ϕ(t) for the mixed distribution becomes:

ϕ(t) =

N
(cid:229)
i=1

pie

(cid:0) 1

2 t2s2

i +itmi

For N = 2, the moments simplify to the following:

M1 = p1m1 + (1 (cid:0) p1) m2
(
(

)

)

M2 = p1

m2

1 + s2
1

+ (1 (cid:0) p1)

(

2 + s2
2

m2
)

M3 = p1m3

1 + (1 (cid:0) p1) m2

m2

2 + 3s2
2
(

+ 3m1 p1

s2
1

)

)

(

M4 = p1

6m2
1

s2
1 + m4

1 + 3s4
1

+ (1 (cid:0) p1)

6m2
2

s2
2 + m4

2 + 3s4
2

b.1 multimodality and fat tails, or the war and peace model

81

Let us consider the different varieties, all characterized by the condition p1 < (1 (cid:0) p1),
m1 < m2, preferably m1 < 0 and m2 > 0, and, at the core, the central property: s

1 > s
2.

Variety 1: War and Peace. Calm period with positive mean and very low volatility, tur-
moil with negative mean and extremely low volatility.

Figure B.3: The War
and
peace model. Kurtosis K=1.7,
much lower than the Gaussian.

Variety 2: Conditional deterministic state
Take a bond B, paying interest r at the end of
a single period. At termination, there is a high probability of getting B(1 + r), a possibility
of defaut. Getting exactly B is very unlikely. Think that there are no intermediary steps
between war and peace: these are separable and discrete states. Bonds don’t just default
"a little bit". Note the divergence, the probability of the realization being at or close to
the mean is about nil. Typically, p(E(x)) the probabilitity densities of the expectation
are smaller than at the different means of regimes, so P(x = E(x)) < P (x = m1) and
< P (x = m2), but in the extreme case (bonds), P(x = E(x)) becomes increasingly small.
The tail event is the realization around the mean.

Figure B.4: The Bond pay-
off/Currency peg model. Ab-
sence of volatility stuck at the
peg, deterministic payoff
in
regime 2, mayhem in regime 1.
Here the kurtosis K=2.5. Note
that the coffee cup is a special
case of both regimes 1 and 2 be-
ing degenerate .

The same idea applies to currency pegs, as devaluations cannot be "mild", with all-or-
nothing type of volatility and low density in the "valley" between the two distinct regimes.

S1S2PrS2S1Pr82

special cases of fat tails

Figure B.5: Pressure on the
peg which may give a Dirac
PDF in the "no devalu-
ation" regime (or, equiva-
lently,low volatility).
It is
typical for ﬁnance imbeciles
to mistake regime S2 for low
volatility.

With option payoffs, this bimodality has the effect of raising the value of at-the-money
options and lowering that of the out-of-the-money ones, causing the exact opposite of the
so-called "volatility smile".

Note the coffee cup has no state between broken and healthy. And the state of being
broken can be considered to be an absorbing state (using Markov chains for transition
probabilities), since broken cups do not end up ﬁxing themselves.
Nor are coffee cups likely to be "slightly broken", as we see in ﬁgure B.1.

A brief list of other situations where bimodality is encountered:

1. Currency pegs
2. Mergers
3. Professional choices and outcomes
4. Conﬂicts: interpersonal, general, martial, any situation in which there is no interme-

diary between harmonious relations and hostility.

5. Conditional cascades

b.2 transition probabilites: what can break will break

So far we looked at a single period model, which is the realistic way since new information
may change the bimodality going into the future: we have clarity over one-step but not
more. But let us go through an exercise that will give us an idea about fragility. Assuming
the structure of the model stays the same, we can look at the longer term behavior under
transition of states. Let P be the matrix of transition probabilitites, where pi,jis the transi-
tion from state i to state j over ∆t, (that is, where S(t) is the regime prevailing over period
t, P

S(t + ∆t) = sj

S(t) = sj

))

(cid:12)
(cid:12)

(

(

P =

)

p1,1
p1,2

p2,1
p2,2

Pn =

(

)

an
cn

bn
dn

After n periods, that is, n steps,

Where

b.2 transition probabilites: what can break will break

83

(p1,1

an =

(cid:0) 1) (p1,1 + p2,2
p1,1 + p2,2

(cid:0) 1) n + p2,2
(cid:0) 2

(cid:0) 1

bn =

cn =

(1 (cid:0) p1,1) ((p1,1 + p2,2

(cid:0) 1) n (cid:0) 1)

p1,1 + p2,2

(cid:0) 2

(1 (cid:0) p2,2) ((p1,1 + p2,2

(cid:0) 1) n (cid:0) 1)

p1,1 + p2,2
(cid:0) 1) (p1,1 + p2,2
p1,1 + p2,2

(cid:0) 2
(cid:0) 1) n + p1,1
(cid:0) 2

(cid:0) 1

(p2,2

dn =

The extreme case to consider is the one with the absorbing state, where p1,1 = 1, hence
(replacing pi,̸=iji=1,2 = 1 (cid:0) pi,i).

and the "ergodic" probabilities:

(

Pn =

)

1
1 (cid:0) pN
2,2

0
pN
2,2

(

)

n!¥ Pn =
lim

1
1

0
0

The implication is that the absorbing state regime 1 S(1) will end up dominating with
probability 1: what can break and is irreversible will eventually break.
With the "ergodic" matrix,

n!¥ Pn = p.1T
lim

where 1T is the transpose of unitary vector f1,1g, p the matrix of eigenvectors.
(

(

)

The eigenvalues become l =

1

p1,1 + p2,2

(cid:0) 1

and associated eigenvectors p=

)

.

1
1

1
1(cid:0)p1,1
1(cid:0)p2,2

C P S E U D O - S T O C H A S T I C V O L AT I L I T Y: A

C A S E S T U D Y

remove and merge

Figure C.1: Running 22-day (i.e., corresponding to monthly) realized volatility (standard deviation) for a
Student T distributed returns sampled daily. It gives the impression of stochastic volatility when in fact the
scale of the distribution is constant.

Fig. C.1 shows the volatility of returns of a market that greatly resemble ones should one
use a standard simple stochastic volatility process. By stochastic volatility we assume the
variance is distributed randomly (although the expressions are usually about the standard
deviation, modeling is off the variance—note that the two have different expectations).
Let X be the returns with mean 0 and scale s, with PDF φ(.):

(

) a+1
2

a
a+ x2
s2
(

asB

φ(x) =

p

) , x 2 ((cid:0)¥, ¥).

a
2 , 1
2

85

5001000150020002500t20406080100σ2286

pseudo-stochastic volatility: a case study

Transforming to get Y = X2 (to get the distribution of the variance), y, the PDF for Y
becomes,

) a+1
2

(

as2
as2+y
(
a
2 , 1
2

y(y) =

sB

) p

ay

, y 2 ((cid:0)¥, ¥),

which we can see transforms into a power law with asymptotic tail exponent
characteristic function wcy = E(exp(iwY)) can be written as
( p
)(cid:0) a

√

(

(

a
2 . The

))

2

((cid:0)iw)

a=2

1 ˜F1

a+1
2 ;

a+2
2 ; (cid:0)ias2w

p

p

as

as2 ((pa) csc)
1

wcy =

p

1 ˜F1( 1

2 ;1(cid:0) a
2 ;(cid:0)ias2w)
(cid:0)
G( a+1
2 )
(

1
as2
)

2B

a
2 , 1
2

From which we get the mean deviation of the variance.

a

5
2

3

7
2

4

9
2

5

5 73=4(7 2 F1( 3

4 , 9

4 ; 11

4 ;(cid:0) 7

4 )
6 ))s2G( 5

11 )3=4

)

s2G( 7
4 )

√

4

(

23=4

5
3

Mean Deviation

2 2 F1( 1

4 , 7
4 ; 5
p

4 ;(cid:0) 5
6 )+3( 6
4 )
pG( 5
6s2
p
6 )(cid:0)3 2 F1( 7
4 ;(cid:0) 7
4 , 9
4 ; 7
p
pG( 7
4 )
6 63=4
)
(
p
21 (cid:0) 7

3

1
7

s2

√

4

3

(

3
2

)

s2G( 11
4 )

4 ;(cid:0) 3
2 )

6( 2

p

5 )3=4(cid:0)6 2 F1( 5
4 ; 9
4 , 11
pG( 9
4 )
5
(cid:0)1
15(cid:0)16 tan

p

(

7

(√

s2

))

5
3

6p

D C A S E S T U D Y: H O W T H E M Y O P I C

L O S S A V E R S I O N I S M I S S P E C I F I E D

We fatten tails of the distribution with stochasticity of, say, the scale parameter, and can
see what happens to some results in the literature that seem absurd at face value, and in
fact are absurd under more rigorous use of probabilistic analyses.

Myopic loss aversion

Figure D.1: The effect of
Ha,p(t) "utility" or prospect
theory of under second or-
der effect on variance. Here
s = 1, m = 1 and t variable.

Take the prospect theory valuation w function for x changes in wealth.

wl,a(x) = x

a 1x(cid:21)0

(cid:0) l((cid:0)x

a

) 1x<0

p

Where ϕmt,s
dard deviation (scaled by t)

t(x) is the Normal Distribution density with corresponding mean and stan-

The expected "utility" (in the prospect sense):

H0(t) =

∫ ¥

(cid:0)¥

wl,a(x)ϕmt,s

p

t(x) dx

(D.1)

87

Higher values of  a0.100.150.200.25t(cid:45)0.09(cid:45)0.08(cid:45)0.07(cid:45)0.06(cid:45)0.05(cid:45)0.04Ha,1288

case study: how the myopic loss aversion is misspecified

The

ratio

Figure D.2:
Ha, 1
(t)
2
H0

or the degradation of
"utility" under second order
effects.

=

1p

p 2

a
2

(cid:0)2

)(cid:0) a

2

(

(

G

(

1
s2t

a + 1
2

) (

(

a=2

sa

t

1p

+

2s

√

)

mG

(

+ 2lst

1
s2t

1F1

(

)

( a
2

+ 1

sa+1t

a
2 +1

))

1 (cid:0) a
2

;

; (cid:0) tm2
2s2

3
2

√

(cid:0) ls

p

t

1
s2t

1
s2t
)

)a=2

(

(cid:0)

a

2

;

1F1

) a+1
2

(

1
s2t

+ sa

t

a+1
2

)

; (cid:0) tm2
1
2s2
2
)a=2
(

1
s2t

(D.2)

We can see from D.2 that the more frequent sampling of the performance translates
into worse utility. So what Benartzi and Thaler did was try to ﬁnd the sampling period
"myopia" that translates into the sampling frequency that causes the "premium" —the error
being that they missed second order effects.
Now under variations of s with stochatic effects, heuristically captured, the story changes:
what if there is a very small probability that the variance gets multiplied by a large number,
with the total variance remaining the same? The key here is that we are not even changing
the variance at all: we are only shifting the distribution to the tails. We are here generously
assuming that by the law of large numbers it was established that the "equity premium
puzzle" was true and that stocks really outperformed bonds.
So we switch between two states, (1 + a) s2 w.p. p and (1 (cid:0) a) w.p. (1 (cid:0) p).
Rewriting D.1

Ha,p(t) =

∫ ¥

(cid:0)¥

(

)

wl,a(x)

p

p ϕm t,

1+a s

p

t(x) + (1 (cid:0) p) ϕm t,

p

1(cid:0)a s

p

t(x)

dx

(D.3)

Result Conclusively, as can be seen in ﬁgures D.1 and D.2, second order effects cancel
the statements made from "myopic" loss aversion. This doesn’t mean that myopia doesn’t
have effects, rather that it cannot explain the "equity premium", not from the outside (i.e.

0.20.40.60.8a1.11.21.31.41.51.6Ha,12H1case study: how the myopic loss aversion is misspecified

89

the distribution might have different returns, but from the inside, owing to the structure
of the Kahneman-Tversky value function v(x).

Comment We used the (1 + a) heuristic largely for illustrative reasons; we could use a
full distribution for s2 with similar results. For instance the gamma distribution with

with expectation V matching the variance used in the "equity

g(cid:0)1e

v

(cid:0)g

(cid:0) av

a )

V ( V
G(g)

density f (v) =
premium" theory.
Rewriting D.3 under that form,

∫ ¥

∫ ¥

(cid:0)¥

0

wl,a(x)ϕm t,

p

v t(x) f (v) dv dx

Which has a closed form solution (though a bit lengthy for here).

True problem with Benartzi and Thaler Of course the problem has to do with fat tails
and the convergence under LLN, which we treat separately.

Time preference under model error

Another example of the effect of the randomization of a parameter.
This author once watched with a great deal of horror one Laibson [103] at a conference
in Columbia University present the idea that having one massage today to two tomorrow,
but reversing in a year from now is irrational and we need to remedy it with some policy.
(For a review of time discounting and intertemporal preferences, see [70], as economists
temps to impart what seems to be a varying "discount rate" in a simpliﬁed model).

1

Intuitively, what if I introduce the probability that the person offering the massage is
full of balloney? It would clearly make me both prefer immediacy at almost any cost and
conditionally on his being around at a future date, reverse the preference. This is what we
will model next.

First, time discounting has to have a geometric form, so preference doesn’t become nega-
tive: linear discounting of the form Ct, where C is a constant ant t is time into the future
is ruled out: we need something like Ct or, to extract the rate, (1 + k)t which can be math-
ematically further simpliﬁed into an exponential, by taking it to the continuous time limit.
(cid:0)k t. Effectively, such a discounting method using
Exponential discounting has the form e
a shallow model prevents "time inconsistency", so with d < t:

(cid:0)k t
e
e(cid:0)k (t(cid:0)d) = e

(cid:0)k d

lim
t!¥

Now add another layer of stochasticity: the discount parameter, for which we use the
symbol l, is now stochastic.
So we now can only treat H(t) as

∫

H(t) =

(cid:0)l tϕ(l) dl

e

1 It came to my attention that [67] Farmer and Geanakoplos have applied a similar approach to Hyperbolic dis-

counting

90

case study: how the myopic loss aversion is misspecified

It is easy to prove the general case that under symmetric stochasticization of intensity ∆l
(that is, with probabilities 1
2 around the center of the distribution) using the same technique
we did in 3.1.1:

′

H

(t, ∆l) =

(

1
2

(cid:0)(l(cid:0)∆l)t + e

e

(cid:0)(l+∆l)t

)

′
(t, ∆l)
H
H′(t, 0)

=

lt

e

1
2

(

e((cid:0)∆l(cid:0)l)t + e(∆l(cid:0)l)t

)

= cosh(∆ lt)

Where cosh is the cosine hyperbolic function (cid:0) which will converge to a certain value
where intertemporal preferences are ﬂat in the future.

Example: Gamma Distribution Under the gamma distribution with support in R+, with

parameters a and b, ϕ(l) =
we get:

b(cid:0)ala(cid:0)1e
G(a)

(cid:0) l
b

H(t, a, b) =

∫ ¥

0

(cid:0)l t

e

(

b(cid:0)ala(cid:0)1e

(cid:0) l
b

)

G(a)

dl = b(cid:0)a

)(cid:0)a

(

1
b + t

so

H(t, a, b)
H(t (cid:0) d, a, b)

lim
t!¥

= 1

Meaning that preferences become ﬂat in the future no matter how steep they are in the
present, which explains the drop in discount rate in the economics literature.

Further, fudging the distribution and normalizing it, when

ϕ(l)=

(cid:0) l
k

e

k

,

we get the normatively obtained so-called hyperbolic discounting:

H(t) =

1
1 + k t

which turns out to not be the empirical pathology that nerdy researchers have claimed it
to be.

E T H E L A R G E D E V I AT I O N P R I N C I P L E ,

I N B R I E F

Let us discuss the Cramer bound with a rapid exposition of the surrounding

rich outliers in 2.1 is that under
literature. The idea behind the tall vs.
some conditions, the tail probabilities decay exponentially. Such a property
that is central in risk management –as we mentioned earlier, the catastrophe
principle explains that for diversiﬁcation to be effective, such exponential

decay is necessary.

The large deviation principle helps us understand such a tail behavior.It also helps us
ﬁgure out why things do not blow-up under thin-tailedness –but, more signiﬁcantly, why
they could under fat tails, or where the Cramèr condition is not satisﬁed.

Let MN be the mean of a sequence of realizations (identically distributed) of N random

variables. For large N, consider the tail probability:

P(MN > x) (cid:25) e

(cid:0)N I(x),

(

)

where I(.) is the Cramer (or rate) function (Varadhan [185], Denbo and Zeitouni [45]). If we
know the distribution of X, then, by Legendre transformation, I(x) = supq>0 (qx (cid:0) l(q)),
where l(q) = log E
is the cumulant generating function.
The behavior of the function q(x) informs us on the contribution of a single event to
(It connects us to the Cramer condition which requires existence of
the overall payoff.
exponential moments).

q(X)

e

A special case for Bernoulli variables is the Chernoff Bound, which provides tight bounds

for such a class of discrete variables with compact support.

simple case: chernoff bound

A binary payoff is subjected to very tight bounds. Let ( Xi)1<i(cid:20)n be a sequence of inde-
pendent Bernouilli trials taking values in f0, 1g, with P(X = 1) = p and P(X = 0) = 1 (cid:0) p.
Consider the sum Sn = (cid:229)1<i(cid:20)n Xi. with expectation E(Sn)= np = m. Taking d as a "distance
from the mean", the Chernoff bounds gives:
For any d > 0

P (S (cid:21) (1 + d)m) (cid:20)

)m

(

d

e
(1 + d)1+d

91

92

the large deviation principle, in brief

and for 0 < d (cid:20) 1

P (S (cid:21) (1 + d)m) (cid:20) 2e

(cid:0) md2
3

Let us compute the probability of coin ﬂips n of having 50% higher than the true mean,
(cid:0)n=24, which for n = 1000 happens every

(cid:20) 2e

3 = e

S (cid:21)

(cid:0) md2

(

(

)

)

3
2

n
2

with p= 1
1 in 1.24 (cid:2) 1018.

2 and m = n

2 : P

The Markov bound gives: P(X (cid:21) c) (cid:20) E(X)
Proof
c
positive function g(x), hence P(g(x) (cid:21) g(c)) (cid:20) E(g(X))
follows, with g(X) = e
Now consider (1 + d), with d > 0, as a "distance from the mean", hence, with w > 0,

, but allows us to substitute X with a

. We will use this property in what

wX.

g(c)

P (Sn (cid:21) (1 + d)m) = P

(

wSn (cid:21) e

e

w(1+d)m

)

(cid:20) e

(cid:0)w(1+d)mE(e

wSn )

(E.1)

w (cid:229)(Xi)) = E(e

wXi )n, by independence of the stopping time, becomes

(

)

wSn ) = E(e
Now E(e
n
wX)
E(e
.
We have E(e

wX) = 1 (cid:0) p + pe

w

. Since 1 + x (cid:20) ex,

E(e

wSn ) (cid:20) e

m(e

wa(cid:0)1)

Substituting in E.1, we get:

(

P

wSn (cid:21) e

e

w(1+d)m

)

(cid:0)w(1+d)m

(cid:20) e

m(e

w(cid:0)1)

e

(E.2)

We tighten the bounds by playing with values of w that minimize the right side. w(cid:3)
{

}

=

m(e

¶e

w :

¶w

w (cid:0)1)(cid:0)(d+1)mw

= 0

yields w(cid:3)

= log(1 + d).

Which recovers the bound: e

dm

(d + 1)((cid:0)d(cid:0)1)m

.

F M A C H I N E L E A R N I N G

C O N S I D E R AT I O N S

We have learned from option trading that you can express any one-dimensional

function as a weighted linear combination of call or put options –smoothed
by adding time value to the option. An option becomes a building block. A
payoff constructed via option is more precisely as follows S = (cid:229)n
i C(Ki, ti),
i
i = 1, 2, . . . , n, where C is the call price (or, rather, valuation), w is a weight
K is the strike price, and t the time to expiration of the option. A European call C delivers
max(S (cid:0) K, 0) at expiration t.
Neural networks and nonlinear regression, the predecessors of machine learning, on the
other hand, focused on the Heaviside step function, again smoothed to produce a sigmoid
type "S" curve. A collection of different sigmoids would ﬁt in sample.

w

1

Figure F.1: The heaviside q
function: note that it is the
payoff of the "binary option"
and can be decomposed as
lim∆K!0

C(K)(cid:0)C(K+∆K)
dK

.

So this discussion is about ...fattailedness and how the different building blocks can ac-
commodate them. Statistical machine learning switched to "ReLu" or "ramp" functions
that act exactly like call options rather than an aggregation of "S" curves. Researchers then
discovered that it allows better handling of out of sample tail events (since there are by def-
inition no unexpected tail events in sample) owing to the latter’s extrapolation properties.
What is a sigmoid? Consider a payoff function as shown in F.7 that can be expressed with
)
kx
formula S : ((cid:0)¥, ¥) ! (0, 1), S(x) = 1
, or, more precisely, a three parameter
p
ai
function Si
(cid:0)bi x)+1

: ((cid:0)¥, ¥) ! (0, a1) Si(x) =

It can also be the cumulative normal

+ 1
2
.

2 tanh

e(ci

)

(

1 This appears to be an independent discovery by traders of the universal approximation theorem, initially for

sigmoid functions, which are discussed further down (Cybenko [39]).

93

xf(x)94

machine learning considerations

distribution, N (m, s) where s controls the smoothness (it then becomes the Heaviside of
Fig. F.7 at the limit of s ! 0). The (bounded) sigmoid is the smoothing using parameters
of the Heaviside function.

Figure F.2: The sigmoid func-
tion ; note that it is bounded to
both the left and right sides ow-
ing to saturation: it looks like
a smoothed Heaviside q.

We can build composite "S" functions with n summands cn(x) = (cid:229)n
i

w

iSi(x) as in F.3. But:

Remark F.1
For cn(x) 2 [0, ¥) _ [(cid:0)¥, 0) _ ((cid:0)¥, ¥), we must have n ! ¥.

We need an inﬁnity of summands for an unbounded function. So wherever the "empirical
distribution" will be maxed, the last observation will match the ﬂat part of the sig. For the
deﬁnition of an empirical distribution see 5.

Figure F.3: A sum of
sigmoids will always be
bounded, so one needs an
inﬁnite sum to replicate
an "open" payoff, one that
is not subjected to satura-
tion.

the

Now let us consider option payoffs. Fig.F.4 shows the payoff of a regular option at expi-
ration –the deﬁnition of which which matches a Rectiﬁer Linear Unit (ReLu) in machine
learning. Now Fig. F.5 shows the following function: consider a function r : ((cid:0)¥, ¥) !
[k, ¥), with K 2 R:

)

(

r(x, K, p) = k +

log

ep(x(cid:0)K) + 1

p

(F.1)

We can sum the function as (cid:229)i = 1nr(x, Ki, pi) to ﬁt a nonlinear function, which in fact
replicates what we did with call options –the parameters pi allow to smooth time value.

Dose(X)0.20.40.60.81.0Response(F(X))Dose(X)-1.0-0.50.51.0Response(F(X))machine learning considerations

95

Figure F.4: An option payoff
at expiration,
open on the
right.

Figure F.5: r function,
from
Eq. F.1 , with k = 0. We
calibrate and smooth the payoff
with different values of p.

Figure F.6: A butterﬂy (built
via a sum of options/ReLu, not
sigmoids), with open tails on
both sides and ﬂipping ﬁrst
and second derivatives. This
example is particularly potent
as it has no verbalistic corre-
spondence but can be under-
stood by option traders and ma-
chine learning.

F.0.1 Calibration via angles

From ﬁgure F.6 we can see that, in the equation, S = (cid:229)n
i corresponds to
i
the arc tangent of the angle made –if positive (as illustrated in ﬁgure F.7), or the negative
of the arctan of the supplementary angle.

i C(Ki, ti), the w

w

xf(x)xf(x)50100150200x-20-1010203040f(x)96

machine learning considerations

Figure F.7:
w = arctan q.
angles we
nonlinear
option summation.

How
By ﬁtting
can translate a
function into its

Summary

We can express all nonlinear univariate functions using a weighted sum of call op-
tions of different strikes, which in machine learning applications maps to the tails
better than a sum of sigmoids (themselves a net of a long and a short options of
neighboring strikes). We can get the weights implicitly using the angles of the func-
tions relative to Cartesian coordinates.

θ2θ1ω1ω20.00.20.40.60.81.0x0.51.01.52.0f(x)Part II

T H E L A W O F M E D I U M N U M B E R S

6 L I M I T D I S T R I B U T I O N S , A

C O N S O L I D AT I O N (cid:3),†

In this expository chapter we proceed to consolidate the literature on

limit distributions seen from our purpose, with some shortcuts where
indicated. First we show the intuition behind the Central Limit Theo-
rem and illustrate how it varies preasymptotically across distributions.
Then we discuss the law of large numbers as applied to higher mo-

ments. A more formal approach is presented in the next chapter.

6.1 central limit in action

Figure 6.1: The fastest CLT: the Uniform becomes Gaussian in a few steps. We have, successively, 1, 2, 3, and
4 summands. With 3 summands we see a well formed bell shape.

The simpliﬁed version of the generalized central limit theorem (GCLT) (formulated by

Paul Lévy) is as follows:

99

0.20.40.60.81.0x0.20.40.60.81.0ϕ11234x0.10.20.30.40.5ϕ22468x0.050.100.150.200.25ϕ351015x0.050.100.15ϕ4100

limit distributions, a consolidation (cid:3),†

Figure 6.2: Paul Lévy, 1886-1971, formu-
lated the generalized central limit theo-
rem.

Let X1, . . . , Xn be independent and identically distributed random variables. Consider

their sum Sn. We have

Sn (cid:0) an
bn

D! Xs,

(6.1)

D! denotes
where Xs follows a stable distribution S, an and bn are norming constants, and
convergence in distribution (the distribution of X as n ! ¥). The properties of S will be
more properly deﬁned and explored in the next chapter. Take it for now that a random
variable Xs follows a stable (or a-stable) distribution, symbolically Xs (cid:24) S(as, b, m, s), if its
characteristic function c(t) = E(eitXs ) is of the form:

c(t) = e(imt(cid:0)jtsja

s (1(cid:0)ib tan( pa

2 )sgn(t))) when as ̸= 1.

(6.2)

1

The constraints are (cid:0)1 (cid:21) b (cid:21) 1 and 0 < as (cid:20) 2.
The designation stable distribution implies that the distribution (or class) is stable under
summation: you sum up random variables following any the various distributions that
are members of the class S explained next chapter (actually the same distribution with
different parametrizations of the characteristic function), and you stay within the same
distribution. Intuitively, c(t)n is the same form as c(t) , with m ! nm, and s ! n
a s. The
well known distributions in the class (or some people call it a "basin") are: the Gaussian,

1

1 We will try to use as 2 (0, 2] to denote the exponent of the limiting and Platonic stable distribution and ap 2 (0, ¥)
the corresponding Paretian (preasymptotic) equivalen but only in situations where there could be some ambiguity.
Plain a should be understood in context.

6.1 central limit in action

101

the Cauchy and the Lévy with a = 2, 1, and 1
2
closed form density.

2 , respectively. Other distributions have no

We note that if X has a ﬁnite variance, Xs will be Gaussian. But note that Xs is a limiting
construct as n ! ¥ and there are many, many complication with "how fast" we get there.
Let us consider 4 cases that illustrate both the idea of CLT and the speed of it.

6.1.1 Fast convergence: the uniform dist.

Consider a uniform distribution –the simplest of all. If its support is in [0, 1], it will simply
have a density of ϕ(x1) = 1 for 0 (cid:20) x1
(cid:20) 1 and integrates to 1. Now add another variable,
x2, identically distributed and independent. The sum x1 + x2 immediately changed in
shape! Look at ϕ
2(.), the density of the sum in Fig. 6.1. It is now a triangle. Add one
variable and now consider the density ϕ
3 of the distribution of X1 + X2 + X3. It is already
almost bell shaped, with n = 3 summands.

The uniform sum distribution
) (

(

ϕn(x) =

n
(cid:229)
k=0

((cid:0)1)k

n
k

x (cid:0) L
H (cid:0) L

(cid:0) k

)

n(cid:0)1

(

sgn

x (cid:0) L
H (cid:0) L

)

(cid:0) k

for nL (cid:20) x (cid:20) nH

6.1.2 Semi-slow convergence: the exponential

Let us consider a sum of exponential random variables.

We have for initial density

3
and for n summands

1(x) = le
ϕ

ϕn(x) =

(

1
l

(cid:0)lx, x (cid:21) 0
)(cid:0)n xn(cid:0)1e
G(n)

(cid:0)lx

We have, replacing x by n=l (and later in the illustrations in Fig. 6.3

l = 1),

(

1
l

)(cid:0)n

l((cid:0)x)

xn(cid:0)1e
G(n)

!
n!¥

l )2

le

l2(x(cid:0) n
(cid:0)
2np
p
2p

,

n
l and variance n
which is the density of the normal distribution with mean n
l2 .

We can see how we get more slowly to the Gaussian, as shown in Fig. 6.3, mostly on
account of its skewness. Getting to the Gaussian requires symmetry.

there are ways to use special functions;

the Stable S with standard parameters a =
(

))

)

(

for instance one discovered accidentally by the au-
2 , b = 1, m = 0, s = 1 , PDF(x) =

3

′

2Ai

p
x2
3 22=3 3

3

used further down in the example on the limit distribution

2 Actually,
thor:
p
3

x3
27

2e

for
(

p
3

(cid:0)

for Pareto sums.

p
3

3xAi

p
x2
3 22=3 3

3
3 32=3

+3

characteristic functions.

3 We derive the density of sums either by convolving, easy in this case, or as we will see with the Pareto, via

102

limit distributions, a consolidation (cid:3),†

Figure 6.3: The exponential distribution,ϕ indexed by the number of summands. Slower but good enough.

6.1.3 The slow Pareto

Consider the simplest Pareto distribution on [1, ¥):

ϕ

1(x) = 2x

(cid:0)3

and inverting the characteristic function,

ϕn(x) =

∫ ¥

(cid:0)¥

1
2p

exp((cid:0)itx)(2E3((cid:0)it))n dt, x (cid:21) n

∫ ¥
1

dtet((cid:0)z)
tn

Where E(.)(.) is the exponential integral En(z) =
. Clearly, the integration is done
numerically (so far nobody has managed to pull out the distribution of a Pareto sum). It
can be exponentially slow (up to 24 hours for n = 50 vs. 45 seconds for n = 2), so we have
used Monte Carlo simulations for Figs. 6.1.1.
Recall from Eq. 6.1 that the convergence requires norming constants an and bn. From
Uchaikin and Zolotarev [182], we have (narrowing the situation for 1 < ap (cid:20) 2):

P(X > x) = cx

(cid:0)ap

1234x0.20.40.60.81.0ϕ12468x0.10.20.3ϕ22468x0.050.100.150.200.25ϕ324681012x0.050.100.150.20ϕ4510152025x0.020.040.060.080.100.120.14ϕ951015202530x0.020.040.060.080.100.12ϕ106.1 central limit in action

103

Figure 6.4: The Pareto distribution. Doesn’t want to lose its skewness, although in this case it should converge
to the Gaussian... eventually.

as x ! ¥ (assume here that c is a constant we will present more formally the "slowly
varying function" in the next chapter, and

P(X < x) = djxj(cid:0)ap
as x ! ¥. The norming constants become an = n E(X) for ap > 1 (for other cases, consult
[182] as these are not likely to occur in practice), and
(

(

)

8
<

bn =

:

pn1=ap
p
c + d

√

2 sin

pap
2
n log(n)

)(cid:0) 1

G(ap)

ap (c + d)1=ap

for 1 < ap < 2
for ap = 2

(6.3)

And the symmetry parameter b = c(cid:0)d
ap is greater than 2 leads to the Gaussian.

c+d . Clearly, the situation where the Paretian parameter

6.1.4 The half-cubic Pareto and its basin of convergence

Of interest is the case of a = 3
2 . Unlike the situations where as in Fig. 6.1.1, the distribution
ends up slowly being symmetric. But, as we will cover in the next chapter, it is erroneous
to conﬂate its properties with those of a stable. It is, in a sense, more fat-tailed.

104

limit distributions, a consolidation (cid:3),†

Figure 6.5: The Pareto distri-
bution, ϕ
100 and ϕ
1000, not
much improvement
towards
Gaussianity, but an al pha = 2
will eventually get you there
if you are patient and have a
long, very long, life.

Figure 6.6: The
Pareto
becomes
n = 104

distribution
symmetric.

half-cubic
never
Here

6.2 cumulants and convergence

Since the Gaussian (as a basin of convergence) has skewness of 0 and (raw) kurtosis of 3,
we can heuristically examine the convergence of these moments to establish the speed of
the workings under CLT.

Deﬁnition 6.1 (Excess p-cumulants)
Let c(w) be characteristic functionof a given distribution, n the number of summands (for inde-

2500030000350004000045000xϕ100006.2 cumulants and convergence

105

Figure 6.7: Behavior of 4th moment under aggregation for a few ﬁnancial securities deemed to converge to the
Gaussian but in fact do not converge (backup data for [159]). There is no conceivable way to claim convergence
to Gaussian for data sampled at a lower frequency.

pendent random variables), p the order of the moment. We deﬁne the ratio of cumulants for the
corresponding pth moment:

K

p
k

≜ ((cid:0)i)p¶p log(c(w)n)
((cid:0)¶2 log(c(w)n))2

K(n) is a metric of excess pth moment over that of a Gaussian, p > 2; in other words, K4
denotes Gaussianity for n independent summands.

n = 0

Remark 6.1
We note that

n!¥ K
lim

p
N = 0

for all probability distributions outside the Power Law class.
p
n is ﬁnite for the thin-tailed class. In other words, we face a clear-cut

We also note that limp!¥ K

basin of converging vs. diverging moments.
For distributions outside the Power Law basin, 8p 2 N>2, K
A sketch of the proof can be done using the stable distribution as the limiting basin and
the nonderivability at order p greater than its tail index, using Eq. 7.4.
Table 6.1 for N-summed p cumulants. It describes K(n)
We would expect a drop at a rate 1
N2 for stochastic volatility (gamma variance wlog).
However, ﬁgure 8.9 shows the drop does not take place at any such speed. Visibly we are
not in the basin. As seen in [159] there is an absence of convergence of kurtosis under
summation across economic variables.

n decays at a rate N p(cid:0)2.

p

10203040Lagn123456KurtCopper10203040Lagn5101520KurtEurodollarDepo3M10203040Lagn510152025KurtGold10203040Lagn2468KurtLiveCattle10203040Lagn2468101214KurtRussiaRTSI10203040Lagn2468101214KurtSoyMeal10203040Lagn123456KurtTY10YNotes10203040Lagn246KurtAustraliaTB10y10203040Lagn246810KurtCoffeeNY106

limit distributions, a consolidation (cid:3),†

Table 6.1: Table of Normalized Cumulants For Thin Tailed Distributions Speed of Convergence for N Indepen-
dent Summands
-

Distr. Poisson Expon. Gamma Symmetric 2-state vol
(a,b)
1

1, s

2)

(l)
1
1
nl
1
nl2

K(2)

K(3)

K(4)

(l)
1
2l
n
3!l2
n

(s
1
0

2
a b n

3!
a2 b2 n

3(1(cid:0)p)p
n

(cid:2) (s2
(ps2
1

(cid:0)s2

2 )2
1
(cid:0)(p(cid:0)1)s2

2 )3

6.3 the law of large numbers

G-Variance
(a, b)
1
0

3b
n

Figure 6.8: The law of large
numbers show a tightening
distribution around the mean
leading to degeneracy converg-
ing to a Dirac stick at the exact
mean.

By the weak law of large numbers, a sum of i.i.d. random variables X1, . . . , Xn with ﬁnite
(cid:229) Xi converges to m in probability, as n ! +¥. Or, for
mean m, that is E(X) < +¥, then 1
)
n
jXn (cid:0) mj> ϵ
any ϵ > 0 limn!+¥ P
= 0.
By standard results, we can observe the law of large numbers at work for the stable
distribution, illustrated in Fig. 6.8:

(

)

n

(

t
n

c

lim
n!+¥

= eimt, 1 < as (cid:20) 2

(6.4)

which is the characteristic functionof a Dirac delta at m, a degenerate distribution, since the
Fourier transform (here parametrized to be the inverse of the characteristic function) is:
]

[

1p

2p

Ft

eimt

(x) = d(m + x).

(6.5)

Further, we can observe the "real-time" operation for all 1 < n < +¥ in the following

way.

6.4 the law of large numbers for higher moments

Dirac-4-2024mean0.51.01.52.02.5pdf6.4 the law of large numbers for higher moments

107

Table 6.2: Fourth noncentral moment at daily, 10-day, and 66-day windows for the random variables

K(1)

K(10) K(66)

Max
Quartic

Years

Dol-

Australian
lar/USD
Australia TB 10y
Australia TB 3y
BeanOil
Bonds 30Y
Bovespa
British Pound/USD
CAC40
Canadian Dollar
Cocoa NY
Coffee NY
Copper
Corn
Crude Oil
CT
DAX
Euro Bund
Euro Currency/DEM
previously
Eurodollar Depo 1M
Eurodollar Depo 3M
FTSE
Gold
Heating Oil
Hogs
Jakarta Stock Index
Japanese Gov Bonds
Live Cattle
Nasdaq Index
Natural Gas
Nikkei
Notes 5Y
Russia RTSI
Short Sterling
Silver
Smallcap
SoyBeans
SoyMeal
Sp500
Sugar #11
SwissFranc
TY10Y Notes
Wheat
Yen/USD

6.3
7.5
7.5
5.5
5.6
24.9
6.9
6.5
7.4
4.9
10.7
6.4
9.4
29.0
7.8
8.0
4.9
5.5
41.5
21.1
15.2
11.9
20.0
4.5
40.5
17.2
4.2
11.4
6.0
52.6
5.1
13.3
851.8
160.3
6.1
7.1
8.9
38.2
9.4
5.1
5.9
5.6
9.7

3.8
6.2
5.4
7.0
4.7
5.0
7.4
4.7
4.1
4.0
5.2
5.5
8.0
4.7
4.8
6.5
3.2
3.8
28.0
8.1
27.4
14.5
4.1
4.6
6.2
16.9
4.9
9.3
3.9
4.0
3.2
6.0
93.0
22.6
5.7
8.8
9.8
7.7
6.4
3.8
5.5
6.0
6.1

2.9
3.5
4.2
4.9
3.9
2.3
5.3
3.6
3.9
5.2
5.3
4.5
5.0
5.1
3.7
3.7
3.3
2.8
6.0
7.0
6.5
16.6
4.4
4.8
4.2
4.3
5.6
5.0
3.8
2.9
2.5
7.3
3.0
10.2
6.8
6.7
8.5
5.1
3.8
2.6
4.9
6.9
2.5

0.12
0.08
0.06
0.11
0.02
0.27
0.05
0.05
0.06
0.04
0.13
0.05
0.18
0.79
0.25
0.20
0.06
0.06
0.31
0.25
0.54
0.04
0.74
0.05
0.19
0.48
0.04
0.13
0.06
0.72
0.06
0.13
0.75
0.94
0.06
0.17
0.09
0.79
0.30
0.05
0.10
0.02
0.27

22.
25.
21.
47.
32.
16.
38.
20.
38.
47.
37.
48.
49.
26.
48.
18.
18.
38.
19.
28.
25.
35.
31.
43.
16.
24.
44.
21.
19.
23.
21.
17.
17.
46.
17.
47.
48.
56.
48.
38.
27.
49.
38.

108

limit distributions, a consolidation (cid:3),†

Figure 6.9: Cumulative mo-
ments p = 1, 2, 3, 4.

Figure 6.10: Gaussian Con-
trol.

Figure 6.11: QQ Plot of Stu-
dent T: left tail ﬁts, not the
right tail.

500010000150000.20.40.60.81.0MaxSum500010000150000.20.40.60.81.0MaxSum-0.10-0.050.000.050.10-0.10-0.050.000.050.106.5 mean deviation for a stable distributions

109

6.5 mean deviation for a stable distributions

Let us prepare a result for the next chapter using the norm L1 for situations of ﬁnite
4
mean but inﬁnite variance.
Clearly we have no way to measure the compression of the
distribution around the mean within the norm L2.
The error of a sum in the norm L1 is as follows. Since sgn(x) = 2q(x) (cid:0) 1:

csgn(x)(t) =

2i
t

(6.6)

Let cd(.) be the characteristic function of any nondegenerate distribution. Convoluting
csgn(x) (cid:3) (cd)n, we obtain the characteristic functionfor the positive variations for n inde-
pendent summands

cm =

csgn(x)(t)cd(u (cid:0) t)ndt.

∫ ¥

(cid:0)¥

In our case of mean absolute deviation being twice that of the positive values of X:

c(jSnj) = (2i)
∫

∫ ¥

c(t (cid:0) u)n
t

du

(cid:0)¥
which is the Hilbert transform of c when
is taken in the p.v. sense (Pinelis, 2015 )[133]. In
our situation, given that all independents summands are copies from the same distribution,
we can replace the product c(t)n with cs(t) which is the same characteristic functionwith
ss = n1=as, b remains the same:

E(jXj) = 2i

¶
¶u

p.v.

∫ ¥

(cid:0)¥

cs(t (cid:0) u)
t

dtj

t=0

(6.7)

Now, [133] the Hilbert transform,

=

2
pi

∫ ¥(cid:0)

0

cs(u + t) (cid:0) cs(u (cid:0) t) dt

which can be rewritten as

(

= (cid:0)i

¶
¶u

1 + cs(u) +

∫ ¥(cid:0)

0

1
pi

cs(u + t) (cid:0) cs(u (cid:0) t) (cid:0) cs(t) + cs((cid:0)t)

)

.

dt
t

(6.8)

Deriving ﬁrst inside the integral and using a change of variable, z = log(t),

EjXj
∫ ¥

(˜a,b,ss,0) =
2iae
(

(cid:0)(ssez)

(cid:0)¥

+ cos

b tan

)a

(

ssez

) (

)a

ssez

a(cid:0)z
( pa
2

(

b tan
))

( pa
2

)

(

sin

b tan

) (

( pa
2

ssez

)

)a

dz

which then integrates nicely to:

EjXj

(˜a,b,ss,0) =

) ((

(

a (cid:0) 1
a

ss
2p

G

1 + ib tan

))

1=a

( pa
2

(

+

1 (cid:0) ib tan

( pa
2

)

))

1=a

.

(6.9)

4 We say, again by convention, inﬁnite for the situation where the random variable, say X2 (or the variance of
any random variable), is one-tailed –bounded on one side– and undeﬁned in situations where the variable is
two-tailed, e.g. the infamous Cauchy.

7 H O W M U C H D ATA D O Y O U N E E D ? A N

O P E R AT I O N A L M E T R I C F O R
FAT-TA I L E D N E S S ‡

In this (research) chapter we discuss the laws of medium number-

sLaw of medium numbers. We present an operational metric for uni-
variate unimodal probability distributions with ﬁnite ﬁrst moment, in
[0, 1] where 0 is maximally thin-tailed (Gaussian) and 1 is maximally
fat-tailed. It is based on "how much data one needs to make meaning-

ful statements about a given dataset?"

Applications: Among others, it

(cid:15) helps assess the sample size n needed for statistical signiﬁcance outside the

Gaussian,

(cid:15) helps measure the speed of convergence to the Gaussian (or stable basin),
(cid:15) allows practical comparisons across classes of fat-tailed distributions,
(cid:15) allows the assessment of the number of securities needed in portfolio construc-

tion to achieve a certain level of stability from diversiﬁcation,

(cid:15) helps understand some inconsistent attributes of the lognormal, pending on the

parametrization of its variance.

The literature is rich for what concerns asymptotic behavior, but there is a large void

for ﬁnite values of n, those needed for operational purposes.

1

: Conventional measures of fat-tailedness, namely 1) the tail index for the
Background
Power Law class, and 2) Kurtosis for ﬁnite moment distributions fail to apply to some
distributions, and do not allow comparisons across classes and parametrization, that is
between power laws outside the Levy-Stable basin, or power laws to distributions in other
classes, or power laws for different number of summands. How can one compare a sum
of 100 Student T distributed random variables with 3 degrees of freedom to one in a Levy-
Stable or a Lognormal class? How can one compare a sum of 100 Student T with 3 degrees
of freedom to a single Student T with 2 degrees of freedom?
We propose an operational and heuristic metric that allows us to compare n-summed
independent variables under all distributions with ﬁnite ﬁrst moment. The method is

1 The author owes the most to the focused comments by Michail Loulakis who, in addition, provided the rigorous
derivations for the limits of the k for the Student T and lognormal distributions, as well as to the patience and
wisdom of Spyros Makridakis. The paper was initially presented at Extremes and Risks in Higher Dimensions, Sept
12-16 2016, at the Lorentz Center, Leiden and at Jim Gatheral’s Festschrift at the Courant Institute, in October
2017. The author thanks Jean-Philippe Bouchaud, John Einmahl, Pasquale Cirillo, and others. Laurens de Haan
suggested changing the name of the metric from "gamma" to "kappa" to avoid confusion. Additional thanks to
Colman Humphrey, Michael Lawler, Daniel Dufresne and others for discussions and insights with derivations.

111

112

how much data do you need? an operational metric for fat-tailedness ‡

based on the rate of convergence of the law of large numbers for ﬁnite sums, n-summands
speciﬁcally.

We get either explicit expressions or simulation results and bounds for the lognormal, ex-
ponential, Pareto, and the Student T distributions in their various calibrations –in addition
to the general Pearson classes.

Figure 7.1: The intuition of
what k is measuring:
how
the mean deviation of the sum
of
identical copies of a r.v.
Sn = X1 + X2 + . . . Xn grows
as the sample increases and
how we can compare preasymp-
totically distributions from dif-
ferent classes.

Figure 7.2: Watching the ef-
fect of the Generalized Cen-
tral Limit Theorem: Pareto
and Student T Distribution, in
the P class, with a exponent,
k converge to 2 (cid:0) (1a<2
a +
1a(cid:21)22), or the Stable S class.
We observe how slow the con-
vergence, even after 1000 sum-
mands. This discounts Man-
delbrot’s assertion that an in-
ﬁnite variance Pareto can be
subsumed into a stable distri-
bution.

Cauchy(κ=1)Pareto1.14CubicStudentTGaussian(κ=0)Degrees ofFat Tailedness246810n246810|Sn=X1+X2+...+Xn|7.1 introduction and definitions

113

7.1 introduction and definitions

How can one compare a Pareto distribution with tail a = 2.1 that is, with ﬁnite variance,
to a Gaussian? Asymptotically, these distributions in the regular variation class with ﬁnite
second moment, under summation, become Gaussian, but pre-asymptotically, we have no
standard way of comparing them given that metrics that depend on higher moments, such
as kurtosis, cannot be of help. Nor can we easily compare an inﬁnite variance Pareto
distribution to its limiting a-Stable distribution (when both have the same tail index or tail
exponent ). Likewise, how can one compare the "fat-tailedness" of, say a Student T with 3
degrees of freedom to that of a Levy-Stable with tail exponent of 1.95? Both distributions
have a ﬁnite mean; of the two, only the ﬁrst has a ﬁnite variance but, for a small number
of summands, behaves more "fat-tailed" according to some operational criteria.

Criterion for "fat-tailedness"
There are various ways to "deﬁne" Fat Tails and rank dis-
tributions according to each deﬁnition.
In the narrow class of distributions having all
moments ﬁnite, it is the kurtosis, which allows simple comparisons and measure depar-
tures from the Gaussian, which is used as a norm. For the Power Law class, it can be the
tail exponent . One can also use extremal values, taking the probability of exceeding a
maximum value, adjusted by the scale (as practiced in extreme value theory). For opera-
tional uses, practitioners’ fat-tailedness is a degree of concentration, such as "how much
of the statistical properties will be attributable to a single observation?", or, appropriately
adjusted by the scale (or the mean dispersion), "how much is the total wealth of a country
in the hands of the richest individual?"

Here we use the following criterion for our purpose, which maps to the measure of
concentration in the past paragraph: "How much will additional data (under such a prob-
ability distribution) help increase the stability of the observed mean". The purpose is not
entirely statistical: it can equally mean: "How much will adding an additional security
into my portfolio allocation (i.e., keeping the total constant) increase its stability?"

Our metric differs from the asymptotic measures (particularly ones used in extreme value

theory) in the fact that it is fundamentally preasymptotic.

Real life, and real world realizations, are outside the asymptote.

What does the metric do?

The metric we propose, k does the following:

(cid:15) Allows comparison of n-summed variables of different distributions for a given num-
ber of summands , or same distribution for different n, and assess the preasymptotic
properties of a given distributions.

(cid:15) Provides a measure of the distance from the limiting distribution, namely the Lévy

a-Stable basin (of which the Gaussian is a special case).

(cid:15) For statistical inference, allows assessing the "speed" of the law of large numbers,
expressed in change of the mean absolute error around the average thanks to the
increase of sample size n.

(cid:15) Allows comparative assessment of the "fat-tailedness" of two different univariate dis-

tributions, when both have ﬁnite ﬁrst moment.

(cid:15) Allows us to know ahead of time how many runs we need for a Monte Carlo simu-

lation.

The state of statistical inference
The last point, the "speed", appears to have been ig-
nored (see earlier comments in Chapter 2 about the 9,400 pages of the Encyclopedia of

114

how much data do you need? an operational metric for fat-tailedness ‡

Statistical Science [101]). It is very rare to ﬁnd a discussion about how long it takes to reach
the asymptote, or how to deal with n summands that are large but perhaps not sufﬁciently
so for the so-called "normal approximation".

To repeat our motto, "statistics is never standard". This metric aims at showing how
standard is standard, and measure the exact departure from the standard from the standpoint
of statistical signiﬁcance.

7.2 the metric

Distribution

Student
(a)

T

Table 7.1: Kappa for 2 summands, k

1.

k

1

2 (cid:0)

2 log(2)
0
@ 22(cid:0)aG(a(cid:0) 1
2 )
2 )2
G( a

1

A+log(p)

2 log

Exponential/Gamma

2 (cid:0) log(2)

2 log(2)(cid:0)1

(cid:25) .21

Pareto (a)

2 (cid:0)

(

log

(a(cid:0)1)2(cid:0)aaa(cid:0)1

∫ 2
a(cid:0)1

0

log(2)

(

)

) a

(cid:0)2a2(y+2)(cid:0)2a(cid:0)1( 2
a(cid:0)1

(cid:0)y)

B 1
y+2

((cid:0)a,1(cid:0)a)(cid:0)B y+1
y+2

((cid:0)a,1(cid:0)a)

dy

Normal
(m, s) with
switching
variance s2a
w.p pb.

Lognormal
(m, s)

2 (cid:0)

(√

0

@

p

2

log

(

√

(√

ap
p(cid:0)1

+s2 +p

(cid:0)2

ap
p(cid:0)1

+s2 +p

p

log(2)
√

+s2 (cid:0)

ap
p(cid:0)1
p

2a( 1
p(cid:0)1
√
ap
p(cid:0)1

a+s2 (cid:0)(p(cid:0)1)

+s2

+2)+4s2 +

p

)

√

a+s2

+

2a( 1
p(cid:0)1

+2)+4s2

))

1

A

(cid:25) 2 (cid:0)

0

B
B
B
B
B
B
B
B
B
B
@

log

log(2)
√
(

(

log

s2

+1

1
2

e
p

2

2

0

B
B
B
B
@

2 erf

(

)

erf

s
p

2

2

1

1

))

.

C
C
C
C
A

C
C
C
C
C
C
C
C
C
C
A

a B.(., .) is the incomplete Beta function: Bz(a, b) =

∫

2p
p

(cid:0)t2

z
0 e

dt.

∫

z

0 ta(cid:0)1(1 (cid:0) t)b(cid:0)1dt; erf(.) is the error function erf(z) =

b See comments and derivations in the appendix for switching both variance and mean as it can produce

negative values for kappa.

Deﬁnition 7.1 (the k metric)
Let X1, . . . , Xn be i.i.d. random variables with ﬁnite mean, that is E(X) < +¥. Let Sn = X1 + X2 +
. . . + Xn be a partial sum. Let M(n) = E(jSn (cid:0) E(Sn)j) be the expected mean absolute deviation
from the mean for n summands. Deﬁne the "rate" of convergence for n additional summands
starting with n0:

7.2 the metric

115

Table 7.2: Summary of main results

Distribution

kn

Exponential/Gamma

Explicit

Lognormal (m, s)

Pareto (a) (Constant)

No explicit kn but explicit
lower and higher bounds
(low or high s or n). Ap-
proximated with Pearson
IV for s in between.

k
Explicit
2
bound for all a).

for

(lower

Student T(a)
function)

(slowly varying

Explicit for k

1 , a = 3.

Figure 7.3: The lognormal distribution
behaves like a Gaussian for low values of s,
but becomes rapidly equivalent to a power
law. This illustrates why, operationally,
the debate on whether the distribution of
wealth was lognormal (Gibrat) or Pareto
(Zipf) doesn’t carry much operational sig-
niﬁcance.

{

kn0,n = min

kn0,n :

M(n)
M(n0)

=

(

n
n0

) 1

2(cid:0)kn0,n

}

, n0 = 1, 2, ...

,

n > n0

(cid:21) 1, hence

k(n0, n) = 2 (cid:0) log(n) (cid:0) log(n0)
) .

(

log

M(n)
M(n0)

(7.1)

Further, for the baseline values n = n0 + 1, we use the shorthand kn0.
We can also decompose k(n0, n) in term of "local" intermediate ones similar to "local"
interest rates, under the constraint.

k(n0, n) = 2 (cid:0) log(n) (cid:0) log(n0)

(cid:229)n

i=0

log(i+1)(cid:0)log(i)
2(cid:0)k(i,i+1)

.

(7.2)

Student T (3)orStable α=1.7Stable α=1.2∼ Gaussian0.51.01.52.02.53.0σ0.20.40.60.81.0κ1116

how much data do you need? an operational metric for fat-tailedness ‡

Table 7.3: Comparing Pareto to Student T (Same tail exponent a)

a

1.25
1.5
1.75
2.
2.25
2.5
2.75
3.
3.25
3.5
3.75
4.

Pareto
k
1
0.829
0.724
0.65
0.594
0.551
0.517
0.488
0.465
0.445
0.428
0.413
0.4

Pareto
k
1,30
0.787
0.65
0.556
0.484
0.431
0.386
0.356
0.3246
0.305
0.284
0.263
0.2532

Pareto
k
1,100
0.771
0.631
0.53
0.449
0.388
0.341
0.307
0.281
0.258
0.235
0.222
0.211

Student
k
1
0.792
0.647
0.543
0.465
0.406
0.359
0.321
0.29
0.265
0.243
0.225
0.209

Student
k
1,30
0.765
0.609
0.483
0.387
0.316
0.256
0.224
0.191
0.167
0.149
0.13
0.126

Student
k
1,100
0.756
0.587
0.451
0.352
0.282
0.227
0.189
0.159
0.138
0.121
0.10
0.093

Use of Mean Deviation Note that we use for measure of dispersion around the mean
the mean absolute deviation, to stay in norm L1 in the absence of ﬁnite variance –actually,
even in the presence of ﬁnite variance, under Power Law regimes, distributions deliver an
unstable and uninformative second moment. Mean deviation proves far more robust there.
(Mean absolute deviation can be shown to be more "efﬁcient" except in the narrow case of
kurtosis equals 3 (the Gaussian), see a longer discussion in [168]; for other advantages, see
[127].)

7.3 stable basin of convergence as benchmark

Deﬁnition 7.2 (the class P)
The P class of power laws (regular variation) is deﬁned for r.v. X as follows:

P = fX : P(X > x) (cid:24) L(x) x

(cid:0)ag

(7.3)

where (cid:24) means that the limit of the ratio or rhs to lhs goes to 1 as x ! ¥. L : [xmin, +¥) !
L(kx)
(0, +¥) is a slowly varying function, deﬁned as limx!+¥
L(x) = 1 for any k > 0. The constant
a > 0.

Next we deﬁne the domain of attraction of the sum of identically distributed variables,

in our case with identical parameters.

Deﬁnition 7.3
(stable S class) A random variable X follows a stable (or a-stable) distribution, symbolically X (cid:24)
S(˜a, b, m, s), if its characteristic functionc(t) = E(eitX) is of the form:

8
>>><
>>>:

c(t) =

e(imt(cid:0)jtsj ˜a(1(cid:0)ib tan( p ˜a

2 )sgn(t)))

2bs log(s)
p

+m

(

)

(cid:0)jtsj

2ibsgn(t) log(jtsj)
p

1+

)

(

it

e

˜a ̸= 1

,

˜a = 1

(7.4)

7.3 stable basin of convergence as benchmark

117

Next, we deﬁne the corresponding stable ˜a:

{

˜a ≜

a 1a<2 + 2 1a(cid:21)2
2

if X is in P
otherwise.

Further discussions of the class S are as follows.

7.3.1 Equivalence for stable distributions

For all n0 and n (cid:21) 1 in the Stable S class with ˜a (cid:21) 1:

simply from the property that

k

(n0,n) = 2 (cid:0) ˜a,

M(n) = n

1

a M(1)

(7.5)

(7.6)

This, simply shows that kn0,n = 0 for the Gaussian.
The problem of the preasymptotics for n summands reduces to:

(cid:15) What is the property of the distribution for n0 = 1 (or starting from a standard, off-the

shelf distribution)?

(cid:15) What is the property of the distribution for n0 summands?
(cid:15) How does kn ! 2 (cid:0) ˜a and at what rate?

7.3.2 Practical signiﬁcance for sample sufﬁciency

Conﬁdence intervals: As a simple heuristic, the higher k, the more disproportionally
insufﬁcient the conﬁdence interval. Any value of k above .15 effectively indicates a
high degree of unreliability of the "normal approximation". One can immediately
doubt the results of numerous research papers in fat-tailed domains.

Computations of the sort done Table 7.2 for instance allows us to compare various dis-
tributions under various parametriazation.
(comparing various Pareto distributions to
symmetric Student T and, of course the Gaussian which has a ﬂat kappa of 0)
As we mentioned in the introduction, required sample size for statistical inference is
driven by n, the number of summands. Yet the law of large numbers is often invoked in
erroneous conditions; we need a rigorous sample size metric.
Many papers, when discussing ﬁnancial matters, say [72] use ﬁnite variance as a binary
classiﬁcation for fat tailedness: power laws with a tail exponent greater than 2 are therefore
classiﬁed as part of the "Gaussian basin", hence allowing the use of variance and other
such metrics for ﬁnancial applications. A much more natural boundary is ﬁniteness of
expectation for ﬁnancial applications [160]. Our metric can thus be useful as follows:
Let Xg,1, Xg,2, . . . , Xg,ng be a sequence of Gaussian variables with mean m and scale s. Let
Xn,1, Xn,2, . . . , Xn,nn be a sequence of some other variables scaled to be of the same M(1),
namely Mn
p s. We would be looking for values of nn corresponding to a
2
given ng.

(1) = Mg(1) =

√

118

how much data do you need? an operational metric for fat-tailedness ‡

kn is indicative of both the rate of convergence under the law of large number, and
for kn ! 0, for rate of convergence of summands to the Gaussian under the central
limit, as illustrated in Figure 7.2.

{

nmin = inf

nn : E

((cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

nn
(cid:229)
i=1

Xn,i

(cid:0) mp
nn

)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:20) E

((cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ng
(cid:229)
i=1

Xg,i

(cid:0) mg
ng

)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

}

, nn > 0

(7.7)

which can be computed using kn = 0 for the Gaussian and backing our from kn for the
target distribution with the simple approximation:

(cid:0) 1
k
1,ng

g

(cid:0)1

(cid:25) n

nn = n

(cid:0) 1
k
1

(cid:0)1

g

, ng > 1

(7.8)

The approximation is owed to the slowness of convergence. So for example, a Student
T with 3 degrees of freedom (a = 3) requires 120 observations to get the same drop in
variance from averaging (hence conﬁdence level) as the Gaussian with 30, that is 4 times as
much. The one-tailed Pareto with the same tail exponent a = 3 requires 543 observations to
match a Gaussian sample of 30, 4.5 times more than the Student, which shows 1) ﬁniteness
of variance is not an indication of fat tailedness (in our statistical sense), 2) neither are tail
exponent s good indicators 3) how the symmetric Student and the Pareto distribution are
not equivalent because of the "bell-shapedness" of the Student (from the slowly varying
function) that dampens variations in the center of the distribution.
We can also elicit quite counterintuitive results. From Eq. 7.8, the "Pareto 80/20" in
the popular mind, which maps to a tail exponent around a (cid:25) 1.14, requires > 109 more
observations than the Gaussian.

7.4 technical consequences

7.4.1 Some oddities with asymmetric distributions

The stable distribution, when skewed, has the same k index as a symmetric one (in other
words, k is invariant to the b parameter in Eq. 7.4, which conserves under summation).
But a one-tailed simple Pareto distribution is fatter tailed (for our purpose here) than an
equivalent symmetric one.

This is relevant because the stable is never really observed in practice and used as some
limiting mathematical object, while the Pareto is more commonly seen. The point is not
well grasped in the literature. Consider the following use of the substitution of a stable for
a Pareto. In Uchaikin and Zolotarev [182]:

Mandelbrot called attention to the fact that the use of the extremal stable distributions
(corresponding to b = 1) to describe empirical principles was preferable to the use
of the Zipf-Pareto distributions for a number of reasons.
It can be seen from many
publications, both theoretical and applied, that Mandelbrot’s ideas receive more and
more wide recognition of experts. In this way, the hope arises to conﬁrm empirically
established principles in the framework of mathematical models and, at the same time,
to clear up the mechanism of the formation of these principles.

These are not the same animals, even for large number of summands.

7.4.2 Rate of convergence of a student T distribution to the Gaussian Basin

7.5 conclusion and consequences

119

1

We show in the appendix –thanks to the explicit derivation of k for the sum of students
with a = 3, the "cubic" commonly noticed in ﬁnance –that the rate of convergence of k
to 0 under summation is
log(n) . This (and the semi-closed form for the density of an n-
summed cubic Student) complements the result in Bouchaud and Potters [20] (see also
[149]), which is as follows. Their approach is to separate the "Gaussian zone" where the
density is approximated by that of a Gaussian, and a "Power Law zone" in the tails which
retains the original distribution with Power Law decline. The "crossover" between the
n log(n) standard deviations) which
two moves right and left of the center at a rate of
is excruciatingly slow.
Indeed, one can note that more summands fall at the center of
the distribution, and fewer outside of it, hence the speed of convergence according to the
central limit theorem will differ according to whether the density concerns the center or
the tails.

√

Further investigations would concern the convergence of the Pareto to a Levy-Stable,

which so far we only got numerically.

7.4.3 The lognormal is neither thin nor fat tailed

Naively, as we can see in Figure 7.2, at low values of the parameter s, the lognormal
behaves like a Gaussian, and, at high s, it appears to have the behavior of a Cauchy of
sorts (a one-tailed Cauchy, rather a stable distribution with a = 1, b = 1), as k gets closer
and closer to 1. This gives us an idea about some aspects of the debates as to whether
some variable is Pareto or lognormally distributed, such as, say, the debates about wealth
[109], [40], [41].
Indeed, such debates can be irrelevant to the real world. As P. Cirillo
[32] observed, many cases of Paretianity are effectively lognormal situations with high
variance; the practical statistical consequences, however, are smaller than imagined.

7.4.4 Can kappa be negative?

Just as kurtosis for a mixed Gaussian (i.e., with stochastic mean, rather than stochastic
volatility ) can dip below 3 (or become "negative" when one uses the convention of mea-
suring kurtosis as excess over the Gaussian by adding 3 to the measure), the kappa metric
can become negative when kurtosis is "negative". These situations require bimodality (i.e.,
a switching process between means under ﬁxed variance, with modes far apart in terms
of standard deviation). They do not appear to occur with unimodal distributions.

Details and derivations are presented in the appendix.

7.5 conclusion and consequences

To summarize, while the limit theorems (the law of large numbers and the central limit)
are concerned with the behavior as n ! +¥, we are interested in ﬁnite and exact n both
small and large.

We may draw a few operational consequences:

120

how much data do you need? an operational metric for fat-tailedness ‡

Figure 7.4: In short, why the
1/n heuristic works:
it takes
many, many more securities to
get the same risk reduction as
via portfolio allocation accord-
ing to Markowitz . We as-
sume to simplify that the se-
curities are independent, which
they are not, something that
compounds the effect.

7.5.1 Portfolio pseudo-stabilization

Our method can also naturally and immediately apply to portfolio construction and the
effect of diversiﬁcation since adding a security to a portfolio has the same "stabilizing"
effect as adding an additional observation for the purpose of statistical signiﬁcance. "How
much data do you need?" translates into "How many securities do you need?". Clearly,
the Markowicz allocation method in modern ﬁnance[112] (which seems to not be used by
Markowitz himself for his own portfolio [120]) applies only for k near 0; people use convex
heuristics, otherwise they will underestimate tail risks and "blow up" the way the famed
portfolio-theory oriented hedge fund Long Term Management did in 1998 [167] [176].)
We mentioned earlier that a Pareto distribution close to the "80/20" requires up to 109
more observations than a Gaussian; consider that the risk of a portfolio under such a dis-
tribution would be underestimated by at least 8 orders of magnitudes if one uses modern
portfolio criteria. Following such a reasoning, one simply needs broader portfolios.

It has also been noted that there is practically no ﬁnancial security that is not fatter tailed
than the Gaussian, from the simple criterion of kurtosis [159], meaning Markowitz port-
folio allocation is never the best solution.
It happens that agents wisely apply a noisy
approximation to the 1
n heuristic which has been classiﬁed as one of those biases by behav-
ioral scientists but has in fact been debunked as false (a false bias is one in which, while
the observed phenomenon is there, it does not constitute a "bias" in the bad sense of the
word; rather it is the researcher who is mistaken owing to using the wrong tools instead of
the decision-maker). This tendency to "overdiversify" has been deemed a departure from
optimal investment behavior by Benartzi and Thaler [13], explained in [12] "when faced
with n options, divide assets evenly across the options. We have dubbed this heuristic the
"1=n rule."" However, broadening one’s diversiﬁcation is effectively as least as optimal as
standard allocation(see critique by Windcliff and Boyle [188] and [46]). In short, an equally
weighted portfolio outperforms the SP500 across a broad range range of metrics. But even
the latter two papers didn’t conceive of the full effect and properties of fat tails, which
we can see here with some precision. Fig. 7.5 shows the effect for securities compared to
Markowitz.

This false bias is one in many examples of policy makers "nudging" people into the wrong
rationality [167] and driving them to increase their portfolio risk many folds.

MarkowitzEstablishedsecuritiesSpeculativesecurities02004006008001000n0.10.20.30.40.50.6Variability7.5 conclusion and consequences

121

A few more comments on ﬁnancial portfolio risks. The SP500 has a k of around .2, but one
needs to take into account that it is itself a basket of n = 500 securities, albeit unweighted
and consisting of correlated members, overweighing stable stocks. Single stocks have
kappas between .3 and .7, meaning a policy of "overdiversiﬁcation" is a must.

Likewise the metric gives us some guidance in the treatment of data for forecasting, by
establishing sample sufﬁciency, to state such matters as how many years of data do we
need before stating whether climate conditions "have changed", see [108].

7.5.2 Other aspects of statistical inference

So far we considered only univariate distributions. For higher dimensions, a potential
area of investigation is an equivalent approach to the multivariate distribution of extreme
fat tailed variables, the sampling of which is not captured by the Marchenko-Pastur (or
Wishhart) distributions. As in our situation, adding variables doesn’t easily remove noise
from random matrices.

7.5.3 Final comment

As we keep saying, "statistics is never standard"; however there are heuristics methods to
ﬁgure out where and by how much we depart from the standard.

appendix

We show here some derivations

7.5.4 Cubic Student T (Gaussian Basin)

The Student T with 3 degrees of freedom is of special interest in the literature owing to its
prevalence in ﬁnance [72]. It is often mistakenly approximated to be Gaussian owing to
the ﬁniteness of its variance. Asymptotically, we end up with a Gaussian, but this doesn’t
tell us anything about the rate of convergence. Mandelbrot and Taleb [111] remarks that
the cubic acts more like a powerlaw in the distribution of the extremes, which we will
elaborate here thanks to an explicit PDF for the sum.

Let X be a random variable distributed with density p(x):

p

p(x) =

6

3

p (x2 + 3)2 , x 2 ((cid:0)¥, ¥)

(7.9)

Proposition 7.1
Let Y be a sum of X1, . . . , Xn, n identical copies of X. Let M(n) be the mean absolute deviation
from the mean for n summands. The "rate" of convergence k

1
2(cid:0)k

k :

is:

{

}

1,n =

M(n)
M(1) = n

k

1,n = 2 (cid:0)

log(n)
log (enn(cid:0)nG(n + 1, n) (cid:0) 1)

(7.10)

where G(., .) is the incomplete gamma function G(a, z) =

∫ ¥
z dtta(cid:0)1e

(cid:0)t.

122

how much data do you need? an operational metric for fat-tailedness ‡

Since the mean deviation M(n):

M(n) =

{

p

2

2

3

p
p
3
p (enn

(cid:0)nG(n + 1, n) (cid:0) 1)

for n = 1
for n > 1

(7.11)

The derivations are as follows. For the pdf and the MAD we followed different routes.

We have the characteristic functionfor n summands:

φ(w) = (1 +

p

3jwj)n e

(cid:0)n

p

3 jwj

The pdf of Y is given by:

p(y) =

∫ ¥

p

(1 +

0

1
p

3 w)n e

(cid:0)n

p

3 w

cos(wy) dw

After arduous integration we get the result in 7.11. Further, since the following result does
not appear to be found in the literature, we have a side useful result: the PDF of Y can be
written as

n(cid:0) iyp
3

e

(

2iyp
3 E(cid:0)n

e

p(y) =

(

)

(

))

n (cid:0) iyp
3

(7.12)

+ E(cid:0)n

n + iyp
3
p
3p
et((cid:0)z)
tn dt.

2
∫ ¥
1

where E(.)(.) is the exponential integral Enz =

Note the following identities (from the updating of Abramowitz and Stegun) [51]

(cid:0)n(cid:0)1G(n + 1, n) = E(cid:0)n(n) = e

n

(cid:0)n (n (cid:0) 1)!
nn

n
(cid:229)
m=0

nm
m!

As to the asymptotics, we have the following result (proposed by Michail Loulakis): Re-
expressing Eq. 7.11:

Further,

M(n) =

p

3n!
2
pnn

n(cid:0)1
(cid:229)
m=0

nm
m!

(cid:0)n

e

n(cid:0)1
(cid:229)
m=0

nm
m!

=

1
2

+ O

(

)

1p
n

(cid:0)n (cid:229)n(cid:0)1
m=0

(From the behavior of the sum of Poisson variables as they converge to a Gaussian by
nm
m! = P(Xn < n) where Xn is a Poisson random vari-
the central limit theorem: e
able with parameter n. Since the sum of n independent Poisson random variables with
parameter 1 is Poisson with parameter n, the Central Limit Theorem says the probabil-
ity distribution of Zn = (Xn (cid:0) n)=
n approaches a standard normal distribution. Thus
2
P(Xn < n) = P(Zn < 0) ! 1=2 as n ! ¥.
For another approach, see [121] for proof that
2! + (cid:1) (cid:1) (cid:1) + nn(cid:0)1
1 + n
(n(cid:0)1)!
Using the property that lim
n!¥

2p, we get the following exact asymptotics:

n!exp(n)
p
nn

1! + n2

(cid:24) en

2 .)

p

p

=

n

2 Robert Israel on Math Stack Exchange

n!¥ log(n)k
lim

1,n =

p2

4

7.5 conclusion and consequences

123

thus k goes to 0 (i.e, the average becomes Gaussian) at speed 1
log(n) , which is excruciatingly
slow. In other words, even with 106 summands, the behavior cannot be summarized as
that of a Gaussian, an intuition often expressed by B. Mandelbrot [111].

7.5.5 Lognormal Sums

From the behavior of its cumulants for n summands, we can observe that a sum behaves
likes a Gaussian when s is low, and as a lognormal when s is high –and in both cases we
know explicitly kn.
The lognormal (parametrized with m and s) doesn’t have an explicit characteristic func-
tion. But we can get cumulants Ki of all orders i by recursion and for our case of summed
identical copies of r.v. Xi, Kn
Cumulants:

i = Ki((cid:229)n Xi) = nKi(X1).

s2
2

Kn
Kn

m+
1 = ne
(
2 = n

e

(

e

Kn
Kn

3 = n
4 = . . .

)

)

e2m+s2
(
2
s2

e

)

+ 2

e3m+ 3s2

2

s2 (cid:0) 1
s2 (cid:0) 1

Which allow us to compute: Skewness =
e2s2

(cid:0)6

+2

+3

s2

s2

(

)

)

(

e

e

p

es2 (cid:0)1

(

s2

e

)

+2

e
p

1

2 (2m+s2)(cid:0)m(cid:0) s2

2

n

and Kurtosis = 3 +

n

We can immediately prove from the cumulants/moments that:

k

lim
n!+¥

1,n = 0, lim
s!0

k

1,n = 0

and our bound on k becomes explicit:
Let k(cid:3)
density, with the same ﬁrst two moments. We have

1,n be the situation under which the sums of lognormal conserve the lognormal

0 (cid:20) k(cid:3)
1,n

(cid:20) 1,

k(cid:3)
1,n = 2 (cid:0)

nerf

0

B
B
B
B
B
B
B
B
@

log

n+e

s2 (cid:0)1
n

log(n)
0
v
(
u
u
t

log

B
B
B
@

p

2

2

)

(

erf

s
p

2

2

1

)

1

C
C
C
A

C
C
C
C
C
C
C
C
A

Heuristic attempt Among other heuristic approaches, we can see in two steps how 1)
under high values of s, k
1,n, since the law of large numbers slows down, and 2)
k(cid:3)
1,n

s!¥! 1.

! k(cid:3)

1,n

124

how much data do you need? an operational metric for fat-tailedness ‡

3

Proving the upper bound, that for high variance k

∫ ¥
m (x (cid:0) m) f (x)dx = 2

Loulakis’ Proof
been shown formally my Michail Loulakis
the identify E (jX (cid:0) mj) = 2
the mean, and ¯FX(.) is the survival function. Further, M(n) = 2
or X = exp
we get M(n) = 2
t) (cid:21) P(max0<i(cid:20)n(Xi) > t) (cid:21) nP(X1 > t) (cid:0) (n
second term to 0 (using Hölder’s inequality).

1,n approaches 1 has
which we summarize as follows. We start with
¯FX(t)dt, where f (.) is the density, m is
s2,
where Z is a standard normal variate. Let Sn be the sum X1 + . . . + Xn;
P(Sn > t)dt. Using the property of subexponentiality ([136]), P(Sn >
s!¥! 1 and the

2)P (X1 > t)2. Now P (X1 > t)

¯F(x)dx. Assume m = 1
2

sZ (cid:0) s2
∫ ¥
2
n

∫ ¥
nm

∫ ¥
m

(

)

Skipping steps, we get lim inf
(cid:20) n. So for s ! ¥ ,

s!¥

bound

M(n)
M(1)

M(n)
M(1)
M(n)
M(1) = n, hence k

s!¥! 1.

1,n

(cid:21) n, while at the same time we need to satisfy the

For computational purposes, for the s pa-
Pearson Family approach for computation
rameter not too large (below (cid:25) .3, we can use the Pearson family for computational con-
venience –although the lognormal does not belong to the Pearson class (the normal does,
but we are close enough for computation). Intuitively, at low sigma, the ﬁrst four moments
can be sufﬁcient because of the absence of large deviations; not at higher sigma for which
conserving the lognormal would be the right method.

The use of Pearson class is practiced in some ﬁelds such as information/communication
theory, where there is a rich literature: for summation of lognormal variates see Nie and
Chen, [122], and for Pearson IV, [29], [47].
The Pearson family is deﬁned for an appropriately scaled density f satisfying the follow-

ing differential equation.

′

f

(x) = (cid:0) (a0 + a1x)

b0 + b1x + b2x2 f (x)

(7.13)

We note that our parametrization of a0, b2, etc. determine the distribution within the
Pearson class –which appears to be the Pearson IV. Finally we get an expression of mean
deviation as a function of n, s, and m.
Let m be the mean. Diaconis et al [49] from an old trick by De Moivre, Suzuki [152] show
that we can get explicit mean absolute deviation. Using, again, the identity E(jX (cid:0) mj) =
∫ ¥
m (x (cid:0) m) f (x)dx and integrating by parts,
2
(

)

E(jX (cid:0) mj) =

2

b0 + b1m + b2m2
(cid:0) 2b2

a1

f (m)

(7.14)

3 Review of this paper; Loulakis proposed a formal proof in place of the heuristic derivation.

7.5 conclusion and consequences

125

We use cumulants of the n-summed lognormal to match the parameters. Setting a1 = 1,
(cid:0)a0
and m = b1
1(cid:0)2b2
8

, we get

+6(n(cid:0)1)e
6(n(cid:0)1)+e2s2
)

+12(n(cid:0)1)e2s2 (cid:0)(8n+1)e3s2
s2
)
(
es2

5es2 +4

(cid:0)3

))

(

+3e5s2

+e6s2

+12

)

(

s2
2

m+

e

(cid:0)12n2+(3(cid:0)10n)e4s2
(

(

)(

e

e2s2
6(n(cid:0)1)+e2s2
)
s2
2

s2 (cid:0)1

s2 (cid:0)1
(
es2
(

m+

s2

e

e

(

2
(

e

2
s2

2e
+3
(
5es2 +4
(
(

)

))

(cid:0)3
(

a0 =

b2 =

b1 =

>>>>>>>>>>>>>><
>>>>>>>>>>>>>>:

(

)

)

)

)

)

s2

e

2
(

s2

(

(cid:0)4n+e
e
6(n(cid:0)1)+e2s2
(
s2

e
6(n(cid:0)1)+e2s2

(cid:0)2(n(cid:0)1)e
(
es2

(

s2
(

s2
(

e
+4
+7
5es2 +4
es2
s2 (cid:0)3n+e3s2
))
)
5es2 +4
(cid:0)3

2(m+s2)
(

2

(cid:0)6n+6
))
)
(cid:0)3
)

+6(n(cid:0)1)

+12(n(cid:0)1)

)

+3

+6(n(cid:0)1)

(

s2 (cid:0)1

e

)

e

b0 = (cid:0) n

Polynomial expansions Other methods, such as Gram-Charlier expansions, such as
Schleher [146], Beaulieu,[10], proved less helpful to obtain kn. At high values of s, the
approximations become unstable as we include higher order Lhermite polynomials. See
review in Dufresne [52] and [53].

7.5.6 Exponential

The exponential is the "entry level" fat tails, just at the border.

f (x) =

le

(cid:0)lx,

x (cid:21) 0.

By convolution the sum Z = X1, X2, . . . Xn we get, by recursion, since f (y) =
x) dx = l2ye

(cid:0)ly:

fn(z) =

(cid:0)lz

lnzn(cid:0)1e
(n (cid:0) 1)!

which is the gamma distribution; we get the mean deviation for n summands:

M(n) =

(cid:0)nnn
2e
lG(n)

,

hence:

k

1,n = 2 (cid:0)

log(n)
n log(n) (cid:0) n (cid:0) log(G(n)) + 1

∫

y

0 f (x) f (y (cid:0)

(7.15)

(7.16)

(7.17)

We can see the asymptotic behavior is equally slow (similar to the student) although the

exponential distribution is sitting at the cusp of subexponentiality:

n!¥ log(n)k
lim

1,n = 4 (cid:0) 2 log(2p)

126

how much data do you need? an operational metric for fat-tailedness ‡

Figure 7.5: Negative kurtosis from B.2 and corresponding kappa.

7.5.7 Negative kappa, negative kurtosis

1, s

2 , X (cid:24) N (m

1) and with probability 1

Consider the simple case of a Gaussian with switching means and variance: with proba-
bility 1
These situations with thinner tails than the Gaussian are encountered with bimodal situa-
tions where m
1 and m
2 are separated; the effect becomes acute when they are separated by
1 = s
several standard deviations. Let d= m
2 (to achieve minimum kurtosis),

2 , X (cid:24) N (m

2 and s = s

2, s

(cid:0) m

2).

1

k

1 =

0

@

p

log(p) (cid:0) 2 log

log(4)
d2
4s2 erf( d
pde
(
d2
4s2 erf

de

2
(cid:0) m

d
p
2s

2.

p

s2e
√

2s )+2
)

+2

p se
2

d2
4s2 +2s
d2
8s2

which we see is negative for wide values of m

1

+ 2

1

A

(7.18)

-10-5510μ1-μ21.52.02.53.0Kurtosis-10-5510μ1-μ2-7-6-5-4-3-2-1kappa8 D I A G N O S T I C T O O L S F O R FAT TA I L S .

W I T H A P P L I C AT I O N T O T H E S P 5 0 0 †

In this (research) chapter we show some reasoning errors in the liter-

ature on the "overestimation" of tail risks in general and in the stock
market and other economic random variables in particular. We isolate
the three various methods to study tail risks and show the statistical
invalidity of the ﬁrst two under the power law/slow variation class.

We propose a battery of tests to assess if one fails to reject Paretianity compared
to other ad hoc adjustments such as stochastic volatility /Poisson. Applying to the
SP500, we show that the process cannot be reliably estimated outside the slow varia-
tion class, with or possibly ﬁnite variance –and more so for the tails.

Analyses in L2 such as GARCH , conditional variance, or stochastic volatility are

methodologically (and practically) invalid.

We also present the notion of ergodicity of portfolio in the context of tail pricing.

We show how conclusions about "overpricing" of tail events tend to underestimate
the tail and "deep tail" (catastrophes) by up to 70 times.

8.1 introduction

The problem If you use the wrong distribution (or method)... then all the consequences
may be wrong, and your approach is most certainly unscientiﬁc. Changing from thin-
tailed to fat-tailed is not just changing the color of the dress. The ﬁnance and economic idiots
hold the message "we know it is fat tailed" but then fail to grasp the consequences on many
things such as the slowness of the law of large numbers and the failure of sample means
to be sufﬁcient statistic ( as well as the ergodicity effect, among others).

Here we just focus on the inadequate estimation of tail risks.

Some practical consequences of the problem For the U.S. stock market indices, while
the ﬁrst method is deemed to be ludicrous, using the second method leads to an underes-
timation of the payoff in the tails of between 5 and 70 times.

The organization of the discussion We contrast the three possible approaches to tail
risks: Methods 1, 2, and 3. We establish that Power Law distributions can only be estimated
by Method 3. We show the battery of tests for failure to reject if a given data belong to the

127

128

diagnostic tools for fat tails. with application to the sp500 †

Figure 8.1: This ﬁgure represents the extent of the underestimation. It shows the relative value of tail CVar-
style measure compared to that from the empirical distribution. The deep tail is underestimated up to 70 times
by current methods, even those deemed "empirical".

Power Law basin (based on criteria of stability of moments and convergence). Method 3 is
extrapolative and extends the tails beyond in-sample extrema.

Next, we show the tail risk underestimation. Finally we connect the point to the regular

argument of ergodicity of probability.

8.2 methods 1 through 3

There are three general statistical methods to capture tail risk while dealing with indepen-
dent data.

Method 1 (Naive Parametric)

Method 1 is the "naive" parametric method based on thin-tails distributions, where the
maximum likelihood of the conditional mean represents the true mean, for all subsegments
≜ E((cid:0)Xj(cid:0)X>K)
≜ E(Xj
X>K) (or m<K
of the distributions. The conditional "tail mean" m
is derived parametrically to match the sample mean m(n) = 1
(cid:229)i(cid:20)n xi and the sample second
n
moment of a parametric distribution, m2(n) = 1
(cid:229)i(cid:20)n x2
thanks to the working of the
i
n
Central Limit Theorem (in its Lindeberg formulation).
Note: in the rest of the paper we will express E((cid:0)Xj(cid:0)X>K) "the negative tail" in positive
numbers except when otherwise mentioned.

K

0.050.100.150.200.250.300.35K10203040506070∫K∞ϕ_sⅆx∫K∞ϕ_e(x)ⅆx8.2 methods 1 through 3

129

Figure 8.2: The great Benoit Mandelbrot linked fractal geometry to statistical distributions via self-afﬁnity at
all scales. When asked to explain his work, he said: "rugosité", meaning"roughness" –it took him ﬁfty years
to realize that was his specialty. (Seahorse Created by Wolfgang Beyer, Wikipedia Commons.)

The limit for n ! ¥:

p

n

((

)

)

1
n

n
(cid:229)
i=1

xi

(cid:0) m

D(cid:0)! N

(

)

0, s2

.

(8.1)

D(cid:0)! indicates convergence in distribution. Critically, Method 1 assumes that n is

where
sufﬁciently large to assume 8.1, and that the same applies to higher moments.

Deﬁnition 8.1 (Method 1)
Method 1 assumes that the data allows the matching of mean and variance to that of a standard para-
metric distribution and, when n is sufﬁciently large, to that of a Gaussian, and estimate conditional
tail means m

K from that of the parametric distribution.
We note that Method 1 is the standard approach in ﬁnance.

Remark 8.1
Method 1 relies on both the central limit theorem (CLT) and the law of large numbers (LLN) .

In other words, Method 1 assumes that the maximum likelihood estimation of the mean
and variance are derived from the sample ﬁrst two moments. The way CLT and LLN mix
inextricably is one of the problems we faced in Chapter x.

130

diagnostic tools for fat tails. with application to the sp500 †

Remark 8.2
Method 1 assumes that the mean and variance are sufﬁcient statistics for conditional tail means m
and m<K.

K

Method 2 (Nonparametric)

Method 2 is the nonparametric method based on the empirical distribution, which appear
to adjust for fat tails. Empirical distribution, or survival function ¯F(t): Let X1, X2, . . . Xn be
independent, identically distributed real random variables with the common cumulative
distribution function F(t).

where 1

A is the indicator function.

¯Fn(t) =

1
n

n
(cid:229)
i=1

1xi

(cid:21)t,

Deﬁnition 8.2
Method 2 is where the conditional mean m> K ≜ E(Xjx > K) is assumed to be represented by the
sample conditional mean

1xi
1xi
and the survival probabilities are represented by ¯Fn(K).

(cid:229)n
i=1 xi
(cid:229)n

mK

≜

i=1

(cid:21)K
(cid:21)K

.

(8.2)

We note that the empirical distributions are necessarily censured on the interval [xmin, xmax].
On the other hand, owing to the ﬁniteness of moments inherent in Method 1, the latter
extrapolate very little outside such a range.
We next discuss the properties of the extremes of Medhod 2.[REPEAT]

The so-called "empirical distribution" is not quite empirical
There is a prevalent con-
fusion about the nonparametric empirical distribution based on the following powerful
property: as n grows, it converges to the Gaussian regardless of the base distribution, even if
fat-tailed (assuming inﬁnite support). For the CDF (or survival functions) are both uniform
n (Fn(x) (cid:0) F(x)) converges
on [0, 1], and, further, by the Donsker theorem, the sequence
in distribution to a Normal Distribution with mean 0 and variance F(x) (1 (cid:0) F(x)) (one
may ﬁnd even stronger forms of convergence via the Glivenko– Cantelli theorem). Owing
to this remarkable property, one may mistakenly assume that the tails of the distribution
converge in the same manner independently of the distribution. Further, and what con-
tributes to the confusion, given the variance, F(x) (1 (cid:0) F(x)) for both empirical CDF and
survival function, drops at the extremes.

p

In truth, and that is a property of extremes, the error effectively increases in the tails if
one multiplies by the divergence. Let cn be the difference between the empirical and the
distributional conditional mean, deﬁned as:

cn =

n
(cid:229)
i=1

xi

1xi

(cid:21)K

(cid:0)

∫ ¥

K

xdF(x)

= K(Fn(K) (cid:0) F(K)) +

(

¯Fn(K + (i + 1)d) (cid:0) ¯Fn(K + idK) (cid:0)

∫

K+(i+1)d

K+id

)

d ¯F(K)

(cid:0)

∫ ¥

max

dF(x),

xmax
d

(cid:229)
i

(8.3)
(cid:0)1(0), that is where the distribution is truncated. cn recovers the dispersion
where xmax = ¯Fn
of the distribution of x which remains fat tailed. Another way to see it is that for fat tailed

8.2 methods 1 through 3

131

variables, probabilities are more stable than their realizations and, more generally, the
lowest moment will always disproportionately be the most stable one.

Biases of the empirical method under Fat Tails

We note that, owing of the convergence to the Gaussian, by Donsker’s theorem:

cn =

∫ ¥

xmax

dF(x) + O

(

)

F(x) (1 (cid:0) F(x))
p
n

so, for sufﬁciently large (but not too large) n,
∫ ¥

cn (cid:25)

dF(x)

xmax

(8.4)

(8.5)

yet, under a Paretian regime, xmax is distributed according to a Fréchet, as we will see
in Section TK.

Theorem 8.1
For an empirical distribution with a sample size n, the underestimation of the conditional tail
expectation cn for a Paretian with scale L and tail index a is:

φ(c, n) =

) 1
a(cid:0)1

(

a (cid:0) 1
a

(

((

a2
1(cid:0)a +1c 1
a(cid:0)1

nL

exp

) a
a(cid:0)1

a (cid:0) 1
a

(

)

))

a2
1(cid:0)a +1

(cid:0)L

n

a
a(cid:0)1

c

(8.6)

and its expectation

E(cn) = G

(

)

a (cid:0) 1
a

a+ 1

a (cid:0)1n

L

a (cid:0)1
1

Proof. The maximum of n variables is in the MDA (Maximum domain of attraction) of
Fréchet with scale b = (Ln)1=a
. We have the conditional expectation > c: E(x)jx>cP(x >
. Randomizing c and doing a probability transformation we get the density
c) =
φ(.).

ac1(cid:0)a
a(cid:0)1

aL

Remark 8.3
Method 2 relies on the law of large numbers without the central limit theorem.

Method 3 (Nonrestricted Maximum Likelihood parametric)

Method 3 is a more general Maximum Likelihood parametric approach that ﬁnds a the-
oretical distribution and a parameter that has converged according to the law of large
numbers. This allows to extend the tails and extrapolate outside the sample based on
statistical properties, not just past maxima.

Deﬁnition 8.3
Method 3 ﬁts those parameters of the distribution that can satisfy the law of large numbers for
sample size n. More speciﬁcally, it does not hold that the LLN holds for the mean n given n
observations, even if m> K < ¥.
Equivalently, Method 3 does not accept the sample conditional mean mK as statistical mean for all

K in the support of the distribution.

Method 3 ﬁnds the parameter for which the law of large numbers holds.

132

diagnostic tools for fat tails. with application to the sp500 †

The latter approach is necessary for distribution in the Power Law basin, which is char-
acterized by the slowness of the mean. [34], [33].

Deﬁnition 8.4 (Power Law Class P)
The r.v. X 2 R belongs to P, the class of slowly varying functions (a.k.a. Paretian tail or power
law-tailed) if its survival function (for the variable taken in absolute value) decays asymptotically
at a ﬁxed exponent a, or a′

, that is

(right tail) or

P(X > x) (cid:24) L(x) x

(cid:0)a

P((cid:0)X > x) (cid:24) L(x) x

(cid:0)a′

(8.7)

(8.8)

(left tail)
where a, a′ > 0 and L : (0, ¥) ! (0, ¥) is a slowly varying function, deﬁned as limx!¥
for all k > 0.

L(kx)
L(x) = 1

The happy result is that the parameter a obeys an inverse gamma distribution that con-
verges rapidly to a Gaussian and does not require a large n to get a good estimate. This is
illustrated in Figure 8.3, where we can see the difference in ﬁt.

8.3 the law of large numbers under paretianity

Figure 8.3: Monte Carlo Sim-
ulation (105) of a comparison
of sample mean (Methods 1
and 2) vs maximum likelihood
mean estimations (Method 3)
for a Pareto Distribution with
a = 1.2 (yellow and blue re-
spectively), for n = 100, 1000.
We can see how the MLE
tracks the distribution more re-
liably. We can also observe the
bias as Methods 1 and 2 un-
derestimate the sample mean in
the presence of skewness in the
data. We need 107 more data
in order to get the same error
rate.

n=10051015200.00.10.20.30.4n=100051015200.00.10.20.30.40.5True mean8.3 the law of large numbers under paretianity

133

Why the sample mean, if it converges asymptotically for a sample size n, as n ! ¥,
does not necessarily behave reliably for ﬁnite n, even larges values; this is a common
mistake in the literature (as researchers use mechanistically asymptotic properties
without verifying pre-asymptotic attributes).

To illustrate the slowness of the LLN in the slowly varying function class: for a distribu-
tion of the type "Pareto 80-20", that is with an exponent a (cid:25) 1.14, it takes > 1011 more data
than the Gaussian for sample mean ! mean; accordingly neither Method 1 nor Method 2
are statistically reliable.
Simply, on the surface, the law of large number can be phrased as follows: For E(X) =
m < ¥ (meaning ﬁnite ﬁrst moment but no restriction on higher moments),

1
n

n
(cid:229)
i=1

P(cid:0)! m

Xi

when n ! ¥

That is to say that for any positive number ϵ:

(

n!¥ Pr
lim

jXn (cid:0) mj> #

)

= 0.

The confusion arises as to "speed". To what extent the n-sized sample ﬁrst moment
1
n

i=1 Xi is representative of m? Well, it depends on a.

(cid:229)n

First, we need to look at "errors" about the mean in mean deviation terms (since we put

no restriction on higher moments than 1).

Deﬁnition 8.5
Let X1, . . . , Xn be i.i.d. random variables with ﬁnite mean, that is E(X) < +¥. Let Sn = X1 + X2 +
. . . + Xn be a partial sum. Let MD(n) = E(jSn (cid:0) E(Sn)j) be the expected mean absolute deviation
from the mean for n summands. Let kn0,n be the "speed" of convergence for additional n summands
given that already we have n0 summands:

For a thin-tailed variable, errors about the mean are, for n summands, simply n

p
2
p .
For a symmetric Stable Distribution (about which, see x) with otherwise same summands
(and adapting to ﬁt a scale s):

(cid:0)2s

MDa,b(n) =

)

(

p

2sG

a (cid:0) 1
a

1
2p

a (cid:0)1
1

n

((

1 + ib tan

))

1=a

( pa
2

(

+

1 (cid:0) ib tan

( pa
2

)

))

1=a

(8.9)

which becomes simple when b = 0 (Samorodnitsky and Takku,[145], Uchaikin and Zolotarev
[182]):

(

)

p

2sG

a(cid:0)1
a
p

MDa(n) =

a (cid:0)1
1

n

(8.10)

and deﬁning n2 the number of summands required to make mean deviation equal to that
of a Gaussian with n1 summands. fn2 : MDa(n2) = MDG(n1)g where "G" is the Gaussian
of the same scale

n2,b =1 = 2

a
1(cid:0)a p

a
2(cid:0)2a

(

p

((

n1

1 + i tan

))

1=a

( pa
2

(

+

1 (cid:0) i tan

( pa
2

))

1=a

)

(

G

a (cid:0) 1
a

)) a
a(cid:0)1

(8.11)

134

diagnostic tools for fat tails. with application to the sp500 †

which simpliﬁes for a symmetric distribution:

0

@

n2 = p

a
2(cid:0)2a

1

A

)

1
1a (cid:0)1

1
(

p

G

n1

a(cid:0)1
a

(8.12)

We can thus verify that for a = 1.14, n2,b=1 > 1011 (cid:2) n1. Again, Figure 8.3 is representative
as applying to a = 1.2, we get 107.
Secondly, there is a problem with ﬁnite variance power laws: forn < 107, even a > 2
becomes meaningless as convergence to the Gaussian basin is very slow. See paper on
Kappa-n, still incomplete.

8.4 distribution of the tail exponent

The distribution of a, the tail exponent of a Paretian distribution is thin-tailed and
converges rapidly to the Gaussian.

Consider wlog the standard Pareto distribution for a random variable X with pdf:

ϕ

X(x) = aL

a

(cid:0)a(cid:0)1 , x > L

x

(8.13)

Assume L = 1 by scaling.
(cid:0)a(cid:0)1
The likelihood function is L = (cid:213)n
i=1
i
tion (assuming we set the minimum value) log(L) = n(log(a) + a log(L)) (cid:0) (a + 1) (cid:229)n
i=1 log Xi
yields: ˆa =
n
the distribution of the average logarithm yield:

. Maximizing the Log of the likelihood func-
i=1 log (xi)
. Using the characteristic function to get

i=1 log(xi) . Now consider l = (cid:0) (cid:229)n

ax

(cid:229)n

n

y(t)n =

(∫ ¥

1

f (x) exp

(

it log(x)
n

)

)

n

(

dx

=

)

n

an
an (cid:0) it

which is the characteristic function of the gamma distribution (n, 1
that ˆa′ ≜ 1
l will follow the inverse gamma distribution with density:

an ). A standard result is

ϕˆa(a) =

)

n

e

(

(cid:0) an
an
ˆa
ˆa
ˆaG(n)

, a > 0

.

Debiasing

Since E(ˆa) = n
n(cid:0)1

which, after scaling, will have for distribution ϕ ˆa′ (a) =

e

a(cid:0)an
a

a(n(cid:0)1)
a

aG(n+1)

n+1

.

a we elect another –unbiased– random variable ˆa′ = n(cid:0)1
n ˆa
(

)

Truncating for a > 1 Given that values of a (cid:20) 1 lead to absence of mean we restrict
the distribution to values greater than 1 + ϵ, ϵ > 0. Our sampling now applies to lower-
truncated values of the estimator, those strictly greater than 1, with a cut point ϵ > 0, that

8.5 dependence and asymmetries

135

log(xi) > 1 + ϵ, or E(ˆaj

is, (cid:229) n(cid:0)1
of the exponent conditional of it being greater than 1 becomes:

ˆa>1+ϵ): ϕ ˆa′′ (a) =

ϕ
ˆa′ (a)
∫ ¥
1+ϵ ϕ
ˆa′ (a) da

, hence the distribution of the values

ϕ ˆa′′ (a) =

an2
a(cid:0)an

e

(

(

(

a

G(n) (cid:0) G

)

n

an2
a(n(cid:0)1)

n,

n2a
(n(cid:0)1)(ϵ+1)

)) , a (cid:21) 1 + ϵ

(8.14)

8.5 dependence and asymmetries

8.5.1 Records and Extrema

The Gumbel record methods is as follows (Embrechts et al [60]). Let X1, X2, . . . be a discrete
time series, with a maximum at period t (cid:21) 2, Mt = max(X1, X2, . . . , Xt), we have the record
counter N1,t for n data points.

N1,t = 1 +

t
(cid:229)
k=2

1Xt>Mt(cid:0)1

(8.15)

Regardless of the underlying distribution, the expectation E(Nt) is the Harmonic Number
Ht, and the variance Ht (cid:0) H(2)
1
ir . We note that the harmonic number is
t
concave and very slow in growth, logarithmic, as it can be approximated with log(n) + g,
(cid:20) Ht (cid:0)
where g is the Euler Mascheroni constant. The approximation is such that
log(t)t (cid:0) g (cid:20) 1

, where Ht = (cid:229)t

2n (Wolfram Mathworld [187]).

1
2(n+1)

i=1

for

Figure 8.4: The record test
shows
ex-
independence
tremes of negative returns, de-
pendence for positive ones. The
number of records for indepen-
dent observations grows over
time at the harmonic number
H(t) (dashed line), (cid:25) logarith-
mic but here appears to grow >
2.5 standard deviations faster
for positive returns, hence we
cannot assume independence
for extremal gains. The test
does not make assertions about
dependence outside extremes.

Remark 8.4
The Gumbel test of independence above is sufﬁcient condition for the convergence of extreme neg-
ative values of the log-returns of the SP500 to the Maximum Domain of Attraction (MDA) of the
extreme value distribution.

Entire series We reshufﬂed the SP500 (i.e. bootstrapped without replacement, using a
sample size equal to the original (cid:25) 17000 points, with 103 repeats) and ran records across
all of them. As shown in Fig. 8.6 and 8.5, the mean was 10.4 (approximated by the
harmonic number, with a corresponding standard deviation.) The survival function S(.) of

GainsLosses050001000015000time51015#records136

diagnostic tools for fat tails. with application to the sp500 †

N1.7(cid:2)104 = 16, S(16) = 1
implausible.

40 which allows us to consider the independence of positive extrema

On the other hand the negative extrema (9 counts) show realizations close to what is
expected (10.3), diverting by 1
2 a s.t.d. from expected, enough to justify a failure to reject
independence.

Subrecords
If instead of taking the data as one block over the entire period, we broke
the period into sub-periods, we get (because of the concavity of the measure and Jensen’s
inequality), Nt1+d,t1+∆+d, we obtain T=d observations. We took ∆ = 103 and d = 102, thus
getting 170 subperiods for the T (cid:25) 17 (cid:2) 103 days. The picture as shown in Fig. 8.7 cannot
reject independence for both positive and reject observations.

It appears that the total is

Conclusion We can at least use EVT for negative observations.

8.6 some properties and tests

8.6.1 Asymmetry right-left tail

8.6.2 Paretianity and moments

Remark 8.5
Given that:
1) the slowly varying class has no higher moments than a, more precisely,

(cid:15) if p > a, E(X p) = ¥ if p is even or the distribution has one-tailed support

and

(cid:15) E(X p) is undeﬁned if p is odd and the distribution has two-tailed support,

and
2) distributions outside the slowly varying class have all moments 8p 2 N+, E(X p) < ¥.
9p 2 N+ s.t. E(X p) is either undeﬁned or inﬁnite , X 2 P.

The rest of the paper examines ways to detect "inﬁnite" moments. Much confusion at-
tends the notion of inﬁnite moments and its identiﬁcation since by deﬁnition sample
moments are ﬁnite and measurable under the counting measure. We will rely on the
nonconvergence of moments. Let ∥X∥

p be the weighted p-norm

∥X∥

p

≜

(

1
n

n
(cid:229)
i=1

jp

jxi

)

1=p

,

we have the property of power laws:

E(X p) ≮ ¥ , ∥x∥

p is not convergent.

We note that, for obvious reasons, belonging to the class of Power Law tails cancels much
of the methods in L (cid:0) 2 such as GARCH and similar studies.

8.7 convergence tests

137

Figure 8.5: The survival function of the records of positive maxima for the resampled SP500 (103 times) by
keeping all returns but reshufﬂing them, thus removing the temporal structure. The mass above 16 (observed
number of maxima records for SP500 over the period) is 1
40 .

Figure 8.6: The CDF of the records of negative extrema for the resampled SP500 (103 times) reshufﬂed as
above. The mass above 9 (observed number of minima records for SP500 over the period) is 2
5 .

8.7 convergence tests

Convergence laws can help us exclude some classes of probability distributions.

8.7.1 Test 1: Kurtosis under Aggregation

Result: The verdict as shown in Figure 8.9 is that the one-month kurtosis is not lower than
the daily kurtosis and, as we add data, no drop in kurtosis is observed. Further we would
(cid:0)2. This allows us to safely eliminate numerous classes, which includes
expect a drop (cid:24) N
stochastic volatility in its simple formulations such as gamma variance. Next we will get
into the technicals of the point and the strength of the evidence.

A typical misunderstanding is as follows. In a note "What can Taleb learn from Markowitz"
[180], Jack L. Treynor, one of the founders of portfolio theory, defended the ﬁeld with the

SP5001950-2017# maximaMean records for maxima of reshuffled returns5101520t0.20.40.60.81.0NtMean records for minima of reshuffled returnsSP5001950-2017# minima5101520t0.20.40.60.81.0Nt138

diagnostic tools for fat tails. with application to the sp500 †

Figure 8.7: Running shorter period, t = 1000 days of overlapping observations for the records of maxima(top)
and minima (bottom), compared to the expected Harmonic number H(1000).

argument that the data may be fat tailed "short term" but in something called the "long
term" things become Gaussian. Sorry, it is not so. (We add the ergodic problem that blurs,
if not eliminate, the distinction between long term and short term).

The reason is that, simply we cannot possibly talk about "Gaussian" if kurtosis is inﬁnite.
Further, for a (cid:25) 3, Central limit operates very slowly, requires n of the order of 106 to
become acceptable, not what we have in the history of markets. [19]

8.7.2 Test 2: Excess Conditional Expectation

Result: The verdict from this test is that, as we can see in Figure 8.11, that the con-
ditional expectation of X (and (cid:0)X), conditional on X is greater than some arbitrary
value K, remains proportional to K.

50100150t51015N50100150t51015N8.7 convergence tests

139

Figure 8.8: We separate positive and negative logarithmic returns and use overlapping cumulative returns
from 1 up to 15. Clearly the negative returns appear to follow a Power Law while the Paretianity of the right
one is more questionable.

Figure 8.9: Visual convergence diagnos-
tics for the kurtosis of the SP500 over
the past 17000 observations. We compute
the kurtosis at different lags for the raw
SP500 and reshufﬂed data. While the 4th
norm is not convergent for raw data, it
is clearly so for the reshufﬂed series. We
can thus assume that the "fat tailedness"
is attributable to the temporal structure of
the data, particularly the clustering of its
volatility. See Table 6.1 for the expected
drop at speed 1=n2 for thin-tailed distri-
butions.

Deﬁnition 8.6
Let K be in R+, the relative excess conditional expectation:

≜

φ+
K

E(X)j
K

X>K

0.050.100.20|X}0.010.050.100.501P>X10.050.100.150.200.25|X}0.010.050.100.501P>X20.050.100.150.200.250.30|X}0.010.050.100.501P>X30.100.150.200.250.30|X}0.010.050.100.501P>X40.100.150.200.250.30|X}0.010.050.100.501P>X50.100.150.200.250.30|X}0.010.050.100.501P>X60.100.150.200.250.30|X}0.010.050.100.501P>X70.100.150.200.250.300.35|X}0.010.050.100.501P>X80.100.150.200.250.300.35|X}0.010.050.100.501P>X90.100.150.200.250.300.35|X}0.010.050.100.501P>X100.100.150.200.250.300.35|X}0.010.050.100.501P>X110.100.150.200.250.300.35|X}0.010.050.100.501P>X12SP500ReshuffledSP500020406080100lag5101520Kurtosis140

diagnostic tools for fat tails. with application to the sp500 †

Figure 8.10: MS Plot (or "law of large numbers for p moments") for p = 4 for the SP500 compared to p = 4
for a Gaussian and stochastic volatility for a matching Kurtosis ( 30) over the entire period. Convergence, if
any, does not take place in any reasonable time. MS Plot for moment p = 3 for the SP500 compared to p = 4
for a Gaussian. We can safely say that the 4th moment is inﬁnite and the 3rd one is indeterminate

Figure 8.11: Condexp as test of scalability

φ(cid:0)
K

≜

E((cid:0)X)j
K

X>K

φ

K = 0

lim
K!¥

We have

50001000015000n0.20.40.60.81.0MS(4)SP500MSPlotfor4thM50001000015000n0.20.40.60.81.0MS(4)4thMomentMSPlotforThinTailedDist.50001000015000n0.20.40.60.81.0MS(4)MSPlotforMatchingStochasticVolatility50001000015000n0.20.40.60.81.0MS(3)SP500MSPlotfor3rdM0.040.060.080.10K1.41.51.61.7(-X-X>K)K8.7 convergence tests

141

Figure 8.12: Visual Identiﬁcation of Paretianity

Figure 8.13: Empirical distri-
bution ﬁts a stable with a
l =
1.62

Figure 8.14: The tails can pos-
sibly ﬁt an inﬁnite mean stable
a
l = 1

for distributions outside the power-law basin, and

19870.010.020.050.100.20|X}0.0010.0100.1001P>X-0.20-0.15-0.10-0.050.050.101020304050600.050.100.150.20|X}0.050.100.501P>XStableDistribution[1,1.,1.,0.0690167,0.00608249]142

diagnostic tools for fat tails. with application to the sp500 †

Figure 8.15:
Comparing
SP500 squared returns
to
those from a Standard cor-
GARCH(1,1)
responding
for 16500 observations,
for
illustrative purposes only. A
more formal proof comes from
the Conditional Expectation
test.

Figure 8.16:
mated empirically.

kappa-n

esti-

lim
K!¥
for distribution satisfying Deﬁnition 1. Note the van der Wijk’s law [32],[159].
Figure 8.11 shows the following:
values, which is incompatible with non-Paretian distributions.

K = k

the conditional expectation does not drop for large

φ

a
1 (cid:0) a

8.7.3 Test 3- Instability of 4th moment

A main argument in [159] is that in 50 years of SP 500 observations, a single one represents
>80 % of the Kurtosis. Similar effect are seen with other socioeconomic variables, such as
gold, oil, silver other stock markets, soft commodities. Such sample dependence of the
kurtosis means that the fourth moment does not have the stability, that is, does not exist.

8.7.4 Test 4: MS Plot

An additional approach consists in examining the behavior of moments in a given sample,
is the Maximum-to-Sum plot, or MS plot as it is plotted as in Figure 8.10. The MS Plot

0.000.010.020.030.040.05Central LimitNegative Ret.Positive and Negative Ret.0100200300400500600n1.751.801.851.901.952.00κn8.7 convergence tests

143

Figure 8.17: Drawdowns and Scalability

Figure 8.18: Paretianity
Drawdowns and Scale

of

relies on a consequence of the law of large numbers [124]. For a sequence X1, X2, ..., Xn of
!a.s.
nonnegative i.i.d. random variables, if for p = 1, 2, 3..., E[X p] < ¥, then R
0 as n ! ¥, where S

p
n = M
p
n) the partial

is the partial sum, and M

p
n = max(X

p
n=S

p
n =

X

p
n

p
1 , ..., X

p
i

n
(cid:229)
i=1

maximum.

We show by comparison the MS plot for a Gaussian and that for a Student T with a tail
exponent of 3. We observe that the SP 500 show the typical characteristics of a steep power
law, as in 16,000 observations (50 years) it does not appear to drop to the point of allowing
the functioning of the law of large numbers.

-0.8-0.6-0.4-0.20.0-0.8-0.6-0.4-0.20.0-0.8-0.6-0.4-0.20.0-0.8-0.6-0.4-0.20.05d100d252 d0.020.050.100.200.50|X}0.0010.0100.1001P>X144

diagnostic tools for fat tails. with application to the sp500 †

Figure 8.19: Fitting a Stable
Distribution to drawdowns

Figure 8.20: Correcting the empirical distribution function with a Frechet

8.8 conclusion

This part will be completed.

0.550.600.650.70|X}0.050.100.501P>XEmpiricalSurvivalFunctionFrechet,1Frechet,lowertailindex0.050.100.150.200.250.30K0.20.40.60.81.0PrPart III

I S S U E S W I T H FAT TA I L S

9 E X P E R T C A L I B R AT I O N U N D E R FAT

TA I L S

Being a good forecaster doesn’t lead to having a good P/L, and vice versa. How
someone’s forecasting record is not a proxy for abilities to perform in the real world
–and can be a reverse indicator under fat tails.

The decision "science" literature revolves around the concept of "calibration"

of probabilistic assessment –an evaluation of how close someone providing
odds of events turns out to be on average (under some operation of the law
of large number deemed satisfactory). The methods are highly unempirical
except in narrow circumstances of binary payoffs (such as those entailing a
"win/lose" outcome) –and generalizing from these payoffs is described as falling for the
"Ludic fallacy" [158]. Further, as we saw in Chapter 2, in the real world, people care about
expectation, not probability, under condition of survival. Most businesses have severely
skewed payoffs, so being calibrated in probability in meaningless there.
Finally, the difference between "binary" and "vanilla" options has been discussed in [156]:
the former are bets paying a ﬁxed amount, the latter have full payoff, and one cannot be
properly replicated using another, especially under fat tails –meaning performance in one
does not translate to performance into the other. And it is highly ﬂawed to separate binary
dF(x) for
probability into the integral – where F is the probability measure, substituting
∫

∫

xdF(x).

Recall that in the real world, it is P/L that counts, and making "calibration" mistakes
where it doesn’t matter should be encouraged, not penalized. If a mistake doesn’t cost you
anything, it is clearly not a mistake. And if it costs you something, and has been present
in society for a long time, consider that there may be hidden evolutionary advantages to
these types of mistakes –of the following sort: mistaking a bear for a stone is worse than
mistaking a stone for a bear. In risk management, one should never operate in probability
space.

In short, probabilistic calibration requires estimation of the zeroth moment while
the real world requires all moments (outside of gambling bets where payoffs are
truncated), and it is a central property of fat tails that higher moments are explosive
(even "inﬁnite") and count more and more.

Worse, the literature operates under the illusion ([50] [98]) that the measure, being prob-
ability distribution-free, is robust. Not quite: since probability is a kernel for summation
or integration, i.e., something that enters into something else, and not the end product,
mistakes on probability will rarely map to mistakes on expectation.

147

148

expert calibration under fat tails

This note will show:

(cid:15) Metrics of calibration of "right" or "wrong" probabilistic assessment –such as the Brier
score– follow a thin-tailed distribution. We will derive the exact distribution of the
metric.

(cid:15) The mean of the n-summed (or averaged) random variable follows what mean by the
law of large numbers – which, for power laws, remains a power law regardless of the
number of summands.

9.0.1 Background: the robustness of metrics

What is the rationale behind the claim that the probability distribution of probability (sic)
is robust? Let us refresh a standard result behind nonparametric discussions and tests,
dating from Kolmogorov [100]. The probability integral transform is as follows.Probability
integral transform Let X have a continuous distribution for which the cumulative distri-
bution function (CDF) is FX. Then –in the absence of additional information –the random
variable U deﬁned as U = FX(X) is uniform between 0 and 1.
The proof is as follows: For t 2 [0, 1],

P(Y (cid:20) u) = P(FX(X) (cid:20) u) = P(X (cid:20) F

(cid:0)1
X (u)) = FX(F

(cid:0)1
X (u)) = u.

(9.1)

which is the cumulative distribution function of the uniform. This is the case regardless of
the probability distribution of X.

Figure 9.1: Maximum vari-
ance for a beta distribution
with support in [0, 1] with pa-
rameters a and b; as both ap-
proach 0 we get a "U" shaped
function, the Arcsine distribu-
tion shown in 9.3.

9.1 the brier score follows a thin tailed distribution

This discussion will use for calibration metric the Brier score (DeFinetti, [44]), deemed
as the gold standard since it doesn’t allow arbitrage and requires perfect probabilistic
calibration: someone betting than an event has a probability 1 of occurring will get a
perfect score only if the event occurs all the time.
Let ln be Brier score for a certain speciﬁc type of events:

9.1 the brier score follows a thin tailed distribution

149

Figure 9.2: Maximum en-
tropy for a beta distribution
with support in [0, 1] with
parameters a and b . Un-
like with the variance, we
have the maximum reached
with the uniform distribution
(a = b = 1) and the mini-
mum reached when either is
0. Axes show a, b,and Enta,b

.

n
(cid:229)
t=1

At)2,

ln =

( ft (cid:0) 1

1
n
where ft 2 [0, 1] is the probability announced by the forecaster for event t, 1
At 2 f0, 1g
an indicator function whether the event under consideration took place or not, and n the
total number of such forecasting events.
Clearly we are dealing with 1) ft beta distributed (either as a special case the uniform
distribution when purely random, as derived above, or a beta distribution when one has
some accuracy, for which the uniform is a special case), and 2) 1
At a Bernoulli variable
with probability p.

Let us consider the general case.

Figure 9.3: Beta distributions
with maximum entropy (Uni-
form) and maximum variance
(Arcsine).

fa,b(x) =

xa(cid:0)1(1 (cid:0) x)b(cid:0)1
B(a, b)

, 0 < x < 1

As we can see in ﬁgures 9.1 and 9.2, we have the maximum uncertainty in a set of param-
eters where lima!0 b!0 fa,b, which produces the Arcsine distribution.

a=b=0(ArcSine)a=b=1(Uniform)0.20.40.60.81.0x0.81.01.21.4PDF(x)150

expert calibration under fat tails

Theorem 9.1 (Convergence in distribution of the Brier score)
Under independence of ( f1
ln

D(cid:0)! N (m, sn) where N denotes the Gaussian distribution.

A1), ldots, ( fn (cid:0) n

An),

(cid:0) 1

Further, a bit unwieldy but controllable:

m =

a2((cid:0)p) + a2 (cid:0) ap + a + b2 p + bp
(a + b)(a + b + 1)

,

sn = (2a(1 + a)b(3(1 + b) + 2a(3 + a + b))

+ (a (cid:0) b)(1 + a + b)(3 + a + b)(2a + a3 (cid:0) b(1 + b)(2 + b) + a2(3 + b)
(cid:0) ab(4 + b))p (cid:0) (a (cid:0) b)2(1 + a + b)2(2 + a + b)(3 + a + b)p2)=(n(a + b)2(1 + a + b)2(2 + a + b)(3 + a + b)).

It will also be shown that the Brier score has thinner tails than the Gaussian as its kurtosis

is lower.

Proof. We start with yt = ( ft (cid:0) 1
where 1F1(.; .; .) is the hypergeometric distribution 1F1(a; b; z) = (cid:229)¥
A = peit (cid:0) p + 1.
tic function of 1
From here we get the characteristic function for ( ft (cid:0) 1
(

At). The characteristic function of ft ,Y
f = 1F1(a; a + b; it),
zk
ak
k !
bk

. The characteris-

At, Y1

At)2

k=0

(

)

p2

(cid:0)a(cid:0)b+1G(a + b)

(1 (cid:0) p) 2 ˜F2

a + 1
2

,

a
2

;

+ p 2 ˜F2

,

(

a + b
1
2
2
b + 1
2

,

(a + b + 1); it

)

b
2

;

a + b
2

,

1
2

(a + b + 1); it

(9.2)

p

Y

y2(t) =

where 2 ˜F2 is the generalized hypergeometric function regularized 2 ˜F2(., .; ., .; .) = 2 F2(a;b;z)
and pFq(a; b; z) has series expansion (cid:229)¥
(a1)k...(ap)k
k=0
(b1)k...(bp)k
bol.

(G(b1)...G(bq))
zk=k!, were (a)(.) is the Pockhammer sym-

We can proceed to prove directly from there the convergence in distribution for the aver-

age:

lim
n!¥

Y

y2(t=n)n = exp

(

(

(cid:0) it

a2(p (cid:0) 1) + a(p (cid:0) 1) (cid:0) b(b + 1)p
(a + b)(a + b + 1)

)

)

which is that of a degenerate Gaussian (Dirac) with location parameter (cid:0) (a2(p(cid:0)1)+a(p(cid:0)1)(cid:0)b(b+1)p)
We can ﬁnally assess the speed of convergence, the rate at which higher moments map to
¶4 log Y.(.)
j
those of a Gaussian distribution: consider the behavior of the 4th cumulant k
¶t4
1) in the maximum entropy case of a = b = 1:

4 = (cid:0)i

(a+b)(a+b+1)

t!0:

.

j

k

4

a=1,b=1= (cid:0) 32

4725n3

9.2 the p/l follows a different distribution

151

2) In the maximum variance case, using l’Hôpital:

4 = (cid:0) (p (cid:0) 1)p(6(p (cid:0) 1)p + 1)

n3

k

lim
a!0
b!0

Se we have k

!0
n!¥ at rate n

4

(cid:0)3.

Further, we can extract its probability density function of the Brier score:

(p (cid:0) 1)za=2

p(zt) = (

(

2

1 (cid:0)
(p

p

)

z
z (cid:0) 1

(

b (cid:0) p
)

1 (cid:0)
zB(a, b)

p

)

a

z

zb=2

, 0 < z < 1.

9.2 the p/l follows a different distribution

(cid:0) fi(xi)), where Xi is the
Now under consideration is the distribution of Gn = 1
n
random variable, fi(xi) a numerical function of the random variable, wi the weight and oi
the contractual price (or the numerical value) for the exposure.

(cid:229)n (w

ioi

If the distribution of Xi is in the power law class and f is linear (seen in the absence
of any higher derivative than the ﬁrst), then, by a standard result, the distribution of Gn
is in the same class. The proof is standard and the convergence of Gn as n gets larger is
explained in [166], as well as the properties of the deviation from its mean.

Part IV

I N E Q U A L I T Y E S T I M AT O R S

10 G I N I E S T I M AT I O N U N D E R I N F I N I T E

V A R I A N C E ‡

This Chapter is about the problems related to the estimation of the Gini

index in presence of a fat-tailed data generating process, i.e. one in the
stable distribution class with ﬁnite mean but inﬁnite variance (i.e. with
tail index a 2 (1, 2)). We show that, in such a case, the Gini coefﬁcient
cannot be reliably estimated using conventional nonparametric meth-
ods, because of a downward bias that emerges under fat tails. This has important
implications for the ongoing discussion about economic inequality.

We start by discussing how the nonparametric estimator of the Gini index undergoes
a phase transition in the symmetry structure of its asymptotic distribution, as the
data distribution shifts from the domain of attraction of a light-tailed distribution
to that of a fat-tailed one, especially in the case of inﬁnite variance. We also show
how the nonparametric Gini bias increases with lower values of a. We then prove
that maximum likelihood estimation outperforms nonparametric methods, requiring
a much smaller sample size to reach efﬁciency.

Finally, for fat-tailed data, we provide a simple correction mechanism to the small
sample bias of the nonparametric estimator based on the distance between the mode
and the mean of its asymptotic distribution.

10.1 introduction

Wealth inequality studies represent a ﬁeld of economics, statistics and econophysics ex-
posed to fat-tailed data generating processes, often with inﬁnite variance [28, 99]. This is
not at all surprising if we recall that the prototype of fat-tailed distributions, the Pareto,
has been proposed for the ﬁrst time to model household incomes [125]. However, the
fat-tailedness of data can be problematic in the context of wealth studies, as the property
of efﬁciency (and, partially, consistency) does not necessarily hold for many estimators of
inequality and concentration [99? ].
The scope of this work is to show how fat tails affect the estimation of one of the most
celebrated measures of economic inequality, the Gini index [56, 79, 99], often used (and
abused) in the econophysics and economics literature as the main tool for describing the
distribution and the concentration of wealth around the world [131? ? ].
The literature concerning the estimation of the Gini index is wide and comprehensive
(e.g. [56? ] for a review), however, strangely enough, almost no attention has been paid to

0 (With A. Fontanari and P. Cirillo), coauthors

155

156

gini estimation under infinite variance ‡

its behavior in presence of fat tails, and this is curious if we consider that: 1) fat tails are
ubiquitous in the empirical distributions of income and wealth [99, 131], and 2) the Gini
index itself can be seen as a measure of variability and fat-tailedness [55, 57, 58, 69].
The standard method for the estimation of the Gini index is nonparametric: one com-
putes the index from the empirical distribution of the available data using Equation (10.5)
below. But, as we show in this paper, this estimator suffers from a downward bias when
we deal with fat-tailed observations. Therefore our goal is to close this gap by deriving
the limiting distribution of the nonparametric Gini estimator in presence of fat tails, and
propose possible strategies to reduce the bias. We show how the maximum likelihood
approach, despite the risk of model misspeciﬁcation, needs much fewer observations to
reach efﬁciency when compared to a nonparametric one.

1

Our results are relevant to the discussion about wealth inequality, recently rekindled by
Thomas Piketty in [131], as the estimation of the Gini index under fat tails and inﬁnite
variance may cause several economic analyses to be unreliable, if not markedly wrong.
Why should one trust a biased estimator?

Figure 10.1: The Italian statistician Cor-
rado Gini, 1884-1965. source: Bocconi

By fat-tailed data we indicate those data generated by a positive random variable X with
cumulative distribution function (c.d.f.) F(x), which is regularly-varying of order a [93],
that is, for ¯F(x) := 1 (cid:0) F(x), one has

a ¯F(x) = L(x),

x!¥ x
lim

(10.1)

L(cx)
L(x) = 1 with c > 0, and where

where L(x) is a slowly-varying function such that limx!¥
a > 0 is called the tail exponent .
Regularly-varying distributions deﬁne a large class of random variables whose proper-
ties have been extensively studied in the context of extreme value theory [? ? ], when
dealing with the probabilistic behavior of maxima and minima. As pointed out in [32],
regularly-varying and fat-tailed are indeed synonyms. It is known that, if X1, ..., Xn are
i.i.d. observations with a c.d.f. F(x) in the regularly-varying class, as deﬁned in Equation
(10.1), then their data generating process falls into the maximum domain of attraction of a

1 A similar bias also affects the nonparametric measurement of quantile contributions, i.e. those of the type “the top
1% owns x% of the total wealth" [170]. This paper extends the problem to the more widespread Gini coefﬁcient,
and goes deeper by making links with the limit theorems.

10.1 introduction

157

Fréchet distribution with parameter r, in symbols X 2 MDA(F(r))[85]. This means that,
for the partial maximum Mn = max(X1, ..., Xn), one has

(

P

(cid:0)1
n (Mn (cid:0) bn) (cid:20) x

a

)

d! F(r) = e

(cid:0)r

(cid:0)x

,

r > 0,

(10.2)

with an > 0 and bn 2 R two normalizing constants. Clearly, the connection between the
regularly-varying coefﬁcient a and the Fréchet distribution parameter r is given by: a = 1
r
[? ].
The Fréchet distribution is one of the limiting distributions for maxima in extreme value
theory, together with the Gumbel and the Weibull; it represents the fat-tailed and un-
bounded limiting case [85]. The relationship between regularly-varying random variables
and the Fréchet class thus allows us to deal with a very large family of random variables
(and empirical data), and allows us to show how the Gini index is highly inﬂuenced by
maxima, i.e. extreme wealth, as clearly suggested by intuition [69, 99], especially under inﬁ-
nite variance. Again, this recommends some caution when discussing economic inequality
under fat tails.

It is worth remembering that the existence (ﬁniteness) of the moments for a fat-tailed
random variable X depends on the tail exponent a, in fact

d

E(X

E(X

) < ¥ if d (cid:20) a,
d
) = ¥ if d > a.

(10.3)

In this work, we restrict our focus on data generating processes with ﬁnite mean and
inﬁnite variance, therefore, according to Equation (10.3), on the class of regularly-varying
distributions with tail index a 2 (1, 2).
Table 10.1 and Figure 10.2 present numerically and graphically our story, already suggest-
ing its conclusion, on the basis of artiﬁcial observations sampled from a Pareto distribution
(Equation (10.13) below) with tail parameter a equal to 1.1.
Table 10.1 compares the nonparametric Gini index of Equation (10.5) with the maximum
likelihood (ML) tail-based one of Section 10.3. For the different sample sizes in Table
10.1, we have generated 108 samples, averaging the estimators via Monte Carlo. As the
ﬁrst column shows, the convergence of the nonparametric estimator to the true Gini value
(g = 0.8333) is extremely slow and monotonically increasing; this suggests an issue not
only in the tail structure of the distribution of the nonparametric estimator but also in its
symmetry.
Figure 10.2 provides some numerical evidence that the limiting distribution of the non-
parametric Gini index loses its properties of normality and symmetry [68], shifting towards
a skewed and fatter-tailed limit, when data are characterized by an inﬁnite variance. As
we prove in Section 10.2, when the data generating process is in the domain of attraction of
a fat-tailed distribution, the asymptotic distribution of the Gini index becomes a skewed-
to-the-right a-stable law. This change of behavior is responsible of the downward bias of
the nonparametric Gini under fat tails. However, the knowledge of the new limit allows
us to propose a correction for the nonparametric estimator, improving its quality, and thus
reducing the risk of badly estimating wealth inequality, with all the possible consequences
in terms of economic and social policies [99, 131].
The rest of the paper is organized as follows. In Section 10.2 we derive the asymptotic
distribution of the sample Gini index when data possess an inﬁnite variance. In Section
10.3 we deal with the maximum likelihood estimator; in Section 10.4 we provide an illus-
tration with Paretian observations; in Section 10.5 we propose a simple correction based

158

gini estimation under infinite variance ‡

Table 10.1: Comparison of the Nonparametric (NonPar) and the Maximum Likelihood (ML) Gini estimators,
using Paretian data with tail a = 1.1 (ﬁnite mean, inﬁnite variance) and different sample sizes. Number of
Monte Carlo simulations: 108.

n

Nonpar

ML

Error Ratio

(number of obs.) Mean
0.711
0.750
0.775
0.790
0.802

103
104
105
106
107

Bias Mean
0.8333
-0.122
0.8333
-0.083
0.8333
-0.058
0.8333
-0.043
0.8333
-0.031

Bias
0
0
0
0
0

1.4
3
6.6
156
105+

2

Figure 10.2:
His-
the Gini
tograms
for
estima-
nonparametric
two Paretian
for
tors
distributions
(type
I)
with different
in-
tail
dices, with ﬁnite and
inﬁnite variance (plots
have been centered to
ease comparison). Sam-
ple size: 103. Number
of samples: 102 for each
distribution.

on the mode-mean distance of the asymptotic distribution of the nonparametric estimator,
to take care of its small-sample bias. Section 10.6 closes the paper. A technical Appendix
contains the longer proofs of the main results in the work.

10.2 asymptotics of the nonparametric estimator under infinite vari-

ance

We now derive the asymptotic distribution for the nonparametric estimator of the Gini
index when the data generating process is fat-tailed with ﬁnite mean but inﬁnite variance.

The so-called stochastic representation of the Gini g is

g =

1
2

E (jX

′ (cid:0) X”j)
m

2 [0, 1],

(10.4)

′

where X
and with ﬁnite mean E(X) = m. The quantity E (jX
Difference" (GMD) [? ]. For later convenience we also deﬁne g =

and X” are i.i.d. copies of a random variable X with c.d.f. F(x) 2 [c, ¥), c > 0,
′ (cid:0) X”j) is known as the "Gini Mean
q
m with q =

E(jX

.

′(cid:0)X”j)
2

The Gini index of a random variable X is thus the mean expected deviation between any
two independent realizations of X, scaled by twice the mean [59].

10.2 asymptotics of the nonparametric estimator under infinite variance

159

The most common nonparametric estimator of the Gini index for a sample X1, ..., Xn is

deﬁned as

GNP(Xn) =

which can also be expressed as

(cid:229)1(cid:20)i<j(cid:20)n

jXi
(n (cid:0) 1) (cid:229)n

(cid:0) Xj
i=1 Xi

j

,

GNP(Xn) =

i=1(2( i(cid:0)1
(cid:229)n
n(cid:0)1
(cid:229)n
i=1 X(i)

(cid:0) 1)X(i)

=

1
n
1
n

(cid:229)n

i=1 Z(i)
(cid:229)n
i=1 Xi

,

(10.5)

(10.6)

)

(

(cid:0) 1

i(cid:0)1
n(cid:0)1

where X(1), X(2), ..., X(n) are the ordered statistics of X1, ..., Xn, such that: X(1) < X(2) < ... <
X(n) and Z(i) = 2
X(i). The asymptotic normality of the estimator in Equation
(10.6) under the hypothesis of ﬁnite variance for the data generating process is known [99?
]. The result directly follows from the properties of the U-statistics and the L-estimators
involved in Equation (10.6)
A standard methodology to prove the limiting distribution of the estimator in Equation
(10.6), and more in general of a linear combination of order statistics, is to show that, in the
limit for n ! ¥, the sequence of order statistics can be approximated by a sequence of i.i.d
random variables [43, 104]. However, this usually requires some sort of L2 integrability of
the data generating process, something we are not assuming here.
Lemma 10.1 (proved in the Appendix) shows how to deal with the case of sequences of
order statistics generated by fat-tailed L1-only integrable random variables.

Lemma 10.1
Consider the following sequence Rn = 1
n
of a uniformly distributed i.i.d random sample. Assume that F
results hold:

(cid:0) U(i))F

i=1( i
n

(cid:229)n

and

Rn

a(cid:0)1
a

n
L0(n)

L1(cid:0)! 0,

L1(cid:0)! 0,

Rn

(cid:0)1(U(i)) where U(i) are the order statistics
(cid:0)1(U) 2 L1. Then the following

(10.7)

(10.8)

with a 2 (1, 2) and L0(n) a slowly-varying function.

10.2.1 A quick recap on a-stable random variables

We here introduce some notation for a-stable distributions, as we need them to study the
asymptotic limit of the Gini index.
A random variable X follows an a-stable distribution, in symbols X (cid:24) S(a, b, g, d), if its
characteristic functionis

{

E(eitX) =

e

e

(cid:0)gajtja
(cid:0)gjtj(1+ib 2

(1(cid:0)ib sign(t)) tan(

pa
2 )+idt

p sign(t)) lnjtj+idt

a ̸= 1
a = 1

,

where a 2 (0, 2) governs the tail, b 2 [(cid:0)1, 1] is the skewness, g 2 R+ is the scale param-
eter, and d 2 R is the location one. This is known as the S1 parametrization of a-stable
distributions [123, 145].
Interestingly, there is a correspondence between the a parameter of an a-stable random
variable, and the a of a regularly-varying random variable as per Equation (10.1): as shown
in [68, 123], a regularly-varying random variable of order a is a-stable, with the same tail

160

gini estimation under infinite variance ‡

coefﬁcient. This is why we do not make any distinction in the use of the a here. Since
we aim at dealing with distributions characterized by ﬁnite mean but inﬁnite variance, we
restrict our focus to a 2 (1, 2), as the two a’s coincide.
Recall that, for a 2 (1, 2], the expected value of an a-stable random variable X is equal to
the location parameter d, i.e. E(X) = d. For more details, we refer to [? ? ].
The standardized a-stable random variable is expressed as

Sa,b (cid:24) S(a, b, 1, 0).

(10.9)

We note that a-stable distributions are a subclass of inﬁnitely divisible distributions.
Thanks to their closure under convolution, they can be used to describe the limiting behav-
ior of (rescaled) partials sums, Sn = (cid:229)n
i=1 Xi, in the General central limit theorem (GCLT)
setting [68]. For a = 2 we obtain the normal distribution as a special case, which is the
limit distribution for the classical CLTs, under the hypothesis of ﬁnite variance.

In what follows we indicate that a random variable is in the domain of attraction of an
a-stable distribution, by writing X 2 DA(Sa). Just observe that this condition for the limit
of partial sums is equivalent to the one given in Equation (10.2) for the limit of partial
maxima [68? ].

10.2.2 The a-stable asymptotic limit of the Gini index

2 , 1), given that r = 1
a .

Consider a sample X1, ..., Xn of i.i.d. observations with a continuous c.d.f. F(x) in the
regularly-varying class, as deﬁned in Equation (10.1), with tail index a 2 (1, 2). The data
generating process for the sample is in the domain of attraction of a Fréchet distribution
with r 2 ( 1
For the asymptotic distribution of the Gini index estimator, as presented in Equation
(10.6), when the data generating process is characterized by an inﬁnite variance, we can
make use of the following two theorems: Theorem 1 deals with the limiting distribution
of the Gini Mean Difference (the numerator in Equation (10.6)), while Theorem 2 extends
the result to the complete Gini index. Proofs for both theorems are in the Appendix.

Theorem 1
Consider a sequence (Xi)1(cid:20)i(cid:20)n of i.i.d random variables from a distribution X on [c, +¥) with
c > 0, such that X is in the domain of attraction of an a-stable random variable, X 2 DA(Sa),
with a 2 (1, 2). Then the sample Gini mean deviation (GMD)
satisﬁes the following limit
in distribution:

i=1 Z(i)
n

(

)

(cid:229)n

a(cid:0)1
a

n
L0(n)

1
n

n
(cid:229)
i=1

(cid:0) q

Z(i)

d! Sa,1,

(10.10)

where Zi = (2F(Xi) (cid:0) 1)Xi, E(Zi) = q, L0(n) is a slowly-varying function such that Equation
(10.37) holds (see the Appendix), and Sa,1 is a right-skewed standardized a-stable random variable
deﬁned as in Equation (10.9).
(cid:229)n
Moreover the statistic 1
n
P! q.
(cid:229)n

i=1 Z(i) is an asymptotically consistent estimator for the GMD, i.e.

1
n

i=1 Z(i)

Note that Theorem 1 could be restated in terms of the maximum domain of attraction
MDA(F(r)) as deﬁned in Equation (10.2).

10.3 the maximum likelihood estimator

161

Theorem 2
Given the same assumptions of Theorem 1, the estimated Gini index GNP(Xn) =
the following limit in distribution

(cid:229)n
i=1 Z(i)
(cid:229)n
i=1 Xi

satisﬁes

(

a(cid:0)1
a

n
L0(n)

GNP(Xn) (cid:0)

)

q
m

d! Q,

(10.11)

P! q

m , 0).

m = g.

is an asymptotically consistent estimator for the Gini index, i.e.

where E(Zi) = q, E(Xi) = m, L0(n) is the same slowly-varying function deﬁned in Theorem 1 and
Q is a right-skewed a-stable random variable S(a, 1, 1
(cid:229)n
i=1 Z(i)
(cid:229)n
i=1 Xi

Furthermore the statistic
(cid:229)n
i=1 Z(i)
(cid:229)n
i=1 Xi
In the case of fat tails with a 2 (1, 2), Theorem 2 tells us that the asymptotic distribu-
tion of the Gini estimator is always right-skewed notwithstanding the distribution of the
underlying data generating process. Therefore heavily fat-tailed data not only induce a
fatter-tailed limit for the Gini estimator, but they also change the shape of the limit law,
which deﬁnitely moves away from the usual symmetric Gaussian. As a consequence, the
Gini estimator, whose asymptotic consistency is still guaranteed [104], will approach its
true value more slowly, and from below. Some evidence of this was already given in Table
10.1.

10.3 the maximum likelihood estimator

Theorem 2 indicates that the usual nonparametric estimator for the Gini index is not the
best option when dealing with inﬁnite-variance distributions, due to the skewness and
the fatness of its asymptotic limit. The aim is to ﬁnd estimators that still preserve their
asymptotic normality under fat tails, which is not possible with nonparametric methods,
as they all fall into the a-stable Central Limit Theorem case [68? ]. Hence the solution is to
use parametric techniques.
Theorem 3 shows how, once a parametric family for the data generating process has been
identiﬁed, it is possible to estimate the Gini index via MLE. The resulting estimator is not
just asymptotically normal, but also asymptotically efﬁcient.
In Theorem 3 we deal with random variables X whose distribution belongs to the large
and ﬂexible exponential family [147], i.e. whose density can be represented as

with q 2 R, and where T(x), h(q), h(x), A(q) are known functions.

fq(x) = h(x)e(h(q)T(x)(cid:0)A(q)),

Theorem 3
Let X (cid:24) Fq such that Fq is a distribution belonging to the exponential family. Then the Gini
index obtained by plugging-in the maximum likelihood estimator of q, GML(Xn)q, is asymptotically
normal and efﬁcient. Namely:

p

(

n

GML (Xn)q (cid:0) gq

)

D! N

(

0, g

′2
q I

(cid:0)1(q)

)

,

(10.12)

where g

′
q = dgq

dq and I(q) is the Fisher Information.

162

gini estimation under infinite variance ‡

Proof. The result follows easily from the asymptotic efﬁciency of the maximum likelihood
estimators of the exponential family, and the invariance principle of MLE. In particular,
the validity of the invariance principle for the Gini index is granted by the continuity and
the monotonicity of gq with respect to q. The asymptotic variance is then obtained by
application of the delta-method [147].

10.4 a paretian illustration

We provide an illustration of the obtained results using some artiﬁcial fat-tailed data. We
choose a Pareto I [125], with density

f (x) = ac

a

(cid:0)a(cid:0)1 , x (cid:21) c.

x

(10.13)

It is easy to verify that the corresponding survival function ¯F(x) belongs to the regularly-
varying class with tail parameter a and slowly-varying function L(x) = c
. We can therefore
apply the results of Section 10.2 to obtain the following corollaries.

a

Corollary 10.1
Let X1, ..., Xn be a sequence of i.i.d. observations with Pareto distribution with tail parameter
a 2 (1, 2). The nonparametric Gini estimator is characterized by the following limit:

0

DNP

n = GNP(Xn) (cid:0) g (cid:24) S

@a, 1,

1

(a (cid:0) 1)
a

A .

, 0

(cid:0) 1
a
C
a
a(cid:0)1
a

n

(10.14)

Proof. Without loss of generality we can assume c = 1 in Equation (10.13). The results is a
mere application of Theorem 2, remembering that a Pareto distribution is in the domain
of attraction of a-stable random variables with slowly-varying function L(x) = 1. The se-
(cid:0) 1
quence cn to satisfy Equation (10.37) becomes cn = n
a
, therefore we have L0(n) = C
,
a
which is independent of n. Additionally the mean of the distribution is also a function of
a, that is m =

(cid:0) 1
1
a
a C
a

a
a(cid:0)1 .

Corollary 10.2
Let the sample X1, ..., Xn be distributed as in Corollary 10.1, let GML
be the maximum likelihood
estimator for the Gini index as deﬁned in Theorem 3. Then the MLE Gini estimator, rescaled by its
true mean g, has the following limit:

q

DML

n = GML

a

(

(Xn) (cid:0) g (cid:24) N

0,

4a2
n(2a (cid:0) 1)4

)

,

(10.15)

where N indicates a Gaussian.

q

1

=

Proof. The functional form of the maximum likelihood estimator for the Gini index is
2aML(cid:0)1 [99]. The result then follows from the fact that the Pareto dis-
known to be GML
tribution (with known minimum value xm) belongs to an exponential family and therefore
satisﬁes the regularity conditions necessary for the asymptotic normality and efﬁciency of
the maximum likelihood estimator. Also notice that the Fisher information for a Pareto
distribution is 1
a2 .

Now that we have worked out both asymptotic distributions, we can compare the qual-
ity of the convergence for both the MLE and the nonparametric case when dealing with

10.4 a paretian illustration

163

Paretian data, which we use as the prototype for the more general class of fat-tailed obser-
vations.

In particular, we can approximate the distribution of the deviations of the estimator from
the true value g of the Gini index for ﬁnite sample sizes, by using Equations (10.14) and
(10.15).

(a) a = 1.8

(b) a = 1.6

(c) a = 1.4

(d) a = 1.2

Figure 10.3: Comparisons between the maximum likelihood and the nonparametric asymptotic distributions
for different values of the tail index a. The number of observations for MLE is ﬁxed to n = 100. Note that, even
if all distributions have mean zero, the mode of the distributions of the nonparametric estimator is different
from zero, because of the skewness.

Figure 10.3 shows how the deviations around the mean of the two different types of esti-
mators are distributed and how these distributions change as the number of observations
increases. In particular, to facilitate the comparison between the maximum likelihood and
the nonparametric estimators, we ﬁxed the number of observation in the MLE case, while
letting them vary in the nonparametric one. We perform this study for different types
of tail indices to show how large the impact is on the consistency of the estimator. It is
worth noticing that, as the tail index decreases towards 1 (the threshold value for a inﬁnite
mean), the mode of the distribution of the nonparametric estimator moves farther away
from the mean of the distribution (centered on 0 by deﬁnition, given that we are dealing
with deviations from the mean). This effect is responsible for the small sample bias ob-
served in applications. Such a phenomenon is not present in the MLE case, thanks to the
the normality of the limit for every value of the tail parameter.

−0.10−0.050.000.050.10020406080100120140Limit distribution for a = 1.8, MLE vs Non−ParametricDeviation from mean valueMLEn = 100n = 500n = 1000−0.15−0.10−0.050.000.050.100.15020406080Limit distribution for a = 1.6, MLE vs Non−ParametricDeviation from mean valueMLEn = 100n = 500n = 1000−0.2−0.10.00.10.201020304050Limit distribution for a = 1.4, MLE vs Non−ParametricDeviation from mean valueMLEn = 100n = 500n = 1000−0.3−0.2−0.10.00.10.2051015202530Limit distribution for a = 1.2, MLE vs Non−ParametricDeviation from mean valueMLEn = 100n = 500n = 1000164

gini estimation under infinite variance ‡

We can make our argument more rigorous by assessing the number of observations ˜n
needed for the nonparametric estimator to be as good as the MLE one, under different tail
scenarios. Let’s consider the likelihood-ratio-type function

r(c, n) =

PS(jDNP
PN(jDML
100

n

j> c)
j> c)

,

(10.16)

n

j> c) and PN(jDML
100

where PS(jDNP
j> c) are the probabilities (a-stable and Gaussian respec-
tively) of the centered estimators in the nonparametric, and in the MLE cases, of exceeding
the thresholds (cid:6)c, as per Equations (10.15) and (10.14).
In the nonparametric case the
number of observations n is allowed to change, while in the MLE case it is ﬁxed to 100.
We then look for the value ˜n such that r(c, ˜n) = 1 for ﬁxed c.

Table 10.2 displays the results for different thresholds c and tail parameters a. In particu-
lar, we can see how the MLE estimator outperforms the nonparametric one, which requires
a much larger number of observations to obtain the same tail probability of the MLE with
n ﬁxed to 100. For example, we need at least 80 (cid:2) 106 observations for the nonparametric
estimator to obtain the same probability of exceeding the (cid:6)0.02 threshold of the MLE one,
when a = 1.2.

Table 10.2: The number of observations ˜n needed for the nonparametric estimator to match the tail probabilities,
for different threshold values c and different values of the tail index a, of the maximum likelihood estimator
with ﬁxed n = 100.

Threshold c as per Equation (10.16):

a
1.8
1.5
1.2

0.005
27 (cid:2) 103
21 (cid:2) 104
33 (cid:2) 108

0.01
12 (cid:2) 105
21 (cid:2) 104
67 (cid:2) 107

0.015
12 (cid:2) 106
46 (cid:2) 105
20 (cid:2) 107

0.02
63 (cid:2) 105
81 (cid:2) 107
80 (cid:2) 106

Interestingly, the number of observations needed to match the tail probabilities in Equa-
tion (10.16) does not vary uniformly with the threshold. This is expected, since as the
threshold goes to inﬁnity or to zero, the tail probabilities remain the same for every value
of n. Therefore, given the unimodality of the limit distributions, we expect that there will
be a threshold maximizing the number of observations needed to match the tail probabili-
ties, while for all the other levels the number of observations will be smaller.

We conclude that, when in presence of fat-tailed data with inﬁnite variance, a plug-in

MLE based estimator should be preferred over the nonparametric one.

10.5 small sample correction

Theorem 2 can be also used to provide a correction for the bias of the nonparametric esti-
mator for small sample sizes. The key idea is to recognize that, for unimodal distributions,
most observations come from around the mode. In symmetric distributions the mode and
the mean coincide, thus most observations will be close to the mean value as well, not so
for skewed distributions: for right-skewed continuous unimodal distributions the mode is
lower than the mean. Therefore, given that the asymptotic distribution of the nonparamet-
ric Gini index is right-skewed, we expect that the observed value of the Gini index will be
usually lower than the true one (placed at the mean level). We can quantify this difference

10.5 small sample correction

165

(i.e. the bias) by looking at the distance between the mode and the mean, and once this
distance is known, we can correct our Gini estimate by adding it back
Formally, we aim to derive a corrected nonparametric estimator GC(Xn) such that

3

.

GC(Xn) = GNP(Xn) + jjm(GNP(Xn)) (cid:0) E(GNP(Xn))jj,

(10.17)

where jjm(GNP(Xn)) (cid:0) E(GNP(Xn))jj is the distance between the mode m and the mean of
the distribution of the nonparametric Gini estimator GNP(Xn).
Performing the type of correction described in Equation (10.17) is equivalent to shifting
the distribution of GNP(Xn) in order to place its mode on the true value of the Gini index.
Ideally, we would like to measure this mode-mean distance jjm(GNP(Xn)) (cid:0) E(GNP(Xn))jj
on the exact distribution of the Gini index to get the most accurate correction. However,
the ﬁnite distribution is not always easily derivable as it requires assumptions on the
parametric structure of the data generating process (which, in most cases, is unknown
for fat-tailed data [99]). We therefore propose to use the limiting distribution for the
nonparametric Gini obtained in Section 10.2 to approximate the ﬁnite sample distribution,
and to estimate the mode-mean distance with it. This procedure allows for more freedom
in the modeling assumptions and potentially decreases the number of parameters to be
estimated, given that the limiting distribution only depends on the tail index and the
mean of the data, which can be usually assumed to be a function of the tail index itself, as
in the Paretian case where m =
By exploiting the location-scale property of a-stable distributions and Equation (10.11),

a
a(cid:0)1 .

we approximate the distribution of GNP(Xn) for ﬁnite samples by

GNP(Xn) (cid:24) S (a, 1, g(n), g) ,

(10.18)

L0(n)
m

is the scale parameter of the limiting distribution.

where g(n) = 1
a(cid:0)1a

n

As a consequence, thanks to the linearity of the mode for a-stable distributions, we have

jjm(GNP(Xn)) (cid:0) E(GNP(Xn))jj(cid:25)

jjm(a, g(n)) + g (cid:0) gjj=

jjm(a, g(n))jj,

where m(a, g(n)) is the mode function of an a-stable distribution with zero mean.

The implication is that, in order to obtain the correction term, knowledge of the true Gini
index is not necessary, given that m(a, g(n)) does not depend on g. We then estimate the
correction term as

ˆm(a, g(n)) = arg max

s(x),

x

(10.19)

where s(x) is the numerical density of the associated a-stable distribution in Equation
(10.18), but centered on 0. This comes from the fact that, for a-stable distributions, the
mode is not available in closed form, but it can be easily computed numerically [123],
using the unimodality of the law.

The corrected nonparametric estimator is thus

GC(Xn) = GNP(Xn) + ˆm(a, g(n)),

whose asymptotic distribution is

GC(Xn) (cid:24) S (a, 1, g(n), g + ˆm(a, g(n))) .

(10.20)

(10.21)

3 Another idea, which we have tested in writing the paper, is to use the distance between the median and the mean;

the performances are comparable.

166

gini estimation under infinite variance ‡

Note that the correction term ˆm (a, g(n)) is a function of the tail index a and is connected
to the sample size n by the scale parameter g(n) of the associated limiting distribution. It
is important to point out that ˆm(a, g(n)) is decreasing in n, and that limn!¥ ˆm(a, g(n)) !
0. This happens because, as n increases, the distribution described in Equation (10.18)
becomes more and more centered around its mean value, shrinking to zero the distance
between the mode and the mean. This ensures the asymptotic equivalence of the corrected
estimator and the nonparametric one. Just observe that

jG(Xn)C (cid:0) GNP(Xn)j =

lim
n!¥

=

jGNP(Xn) + ˆm(a, g(n)) (cid:0) GNP(Xn)j
j ˆm(a, g(n))j! 0.

lim
n!¥
lim
n!¥

Naturally, thanks to the correction, GC(Xn) will always behave better in small samples.
Consider also that, from Equation (10.21), the distribution of the corrected estimator has
now for mean g + ˆm(a, g(n)), which converges to the true Gini g as n ! ¥.
From a theoretical point of view, the quality of this correction depends on the distance
between the exact distribution of GNP(Xn) and its a-stable limit; the closer the two are to
each other, the better the approximation. However, given that, in most cases, the exact
distribution of GNP(Xn) is unknown, it is not possible to give more details.
From what we have written so far, it is clear that the correction term depends on the tail
index of the data, and possibly also on their mean. These parameters, if not assumed to
be known a priori, must be estimated. Therefore the additional uncertainty due to the
estimation will reﬂect also on the quality of the correction.

We conclude this Section with the discussion of the effect of the correction procedure
with a simple example. In a Monte Carlo experiment, we simulate 1000 Paretian samples
of increasing size, from n = 10 to n = 2000, and for each sample size we compute both
the original nonparametric estimator GNP(Xn) and the corrected GC(Xn). We repeat the
experiment for different a’s. Figure 10.4 presents the results.
It is clear that the corrected estimators always perform better than the uncorrected ones
in terms of absolute deviation from the true Gini value.
In particular, our numerical
experiment shows that for small sample sizes with n (cid:20) 1000 the gain is quite remarkable
for all the different values of a 2 (1, 2). However, as expected, the difference between the
estimators decreases with the sample size, as the correction term decreases both in n and
in the tail index a. Notice that, when the tail index equals 2, we obtain the symmetric
Gaussian distribution and the two estimators coincide, given that, thanks to the ﬁniteness
of the variance, the nonparametric estimator is no longer biased.

10.6 conclusions

In this paper we address the issue of the asymptotic behavior of the nonparametric esti-
mator of the Gini index in presence of a distribution with inﬁnite variance, an issue that
has been curiously ignored by the literature. The central mistake in the nonparametric
methods largely used is to believe that asymptotic consistency translates into equivalent
pre-asymptotic properties.

We show that a parametric approach provides better asymptotic results thanks to the
properties of maximum likelihood estimation. Hence we strongly suggest that, if the col-
lected data are suspected to be fat-tailed, parametric methods should be preferred.

10.6 conclusions

167

(a) a = 1.8

(b) a = 1.6

(c) a = 1.4

(d) a = 1.2

Figure 10.4: Comparisons between the corrected nonparametric estimator (in red, the one on top) and the usual
nonparametric estimator (in black, the one below). For small sample sizes the corrected one clearly improves
the quality of the estimation.

05001000150020000.00.20.40.60.81.0Corrected vs Original Estimator, data Tail index = 1.8Sample sizeEstimator ValuesCorrected EstimatorOriginal EstimatorTrue Value05001000150020000.00.20.40.60.81.0Corrected vs Original Estimator, data Tail index = 1.6Sample sizeEstimator ValuesCorrected EstimatorOriginal EstimatorTrue Value05001000150020000.00.20.40.60.81.0Corrected vs Original Estimator, data Tail index = 1.4Sample sizeEstimator ValuesCorrected EstimatorOriginal EstimatorTrue Value05001000150020000.00.20.40.60.81.0Corrected vs Original Estimator, data Tail index = 1.2Sample sizeEstimator ValuesCorrected EstimatorOriginal EstimatorTrue Value168

gini estimation under infinite variance ‡

In situations where a fully parametric approach cannot be used, we propose a simple
correction mechanism for the nonparametric estimator based on the distance between the
mode and the mean of its asymptotic distribution. Even if the correction works nicely,
we suggest caution in its use owing to additional uncertainty from the estimation of the
correction term.

technical appendix

Proof of Lemma 10.1

Let U = F(X) be the standard uniformly distributed integral probability transform of the
random variable X. For the order statistics, we then have [? ]: X(i)

(cid:0)1(U(i)). Hence

a.s.
= F

Rn =

1
n

n
(cid:229)
i=1

(i=n (cid:0) U(i))F

(cid:0)1(U(i)).

Now by deﬁnition of empirical c.d.f it follows that

Rn =

1
n

n
(cid:229)
i=1

(Fn(U(i)) (cid:0) U(i))F

(cid:0)1(U(i)),

(10.22)

(10.23)

where Fn(u) = 1
n

(cid:229)n

To show that Rn

we notice that

i=1 1Ui
(cid:20)u is the empirical c.d.f of uniformly distributed random variables.
L1(cid:0)! 0, we are going to impose an upper bound that goes to zero. First

EjRnj(cid:20) 1
n

n
(cid:229)
i=1

Ej(Fn(U(i)) (cid:0) U(i))F

(cid:0)1(U(i))j.

(10.24)

To build a bound for the right-hand side (r.h.s) of (10.24), we can exploit the fact that, while
(cid:0)1(U(i)) might be just L1-integrable, Fn(U(i)) (cid:0) U(i) is L
¥
F
integrable, therefore we can use
Hölder’s inequality with q = ¥ and p = 1. It follows that

1
n

n
(cid:229)
i=1

Ej(Fn(U(i)) (cid:0) U(i))F

(cid:0)1(U(i))j(cid:20) 1
n

n
(cid:229)
i=1

E sup
U(i)

j(Fn(U(i)) (cid:0) U(i))jEjF

(cid:0)1(U(i))j.

(10.25)

Then, thanks to the Cauchy-Schwarz inequality, we get

1
n

n
(cid:229)
i=1
(

E sup
U(i)

j(Fn(U(i)) (cid:0) U(i))jEjF

(cid:0)1(U(i))j

(cid:20)

1
n

n
(cid:229)
i=1

(E sup
U(i)

j(Fn(U(i)) (cid:0) U(i))j)2 1
n

n
(cid:229)
i=1

(E(F

(cid:0)1(U(i))))2

) 1
2

.

(10.26)

Now, ﬁrst recall that (cid:229)n
sequence, then notice that E(F
becomes

i=1 F

(cid:0)1(U(i))

i=1 F

a.s.
= (cid:229)n

(cid:0)1(Ui) with Ui, i = 1, ..., n, being an i.i.d
(cid:0)1(Ui)) = m, so that the second term of Equation (10.26)
) 1
2

(

j(Fn(U(i)) (cid:0) U(i))j)2

.

(10.27)

m

1
n

n
(cid:229)
i=1

(E sup
U(i)

10.6 conclusions

169

The ﬁnal step is to show that Equation (10.27) goes to zero as n ! ¥.
We know that Fn is the empirical c.d.f of uniform random variables. Using the triangular
inequality the inner term of Equation (10.27) can be bounded as

1
n

n
(cid:229)
i=1
(cid:20) 1
n

(E sup
U(i)

j(Fn(U(i)) (cid:0) U(i))j)2

(10.28)

n
(cid:229)
i=1

(E sup
U(i)

j(Fn(U(i)) (cid:0) F(U(i)))j)2 +

1
n

n
(cid:229)
i=1

(E sup
U(i)

j(F(U(i)) (cid:0) U(i))j)2.

Since we are dealing with uniforms, we known that F(U) = u, and the second term in the
r.h.s of (10.28) vanishes.
We can then bound E(supU(i)
(VC) inequality, a uniform bound for empirical processes [21, 42, 183], getting

j(Fn(U(i)) (cid:0) F(U(i))j) using the so called Vapnik-Chervonenkis

√

j(Fn(U(i)) (cid:0) F(U(i))j(cid:20)

E sup
U(i)

log(n + 1) + log(2)
n

.

(10.29)

Combining Equation (10.29) with Equation (10.27) we obtain

(

m

1
n

n
(cid:229)
i=1

(E sup
U(i)

j(Fn(U(i)) (cid:0) U(i))j)2

) 1
2

√

(cid:20) m

log(n + 1) + log(2)
n

,

(10.30)

which goes to zero as n ! ¥, thus proving the ﬁrst claim.
For the second claim, it is sufﬁcient to observe that the r.h.s of (10.30) still goes to zero

when multiplied by n

a(cid:0)1a
L0(n) if a 2 (1, 2).

Proof of Theorem 1

The ﬁrst part of the proof consists in showing that we can rewrite Equation (10.10) as a
function of i.i.d random variables in place of order statistics, to be able to apply a Central
Limit Theorem (CLT) argument.

Let’s start by considering the sequence

1
n

n
(cid:229)
i=1

Z(i) =

(

i (cid:0) 1
n (cid:0) 1

2

1
n

n
(cid:229)
i=1

)

(cid:0) 1

(cid:0)1(U(i)).

F

(10.31)

Using the integral probability transform X
adding and removing 1
n
rewritten as

2U(i)

(cid:0) 1

(cid:229)n

i=1

F

(

)

d
= F

(cid:0)1(U) with U standard uniform, and
in Equation (10.31) can be

(cid:0)1(U(i)), the r.h.s.

1
n

n
(cid:229)
i=1

Z(i) =

1
n

n
(cid:229)
i=1

(2U(i)

(cid:0) 1)F

(cid:0)1(U(i)) +

(

i (cid:0) 1
n (cid:0) 1

1
n

n
(cid:229)
i=1

2

)

(cid:0) U(i)

(cid:0)1(U(i)).

F

(10.32)

170

gini estimation under infinite variance ‡

Then, by using the properties of order statistics [43] we obtain the following almost sure
equivalence

1
n

n
(cid:229)
i=1

Z(i)

a.s.
=

1
n

n
(cid:229)
i=1

(2Ui

(cid:0) 1)F

(cid:0)1(Ui) +

(

i (cid:0) 1
n (cid:0) 1

1
n

n
(cid:229)
i=1

2

)

(cid:0) U(i)

(cid:0)1(U(i)).

F

(10.33)

Note that the ﬁrst term in the r.h.s of (10.33) is a function of i.i.d random variables as
desired, while the second term is just a reminder, therefore

1
n

n
(cid:229)
i=1

Z(i)

a.s.
=

1
n

n
(cid:229)
i=1

Zi + Rn,

(cid:0) 1)F

with Zi = (2Ui

i=1(2( i(cid:0)1
(cid:229)n
n(cid:0)1
Given Equation (10.10) and exploiting the decomposition given in (10.33) we can rewrite
our claim as

(cid:0)1(Ui) and Rn = 1

(cid:0) U(i)))F

(cid:0)1(U(i)).

n

(

)

(

)

a(cid:0)1
a

n
L0(n)

1
n

n
(cid:229)
i=1

(cid:0) q

Z(i)

=

a(cid:0)1
a

n
L0(n)

1
n

n
(cid:229)
i=1

(cid:0) q

Zi

+

a(cid:0)1
a

n
L0(n)

Rn.

(10.34)

From the second claim of the Lemma 10.1 and Slutsky Theorem, the convergence in
Equation (10.10) can be proven by looking at the behavior of the sequence

a(cid:0)1
a

n
L0(n)

(

)

1
n

n
(cid:229)
i=1

(cid:0) q

Zi

,

(10.35)

(cid:0)a

(cid:0) 1)F

(cid:0)1(Ui) = (2F(Xi) (cid:0) 1)Xi. This reduces to proving that Zi is in the fat

where Zi = (2Ui
tails domain of attraction.
Recall that by assumption X 2 DA(Sa) with a 2 (1, 2). This assumption enables us
to use a particular type of CLT argument for the convergence of the sum of fat-tailed
random variables. However, we ﬁrst need to prove that Z 2 DA(Sa) as well, that is
P(jZj> z) (cid:24) L(z)z
Notice that

, with a 2 (1, 2) and L(z) slowly-varying.

P(j ˜Zj> z) (cid:20) P(jZj> z) (cid:20) P(2X > z),
where ˜Z = (2U (cid:0) 1)X and U ? X. The ﬁrst bound holds because of the positive dependence
between X and F(X) and it can be proven rigorously by noting that 2UX (cid:20) 2F(X)X by the
so-called re-arrangement inequality [87]. The upper bound conversely is trivial.
Using the properties of slowly-varying functions, we have P(2X > z) (cid:24) 2
. To
show that ˜Z 2 DA(Sa), we use the Breiman’s Theorem, which ensure the stability of the
a-stable class under product, as long as the second random variable is not too fat-tailed
[190].
To apply the Theorem we re-write P(j ˜Zj> z) as

L(z)z

(cid:0)a

a

P(j ˜Zj> z) = P( ˜Z > z) + P((cid:0) ˜Z > z) = P( ˜UX > z) + P((cid:0) ˜UX > z),

where ˜U is a standard uniform with ˜U ? X.
We focus on P( ˜UX > z) since the procedure is the same for P((cid:0) ˜UX > z). We have

P( ˜UX > z) = P( ˜UX > zj ˜U > 0)P( ˜U > 0) + P( ˜UX > zj ˜U (cid:20) 0)P( ˜U (cid:20) 0),

10.6 conclusions

171

for z ! +¥.
Now, we have that P( ˜UX > zj ˜U (cid:20) 0) ! 0, while, by applying Breiman’s Theorem,
P( ˜UX > zj ˜U > 0) becomes

P( ˜UX > zj ˜U > 0) ! E( ˜U

ajU > 0)P(X > z)P(U > 0).

Therefore

P(j ˜Zj> z) ! 1
2

E( ˜U

ajU > 0)P(X > z) +

1
2

E(((cid:0) ˜U)

ajU (cid:20) 0)P(X > z).

From this

P(j ˜Zj> z) ! 1
2

ajU > 0) + E(((cid:0) ˜U

ajU (cid:20) 0)]

P(X > z)[E( ˜U)
a
1 (cid:0) a P(X > z) (cid:24) 2

2

a

1 (cid:0) a L(z)z

(cid:0)a

.

=

We can then conclude that, by the squeezing Theorem [? ],

P(jZj> z) (cid:24) L(z)z

(cid:0)a

,

as z ! ¥. Therefore Z 2 DA(Sa).
We are now ready to invoke the Generalized Central Limit Theorem (GCLT) [? ] for the

sequence Zi, i.e.

nc

(cid:0)1
n

(

1
n

n
(cid:229)
i=1

)

Zi

(cid:0) E(Zi)

d! Sa,b.

(10.36)

with E(Zi) = q, Sa,b a standardized a-stable random variable, and where cn is a sequence
which must satisfy

lim
n!¥

nL(cn)
ca
n

=

G(2 (cid:0) a)jcos(

pa

2 )j

a (cid:0) 1

= Ca.

(10.37)

Notice that cn can be represented as cn = n
function possibly different from L(n).
The skewness parameter b is such that

1
a L0(n), where L0(n) is another slowly-varying

P(Z > z)
P(jZj> z)

! 1 + b
2

.

Recalling that, by construction, Z 2 [(cid:0)c, +¥), the above expression reduces to

P(Z > z)
P(Z > z) + P((cid:0)Z > z)

! P(Z > z)
P(Z > z)

= 1 ! 1 + b
2

,

(10.38)

therefore b = 1. This, combined with Equation (10.34), the result for the reminder Rn of
Lemma 10.1 and Slutsky Theorem, allows us to conclude that the same weak limits holds
for the ordered sequence of Z(i) in Equation (10.10) as well.

172

gini estimation under infinite variance ‡

Proof of Theorem 2

The ﬁrst step of the proof is to show that the ordered sequence
(cid:229)n
(cid:229)n

Gini index, is equivalent in distribution to the i.i.d sequence
it is sufﬁcient to apply the factorization in Equation (10.33) to Equation (10.11), getting

(cid:229)n
i=1 Z(i)
(cid:229)n
i=1 Xi
i=1 Zi
. In order to prove this,
i=1 Xi

, characterizing the

(

a(cid:0)1
a

n
L0(n)

(cid:229)n
(cid:229)n

i=1 Zi
i=1 Xi

(cid:0)

q
m

)

a(cid:0)1
a

n
L0(n)

+

Rn

n
i=1 Xi

(cid:229)n

.

(10.39)

By Lemma 10.1 and the application of the continuous mapping and Slutsky Theorems, the
second term in Equation (10.39) goes to zero at least in probability. Therefore to prove the
claim it is sufﬁcient to derive a weak limit for the following sequence

a(cid:0)1
a

n

1
L0(n)

(

(cid:229)n
(cid:229)n

i=1 Zi
i=1 Xi

(cid:0)

q
m

)

.

(10.40)

Expanding Equation (10.40) and recalling that Zi = (2F(Xi) (cid:0) 1)Xi, we get
(

(

a(cid:0)1
a

n
L0(n)

n
i=1 Xi

(cid:229)n

1
n

n
(cid:229)
i=1

Xi

2F(Xi) (cid:0) 1 (cid:0)

.

(10.41)

))

q
m

in Equation (10.41) converges in probability to 1

The term n
m by an application of
(cid:229)n
the continuous mapping Theorem, and the fact that we are dealing with positive random
variables X. Hence it will contribute to the ﬁnal limit via Slutsky Theorem.

i=1 Xi

We ﬁrst start by focusing on the study of the limit law of the term

a(cid:0)1
a

n
L0(n)

1
n

n
(cid:229)
i=1

(

Xi

2F(Xi) (cid:0) 1 (cid:0)

)

.

q
m

(10.42)

Set ˆZi = Xi(2F(Xi) (cid:0) 1 (cid:0) q
In order to apply a GCLT argument to characterize the limit distribution of the sequence
a(cid:0)1a
ˆZi we need to prove that ˆZ 2 DA(Sa). If so then we can apply GCLT to
n
L0(n)

m ) and note that E( ˆZi) = 0, since E(Zi) = q and E(Xi) = m.

(cid:229)n

i=1

1
n

(

a(cid:0)1
a

n
L0(n)

ˆZi

(cid:229)n

i=1
n

)

(cid:0) E( ˆZi)

.

(10.43)

Note that, since E( ˆZi) = 0, Equation (10.43) equals Equation (10.42).
To prove that ˆZ 2 DA(Sa), remember that ˆZi = Xi(2F(Xi) (cid:0) 1 (cid:0) q
m ) is just Zi = Xi(2F(Xi) (cid:0)
q
m . Therefore the same argument used in Theorem 1 for Z applies here to
1) shifted by
show that ˆZ 2 DA(Sa). In particular we can point out that ˆZ and Z (therefore also X)
share the same a and slowly-varying function L(n).
Notice that by assumption X 2 [c, ¥) with c > 0 and we are dealing with continuous
q
distributions, therefore ˆZ 2 [(cid:0)c(1 +
m ), ¥). As a consequence the left tail of ˆZ does not
contribute to changing the limit skewness parameter b, which remains equal to 1 (as for
Z) by an application of Equation (10.38).

10.6 conclusions

173

Therefore, by applying the GCLT we ﬁnally get

a(cid:0)1
a

n

1
L0(n)

(

(cid:229)n
(cid:229)n

i=1 Zi
i=1 Xi

(cid:0)

q
m )

d(cid:0)! 1

m S(a, 1, 1, 0).

(10.44)

We conclude the proof by noting that, as proven in Equation (10.39), the weak limit of the
Gini index is characterized by the i.i.d sequence of
rather than the ordered one, and
that an a-stable random variable is closed under scaling by a constant [145].

i=1 Zi
i=1 Xi

(cid:229)n
(cid:229)n

11 O N T H E S U P E R - A D D I T I V I T Y A N D

E S T I M AT I O N B I A S E S O F Q U A N T I L E
C O N T R I B U T I O N S ‡

Sample measuresa of top centile contributions to the total (concentration)

are downward biased, unstable estimators, extremely sensitive to sam-
ple size and concave in accounting for large deviations. It makes them
particularly unﬁt in domains with Power Law tails, especially for low
values of the exponent. These estimators can vary over time and in-
crease with the population size, as shown in this article, thus providing the illusion of
structural changes in concentration. They are also inconsistent under aggregation and
mixing distributions, as the weighted average of concentration measures for A and B
will tend to be lower than that from A [ B. In addition, it can be shown that under
such fat tails, increases in the total sum need to be accompanied by increased sample
size of the concentration measurement. We examine the estimation superadditivity
and bias under homogeneous and mixed distributions.

a With R. Douady

11.1 introduction

Vilfredo Pareto noticed that 80% of the land in Italy belonged to 20% of the population, and
vice-versa, thus both giving birth to the power law class of distributions and the popular
saying 80/20. The self-similarity at the core of the property of power laws [109] and [110]
allows us to recurse and reapply the 80/20 to the remaining 20%, and so forth until one
obtains the result that the top percent of the population will own about 53% of the total
wealth.

It looks like such a measure of concentration can be seriously biased, depending on
how it is measured, so it is very likely that the true ratio of concentration of what Pareto
observed, that is, the share of the top percentile, was closer to 70%, hence changes year-
on-year would drift higher to converge to such a level from larger sample.
In fact, as
we will show in this discussion, for, say wealth, more complete samples resulting from
technological progress, and also larger population and economic growth will make such a
measure converge by increasing over time, for no other reason than expansion in sample
space or aggregate value.

The core of the problem is that, for the class one-tailed fat-tailed random variables, that is,
bounded on the left and unbounded on the right, where the random variable X 2 [xmin, ¥),

175

176

on the super-additivity and estimation biases of quantile contributions ‡

Figure 11.1: The young Vil-
fredo Pareto, before he discov-
ered power laws.

the in-sample quantile contribution is a biased estimator of the true value of the actual
quantile contribution.

Let us deﬁne the quantile contribution

kq = q

E[XjX > h(q)]
E[X]

where h(q) = inffh 2 [xmin, +¥) , P(X > h) (cid:20) qg is the exceedance threshold for the
probability q.
For a given sample (Xk)1(cid:20)k(cid:20)n, its "natural" estimator bkq (cid:17) qthpercentile
academic studies, can be expressed, as

, used in most

total

bkq (cid:17)

(cid:229)n

i=1

1

Xi> ˆh(q)Xi
i=1 Xi

(cid:229)n

where ˆh(q) is the estimated exceedance threshold for the probability q :

ˆh(q) = inffh :

1
n

n
(cid:229)
i=1

1

x>h

(cid:20) qg

11.2 estimation for unmixed pareto-tailed distributions

177

We shall see that the observed variable bkq is a downward biased estimator of the true ratio
kq, the one that would hold out of sample, and such bias is in proportion to the fatness of
tails and, for very fat tailed distributions, remains signiﬁcant, even for very large samples.

11.2 estimation for unmixed pareto-tailed distributions

Let X be a random variable belonging to the class of distributions with a "power law" right
tail, that is:

P(X > x) (cid:24) L(x) x

(cid:0)a

(11.1)

where L : [xmin, +¥) ! (0, +¥) is a slowly varying function, deﬁned as limx!+¥
for any k > 0.
There is little difference for small exceedance quantiles (<50%) between the various pos-
sible distributions such as Student’s t, Lévy a-stable, Dagum,[40],[41] Singh-Maddala dis-
tribution [148], or straight Pareto.
For exponents 1 (cid:20) a (cid:20) 2, as observed in [? ], the law of large numbers operates, though
extremely slowly. The problem is acute for a around, but strictly above 1 and severe, as it
diverges, for a = 1.

L(kx)
L(x) = 1

11.2.1 Bias and Convergence

Simple Pareto Distribution
bution bounded from below by xmin > 0, in other words: ϕa(x) = ax
and P(X > x) =
(cid:0)1=a
xmin q

Let us ﬁrst consider ϕa(x) the density of a a-Pareto distri-
(cid:0)a(cid:0)11x(cid:21)xmin ,
. Under these assumptions, the cutpoint of exceedance is h(q) =

and we have:

a
minx

xmin
x

(

)

a

kq =

∫ ¥
h(q) x ϕ(x)dx
∫ ¥
x ϕ(x)dx
xmin

=

(

)

h(q)
xmin

1(cid:0)a

a(cid:0)1
a

= q

(11.2)

If the distribution of X is a-Pareto only beyond a cut-point xcut, which we assume to
for some l > 0, then we still have
be below h(q), so that we have P(X > x) =
h(q) = lq

(cid:0)1=a

and

l
x

(

)

a

kq =

a
a (cid:0) 1

l
E [X]

a(cid:0)1
a

q

The estimation of kq hence requires that of the exponent a as well as that of the scaling
parameter l, or at least its ratio to the expectation of X.
Table 11.1 shows the bias of bkq as an estimator of kq in the case of an a-Pareto distribution
for a = 1.1, a value chosen to be compatible with practical economic measures, such as the
1
wealth distribution in the world or in a particular country, including developped ones.
In such a case, the estimator is extemely sensitive to "small" samples, "small" meaning in
practice 108. We ran up to a trillion simulations across varieties of sample sizes. While
(cid:25) 0.657933, even a sample size of 100 million remains severely biased as seen in the
k

0.01
table.
Naturally the bias is rapidly (and nonlinearly) reduced for a further away from 1, and
becomes weak in the neighborhood of 2 for a constant a, though not under a mixture
1 This value, which is lower than the estimated exponents one can ﬁnd in the literature – around 2 – is, following

[64], a lower estimate which cannot be excluded from the observations.

178

on the super-additivity and estimation biases of quantile contributions ‡

distribution for a, as we shall se later. It is also weaker outside the top 1% centile, hence
this discussion focuses on the famed "one percent" and on low values of the a exponent.

Table 11.1: Biases of Estimator of k = 0.657933 From 1012 Monte Carlo Realizations
Median

Mean

bk(n)

bk(103)
bk(104)
bk(105)
bk(106)
bk(107)
bk(108)

0.405235
0.485916
0.539028
0.581384
0.591506
0.606513

0.367698
0.458449
0.516415
0.555997
0.575262
0.593667

STD
across MC runs
0.160244
0.117917
0.0931362
0.0853593
0.0601528
0.0461397

In view of these results and of a number of tests we have performed around them, we
(cid:0)b(q)(a(cid:0)1) where constants
can conjecture that the bias kq (cid:0) bkq(n) is "of the order of" c(a, q)n
b(q) and c(a, q) need to be evaluated. Simulations suggest that b(q) = 1, whatever the value
of a and q, but the rather slow convergence of the estimator and of its standard deviation
to 0 makes precise estimation difﬁcult.

General Case

In the general case, let us ﬁx the threshold h and deﬁne:

k

h = P(X > h)

E[XjX > h]
E[X]

=

E[X1

X>h]

E[X]

so that we have kq = k

h(q). We also deﬁne the n-sample estimator:

(cid:17)

bk
h

(cid:229)n

1
i=1
(cid:229)n

Xi>hXi

i=1 Xi

where Xi are n independent copies of X. The intuition behind the estimation bias of
kq by bkq lies in a difference of concavity of the concentration measure with respect to
an innovation (a new sample value), whether it falls below or above the threshold. Let
Ah(n) = (cid:229)n
i=1

Xi>hXi and S(n) = (cid:229)n

and assume a frozen thresh-

i=1 Xi, so that bk

h(n) =

1

Ah(n)
S(n)

. The

h(n + 1) =

h(n + 1) (cid:25) Ah(n)+Xn+1
S(n)+Xn+1

Ah(n)
old h. If a new sample value Xn+1 < h then the new value is bk
S(n) + Xn+1
value is convex in Xn+1 so that uncertainty on Xn+1 increases its expectation. At variance,
(cid:0)h = 1 (cid:0) S(n)(cid:0)Ah(n)
(cid:0)h
if the new sample value Xn+1 > h, the new value bk
(cid:0)h ,
which is now concave in Xn+1, so that uncertainty on Xn+1 reduces its value. The com-
petition between these two opposite effects is in favor of the latter, because of a higher
concavity with respect to the variable, and also of a higher variability (whatever its mea-
surement) of the variable conditionally to being above the threshold than to being below.
The fatter the right tail of the distribution, the stronger the effect. Overall, we ﬁnd that
E [bk
h (note that unfreezing the threshold ˆh(q) also tends to reduce the
concentration measure estimate, adding to the effect, when introducing one extra sample
because of a slight increase in the expected value of the estimator ˆh(q), although this effect
is rather negligible). We have in fact the following:

E [Ah(n)]
E [S(n)]

h(n)] (cid:20)

S(n)+Xn+1

= k

11.2 estimation for unmixed pareto-tailed distributions

179

Proposition 11.1
Let X = (X)n

deﬁne: bk

h(X ⊔ Y) =

(cid:229)n

i=1

Xi>hXi + 1
1
(cid:229)n
i=1 Xi + Y

Y>hY

i=1 a random sample of size n > 1

q , Y = Xn+1 an extra single random observation, and

. We remark that, whenever Y > h, one has:

¶2bk

h(X ⊔ Y)
¶Y2

(cid:20) 0.

This inequality is still valid with bkq as the value ˆh(q, X ⊔ Y) doesn’t depend on the particular value
of Y > ˆh(q, X).

We face a different situation from the common small sample effect resulting from high
impact from the rare observation in the tails that are less likely to show up in small samples,
a bias which goes away by repetition of sample runs. The concavity of the estimator
constitutes a upper bound for the measurement in ﬁnite n, clipping large deviations, which
leads to problems of aggregation as we will state below in Theorem 1.

Figure 11.2: Effect of addi-
tional observations on k

Figure 11.3: Effect of addi-
tional observations on k, we
can see convexity on both sides
of h except for values of no ef-
fect to the left of h, an area of
order 1=n

In practice, even in very large sample, the contribution of very large rare events to kq
slows down the convergence of the sample estimator to the true value. For a better, un-
biased estimate, one would need to use a different path: ﬁrst estimating the distribution
and only then, estimating the theoretical tail contribution kq(ˆa, ˆl). Falk
parameters

ˆa, ˆl

(

)

20000400006000080000100000Y0.650.700.750.800.850.900.95Κ(cid:72)(cid:83)Xi(cid:43)Y(cid:76)20406080100Y0.6220.6240.626Κ(cid:72)(cid:83)Xi(cid:43)Y(cid:76)180

on the super-additivity and estimation biases of quantile contributions ‡

[64] observes that, even with a proper estimator of a and l, the convergence is extremely
(cid:0)d=ln n, where the exponent d depends on a and on the toler-
slow, namely of the order of n
ance of the actual distribution vs. a theoretical Pareto, measured by the Hellinger distance.
In particular, d ! 0 as a ! 1, making the convergence really slow for low values of a.

11.3 an inequality about aggregating inequality

j
i, in m sub-samples of size ni each for
For the estimation of the mean of a fat-tailed r.v. (X)
a total of n = (cid:229)m
i=1 ni, the allocation of the total number of observations n between i and
j does not matter so long as the total n is unchanged. Here the allocation of n samples
between m sub-samples does matter because of the concavity of k.
Next we prove that
global concentration as measured by bkq on a broad set of data will appear higher than local
concentration, so aggregating European data, for instance, would give a bkq higher than the
average measure of concentration across countries – an "inequality about inequality".
In
other words, we claim that the estimation bias when using bkq(n) is even increased when
dividing the sample into sub-samples and taking the weighted average of the measured
values bkq(ni).

2

Theorem 11.1
Partition the n data into m sub-samples N = N1
i=1 ni = n, and let S1, . . . , Sm be the sum of variables over each sub-sample, and S = (cid:229)m
(cid:229)m
that over the whole sample. Then we have:

[ . . . [ Nm of respective sizes n1, . . . , nm, with
i=1 Si be

[

E

bkq(N)

]

(cid:21)

[

]

Si
S

E

m
(cid:229)
i=1

[

E

]

bkq(Ni)

If we further assume that the distribution of variables Xj is the same in all the sub-samples. Then
we have:

[

]

E

bkq(N)

(cid:21)

[

]

E

bkq(Ni)

m
(cid:229)
i=1

ni
n

In other words, averaging concentration measures of subsamples, weighted by the to-
tal sum of each subsample, produces a downward biased estimate of the concentration
measure of the full sample.

(

Proof. An elementary induction reduces the question to the case of two sub-samples. Let
)
′
q 2 (0, 1) and (X1, . . . , Xm) and
be two samples of positive i.i.d. random
n
′
j’s having distribution p
variables, the Xi’s having distributions p(dx) and the X
). For
m
(cid:229)
i=1

simplicity, we assume that both qm and qn are integers. We set S =

(dx
n
(cid:229)
i=1

′
1, . . . , X

Xi and S

′
i . We

X

X

=

′

′

′

deﬁne A =

X[i] where X[i] is the i-th largest value of (X1, . . . , Xm), and A

′

=

mq
(cid:229)
i=1

′
[i] where

X

mq
(cid:229)
i=1

X

′
[i] is the i-th largest value of

(

X

′
1, . . . , X

′
n

)

. We also set S

′′

′

= S + S

and A” =

where X

′′
[i] is the i-th largest value of the joint sample (X1, . . . , Xm, X

′
1, . . . , X

′
n).

(m+n)q
(cid:229)
i=1

′′
[i]

X

2 The same concavity – and general bias – applies when the distribution is lognormal, and is exacerbated by high

variance.

11.3 an inequality about aggregating inequality

181

The q-concentration measure for the samples X = (X1, ..., Xm), X

(X1, . . . , Xm, X

′
1, . . . , X

′
n) are:

′

= (X

′
1, ..., X

′
n) and X

′′

=

k =

A
S

k′

=

′
A
S′

k′′

=

′′
A
S′′

We must prove that he following inequality holds for expected concentration measures:

]

[

k′′

E

(cid:21) E

[

]

S
S′′

E [k] + E

]

[

′
S
S′′

]

[

k′

E

We observe that:

A = max

(cid:229)
i2J

Xi

J(cid:26)f1,...,mg
jJj=qm
′
i and A

′′

′

= maxJ′(cid:26)f1,...,ng,jJ′j=qn (cid:229)i2J′ X

and, similarly A
where we have denoted Xm+i = X
′′
fm + 1, ..., m + ng , jJ
A

′
i for i = 1 . . . n. If J (cid:26) f1, ..., mg , jJj = qm and J
= J [ J
= (cid:229)i2J′′ Xi
has cardinal m + n, hence A + A
′
S′′ k′
, whatever the particular sample. Therefore k′′ (cid:21) S
S′′ k + S
]
[

= maxJ′′(cid:26)f1,...,m+ng,jJ′′j=q(m+n) (cid:229)i2J′′ Xi,
′ (cid:26)
(cid:20)

′j = qn, then J

and we have:

[

]

′′

′

′

]

[

E

k′′

(cid:21) E

k

S
S′′

+ E

′
S
S′′

k′

Let us now show that:

[

]

[

]

[

]

[

]

E

k

S
S′′

= E

A
S′′

(cid:21) E

S
S′′

E

A
S

If this is the case, then we identically get for k′
]

[

]

[

:

E

k′

= E

′
S
S′′

′
A
S′′

(cid:21) E

]

[

E

[

′
S
S′′

]

′
A
S′

hence we will have:

]

[

k′′

E

(cid:21) E

[

]

S
S′′

E [k] + E

]

[

′
S
S′′

]

[

k′

E

Xi

1Xi

m
(cid:229)
i=1

(cid:21)T and let B = S (cid:0) A =

Let T = X[mq] be the cut-off point (where [mq] is the integer part of mq), so that A =
m
(cid:229)
1Xi<T. Conditionally to T, A and B are independent:
i=1
A is a sum if mq samples constarined to being above T, while B is the sum of m(1 (cid:0) q)
independent samples constrained to being below T. They are also independent of S
. Let
pA(t, da) and pB(t, db) be the distribution of A and B respectively, given T = t. We recall
that p
[

) is the distribution of S
]

and denote q(dt) that of T. We have:

(ds

Xi

′

′

′

′

E

k

S
S′′

=

x

a + b
a + b + s′

a
a + b

pA(t, da) pB(t, db) q(dt) p

′

′

)

(ds

182

on the super-additivity and estimation biases of quantile contributions ‡

For given b, t and s

′

, a ! a+b

a+b+s′ and a ! a

variable a, hence conditionally to T, B and S
]

[

]

[

(cid:12)
(cid:12)
(cid:12)
(cid:12) T, B, S

E

k

S
S′′

′

= E

A
A + B + S′

(cid:12)
(cid:12)
(cid:12)
(cid:12) T, B, S

a+b are two increasing functions of the same
, we have:

′

′

[

(cid:21) E

A + B
A + B + S′

(cid:12)
(cid:12)
(cid:12)
(cid:12) T, B, S

′

]

[

E

]

(cid:12)
(cid:12)
(cid:12)
(cid:12) T, B, S

′

A
A + B

This inequality being valid for any values of T, B and S
expectation, and we have:

]

[

[

]

[

′

, it is valid for the unconditional
]

E

S
S′′

k

(cid:21) E

S
S′′

E

A
S

If the two samples have the same distribution, then we have:

]

[

k′′

E

(cid:21) m

m + n
[

E [k] +
]

n
m + n

]

[

k′

E

Indeed, in this case, we observe that E
identically distributed, hence E
therefore E
n)E

S
S′′
= mE
m+n . Similarly, E

= 1

S
S′′

X
S′′

X
S′′

[

[

]

[

]

]

[

m+n . Indeed S = (cid:229)m
= m
]
. But we also have E
]

′′
S
S′′
m+n , yielding the result.

X
S′′
[
′
S
S′′

= n

[

]

i=1 Xi and the Xi are

= 1 = (m +

This ends the proof of the theorem.

Let X be a positive random variable and h 2 (0, 1). We remind the theoretical h-concentration
measure, deﬁned as:

k

h =

P(X > h)E [X jX > h ]
E [X]

whereas the n-sample q-concentration measure is bk
S(n) , where A(n) and S(n) are
deﬁned as above for an n-sample X = (X1, . . . , Xn) of i.i.d. variables with the same distri-
bution as X.

h(n) = A(n)

Theorem 11.2
For any n 2 N, we have:

and

E [bk

h(n)] < k

h

lim
n!+¥

bk
h(n) = k

h

a.s. and in probability

h(n)] is an increasing sequence. Moreover, thanks to the law of large numbers, 1

Proof. The above corrolary shows that the sequence nE [bk
E [bk
converges almost surely and in probability to E [X] and 1
and in probability to E [X1
almost surely to k
convergence theorem concludes the argument about the convergence in probability.

h(n)] is super-additive, hence
n S(n)
n A(n) converges almost surely
X>h] = P(X > h)E [X jX > h ], hence their ratio also converges
h. On the other hand, this ratio is bounded by 1. Lebesgue dominated

11.4 mixed distributions for the tail exponent

Consider now a random variable X, the distribution of which p(dx) is a mixture of paramet-
ric distributions with different values of the parameter: p(dx) = (cid:229)m
i (dx). A typical
i=1

i pa

w

11.4 mixed distributions for the tail exponent

183

n-sample of X can be made of ni = w
theorem shows that, in this case, we have:
[

[

]

E

bkq(n, X)

(cid:21)

E

m
(cid:229)
i=1

S(w

in, Xa
S(n, X)

]

i )

[

E

]

bkq(w

in, Xa

i )

in samples of Xa

i with distribution pa

i . The above

When n ! +¥, each ratio

S(w

in, Xa
S(n, X)

i )

converges almost surely to w

i respectively, therefore

we have the following convexity inequality:

kq(X) (cid:21)

m
(cid:229)
i=1

w

kq(Xa

i

i )

The case of Pareto distribution is particularly interesting. Here, the parameter a repre-
sents the tail exponent of the distribution. If we normalize expectations to 1, the cdf of Xa
is Fa(x) = 1 (cid:0)

and we have:

)(cid:0)a

(

x
xmin

and

kq(Xa) = q

a(cid:0)1
a

d2
da2

kq(Xa) = q

a(cid:0)1
a

(log q)2
a3

> 0

Hence kq(Xa) is a convex function of a and we can write:

kq(X) (cid:21)

m
(cid:229)
i=1

w

kq(Xa

i

i ) (cid:21) kq(X¯a)

where ¯a = (cid:229)m
i=1

w

a.

i

Suppose now that X is a positive random variable with unknown distribution, except
that its tail decays like a power low with unknown exponent. An unbiased estimation of
the exponent, with necessarily some amount of uncertainty (i.e., a distribution of possible
true values around some average), would lead to a downward biased estimate of kq.

Because the concentration measure only depends on the tail of the distribution, this in-
equality also applies in the case of a mixture of distributions with a power decay, as in
Equation 15.1:

P(X > x) (cid:24)

w

i Li(x)x

(cid:0)a

j

N
(cid:229)
j=1

(11.3)

The slightest uncertainty about the exponent increases the concentration index. One can
get an actual estimate of this bias by considering an average ¯a > 1 and two surrounding
values a+ = a + d and a(cid:0)

= a (cid:0) d. The convexity inequaly writes as follows:

kq(¯a) = q1(cid:0) 1

¯a <

(

1
2

)

q1(cid:0) 1

a+d + q1(cid:0) 1

a(cid:0)d

So in practice, an estimated ¯a of around 3=2, sometimes called the "half-cubic" expo-
nent, would produce similar results as value of a much closer ro 1, as we used in the

184

on the super-additivity and estimation biases of quantile contributions ‡

previous section. Simply kq(a) is convex, and dominated by the second order effect
ln(q)q1(cid:0) 1
, an effect that is exacerbated at lower values of a.

a+d (ln(q)(cid:0)2(a+d))

(a+d)4

To show how unreliable the measures of inequality concentration from quantiles, consider
that a standard error of 0.3 in the measurement of a causes kq(a) to rise by 0.25.

11.5 a larger total sum is accompanied by increases in bkq

n
(cid:229)
j=1

There is a large dependence between the estimator bkq and the sum S =

Xj : conditional

on an increase in bkq the expected sum is larger. Indeed, as shown in theorem 11.1, bkq and
S are positively correlated.

For the case in which the random variables under concern are wealth, we observe as in
Figure 11.4 such conditional increase; in other words, since the distribution is of the class
of fat tails under consideration, the maximum is of the same order as the sum, additional
wealth means more measured inequality. Under such dynamics, is quite absurd to assume
that additional wealth will arise from the bottom or even the middle. (The same argument
can be applied to wars, epidemics, size or companies, etc.)

Figure 11.4: Effect of addi-
tional wealth on ˆk

11.6 conclusion and proper estimation of concentration

Concentration can be high at the level of the generator, but in small units or subsections we
will observe a lower kq. So examining times series, we can easily get a historical illusion of
rise in, say, wealth concentration when it has been there all along at the level of the process;
and an expansion in the size of the unit measured can be part of the explanation.
Even the estimation of a can be biased in some domains where one does not see the entire
picture: in the presence of uncertainty about the "true" a, it can be shown that, unlike other
parameters, the one to use is not the probability-weighted exponents (the standard average)
but rather the minimum across a section of exponents [? ].

3

3 Accumulated wealth is typically thicker tailed than income, see [72].

6000080000100000120000Wealth0.30.40.50.60.70.80.91.0Κ(cid:72)n(cid:61)104(cid:76)11.6 conclusion and proper estimation of concentration

185

One must not perform analyses of year-on-year changes in bkq without adjustment. It did
not escape our attention that some theories are built based on claims of such "increase" in
inequality, as in [131], without taking into account the true nature of kq, and promulgating
theories about the "variation" of inequality without reference to the stochasticity of the
estimation (cid:0) and the lack of consistency of kq across time and sub-units. What is worse,
rejection of such theories also ignored the size effect, by countering with data of a different
4
sample size, effectively making the dialogue on inequality uninformational statistically.

The mistake appears to be commonly made in common inference about fat-tailed data in
the literature. The very methodology of using concentration and changes in concentration
is highly questionable. For instance, in the thesis by Steven Pinker [134] that the world is
becoming less violent, we note a fallacious inference about the concentration of damage
5
from wars from a bkq with minutely small population in relation to the fat-tailedness.
Owing to the fat-tailedness of war casualties and consequences of violent conﬂicts, an
adjustment would rapidly invalidate such claims that violence from war has statistically
experienced a decline.

11.6.1 Robust methods and use of exhaustive data

We often face argument of the type "the method of measuring concentration from quantile
contributions ˆk is robust and based on a complete set of data". Robust methods, alas, tend
to fail with fat-tailed data, see [? ]. But, in addition, the problem here is worse: even
if such "robust" methods were deemed unbiased, a method of direct centile estimation is
still linked to a static and speciﬁc population and does not aggregage. Accordingly, such
techniques do not allow us to make statistical claims or scientiﬁc statements about the true
properties which should necessarily carry out of sample.

Take an insurance (or, better, reinsurance) company. The "accounting" proﬁts in a year
in which there were few claims do not reﬂect on the "economic" status of the company
and it is futile to make statements on the concentration of losses per insured event based
on a single year sample. The "accounting" proﬁts are not used to predict variations year-
on-year, rather the exposure to tail (and other) events, analyses that take into account the
stochastic nature of the performance. This difference between "accounting" (deterministic)
and "economic" (stochastic) values matters for policy making, particularly under fat tails.
The same with wars: we do not estimate the severity of a (future) risk based on past
in-sample historical data.

11.6.2 How Should We Measure Concentration?

Practitioners of risk managers now tend to compute CVaR and other metrics, methods that
are extrapolative and nonconcave, such as the information from the a exponent, taking the
one closer to the lower bound of the range of exponents, as we saw in our extension to
Theorem 2 and rederiving the corresponding k, or, more rigorously, integrating the func-
tions of a across the various possible states. Such methods of adjustment are less biased
and do not get mixed up with problems of aggregation –they are similar to the "stochastic

4 Financial Times, May 23, 2014 "Piketty ﬁndings undercut by errors" by Chris Giles.
5 Using Richardson’s data, [134]: "(Wars) followed an 80:2 rule: almost eighty percent of the deaths were caused
by two percent (his emph.) of the wars". So it appears that both Pinker and the literature cited for the quantitative
properties of violent conﬂicts are using a ﬂawed methodology, one that produces a severe bias, as the centile
estimation has extremely large biases with fat-tailed wars. Furthermore claims about the mean become spurious
at low exponents.

186

on the super-additivity and estimation biases of quantile contributions ‡

volatility" methods in mathematical ﬁnance that consist in adjustments to option prices by
adding a "smile" to the standard deviation, in proportion to the variability of the parame-
ter representing volatility and the errors in its measurement. Here it would be "stochastic
alpha" or "stochastic tail exponent "
By extrapolative, we mean the built-in extension of
the tail in the measurement by taking into account realizations outside the sample path
that are in excess of the extrema observed.

7 8

6

acknowledgment

The late Benoît Mandelbrot, Branko Milanovic, Dominique Guéguan, Felix Salmon, Bruno
Dupire, the late Marc Yor, Albert Shiryaev, an anonymous referee, the staff at Luciano
Restaurant in Brooklyn and Naya in Manhattan.

6 Also note that, in addition to the centile estimation problem, some authors such as [132] when dealing with
censored data, use Pareto interpolation for unsufﬁcient information about the tails (based on tail parameter),
ﬁlling-in the bracket with conditional average bracket contribution, which is not the same thing as using full
power-law extension; such a method retains a signiﬁcant bias.

7 Even using a lognormal distribution, by ﬁtting the scale parameter, works to some extent as a rise of the standard

deviation extrapolates probability mass into the right tail.

8 We also note that the theorems would also apply to Poisson jumps, but we focus on the powerlaw case in
the application, as the methods for ﬁtting Poisson jumps are interpolative and have proved to be easier to ﬁt
in-sample than out of sample, see [? ].

Part V

S H A D O W M O M E N T S PA P E R S

12 O N T H E S H A D O W M O M E N T S O F

A P PA R E N T LY I N F I N I T E - M E A N
P H E N O M E N A ( W I T H P. C I R I L L O ) ‡

This Chapter proposes an approach to compute the conditional moments

of fat-tailed phenomena that, only looking at data, could be mistakenly
considered as having inﬁnite mean. This type of problems manifests
itself when a random variable Y has a heavy-tailed distribution with an
extremely wide yet bounded support.

We introduce the concept of dual distribution, by means of a log-transformation
that smoothly removes the upper bound. The tail of the dual distribution can then
be studied using extreme value theory, without making excessive parametric assump-
tions, and the estimates one obtains can be used to study the original distribution and
compute its moments by reverting the transformation.

The central difference between our approach and a simple truncation is in the
smoothness of the transformation between the original and the dual distribution, al-
lowing use of extreme value theory.

War casualties, operational risk, environment blight, complex networks and many

other econophysics phenomena are possible ﬁelds of application.

12.1 introduction

Consider a heavy-tailed random variable Y with ﬁnite support [L, H]. W.l.o.g. set L >> 0
for the lower bound, while for upper one H, assume that its value is remarkably large,
yet ﬁnite. It is so large that the probability of observing values in its vicinity is extremely
small, so that in data we tend to ﬁnd observations only below a certain M << H < ¥.
Figure 12.1 gives a graphical representation of the problem. For our random variable Y
with remote upper bound H the real tail is represented by the continuous line. However, if
we only observe values up to M << H, and - willing or not - we ignore the existence of H,
which is unlikely to be seen, we could be inclined to believe the the tail is the dotted one,
the apparent one. The two tails are indeed essentially indistinguishable for most cases, as
the divergence is only evident when we approach H.

Now assume we want to study the tail of Y and, since it is fat-tailed and despite H <
¥, we take it to belong to the so-called Fréchet class
. In extreme value theory [124], a
distribution F of a random variable Y is said to be in the Fréchet class if ¯F(y) = 1 (cid:0) F(y) =

1

1 Note that treating Y as belonging to the Fréchet class is a mistake. If a random variable has a ﬁnite upper bound,

it cannot belong to the Fréchet class, but rather to the Weibull class [85].

189

190

on the shadow moments of apparently infinite-mean phenomena ( with p. cirillo) ‡

(cid:0)a

L(y), where L(y) is a slowly varying function. In other terms, the Fréchet class is the

y
class of all distributions whose right tail behaves as a power law.

Looking at the data, we could be led to believe that the right tail is the dotted line in
Figure 12.1, and our estimation of a shows it be smaller than 1. Given the properties of
power laws, this means that E[Y] is not ﬁnite (as all the other higher moments). This also
implies that the sample mean is essentially useless for making inference, in addition to
any considerations about robustness [114]. But if H is ﬁnite, this cannot be true: all the
moments of a random variable with bounded support are ﬁnite.

A solution to this situation could be to ﬁt a parametric model, which allows for fat tails
and bounded support, such as for example a truncated Pareto [1]. But what happens if
Y only shows a Paretian behavior in the upper tail, and not for the whole distribution?
Should we ﬁt a mixture model?

In the next section we propose a simple general solution, which does not rely on strong

parametric assumptions.

Figure 12.1: Graphical representation of what may happen if one ignores the existence of the ﬁnite upper bound
H, since only M is observed.

12.2 the dual distribution

Instead of altering the tails of the distribution we ﬁnd it more convenient to transform the
data and rely on distributions with well known properties. In Figure 12.1, the real and the
apparent tails are indistinguishable to a great extent. We can use this fact to our advantage,
by transforming Y to remove its upper bound H, so that the new random variable Z - the
dual random variable - has the same tail as the apparent tail. We can then estimate the

                     M      H                                 yRight Tail: 1-F(y)Real TailApparent Tail12.3 back to y: the shadow mean (or population mean)

191

shape parameter a of the tail of Z and come back to Y to compute its moments or, to
be more exact, to compute its excess moments, the conditional moments above a given
threshold, view that we will just extract the information from the tail of Z.

Take Y with support [L, H], and deﬁne the function

φ(Y) = L (cid:0) H log

(

)

.

H (cid:0) Y
H (cid:0) L

(12.1)

¥

, φ(cid:0)1(¥) = H, and φ(cid:0)1(L) = φ(L) = L. Then
We can verify that φ is "smooth": φ 2 C
Z = φ(Y) deﬁnes a new random variable with lower bound L and an inﬁnite upper bound.
Notice that the transformation induced by φ((cid:1)) does not depend on any of the parameters
of the distribution of Y.
By construction, z = φ(y) (cid:25) y for very large values of H. This means that for a very large
upper bound, unlikely to be touched, the results we get for the tail of Y and Z = φ(Y) are
essentially the same, until we do not reach H. But while Y is bounded, Z is not. Therefore
we can safely model the unbounded dual distribution of Z as belonging to the Fréchet
class, study its tail, and then come back to Y and its moments, which under the dual
2
distribution of Z could not exist.
The tail of Z can be studied in different ways, see for instance [124] and [65]. Our
suggestions is to rely on the so-called de Pickands, Balkema and de Haan’s Theorem [85].
This theorem allows us to focus on the right tail of a distribution, without caring too much
about what happens below a given threshold threshold u. In our case u (cid:21) L.
Consider a random variable Z with distribution function G, and call Gu the conditional
df of Z above a given threshold u. We can then deﬁne the r.v. W, representing the rescaled
excesses of Z over the threshold u, so that

Gu(w) = P(Z (cid:0) u (cid:20) wjZ > u) =

G(u + w) (cid:0) G(u)
1 (cid:0) G(u)

,

(cid:0) u, where zG is the right endpoint of G.

for 0 (cid:20) w (cid:20) zG
Pickands, Balkema and de Haan have showed that for a large class of distribution func-
tions G, and a large u, Gu can be approximated by a Generalized Pareto distribution, i.e.
Gu(w) ! GPD(w; x, s), as u ! ¥ where

{

GPD(w; x, s) =

(cid:0)1=x

1 (cid:0) (1 + x w
s )
(cid:0) w
1 (cid:0) e
s

i f x ̸= 0
i f x = 0

, w (cid:21) 0.

(12.2)

The parameter x, known as the shape parameter, and corresponding to 1=a, governs the
fatness of the tails, and thus the existence of moments. The moment of order p of a
Generalized Pareto distributed random variable only exists if and only if x < 1=p, or
3
a > p [124]. Both x and s can be estimated using MLE or the method of moments [85].

12.3 back to y: the shadow mean (or population mean)

With f and g, we indicate the densities of Y and Z.

2 Note that the use of logarithmic transformation is quite natural in the context of utility.
3 There are alternative methods to face ﬁnite (or concave) upper bounds, i.e., the use of tempered power laws (with
exponential dampening)[137] or stretched exponentials [102]; while being of the same nature as our exercise, these
methods do not allow for immediate applications of extreme value theory or similar methods for parametrization.

192

on the shadow moments of apparently infinite-mean phenomena ( with p. cirillo) ‡

We know that Z = φ(Y), so that Y = φ(cid:0)1(Z) = (L (cid:0) H)e
Now, let’s assume we found u = L

L(cid:0)Z
H + H.

(cid:3) (cid:21) L, such that Gu(w) (cid:25) GPD(w; x, s). This implies
that we ﬁnd for Z, can be obtained from the tail

(cid:3)

that the tail of Y, above the same value L
of Z, i.e. Gu.

First we have

And we know that

∫ ¥
L(cid:3) g(z) dz =

∫ φ(cid:0)1(¥)
L(cid:3)

f (y) dy.

g(z; x, s) =

)(cid:0) 1

x (cid:0)1

(

1
s

xz
s

1 +

,

z 2 [L

(cid:3)

, ¥).

Setting a = x(cid:0)1, we get

(12.3)

(12.4)

f (y; a, s) =

(

H

1 + H(log(H(cid:0)L)(cid:0)log(H(cid:0)y))
as
s(H (cid:0) y)

)(cid:0)a(cid:0)1

, y 2 [L

(cid:3)

, H],

(12.5)

or, in terms of distribution function,
(

F(y; a, s) = 1 (cid:0)

1 +

H(log(H (cid:0) L) (cid:0) log(H (cid:0) y))
as

)(cid:0)a

.

(12.6)

Clearly, given that φ is a one-to-one transformation, the parameters of f and g obtained
by maximum likelihood methods will be the same —the likelihood functions of f and g
differ by a scaling constant.

We can derive the shadow mean

4

of Y, conditionally on Y > L

(cid:3)

, as

E[YjY > L

(cid:3)

] =

∫

H

L(cid:3) y f (y; a, s) dy,

(12.7)

obtaining

E[YjZ > L

] = (H (cid:0) L

(cid:3)

(cid:3)

)e

as
H

)a

(

G

1 (cid:0) a,

)

as

(cid:3)

+ L

.

( as
H

H
(cid:3) (cid:21) L can then be estimated by simply plugging in
The conditional mean of Y above L
the estimates ˆa and ˆs, as resulting from the GPD approximation of the tail of Z. It is worth
noticing that if L
] = E[Y], i.e. the conditional mean of Y above Y is
exactly the mean of Y.

= L, then E[YjY > L

(cid:3)

(cid:3)

(12.8)

Naturally, in a similar way, we can obtain the other moments, even if we may need

numerical methods to compute them.

Our method can be used in general, but it is particularly useful when, from data, the tail
of Y appears so fat that no single moment is ﬁnite, as it is often the case when dealing
with operational risk losses, the degree distribution of large complex networks, or other
econophysical phenomena.
For example, assume that for Z we have x > 1. Then both E[ZjZ > L
ﬁnite

] and E[Z] are not
. Figure 12.1 tells us that we might be inclined to assume that also E[Y] is inﬁnite -

(cid:3)

5

4 We call the population average –as opposed to the sample one – "shadow", as it is not immediately visible from

the data.

5 Remember that for a GPD random variable Z, E [Zp] < ¥ iff x < 1=p.

12.4 comparison to other methods

193

and this is what the data are likely to tell us if we estimate ˆx from the tail
of Y. But this
cannot be true because H < ¥, and even for x > 1 we can compute the expected value
E[YjZ > L

] using equation (12.8).

(cid:3)

6

Value-at-Risk and Expected Shortfall

Thanks to equation (12.6), we can compute by inversion the quantile function of Y when
Y (cid:21) L
(

, that is

)

(cid:3)

Q(p; a, s, H, L) = e

(cid:0)g(p)

(cid:3)

as
H + He

e

L

g(p) (cid:0) He

as
H

,

(12.9)

(cid:0)1=a

.

and p 2 [0, 1]. Again, this quantile function is conditional on Y

as(1(cid:0)p)
where g(p) =
H
(cid:3)
being larger than L
From equation (12.9), we can easily compute the Value-at-Risk (VaR) of YjY (cid:21) L
for
whatever conﬁdence level. For example, the 95% VaR of Y, if Y represents operational
losses over a 1-year time horizon, is simply VaRY
Another quantity we might be interested in when dealing with the tail risk of Y is the
so-called expected shortfall (ES), that is E[YjY > u (cid:21) L
]. This is nothing more than a
generalization of equation (12.8).
We can obtain the expected shortfall by ﬁrst computing the mean excess function of
YjY (cid:21) L

0.95 = Q(0.95; a, s, H, L).

, deﬁned as

(cid:3)

(cid:3)

(cid:3)

eu(Y) = E[Y (cid:0) ujY > u] =

∫ ¥
u (u (cid:0) y) f (y; a, s)dy
1 (cid:0) F(u)

,

for y (cid:21) u (cid:21) L

(cid:3)

. Using equation (12.5), we get

eu(Y) = (H (cid:0) L)e

)a

0

@

as
H

( as
H

(

H(cid:0)L
H(cid:0)u

H log

)

1

a

A

+ 1

(cid:2)

as
))

(

G

1 (cid:0) a,

(

as

H

+ log

H (cid:0) L
H (cid:0) u

.

(12.10)

The Expected Shortfall is then simply computed as

E[YjY > u (cid:21) L

(cid:3)

] = eu(Y) + u.

As in ﬁnance and risk management, ES and VaR can be combined. For example we could
be interested in computing the 95% ES of Y when Y (cid:21) L
0.95 +
eVaRY

. This is simply given by VaRY

(Y).

(cid:3)

0.95

12.4 comparison to other methods

There are three ways to go about explicitly cutting a Paretian distribution in the tails (not
counting methods to stretch or "temper" the distribution).

6 Because of the similarities between 1 (cid:0) F(y) and 1 (cid:0) G(z), at least up until M, the GPD approximation will give

two statistically undistinguishable estimates of x for both tails [124].

194

on the shadow moments of apparently infinite-mean phenomena ( with p. cirillo) ‡

1) The ﬁrst one consists in hard truncation, i.e.
in setting a single endpoint for the
distribution and normalizing. For instance the distribution would be normalized between
L and H, distributing the excess mass across all points.
2) The second one would assume that H is an absorbing barrier, that all the realizations
of the random variable in excess of H would be compressed into a Dirac delta function at
H –as practiced in derivative models. In that case the distribution would have the same
density as a regular Pareto except at point H.
3) The third is the one presented here.
The same problem has cropped up in quantitative ﬁnance over the use of truncated
normal (to correct for Bachelier’s use of a straight Gaussian) vs. logarithmic transformation
(Sprenkle, 1961 [150]), with the standard model opting for logarithmic transformation and
the associated one-tailed lognormal distribution. Aside from the additivity of log-returns
and other such beneﬁts, the models do not produce a "cliff", that is an abrupt change in
density below or above, with the instability associated with risk measurements on non-
smooth function.
As to the use of extreme value theory, Breilant et al. (2014)[? ] go on to truncate the dis-
tribution by having an excess in the tails with the transformation Y
) and
apply EVT to the result. Given that the transformation includes the estimated parameter, a
new MLE for the parameter a is required. We ﬁnd issues with such a non-smooth transfor-
mation. The same problem occurs as with ﬁnancial asset models, particularly the presence
an abrupt "cliff" below which there is a density, and above which there is none. The effect
is that the expectation obtained in such a way will be higher than ours, particularly at
values of a < 1, as seen in Figure ??.
We can demonstrate the last point as follows. Assume we observe distribution is Pareto
, x 2
that is in fact truncated but treat it as a Pareto. The density is f (x) = 1
s

(cid:0)a ! (Y

x(cid:0)L
as + 1

(cid:0)a (cid:0) H

)(cid:0)a(cid:0)1

(cid:0)a

(

[L, ¥). The truncation gives g(x) =

( x(cid:0)L

as +1)

(cid:0)a(cid:0)1

s(1(cid:0)aasa(as+H(cid:0)L)(cid:0)a) , x 2 [L, H].

Moments of order p of the truncated Pareto (i.e. what is seen from realizations of the

process), M(p) are:

M(p) =ae
(

(cid:0)ip p(as)

B H

L(cid:0)as

a

(as (cid:0) L)p(cid:0)a
(p + 1, (cid:0)a) (cid:0) B L
L(cid:0)as
)a (cid:0) 1
as
as+H(cid:0)L

(

(p + 1, (cid:0)a)

)

(12.11)

where B(., .) is the Euler Beta function, B(a, b) =

∫

G(a)G(b)
G(a+b) =

1

0 ta(cid:0)1(1 (cid:0) t)b(cid:0)1 dt.

We end up with r(H, a), the ratio of the mean of the soft truncated to that of the truncated
Pareto.
)a ( a

)(cid:0)a

)(cid:0)a

(

r(H, a) =e

(cid:0) a
H

(12.12)

( a
H
(

(cid:0)
((

(a (cid:0) 1)

a + H
a

)

(

a
H

(

a + H
)a
a+H
a
)a (cid:0)
∫ ¥
1

(

)

a
H

Ea

+ H + 1
)a)

a+H
H

et((cid:0)a)
tn dt.

where Ea

(

)

a
H

is the exponential integral eaz =

12.5 applications

195

Figure 12.2: Ratio of the expectation of smooth transformation to truncated.

12.5 applications

Operational risk
known maximum losses.

The losses for a ﬁrm are bounded by the capitalization, with well-

Capped Reinsurance Contracts Reinsurance contracts almost always have caps (i.e., a
maximum claim); but a reinsurer can have many such contracts on the same source of risk
and the addition of the contract pushes the upper bound in such a way as to cause larger
potential cumulative harm.

Violence While wars are extremely fat-tailed, the maximum effect from any such event
cannot exceed the world’s population.

Credit risk A loan has a ﬁnite maximum loss, in a way similar to reinsurance contracts.

City size While cities have been shown to be Zipf distributed, the size of a given city
cannot exceed that of the world’s population.

Environmental harm While these variables are exceedingly fat-tailed, the risk is con-
ﬁned by the size of the planet (or the continent on which they take place) as a ﬁrm upper
bound.

H=105H=1080.40.60.81.01.2α0.20.40.60.81.0E[Xsmooth]E[Xtruncated]196

on the shadow moments of apparently infinite-mean phenomena ( with p. cirillo) ‡

Complex networks

The number of connections is ﬁnite.

Company size

The sales of a company is bound by the GDP.

Earthquakes

The maximum harm from an earthquake is bound by the energy.

Hydrology

The maximum level of a ﬂood can be determined.

13 O N T H E TA I L R I S K O F V I O L E N T

C O N F L I C T A N D I T S
U N D E R E S T I M AT I O N ( W I T H P.
C I R I L L O ) ‡

We examine all possible statistical pictures of violent conﬂicts over common era his-
tory with a focus on dealing with incompleteness and unreliability of data. We apply
methods from extreme value theory on log-transformed data to remove compact sup-
port, then, owing to the boundedness of maximum casualties, retransform the data
and derive expected means. We ﬁnd the estimated mean likely to be at least three
times larger than the sample mean, meaning severe underestimation of the severity of
conﬂicts from naive observation. We check for robustness by sampling between high
and low estimates and jackkniﬁng the data. We study inter-arrival times between
tail events and ﬁnd (ﬁrst-order) memorylessless of events. The statistical pictures
obtained are at variance with the claims about "long peace".

13.1 introduction/summary

Figure 13.1: Values of the tail
exponent a from Hill estimator
obtained across 100,000 differ-
ent rescaled casualty numbers
uniformly selected between low
and high estimates of con-
ﬂict. The exponent is slightly
(but not meaningfully) differ-
ent from the Maximum Likeli-
hood for all data as we focus on
top 100 deviations.

This study is as much about new statistical methodologies with fat tailed (and unreliable
data), as well as bounded random variables with local Power Law behavior, as it is about
the properties of violence.

1

1 Acknowledgments: Captain Mark Weisenborn engaged in the thankless and gruesome task of compiling the data,
checking across sources and linking each conﬂict to a narrative on Wikipedia (see Appendix 1). We also beneﬁted

197

0.480.500.520.540.560.58α0.000.020.040.060.080.100.12Pr198

on the tail risk of violent conflict and its underestimation (with p. cirillo) ‡

Figure 13.2: Q-Q plot of the
rescaled data in the near-tail
plotted against a Pareto II-
Lomax Style distribution.

Figure 13.3: Death toll from
"named conﬂicts" over time.
Conﬂicts lasting more than 25
years are disaggregated into
two or more conﬂicts, each one
lasting 25 years.

Figure 13.4: Rescaled death
toll of armed conﬂict and
regimes over time. Data are
rescaled w.r.t.
today’s world
population. Conﬂicts lasting
more than 25 years are dis-
aggregated into two or more
conﬂicts, each one lasting 25
years.

from generous help on social networks where we put data for scrutiny, as well as advice from historians thanked
in the same appendix. We also thank the late Benoit Mandelbrot for insights on the tail properties of wars and
conﬂicts, as well as Yaneer Bar-Yam, Raphael Douady...

0100000200000300000400000010000020000030000040000013.1 introduction/summary

199

Figure 13.5: Observed "jour-
nalistic" mean compared to
MLE mean (derived
from
rescaling back the data to
compact support) for different
values of a (hence for permuta-
tions of the pair (sa, a)). The
"range of a is the one we get
of
from possible variations
the data from bootstrap and
reliability simulations.

Violence is much more severe than it seems from conventional analyses and the prevail-
ing "long peace" theory which claims that violence has declined. Adapting methods from
extreme value theory, and adjusting for errors in reporting of conﬂicts and historical es-
timates of casualties, we look at the various statistical pictures of violent conﬂicts, with
focus for the parametrization on those with more than 50k victims (in equivalent ratio of
today’s population, which would correspond to (cid:25) 5k in the 18th C.). Contrary to current
discussions, all statistical pictures thus obtained show that 1) the risk of violent conﬂict
has not been decreasing, but is rather underestimated by techniques relying on naive year-
on-year changes in the mean, or using sample mean as an estimator of the true mean
of an extremely fat-tailed phenomenon; 2) armed conﬂicts have memoryless inter-arrival
times, thus incompatible with the idea of a time trend. Our analysis uses 1) raw data,
as recorded and estimated by historians; 2) a naive transformation, used by certain his-
torians and sociologists, which rescales past conﬂicts and casualties with respect to the
actual population; 3) more importantly, a log transformation to account for the fact that
the number of casualties in a conﬂict cannot be larger than the world population. (This is
similar to the transformation of data into log-returns in mathematical ﬁnance in order to
use distributions with support on the real line.)
All in all, among the different classes of data (raw and rescaled), we observe that 1) casu-
In the case of log-rescaled data we observe .4 (cid:20) a (cid:20) .7,
alties are Power Law distributed.
thus indicating an extremely fat-tailed phenomenon with an undeﬁned mean (a result that
is robustly obtained); 2) the inter-arrival times of conﬂicts above the 50k threshold follow a
homogeneous Poisson process, indicating no particular trend, and therefore contradicting
a popular narrative about the decline of violence; 3) the true mean to be expected in the
future, and the most compatible with the data, though highly stochastic, is (cid:25) 3(cid:2) higher
than past mean.
Further, we explain: 1) how the mean (in terms of expected casualties) is severely un-
derestimated by conventional data analyses as the observed mean is not an estimator of
true mean (unlike the tail exponent that provides a picture with smaller noise); 2) how
misconceptions arise from the deceiving lengthy (and volatile) inter-arrival times between
large conﬂicts.

2

2 Many earlier studies have found Paretianity in data, [? ],[27]. Our study, aside from the use of extreme value
techniques, reliability bootstraps, and compact support transformations, varies in both calibrations and interpre-
tation.

Sample ("journalistic") MeanMax Likelihood MeanRange of α0.400.450.500.550.600.650.70α1×1072×1073×1074×1075×1076×1077×1078×107Mean200

on the tail risk of violent conflict and its underestimation (with p. cirillo) ‡

To remedy the inaccuracies of historical numerical assessments, we provide a standard
bootstrap analysis of our estimates, in addition to Monte Carlo checks for unreliability of
wars and absence of events from currently recorded history.

13.2 summary statistical discussion

13.2.1 Results

Peak-Over-Threshold methods show (both raw and rescaled variables) ex-
, where
l(kx)
l(x) = 1 for any

Paretian tails
hibit strong Paretian tail behavior, with survival probability P(X > x) (cid:24) l(x)x
l : [L, +¥) ! (0, +¥) is a slowly varying function, deﬁned as limx!+¥
k > 0.
We parametrize G(.), a Generalized Pareto Distribution (GPD) , see Table 13.4, G(x) =
1 (cid:0) (1 + xy=b)
, with x (cid:25) 1.88, (cid:6).14 for rescaled data which corresponds to a tail
a = 1

(cid:0)1=x
x = .53, (cid:6).04.

(cid:0)a

Tables 13.2 and 13.3 show inter-arrival times,
Memorylessness of onset of conﬂicts
meaning one can wait more than a hundred years for an event such as WWII without
changing one’s expectation. There is no visible autocorrelation, no statistically detectable
temporal structure (i.e. we cannot see the imprint of a self-exciting process), see Figure
13.8.

Full distribution(s) Rescaled data ﬁts a Lomax-Style distribution with same tail as ob-
tained by POT, with strong goodness of ﬁt. For events with casualties > L = 10K, 25K, 50K, etc.
we ﬁt different Pareto II (Lomax) distributions with corresponding tail a (ﬁt from GPD),

with scale s = 84, 360, i.e., with density

a( (cid:0)L+s+x
s
s
We also consider a wider array of statistical "pictures" from pairs a, sa across the data
from potential alternative values of a, with recalibration of maximum likelihood s, see
Figure 13.5.

, x (cid:21) L.

)

(cid:0)a(cid:0)1

: Table 13.1 shows
Difference between sample mean and maximum likelihood mean
the true mean using the parametrization of the Pareto distribution above and inverting the
transformation back to compact support. "True" or maximum likelihood, or "statistical"
mean is between 3 and 4 times observed mean.
This means the "journalistic" observation of the mean, aside from the conceptual mistake
of relying on sample mean, underestimates the true mean by at least 3 times and higher
future observations would not allow the conlusion that violence has "risen".

13.2.2 Conclusion

History as seen from tail analysis is far more risky, and conﬂicts far more violent than
acknowledged by naive observation of behavior of averages in historical time series.

Table 13.1: Sample means and estimated maximum likelihood mean across minimum values L –Rescaled data.

13.3 methodological discussion

201

L
10K
25K
50K
100K
200K
500K

Sample Mean ML Mean Ratio
3.43
3.69
3.67
3.53
3.79
3.31

9.079 (cid:2) 106
9.82 (cid:2) 106
1.12 (cid:2) 107
1.34 (cid:2) 107
1.66 (cid:2) 107
2.48 (cid:2) 107

3.11 (cid:2) 107
3.62 (cid:2) 107
4.11 (cid:2) 107
4.74 (cid:2) 107
6.31 (cid:2) 107
8.26 (cid:2) 107

Table 13.2: Average inter-arrival times and their mean absolute deviation for events with more than 1, 2, 5
and 10 million casualties, using actual estimates.

Threshold Average MAD
31.66
1
47.31
2
68.60
5
144.47
10

26.71
42.19
57.74
101.58

Table 13.3: Average inter-arrival times and their mean absolute deviation for events with more than 1, 2, 5,
10, 20, and 50 million casualties, using rescaled amounts.

Threshold Average MAD
12.59
1
18.13
2
5
27.29
41.30
10
52.14
20
78.57
50

11.27
16.84
26.31
37.39
48.47
67.88

Table 13.4: Estimates (and standard errors) of the Generalized Pareto Distribution parameters for casualties
over a 50k threshold. For both actual and rescaled casualties, we also provide the number of events lying above
the threshold (the total number of events in our data is 99).

Data
Raw Data

Nr. Excesses
307

Naive Rescaling

Log-rescaling

524

524

x
1.5886
(0.1467)
1.8718
(0.1259)
1.8717
(0.1277)

b
3.6254
(0.8191)
14.3254
(2.1111)
14.3261
(2.1422)

13.3 methodological discussion

202

on the tail risk of violent conflict and its underestimation (with p. cirillo) ‡

13.3.1 Rescaling method

We remove the compact support to be able to use power laws as follows (see Taleb(2015)
[? ]). Using Xt as the r.v. for number of incidences from conﬂict at times t, consider ﬁrst
a naive rescaling of X
, where Ht is the total human population at period t. See
appendix for methods of estimation of Ht.

′
t = Xt
Ht

Next, with today’s maximum population H and L the naively rescaled minimum for
our deﬁnition of conﬂict, we introduce a smooth rescaling function φ : [L, H] ! [L, ¥)
satisfying:

i φ is "smooth": φ 2 C
ii φ(cid:0)1(¥) = H,
iii φ(cid:0)1(L) = φ(L) = L.

¥

,

In particular, we choose:

φ(x) = L (cid:0) H log

(

)

.

H (cid:0) x
H (cid:0) L

(13.1)

We can perform appropriate analytics on xr = φ(x) given that it is unbounded, and
properly ﬁt Power Law exponents. Then we can rescale back for the properties of X. Also
notice that the φ(x) (cid:25) x for very large values of H. This means that for a very large
upper bound, the results we will get for x and φ(x) will be essentially the same. The big
difference is only from a philosophical/methodological point of view, in the sense that we
remove the upper bound (unlikely to be reached).
In what follows we will use the naively rescaled casualties as input for the φ((cid:1)) function.
We pick H = Pt0 for the exercise.
The distribution of x can be rederived as follows from the distribution of xr:

∫ ¥

L

∫ φ(cid:0)1(¥)

f (xr) dxr =

L

g(x) dx,

(13.2)

where φ(cid:0)1(u) = (L (cid:0) H)e
In this case, from the Pareto-Lomax selected:

L(cid:0)u
H + H

(

a

(cid:0)L+s+xr
s

)(cid:0)a(cid:0)1

s

f (xr) =

, xr 2 [L, ¥)

(13.3)

∫

which veriﬁes

)(cid:0)a(cid:0)1

(

aH

s(cid:0)H log( H(cid:0)x
H(cid:0)L )
s
s(H (cid:0) x)

g(x) =

, x 2 [L, H],

H
L x g(x) dx = 1. Hence the expectation

Eg(x; L, H, s, a) =

∫

H

L

x g(x) dx,

Eg(X; L, H, s, a) = aH

(

1
a

(cid:0) (H (cid:0) L)e

s=H Ea+1
H

)

)

(

s
H

(13.4)

(13.5)

13.3 methodological discussion

203

where E.(.) is the exponential integral Enz =

∫ ¥
1
Note that we rely on the invariance property:

et((cid:0)z)
tn dt.

Remark 13.1
If ˆq is the maximum likelihood estimator (MLE) of q, then for an absolutely continuous function ϕ,
ϕ( ˆq) is the MLE estimator of ϕ(q).

For further details see [147].

13.3.2 Expectation by Conditioning (less rigorous)

¥

by a Heaviside step function, that is the

We would be replacing a smooth function in C
indicator function 1 : R ! f0, 1g, written as 1
X2[L,H]:
∫

E(1

X2[L,H]) =

which for the Pareto Lomax becomes:

H
L x f (x) dx
∫
H
L f (x) dx

E(1

X2[L,H]) =

(H(cid:0)L)

asa
sa(cid:0)(H(cid:0)L+s)a + (a (cid:0) 1)L + s
a (cid:0) 1

(13.6)

13.3.3 Reliability of data and effect on tail estimates

Data from violence is largely anecdotal, spreading via citations, often based on some vague
estimate, without anyone’s ability to verify the assessments using period sources. An event
that took place in the seventh century, such as the an Lushan rebellion, is "estimated" to
have killed 26 million people, with no precise or reliable methodology to allow us to trust
the number. The independence war of Algeria has various estimates, some from France,
others from the rebels, and nothing scientiﬁcally or professionally obtained.

As said earlier, in this paper, we use different data: raw data, naively rescaled data w.r.t.
the current world population, and log-rescaled data to avoid the theoretical problem of the
upper bound.

For some observations, together with the estimated number of casualties, as resulting
from historical sources, we also have a lower and upper bound available. Let Xt be the
number of casualties in a given conﬂict at time t. In principle, we can deﬁne triplets like

(cid:15) fXt, Xl

t, Xu
t

g for the actual estimates (raw data), where Xl

t and Xu

t represent the lower

and upper bound, if available.
, Yu
, Yl

(cid:15) fYt = Xt

P20015
Pt
is the world population in 2015 and Pt is the population at time t = 1, ..., 2014.

g for the naively rescaled data, where P2015

t = Xu
t

t = Xl
t

P20015
Pt

P20015
Pt

(cid:15) fZt = φ(Yt), Zl

t = φ(Yl

t ), Zu

t = φ(Yu

t )g for the log-rescaled data.

To prevent possible criticism about the use of middle estimates, when bounds are present,
we have decided to use the following Monte Carlo procedure (for more details [139]),
obtaining no signiﬁcant different in the estimates of all the quantities of interest (like the
tail exponent a = 1=x):

1. For each event X for which bounds are present, we have assumed casualties to be
uniformly distributed between the lower and the upper bound, i.e. X (cid:24) U(Xl, Xu).
The choice of the uniform distribution is to keep things simple. All other bounded

204

on the tail risk of violent conflict and its underestimation (with p. cirillo) ‡

distributions would in fact generate the same results in the limit, thanks to the central
limit theorem.

2. We have then generated a large number of Monte Carlo replications, and in each
replication we have assigned a random value to each event X according to U(Xl, Xu).
3. For each replication we have computed the statistics of interest, typically the tail

exponent, obtaining values that we have later averaged.

This procedure has shown that the precision of estimates does not affect the tail of the

distribution of casualties, as the tail exponent is rather stable.

For those events for which no bound is given, the options were to use them as they
are, or to perturb them by creating ﬁctitious bounds around them (and then treat them
as the other bounded ones in the Monte Carlo replications). We have chosen the second
approach.

The above also applies to Yt and Zt.
Note that the tail a derived from an average is different from an average alpha across
different estimates, which is the reason we perform the various analyses across estimates.

Technical comment
These simulations are largely looking for a "stochastic alpha" bias
from errors and unreliability of data (Chapter x). With a sample size of n, a parameter ˆqm
will be the average parameter obtained across a large number of Monte Carlo runs. Let
Xi be a given Monte Carlo simulated vector indexed by i and Xm is the middle estimate
between high and low bounds. Since, with 1
(cid:229)(cid:20)m
1 across Monte Carlo runs
m
bq(Xm). For instance, consider the maximum
bq(Xj) ̸=
but 8j , ∥Xj
1,
))(cid:0)1
. With ∆ (cid:21) xm, deﬁne
likelihood estimation of a Paretian tail, ba(Xi) ≜ n

1= ∥Xm∥

bqm = 1

(cid:229)1(cid:20)i(cid:20)n log

̸= ∥Xm∥

(cid:229)(cid:20)m

∥Xj

∥

∥

(

(

m

1

xi
L

0

@

ba(Xi

⊔ ∆) ≜ 1
2

n
)

(

xi
L

(cid:0) log

) +

(

∆
L

(cid:229)n

i=1 log

(

n
)

xi
L

(

∆
L

+ log

(cid:229)n

i=1 log

1

A

)

which, owing to the concavity of the logarithmic function, gives the inequality

8∆ (cid:21) xm, ba(Xi

⊔ ∆) (cid:21) ba(Xi).

13.3.4 Deﬁnition of an "event"

"Named" conﬂicts are an arbitrary designation that, often, does not make sense statisti-
cally: a conﬂict can have two or more names; two or more conﬂicts can have the same
name, and we found no satisfactory hierarchy between war and conﬂict. For uniformity,
we treat events as the shorter of event or its disaggregation into units with a maximum
duration of 25 years each. Accordingly, we treat Mongolian wars, which lasted more than
a century and a quarter, as more than a single event. It makes little sense otherwise as it
would be the equivalent of treating the period from the Franco-Prussian war to WW II as
"German(ic) wars", rather than multiple events because these wars had individual names
in contemporary sources. Effectively the main sources such as the Encyclopedia of War [129]
list numerous conﬂicts in place of "Mongol Invasions" –the more sophisticated the histori-
ans in a given area, the more likely they are to break conﬂicts into different "named" events
and, depending on historians, Mongolian wars range between 12 and 55 conﬂicts.

13.4 data analysis

205

What controversy about the deﬁnition of a "name" can be, once again, solved by boot-
strapping. Our conclusion, incidentally, is invariant to the bundling or unbundling of the
Mongolian wars.

Further, in the absence of a clearly deﬁned protocol in historical studies, it has been hard
to disentangle direct death from wars and those from less direct effects on populations
(say blocades, famine). For instance the First Jewish War has confused historians as an
estimated 30K death came from the war, and a considerably higher (between 350K and the
number 1M according to Josephus) from the famine or civilian casualties.

13.3.5 Missing events

We can assume that there are numerous wars that are not part of our sample, even if we
doubt that such events are in the "tails" of the distribution, given that large conﬂicts are
more likely to be reported by historians. Further, we also assume that their occurrence is
random across the data (in the sense that they do not have an effect on clustering).

But we are aware of a bias from differential in both accuracy and reporting across time:
events are more likely to be recorded in modern times than in the past. Raising the min-
imum value L the number of such "missed" events and their impact are likely to drop
rapidly. Indeed, as a robustness check, raising the bar to a minimum L = 500K does not
change our analysis.

A simple jackknife procedure, performed by randomly removing a proportion of events
from the sample and repeating analyses, shows us the dependence of our analysis on
missing events, dependence that we have found to be insigniﬁcant, when focusing on
the tail of the distribution of casualties. In other words, given that we are dealing with
extremes, if removing 30% of events and checking the effects on parameters produce no
divergence from initial results, then we do not need to worry of having missed 30% of
events, as missing events are not likely to cause thinning of the tails.

3

13.3.6 Survivorship Bias

We did not take into account of the survivorship biases in the analysis, assuming it to be
negligible before 1960, as the probability of a conﬂict affecting all of mankind was negli-
gible. Such probability (and risk) became considerably higher since, especially because of
nuclear and other mass destruction weapons.

13.4 data analysis

Figures 13.3 and 13.4 graphically represent our data: the number of casualties over time.
Figure 13.3 refers to the estimated actual number of victims, while Figure 13.4 shows the
rescaled amounts, obtained by rescaling the past observation with respect to the world
population in 2015 (around 7.2 billion people)
. Figures 13.3 might suggest an increase in
the death toll of armed conﬂicts over time, thus supporting the idea that war violence has

4

3 The opposite is not true, which is at the core of the Black Swan asymmetry: such procedure does not remedy the
missing of tail, "Black Swan" events from the record. A single "Black Swan" event can considerably fatten the tail.
In this case the tail is fat enough and no missing information seems able to make it thinner.

4 Notice that, in equation (13.1), for H = 7.2 billion, φ(x) (cid:25) x. Therefore Figure 13.4 is also representative for

log-rescaled data.

206

on the tail risk of violent conflict and its underestimation (with p. cirillo) ‡

increased. Figure 13.4, conversely, seems to suggest a decrease in the (rescaled) number
of victims, especially in the last hundred years, and possibly in violence as well. In what
follows we show that both interpretations are surely naive, because they do not take into
consideration the fact that we are dealing with extreme events.

13.4.1 Peaks over Threshold

Given the fat-tailed nature of the data, which can be easily observed with some basic
graphical tools like histograms on the logs and QQplots (Figure 13.6 shows the QQplot
of actual casualties against an exponential distribution: the clear concavity is a signal of
fat-tailed distribution), it seems appropriate to use a well-known method of extreme value
theory to model war casualties over time: the Peaks-over-Threshold or POT [124].
According to the POT method, excesses of an i.i.d. sequence over a high threshold u
(that we have to identify) occur at the times of a homogeneous Poisson process, while the
excesses themselves can be modeled with a Generalized Pareto Distribution (GPD). Arrival
times and excesses are assumed to be independent of each other.

In our case, assuming the independence of the war events does not seem a strong assump-
tion, given the time and space separation among them. Regarding the other assumptions,
on the contrary, we have to check them.

We start by identifying the threshold u above which the GPD approximation may hold.
Different heuristic tools can be used for this purpose, from Zipf plot to mean excess func-
tion plots, where one looks for the linearity which is typical of fat-tailed phenomena
[32, 60]. Figure 13.7 shows the mean excess function plot for actual casualties
: an up-
ward trend is clearly present, already starting with a threshold equal to 5k victims. For the
6
goodness of ﬁt, it might be appropriate to choose a slightly larger threshold, like u = 50k
.

5

Figure 13.6: QQplot of actual casualties against standard exponential quantile. The concave curvature of data
points is a clear signal of heavy tails.

5 Similar results hold for the rescaled amounts (naive and log). For the sake of brevity we always show plots for

one of the two variables, unless a major difference is observed.

6 This idea has also been supported by subsequent goodness-of-ﬁt tests.

13.4 data analysis

207

Figure 13.7: Mean excess function plot (MEPLOT) for actual casualties. An upward trend - almost linear in
the ﬁrst part of the graph - is present, suggesting the presence of a fat right tail. The variability of the mean
excess function for higher thresholds is due to the small number of observation exceeding those thresholds and
should not be taken into consideration.

13.4.2 Gaps in Series and Autocorrelation

To check whether events over time occur according to a homogeneous Poisson process, a
basic assumption of the POT method, we can look at the distribution of the inter-arrival
times or gaps, which should be exponential. Gaps should also show no autocorrelation.

Figure 13.8: ACF plot of gaps for actual casualties, no signiﬁcant autocorrelation is visible.

Figure 13.8 clearly shows the absence of autocorrelation. The plausibility of an exponen-
tial distribution for the inter-arrival times can be positively checked using both heuristic
and analytic tools. Here we omit the positive results for brevity.

208

on the tail risk of violent conflict and its underestimation (with p. cirillo) ‡

7

However, in order to provide some extra useful information, in Tables 13.2 and 13.3 we
provide some basic statistics about the inter-arrival times for very catastrophic events in
terms of casualties
. The simple evidence there contained should already be sufﬁcient to
underline how unreliable can be the statement that war violence has been decreasing over
time. For an events with more than 10 million victims, if we refer to actual estimates, the
8
average time delay is 101.58 years, with a mean absolute deviation of 144.47 years
. This
means that it is totally plausible that in the last few years we have not observed such a large
event. It could simply happen tomorrow or some time in the future. This also means that
every trend extrapolation makes no great sense for this type of extreme events. Finally,
we have to consider that an event as large as WW2 happened only once in 2014 years,
if we deal with actual casualties (for rescaled casualties we can consider the An Lushan
rebellion); in this case the possible waiting time is even longer.

13.4.3 Tail analysis

Given that the POT assumptions about the Poisson process seem to be conﬁrmed by data,
it is ﬁnally the time to ﬁt a Generalized Pareto Distribution to the exceedances.

Consider a random variable X with df F, and call Fu the conditional df of X above a given
threshold u. We can then deﬁne a r.v. Y, representing the rescaled excesses of X over the
threshold u, getting [124]

Fu(y) = P(X (cid:0) u (cid:20) yjX > u) =

F(u + y) (cid:0) F(u)
1 (cid:0) F(u)

(cid:0) u, where xF is the right endpoint of the underlying distribution F.
for 0 (cid:20) y (cid:20) xF
Pickands [130], Balkema and de Haan [6], [7] and [8] showed that for a large class of
underlying distribution functions F (following in the so-called domain of attraction of the
GEV distribution [124]), and a large u, Fu can be approximated by a Generalized Pareto
distribution: Fu(y) ! G(y), as u ! ¥ where

{

G(y) =

(cid:0)1=x

1 (cid:0) (1 + xy=b)
(cid:0)y=b
1 (cid:0) e

i f x ̸= 0
i f x = 0.

.

(13.7)

It can be shown that the GPD distribution is a distribution interpolating between the
exponential distribution (for x = 0) and a class of Pareto distributions. We refer to [124] for
more details.
The parameters in (13.7) can be estimated using methods like maximum likelihood or
probability weighted moments [124]. The goodness of ﬁt can then be tested using bootstrap-
based tests [186].
Table 13.4 contains our mle estimates for actual and rescaled casualties above a 50k vic-
tims threshold. This threshold is in fact the one providing the best compromise between
goodness of ﬁt and a sufﬁcient number of observation, so that standard errors are reliable.
The actual and both the rescaled data show two different sets of estimates, but their inter-
pretation is strongly consistent. For this reason we just focus on actual casualties for the
discussion.

7 Table 13.2 does not show the average delay for events with 20M(50M) or more casualties. This is due to the
limited amount of these observations in actual, non-rescaled data. In particular, all the events with more than 20
million victims have occurred during the last 150 years, and the average inter-arrival time is below 20 years. Are
we really living in more peaceful world?

8 In case of rescaled amounts, inter-arrival times are shorter, but the interpretation is the same

13.4 data analysis

209

The parameter x is the most important for us: it is the parameter governing the fatness
of the right tail. A x greater than 1 (we have 1.5886) signiﬁes that no moment is deﬁned
for our Generalized Pareto: a very fat-tailed situation. Naturally, in the sample, we can
compute all the moments we are interested in, but from a theoretical point of view they are
completely unreliable and their interpretation is extremely ﬂawed (a very common error
though). According to our ﬁtting, very catastrophic events are not at all improbable. It is
worth noticing that the estimates is signiﬁcant, given that its standard error is 0.1467.

9

Figures 13.9 and 13.10 compare our ﬁttings to actual data. In both ﬁgures it is possible
to see the goodness of the GPD ﬁtting for most of the observations above the 50k victims
threshold. Some problems arise for the very large events, like WW2 and the An Lushan
rebellion
. In this case it appears that our ﬁtting expects larger events to have happened.
This is a well-known problem for extreme data [124]. The very large event could just be
behind the corner.
Similarly, events with 5 to 10 million victims (not at all minor ones!) seem to be slightly
more frequent than what is expected by our GPD ﬁtting. This is another signal of the
extreme character of war casualties, which does not allow for the extrapolation of simplistic
trends.

Figure 13.9: GPD tail ﬁtting to actual casualties’ data (in 10k). Parameters as per Table 13.4, ﬁrst line.

13.4.4 An alternative view on maxima

Another method is the block-maxima approach of extreme value theory. In this approach
data are divided into blocks, and within each block only the maximum value is taken
into consideration. The Fisher-Tippet theorem [124] then guarantees that the normalized
maxima converge in distribution to a Generalized Extreme Value Distribution, or GEV.

GEV(x; x) =

8
<

:

(

(cid:0) 1
x

(cid:0)(1 + xx)

exp
exp ((cid:0) exp((cid:0)x))

)

x ̸= 0
x = 0

, 1 + xx > 0

9 If we remove the two largest events from the data, the GPD hypothesis cannot be rejected at the 5% signiﬁcance

level.

210

on the tail risk of violent conflict and its underestimation (with p. cirillo) ‡

Figure 13.10: GPD cumulative distribution ﬁtting to actual casualties’ data (in 10k). Parameters as per Table
13.4, ﬁrst line.

This distribution is naturally related to the GPD, and we refer to [124] for more details.
If we divide our data into 100-year blocks, we obtain 21 observation (the last block is the
residual one from 2001 to 2014). Maximum likelihood estimations give a x larger than 2,
indicating that we are in the so-called Fréchet maximum domain of attraction, compatible
with very heavy-tailed phenomena. A value of x greater than 2 under the GEV distribution
further conﬁrms the idea of the absence of moments, a clear signal of a very heavy right
tail.

13.4.5 Full Data Analysis

Naturally, being aware of limitations, we can try to ﬁt all our data, while for casualties in
excess of 10000, we ﬁt the Pareto Distribution from Equation 13.3 with a (cid:25) 0.53 throughout.
The goodness of ﬁt for the "near tail" (L=10K) can be see in Figure 13.2. Similar results to
Figure 13.2 are seen for different values in table below, all with the same goodness of ﬁt.

s

L
10K
84, 260
25K 899, 953
50K 116, 794
100K 172, 733
200K 232, 358
500K 598, 292

The different possible values of the mean in Equation 13.4 can be calculated across different
set values of a, with one single degree of freedom: the corresponding s is a MLE estimate
using such a as ﬁxed: for a sample size n, and xi the observations higher than L, sa =
{

an
s (cid:0) (a + 1) (cid:229)n
i=1

s :
The sample average for L = 10K is 9.12 (cid:2) 106, across 100K simulations, with the spread
in values showed in Figure 13.15.

(cid:0)L+s = 0, s > 0

}

xi

1

.

13.5 additional robustness and reliability tests

211

The "true" mean from Equation 13.4 yields 3.1 (cid:3) 107 , and we repeated for L =10K, 20K,
50K, 100K, 200K, and 500K, ﬁnding ratios of true estimated mean to observed safely be-
tween 3 and 4., see Table 13.1. Notice that this value for the mean of (cid:25) 3.5 times the
observed sample mean is only a general guideline, since, being stochastic, does not re-
veal any precise information other than prevent us from taking the naive mean estimation
seriously.
For under fat tails, the mean derived from estimates of a is more rigorous and has a
smaller error, since the estimate of a is asymptotically Gaussian while the average of a
power law, even when it exists, is considerably more stochastic. See the discussion on
"slowness of the law of large numbers" in [? ] in connection with the point.
We get the mean by truncation for L=10K a bit lower, under equation 13.6; around
1.8835 (cid:2) 107.
We ﬁnally note that, for values of L considered, 96 % of conﬂicts with more than 10,000
victims are below the mean: where m is the mean,

0

P(X < m) = 1 (cid:0)

@1 (cid:0)

(

H log

ae

s=H Ea+1
s

(

s
H

))

1

(cid:0)a

A

.

13.5 additional robustness and reliability tests

13.5.1 Bootstrap for the GPD

In order to check our sensitivity to the quality/precision of our data, we have decided to
perform some bootstrap analysis. For both raw data and the rescaled ones we have gener-
ated 100K new samples by randomly selecting 90% of the observations, with replacement.
Figures 13.11, 13.12 and 13.13 show the stability of our x estimates. In particular x > 0
in all samples, indicating the extreme fat-tailedness of the number of victims in armed
conﬂicts. The x estimates in Table 13.4 appear to be good approximations for our GPD real
shape parameters, notwithstanding imprecisions and missing observations in the data.

Figure 13.11: x parameter’s
distribution over 100K boot-
strap samples for actual data.
Each sample is randomly se-
lected with replacement using
90% of the original observa-
tions.

13.5.2 Perturbation across bounds of estimates

We performed analyses for the "near tail" using the Monte Carlo techniques discussed in
section 13.3.3. We look at second order "p-values", that is the sensitivity of the p-values

Raw data: 100k bootstrap samplesxFrequency1.01.21.41.61.82.02.2010000212

on the tail risk of violent conflict and its underestimation (with p. cirillo) ‡

samples

Figure 13.12: x parameter’s
distribution over 100K boot-
for naively
strap
Each sample
rescaled data.
is
selected with
randomly
replacement using 90% of the
original observations.

Figure 13.13: x parameter’s
distribution over 100K boot-
strap samples for log-rescaled
data. Each sample is randomly
selected with replacement us-
ing 90% of the original obser-
vations.

across different estimates in Figure 13.14 –practically all results meet the same statistical
signiﬁcance and goodness of ﬁt.

In addition, we look at values of both the sample means and the alpha-derived MLE
mean across permutations, see Figures 13.15 and 13.16.

P-Values
across

Figure 13.14:
of
Pareto-Lomax
100K
combinations. This is not to
ascertain the p-value, rather
to check the robustness by
looking at the variations across
permutations of estimates.

13.6 conclusion:

is the world more unsafe than it seems?

Naively rescaled data: 100k bootstrap samplesxFrequency1.41.61.82.02.22.4010000Log−rescaled data: 100k bootstrap samplesxFrequency1.41.61.82.02.22.40100000.60.70.80.91.0pval0.000.050.100.150.200.25Pr13.6 conclusion:

is the world more unsafe than it seems?

213

Figure 13.15: Rescaled sample
mean across 100K estimates be-
tween high-low.

Figure 13.16: Rescaled MLE
mean across 100K estimates be-
tween high-low.

Figure 13.17: Loglogplot com-
parison of
f and g, showing
a pasting-boundary style cap-
ping around H.

To put our conclusion in the simplest of terms: the occurrence of events that would
raise the average violence by a multiple of 3 would not cause us to rewrite this paper,
nor to change the parameters calibrated within.

8.5×1069.0×1069.5×106m0.000.020.040.060.080.10Pr3.0×1073.2×1073.4×1073.6×1073.8×107m0.000.020.040.060.08Prfg101001000104Log(x)10-510-40.0010.0100.100Log(P>x)214

on the tail risk of violent conflict and its underestimation (with p. cirillo) ‡

(cid:15) Indeed, from statistical analysis alone, the world is more unsafe than casually exam-
ined numbers. Violence is underestimated by journalistic nonstatistical looks at the
mean and lack of understanding of the stochasticity of under inter-arrival times.
(cid:15) The transformation into compact support allowed us to perform the analyses and
gauge such underestimation which , if noisy, gives us an idea of the underestimation
and its bounds.

(cid:15) In other words, a large event and even a rise in observed mean violence would not
be inconsistent with statistical properties, meaning it would justify a "nothing has
changed" reaction.

(cid:15) We avoided discussions of homicide since we limited L to values > 10, 000, but its
rate doesn’t appear to have a particular bearing on the tails. It could be a drop in the
bucket. It obeys different dynamics. We may have observed lower rate of homicide
in societies but most risks of death come from violent conﬂict. (Casualties from homi-
cide by rescaling from the rate 70 per 100k, gets us 5.04 (cid:2) 106 casualties per annum
at today’s population. A drop to minimum levels stays below the difference between
errors on the mean of violence from conﬂicts with higher than 10,000 casualties.)
(cid:15) We ignored survivorship bias in the data analysis (that is, the fact that had the world
been more violent, we wouldn’t be here to talk about it). Adding it would increase the risk.
The presence of tail effects today makes further analysis require taking it into account.
Since 1960, a single conﬂict –which almost happened– has the ability to reach the max
casualties, something we did not have before.(We can rewrite the model with one of
fragmentation of the world, constituted of "separate" isolated n independent random
variables Xi, each with a maximum value Hi, with the total (cid:229)n
i Hi = H, with all
wi > 0, (cid:229)n
i = 1. In that case the maximum (that is worst conﬂict) could require
the joint probabilities that all X1, X2, (cid:1) (cid:1) (cid:1) Xn are near their maximum value, which,
under subexponentiality, is an event of much lower probability than having a single
variable reach its maximum.)

w

w

The data was compiled by Captain Mark Weisenborn. We thank Ben Kiernan for com-

ments on East Asian conﬂicts.

G W H AT A R E T H E C H A N C E S O F A

T H I R D W O R L D W A R ? (cid:3),†

This is from an article that is part of the debate with public intellectuals

who claim that violence have dropped "from data", without realizing that
science is hard; signiﬁcance requires further data under fat tails and more
careful examination. We responded; our response provides a way to sum-
marize the main problem with naive empiricism under fat tails.

In a recent issue of Signiﬁcance Mr. Peter McIntyre asked what the chances are that World
War III will occur this century. Prof. Michael Spagat wrote that nobody knows, nobody
can really answer–and we totally agree with him on this. Then he adds that "a really
huge war is possible but, in my view, extremely unlikely." To support his statement, Prof.
Spagat relies partly on the popular science work of Prof. Steven Pinker, expressed in The
Better Angels of our Nature and journalistic venues. Prof. Pinker claims that the world has
experienced a long-term decline in violence, suggesting a structural change in the level of
belligerence of humanity.

It is unfortunate that Prof. Spagat, in his answer, refers to our paper (this volume, Chapter
13 ), which is part of a more ambitious project we are working on related to fat-tailed
variables.

What characterizes fat tailed variables? They have their properties (such as the mean)
dominated by extreme events, those "in the tails". The most popularly known version is
the "Pareto 80/20".
We show that, simply, data do not support the idea of a structural change in human
belligerence. So Prof. Spagat’s ﬁrst error is to misread our claim: we are making neither
pessimistic nor optimistic declarations: we just believe that statisticians should abide by
the foundations of statistical theory and avoid telling data what to say.

Let us go back to ﬁrst principles.

Foundational Principles

Fundamentally, statistics is about ensuring people do not build scientiﬁc theories from hot
air, that is without signiﬁcant departure from random. Otherwise, it is patently "fooled by
randomness".

Further, for fat tailed variables, the conventional mechanism of the law of large numbers
is considerably slower and signiﬁcance requires more data and longer periods. Ironically,
there are claims that can be done on little data: inference is asymmetric under fat-tailed
domains. We require more data to assert that there are no Black Swans than to assert that

215

216

what are the chances of a third world war? (cid:3),†

Figure G.1: After Napoleon, there was a lull in Europe. Until nationalism came to change the story.

there are Black Swans hence we would need much more data to claim a drop in violence
than to claim a rise in it.

Finally, statements that are not deemed statistically signiﬁcant –and shown to be so –

should never be used to construct scientiﬁc theories.

These foundational principles are often missed because, typically, social scientists’ statis-
tical training is limited to mechanistic tools from thin tailed domains [2]. In physics, one
can often claim evidence from small data sets, bypassing standard statistical methodolo-
gies, simply because the variance for these variables is low. The higher the variance, the
more data one needs to make statistical claims. For fat-tails, the variance is typically high
and underestimated in past data.

The second –more serious –error Spagat and Pinker made is to believe that tail events and
the mean are somehow different animals, not realizing that the mean includes these tail
events. For fat-tailed variables, the mean is almost entirely determined by extremes. If you
are uncertain about the tails, then you are uncertain about the mean. It is thus incoherent
to say that violence has dropped but maybe not the risk of tail events; it would be like
saying that someone is "extremely virtuous except during the school shooting episode
when he killed 30 students".

Robustness

Our study tried to draw the most robust statistical picture of violence, relying on methods
from extreme value theory and statistical methods adapted to fat tails. We also put robust-
ness checks to deal with the imperfection of data collected some thousand years ago: our
results need to hold even if a third (or more) of the data were wrong.

Inter-arrival times

We show that the inter-arrival times among major conﬂicts are extremely long, and con-
sistent with a homogenous Poisson process: therefore no speciﬁc trend can be established:

what are the chances of a third world war? (cid:3),†

217

we as humans can not be deemed as less belligerent than usual. For a conﬂict generating at
least 10 million casualties, an event less bloody than WW1 or WW2, the waiting time is on
average 136 years, with a mean absolute deviation of 267 (or 52 years and 61 deviations for
data rescaled to today’s population). The seventy years of what is called the "Long Peace"
are clearly not enough to state much about the possibility of WW3 in the near future.

Underestimation of the mean

We also found that the average violence observed in the past underestimates the true
statistical average by at least half. Why? Consider that about 90-97% of the observations
fall below the mean, which requires some corrections with the help of extreme value theory.
(Under extreme fat tails, the statistical mean can be closer to the past maximum observation
than sample average.)

A common mistake

Similar mistakes have been made in the past. In 1860, one H.T. Buckle
unstatistical reasoning as Pinker and Spagat.

1

used the same

That this barbarous pursuit is, in the progress of society, steadily declining,
If we
must be evident, even to the most hasty reader of European history.
compare one country with another, we shall ﬁnd that for a very long period
wars have been becoming less frequent; and now so clearly is the movement
marked, that, until the late commencement of hostilities, we had remained at
peace for nearly forty years: a circumstance unparalleled (...) The question
arises, as to what share our moral feelings have had in bringing about this
great improvement.

Moral feelings or not, the century following Mr. Buckle’s prose turned out to be the most

murderous in human history.

We conclude by saying that we ﬁnd it ﬁtting –and are honored –to expose fundamen-
tal statistical mistakes in a journal called Signiﬁcance, as the problem is precisely about
signiﬁcance and conveying notions of statistical rigor to the general public.

Gini

1 Buckle, H.T. (1858) History of Civilization in England, Vol. 1, London: John W. Parker and Son.

Part VI

M E TA P R O B A B I L I T Y PA P E R S

14 H O W FAT TA I L S E M E R G E F R O M

R E C U R S I V E E P I S T E M I C
U N C E R TA I N T Y †

The Opposite of Central Limit:a With the Central Limit Theorem, we start

with a distribution and end with a Gaussian. The opposite is more likely to
be true. Recall how we fattened the tail of the Gaussian by stochasticizing
the variance? Now let us use the same metaprobability method, putting

additional layers of uncertainty.

a A version of this chapter was presented at Benoit Mandelbrot’s Scientiﬁc Memorial on April 29, 2011,in

New Haven, CT.

The Regress Argument (Error about Error)
The main problem behind The Black Swan
is the limited understanding of model (or representation) error, and, for those who get it,
a lack of understanding of second order errors (about the methods used to compute the
errors) and by a regress argument, an inability to continuously reapplying the thinking all
the way to its limit ( particularly when they provide no reason to stop). Again, there is
no problem with stopping the recursion, provided it is accepted as a declared a priori that
escapes quantitative and statistical methods.

Epistemic not statistical re-derivation of power laws: Note that previous derivations of
power laws have been statistical (cumulative advantage, preferential attachment, winner-
take-all effects, criticality), and the properties derived by Yule, Mandelbrot, Zipf, Simon,
Bak, and others result from structural conditions or breaking the independence assump-
tions in the sums of random variables allowing for the application of the central limit
theorem. This work is entirely epistemic, based on standard philosophical doubts and
regress arguments.

14.1 methods and derivations

14.1.1 Layering Uncertainties

Take a standard probability distribution, say the Gaussian. The measure of dispersion,
here s, is estimated, and we need to attach some measure of dispersion around it. The
uncertainty about the rate of uncertainty, so to speak, or higher order parameter, similar
to what called the “volatility of volatility” in the lingo of option operators (see Taleb, 1997,
Derman, 1994, Dupire, 1994, Hull and White, 1997) –here it would be “uncertainty rate
about the uncertainty rate”. And there is no reason to stop there: we can keep nesting
these uncertainties into higher orders, with the uncertainty rate of the uncertainty rate of

221

222

how fat tails emerge from recursive epistemic uncertainty †

Figure 14.1: A version of
this chapter was presented at
Benoit Mandelbrot’s memorial.

the uncertainty rate, and so forth. There is no reason to have certainty anywhere in the
process.

14.1.2 Higher order integrals in the Standard Gaussian Case

We start with the case of a Gaussian and focus the uncertainty on the assumed standard
deviation. Deﬁne ϕ(m,s,x) as the Gaussian PDF for value x with mean m and standard
deviation s.
A 2ndorder stochastic standard deviation is the integral of ϕ across values of s 2 R+,
under the measure f ( ¯s, s
1 its scale parameter (our approach to trach the error
of the error), not necessarily its standard deviation; the expected value of s
∫ ¥

1, s) , with s

1 is s
1.

f (x)1 =

0

ϕ(m, s, x) f ( ¯s, s

1, s) ds

Generalizing to the Nth order, the density function f(x) becomes

14.1 methods and derivations

223

f (x)N =

∫ ¥

∫ ¥

...

0

0

ϕ(m, s, x) f ( ¯s, s

1, s) f (s

1, s

2, s

1) ... f (s

N(cid:0)1, s

N, s

N(cid:0)1) ds ds

1 ds

2 ... ds
N
(14.1)

The problem is that this approach is parameter-heavy and requires the speciﬁcations of
the subordinated distributions (in ﬁnance, the lognormal has been traditionally used for
s2
s2 (or Gaussian for the ratio Log[
t
s2 ] since the direct use of a Gaussian allows for negative
values). We would need to specify a measure f for each layer of error rate. Instead this
can be approximated by using the mean deviation for s, as we will see next.
Discretization using nested series of two-states for s- a simple multiplicative process
We saw in the last chapter a quite effective simpliﬁcation to capture the convexity, the
ratio of (or difference between) ϕ(m,s,x) and
1, s) ds (the ﬁrst order
standard deviation) by using a weighted average of values of s, say, for a simple case of
one-order stochastic volatility:

ϕ(m, s, x) f ( ¯s, s

∫ ¥
0

s(1 (cid:6) a(1))
with 0 (cid:20) a(1) < 1, where a(1) is the proportional mean absolute deviation for s, in other
word the measure of the absolute error rate for s. We use 1
2 as the probability of each state.
Unlike the earlier situation we are not preserving the variance, rather the STD.

Thus the distribution using the ﬁrst order stochastic standard deviation can be expressed

as:

(

1
2

f (x)1 =

ϕ(m, s (1 + a(1)), x) + ϕ(m, s(1 (cid:0) a(1)), x)

(14.2)

)

Now assume uncertainty about the error rate a(1), expressed by a(2), in the same manner
as before. Thus in place of a(1) we have 1
The second order stochastic standard deviation:

2 a(1)( 1(cid:6) a(2)).

(

(

)

ϕ

m, s(1 + a(1)(1 + a(2))), x

+

f (x)2 =

1
4

(

)

(

ϕ

m, s(1 (cid:0) a(1)(1 + a(2))), x) + ϕ(m, s(1 + a(1)(1 (cid:0) a(2)), x

+ ϕ

m, s(1 (cid:0) a(1)(1 (cid:0) a(2))), x

)

)

(14.3)

and the Nth order:

f (x)N =

1
2N

2N
(cid:229)
i=1

ϕ(m, sMN
i

, x)

where MN
i

is the ith scalar (line) of the matrix MN

(

2N (cid:2) 1

)

MN =

(

N
(cid:213)
j=1

(a(j)Ti,j + 1)

)

2N

i=1

224

how fat tails emerge from recursive epistemic uncertainty †

Figure 14.2: Three levels of error rates for s following a multiplicative process

and Ti,j the element of ithline and jthcolumn of the matrix of the exhaustive combination
of n-Tuples of the set f(cid:0)1, 1g,that is the sequences of n length (1, 1, 1, ...) representing all
combinations of 1 and (cid:0)1.
for N=3,

0

B
B
B
B
B
B
B
B
B
B
@

1

C
C
C
C
C
C
C
C
C
C
A

1
1
1
1 (cid:0)1
1
1 (cid:0)1
1
1 (cid:0)1 (cid:0)1
(cid:0)1
1
1
1 (cid:0)1
(cid:0)1
(cid:0)1 (cid:0)1
1
(cid:0)1 (cid:0)1 (cid:0)1

T =

ΣH1-a1LΣHa1+1LΣHa1+1LH1-a2LΣHa1+1LHa2+1LΣH1-a1LH1-a2LΣH1-a1LHa2+1LΣH1-a1LH1-a2LH1-a3LΣH1-a1LHa2+1LH1-a3LΣHa1+1LH1-a2LH1-a3LΣHa1+1LHa2+1LH1-a3LΣH1-a1LH1-a2LHa3+1LΣH1-a1LHa2+1LHa3+1LΣHa1+1LH1-a2LHa3+1LΣHa1+1LHa2+1LHa3+1LΣ14.1 methods and derivations

225

and

M3 =

0

B
B
B
B
B
B
B
B
B
B
@

(1 (cid:0) a(1))(1 (cid:0) a(2))(1 (cid:0) a(3))
(1 (cid:0) a(1))(1 (cid:0) a(2))(a(3) + 1)
(1 (cid:0) a(1))(a(2) + 1)(1 (cid:0) a(3))
(1 (cid:0) a(1))(a(2) + 1)(a(3) + 1)
(a(1) + 1)(1 (cid:0) a(2))(1 (cid:0) a(3))
(a(1) + 1)(1 (cid:0) a(2))(a(3) + 1)
(a(1) + 1)(a(2) + 1)(1 (cid:0) a(3))
(a(1) + 1)(a(2) + 1)(a(3) + 1)

1

C
C
C
C
C
C
C
C
C
C
A

So M3

1 = f(1 (cid:0) a(1))(1 (cid:0) a(2))(1 (cid:0) a(3))g, etc.

Figure 14.3: Thicker tails (higher peaks) for higher values of N; here N = 0, 5, 10, 25, 50, all values of a= 1
10

Note that the various error rates a( i) are not similar to sampling errors, but rather

projection of error rates into the future. They are, to repeat, epistemic.

The Final Mixture Distribution
ϕ is the ordinary Gaussian PDF with mean m, std s for the random variable x).

The mixture weighted average distribution (recall that

f (xjm, s, M, N) = 2

(cid:0)N

(

)

ϕ

m, sMN
i

, x

2N
(cid:229)
i=1

It could be approximated by a lognormal distribution for s and the corresponding V as
its own variance. But it is precisely the V that interest us, and V depends on how higher
order errors behave.

Next let us consider the different regimes for higher order errors.

-6-4-22460.10.20.30.40.50.6226

how fat tails emerge from recursive epistemic uncertainty †

regime 1 (explosive): case of a constant parameter a

Special case of constant
the case of ﬂat pro-
portional error rate a. The Matrix M collapses into a conventional binomial tree for the
dispersion at the level N.

a: Assume that a(1)=a(2)=...a(N)=a, i.e.

f (xjm, s, M, N) = 2

(cid:0)N

)

(

ϕ

(

N
(cid:229)
j=0

N
j

m, s(a + 1)j(1 (cid:0) a)N(cid:0)j, x

)

(14.4)

Because of the linearity of the sums, when a is constant, we can use the binomial distri-
bution as weights for the moments (note again the artiﬁcial effect of constraining the ﬁrst
moment m in the analysis to a set, certain, and known a priori).

0

B
B
B
B
B
@

1
2
3
4 6m2s2

(

)

Moment
m
a2 + 1
(
a2 + 1
+ m4 + 3

N
)

(

s2
3ms2
)
N

a2 + 1

+ m2
N
+ m3
(
a4 + 6a2 + 1

1

C
C
C
C
C
A

)

N s4

For clarity, we simplify the table of moments, with m=0
0

1

B
B
B
B
B
B
B
B
B
B
B
B
B
B
@

1
2
3

4
5

6
7

8 105

(

Moment
0
a2 + 1
0

)

N s2

)

(

3

a4 + 6a2 + 1

N s4

)

(

0
a6 + 15a4 + 15a2 + 1
0
a8 + 28a6 + 70a4 + 28a2 + 1

N s6

15

(

)

N s8

C
C
C
C
C
C
C
C
C
C
C
C
C
C
A

Note again the oddity that in spite of the explosive nature of higher moments, the expec-
tation of the absolute value of x is both independent of a and N, since the perturbations
of s do not affect the ﬁrst absolute moment =
p s (that is, the initial assumed s). The
2
situation would be different under addition of x.
Every recursion multiplies the variance of the process by (1 + a2 ). The process is similar
to a stochastic volatility model, with the standard deviation (not the variance) following
a lognormal distribution, the volatility of which grows with M, hence will reach inﬁnite
variance at the limit.

√

Consequences
For a constant a > 0, and in the more general case with variable a where a(n) (cid:21) a(n-1),
the moments explode.
A- Even the smallest value of a >0, since
is unbounded, leads to the second
moment going to inﬁnity (though not the ﬁrst) when N! ¥. So something as small as a
.001% error rate will still lead to explosion of moments and invalidation of the use of the
class of L2 distributions.
B- In these conditions, we need to use power laws for epistemic reasons, or, at least,
distributions outside the L2 norm, regardless of observations of past data.

1 + a2

(

)

N

14.1 methods and derivations

227

Figure 14.4: LogLog Plot of the probability of exceeding x showing power law-style ﬂattening as N rises. Here
all values of a= 1/10

Note that we need an a priori reason (in the philosophical sense) to cutoff the N some-

where, hence bound the expansion of the second moment.

Convergence to Properties Similar to Power Laws
We can see on the example next Log-Log plot (Figure 1) how, at higher orders of stochastic
volatility, with equally proportional stochastic coefﬁcient, (where a(1)=a(2)=...=a(N)= 1
10 )
how the density approaches that of a Power Law (just like the Lognormal distribution at
higher variance), as shown in ﬂatter density on the LogLog plot. The probabilities keep
rising in the tails as we add layers of uncertainty until they seem to reach the boundary of
the power law, while ironically the ﬁrst moment remains invariant.
The same effect takes place as a increases towards 1, as at the limit the tail exponent P>x
approaches 1 but remains >1.

14.1.3 Effect on Small Probabilities

Next we measure the effect on the thickness of the tails. The obvious effect is the rise of
small probabilities.

Take the exceedant probability,that is, the probability of exceeding K, given N, for param-

eter a constant:

10.05.02.020.03.030.01.515.07.0Logx10-1310-1010-710-40.1LogPrHxLa=110,N=0,5,10,25,50228

how fat tails emerge from recursive epistemic uncertainty †

P > KjN =

(cid:0)N(cid:0)1

2

N
(cid:229)
j=0

(

)

N
j

(

erfc

p

)

(14.5)

K
2s(a + 1)j(1 (cid:0) a)N(cid:0)j
∫

where erfc(.) is the complementary of the error function, 1-erf(.), erf(z) = 2p
p

z
0 e

(cid:0)t2

dt

Convexity effect
values of N divided by the probability in the case of a standard Gaussian.

The next Table shows the ratio of exceedant probability under different

Table 14.1: Case of a = 1
10
P>3,N
P>10,N
P>10,N=0
P>3,N=0
7
1.01724
45
1.0345
221
1.05178
922
1.06908
3347
1.0864

P>5,N
P>5,N=0
1.155
1.326
1.514
1.720
1.943

Table 14.2: Case of a = 1
100
P>10,N
P>3,N
P>3,N=0
P>10,N=0
1.09 (cid:2) 1012
2.74
8.99 (cid:2) 1015
4.43
2.21 (cid:2) 1017
5.98
1.20 (cid:2) 1018
7.38
3.62 (cid:2) 1018
8.64

P>5,N
P>5,N=0
146
805
1980
3529
5321

N
5
10
15
20
25

N
5
10
15
20
25

14.2 regime 2: cases of decaying parameters a( n)

As we said, we may have (actually we need to have) a priori reasons to decrease the
parameter a or stop N somewhere. When the higher order of a(i) decline, then the
moments tend to be capped (the inherited tails will come from the lognormality of s).

14.2.1 Regime 2-a;“bleed” of higher order error

Take a “bleed” of higher order errors at the rate l, 0(cid:20) l < 1 , such as a(N) = l a(N-1),
hence a(N) =lN a(1), with a(1) the conventional intensity of stochastic standard deviation.
Assume m=0.
With N=2 , the second moment becomes:

(

)

(

)

M2(2) =

a(1)2 + 1

s2

a(1)2l2 + 1

With N=3,

(

) (

) (

)

M2(3) = s2

1 + a(1)2

1 + l2a(1)2

1 + l4a(1)2

14.2 regime 2: cases of decaying parameters a( n)

229

ﬁnally, for the general N:

(

)

M3(N) =

a(1)2 + 1

s2

)

a(1)2l2i + 1

(

N(cid:0)1
(cid:213)
i=1

We can reexpress 14.6 using the Q-Pochhammer symbol (a; q)N = (cid:213)N(cid:0)1

i=1

(

)

M2(N) = s2

(cid:0)a(1)2; l2

N

(14.6)

(

1 (cid:0) aqi

)

Which allows us to get to the limit

(

M2(N) = s2

lim
N!¥

As to the fourth moment:

By recursion:

)

(

)

l2; l2
a(1)2; l2
(l2 (cid:0) 1)2 (l2 + 1)

2

¥

)

M4(N) = 3s4

(

N(cid:0)1
(cid:213)
i=0

6a(1)2l2i + a(1)4l4i + 1

M4(N) = 3s4

(

(

(cid:0)

p

)

3 + 2

2

)

a(1)2; l2

N(14.7)

((

p

2

)

2 (cid:0) 3

a(1)2; l2

)

N

(

(

(cid:0)

p

)

3 + 2

2

M4(N) = 3s4

lim
N!¥
)

a(1)2; l2

¥(14.8)

((

p

2

)

2 (cid:0) 3

a(1)2; l2

)

¥

So the limiting second moment for l=.9 and a(1)=.2 is just 1.28 s2, a signiﬁcant but
relatively benign convexity bias. The limiting fourth moment is just 9.88s4, more than 3
times the Gaussian’s (3 s4), but still ﬁnite fourth moment. For small values of a and values
of l close to 1, the fourth moment collapses to that of a Gaussian.

14.2.2 Regime 2-b; Second Method, a Non Multiplicative Error Rate

For N recursions,

s(1 (cid:6) (a(1)(1 (cid:6) (a(2)(1 (cid:6) a(3)( ...)))

P(x, m, s, N) =

1
L

L
(cid:229)
i=1

(

(

(

f

x, m, s

1 +

TN.AN

)

)

i

(MN.T + 1)i) is the ith component of the (N (cid:2) 1) dot product of TN the matrix of Tuples
in (xx) , L the length of the matrix, and A contains the parameters

(

)

AN =

aj

j=1,...N

230

how fat tails emerge from recursive epistemic uncertainty †

So for instance, for N = 3, T =

(

1, a, a2, a3

)

A3 T3 =

0

B
B
B
B
B
B
B
B
B
B
@

1

C
C
C
C
C
C
C
C
C
C
A

a3 + a2 + a
(cid:0)a3 + a2 + a
a3 (cid:0) a2 + a
(cid:0)a3 (cid:0) a2 + a
a3 + a2 (cid:0) a
(cid:0)a3 + a2 (cid:0) a
a3 (cid:0) a2 (cid:0) a
(cid:0)a3 (cid:0) a2 (cid:0) a

The moments are as follows:

M1(N) = m

M2(N) = m2 + 2s

M4(N) = m4 + 12m2s + 12s2

N
(cid:229)
i=0

a2i

lim
N!¥

M4(N) =

12s2
1 (cid:0) a2 + m4 + 12m2s

At the limit:

which is very mild.

15 S T O C H A S T I C TA I L E X P O N E N T F O R

A S Y M M E T R I C P O W E R L A W S †

We examine random variables in the power law/slowly varying class with

stochastic tail exponent , the exponent a having its own distribution.
We show the effect of stochasticity of a on the expectation and higher
moments of the random variable. For instance, the moments of a right-
tailed or right-asymmetric variable, when ﬁnite, increase with the vari-
ance of a; those of a left-asymmetric one decreases. The same applies to conditional
shortfall (CVar), or mean-excess functions.

We prove the general case and examine the speciﬁc situation of lognormally dis-
tributed a 2 [b, ¥), b > 1.
The stochasticity of the exponent induces a signiﬁcant bias in the estimation of the
mean and higher moments in the presence of data uncertainty. This has consequences
on sampling error as uncertainty about a translates into a higher expected mean.
The bias is conserved under summation, even upon large enough a number of sum-
mands to warrant convergence to the stable distribution. We establish inequalities
related to the asymmetry.

We also consider the situation of capped power laws (i.e. with compact support),
and apply it to the study of violence by Cirillo and Taleb (2016). We show that
uncertainty concerning the historical data increases the true mean.

15.1 background

stochastic volatility has been introduced heuristically in mathematical ﬁnance by traders
looking for biases on option valuation, where a Gaussian distribution is considered to have
several possible variances, either locally or at some speciﬁc future date. Options far from
the money (i.e. concerning tail events) increase in value with uncertainty on the variance
of the distribution, as they are convex to the standard deviation.

This led to a family of models of Brownian motion with stochastic variance (see review
in Gatheral [73]) and proved useful in tracking the distributions of the underlying and the
effect of the nonGaussian character of random processes on functions of the process (such
as option prices).

0 Conference: Extremes and Risks in Higher Dimensions, Lorentz Center, Leiden, The Netherlands, September

2016.

231

232

stochastic tail exponent for asymmetric power laws †

Just as options are convex to the scale of the distribution, we ﬁnd many situations where

expectations are convex to the Power Law tail exponent . This note examines two cases:

(cid:15) The standard power laws, one-tailed or asymmetric.
(cid:15) The pseudo-power law, where a random variable appears to be a Power Law but has
compact support, as in the study of violence [34] where wars have the number of
casualties capped at a maximum value.

15.2 one tailed distributions with stochastic alpha

15.2.1 General Cases

Deﬁnition 15.1
Let X be a random variable belonging to the class of distributions with a "power law" right tail,
that is support in [x0, +¥) , 2 R:
Subclass P1:

fX : P(X > x) = L(x)x

(cid:0)a

,

¶q L(x)
¶xq = 0 for q (cid:21) 1g

(15.1)

We note that x_0 can be negative by shifting, so long as x0 > (cid:0)¥.
Class P:

fX : P(X > x) (cid:24) L(x) x

(15.2)
where (cid:24) means that the limit of the ratio or rhs to lhs goes to 1 as x ! ¥. L : [xmin, +¥) !
(0, +¥) is a slowly varying function, deﬁned as limx!+¥
(x) is
monotone. The constant a > 0.
We further assume that:

L(kx)
L(x) = 1 for any k > 0. L

′

(cid:0)ag

We have

′

x!¥ L
lim
′′
x!¥ L
lim

(x) x = 0

(x) x = 0

P1

(cid:26) P

(15.3)

(15.4)

We note that the ﬁrst class corresponds to the Pareto distributions (with proper shifting

and scaling), where L is a constant and P to the more general one-sided power laws.

15.2.2 Stochastic Alpha Inequality

Throughout the rest of the paper we use for notation X
X, the constant a case.

′

for the stochastic alpha version of

Proposition 15.1
′
Let p = 1, 2, ..., X
class), with x0

be the same random variable as X above in P1 (the one-tailed regular variation

(cid:21) 0, except with stochastic a with all realizations > p that preserve the mean ¯a,

′

E(X

p) (cid:21) E(X p).

15.2 one tailed distributions with stochastic alpha

233

Proposition 15.2
Let K be a threshold. With X in the P class, we have the expected conditional shortfall (CVar):

E(X

′ j

lim
K!¥

X′>K) (cid:21) lim
K!¥

E(Xj

X>K).

The sketch of the proof is as follows.
We remark that E(X p) is convex to a, in the following sense. Let Xa
i > p, 8i, and w
i, with a
able distributed with constant tail exponent a
i = ¯a. By Jensen’s inequality:
a
w
positive weights: (cid:229)i
i

j(cid:20) 1, (cid:229)i

w

i

i be the random vari-
i be the normalized

i = 1, 0 (cid:20) jw
i (cid:229)

w

E(X

p
a

i ) (cid:21) E((cid:229)

(w

iX

p
i )).
a

i

i

As the classes are deﬁned by their survival functions, we ﬁrst need to solve for the corre-
L(1,0)(x, a) and get the normalizing constant.
sponding density: φ(x) = ax

(cid:0)a(cid:0)1L(x, a) (cid:0) x

(cid:0)a

L(x0, a) = x

a
0

(cid:0) 2x0L(1,0)(x0, a)
a (cid:0) 1

(cid:0) 2x2

0 L(2,0)(x0, a)
(a (cid:0) 1)(a (cid:0) 2)

,

(15.5)

a ̸= 1, 2 when the ﬁrst and second derivative exist, respectively. The slot notation L(p,0)(x0, a)
is short for
By the Karamata representation theorem, [14],[174], a function L on [x0, +¥) is slowly
moving (Deﬁnition) if and only if it can be written in the form

¶p L(x,a)
¶xp

jx=x0.

L(x) = exp

(∫

x

x0

ϵ(t)
t

)

dt

+ h(x)

where h(.) is a bounded measurable function converging to a ﬁnite number as x ! +¥,
and ϵ(x) is a bounded measurable function converging to zero as x ! +¥.
Accordingly, L
goes to 0 faster than x and L

(x) goes to 0 as x ! ¥. (We further assumed in 15.3 and 15.4 that L

(x) goes to 0 faster than x2). Integrating by parts,

(x)

′′

′

′

E(X p) = x

p
0 + p

∫ ¥

x0

xp(cid:0)1 d ¯F(x)

where ¯F is the survival function in Eqs. 15.1 and 15.2. Integrating by parts three additional
times and eliminating derivatives of L(.) of higher order than 2:

E(X p) =

x

p(cid:0)a
0

L(x0, a)
p (cid:0) a

(cid:0) x

p(cid:0)a+1
L(1,0)(x0, a)
0
(p (cid:0) a)(p (cid:0) a + 1)

+

which, for the special case of X in P1 reduces to:

E(X p) = x

p
0

a
a (cid:0) p

p(cid:0)a+2
0

x

L(2,0)(x0, a)

(p (cid:0) a)(p (cid:0) a + 1)(p (cid:0) a + 2)

(15.6)

(15.7)

As to Proposition 2, we can approach the proof from the property that limx!¥ L
(x) =
0. This allows a proof of var der Mijk’s law that Paretian inequality is invariant to the
converges to a constant as K ! +¥. Equation 15.6
threshold in the tail, that is
presents the exact conditions on the functional form of L(x) for the convexity to extend to
sub-classes between P1 and P.

E(Xj
X>K)
K

′

234

stochastic tail exponent for asymmetric power laws †

Our results hold to distributions that are transformed by shifting and scaling, of the sort:
x 7! x (cid:0) m + x0 (Pareto II), or with further transformations to Pareto types II and IV.
We note that the representation P1 uses the same parameter, x0, for both scale and mini-

mum value, as a simpliﬁcation.
We can verify that the expectation from Eq. 15.7 is convex to a:

¶E(X p)

¶a2 = x

p
0

2
(a(cid:0)1)3 .

15.2.3 Approximations for the Class P

For P n P1, our results hold when we can write an approximation the expectation of X as
a constant multiplying the integral of x
, namely

(cid:0)a

E(X) (cid:25) k

n(a)
a (cid:0) 1

(15.8)

where k is a positive constant that does not depend on a and n(.) is approximated by a
linear function of a (plus a threshold). The expectation will be convex to a.

For the Student T distribution with tail a, the "sophis-
Example: Student T Distribution
ticated" slowly varying function in common use for symmetric power laws in quantitative
ﬁnance, the half-mean or the mean of the one-sided distribution (i.e. with support on R+
)
becomes

(

p

aG
p

pG

a+1
2
(
a
2

) (cid:25) a (1 + log(4))

,

p

2n(a) = 2

where G(.) is the gamma function.

15.3 sums of power laws

As we are dealing from here on with convergence to the stable distribution, we consider
situations of 1 < a < 2, hence p = 1 and will be concerned solely with the mean.
We observe that the convexity of the mean is invariant to summations of Power Law
distributed variables as X above. The Stable distribution has a mean that in conventional
parameterizations does not appear to depend on a –but in fact depends on it.
(cid:0)a(cid:0)1, y (cid:21)
Let Y be distributed according to a Pareto distribution with density f (y) ≜ ala
l > 0 and with its tail exponent 1 < a < 2. Now, let Y1, Y2, . . . Yn be identical and
independent copies of Y. Let c(t) be the characteristic function for f (y). We have c(t) =
aG((cid:0)a, (cid:0)it), where g(., .) is the incomplete gamma function. We can get the mean
a((cid:0)it)
from the characteristic function of the average of n summands 1
n (Y1 + Y2 + ...Yn), namely
c( t

n )n. Taking the ﬁrst derivative:

y

(cid:0)i

¶c( t
n )n
¶t

= ((cid:0)i)

a(n(cid:0)1)n1(cid:0)ananla(n(cid:0)1)t

a(n(cid:0)1)(cid:0)1G

(cid:0)a,

(

(15.9)

)

n(cid:0)1

(

(cid:0) itl
n

((cid:0)i)

aala

aG

t

(

(cid:0)a, (cid:0) itl
n

)

)

a

(cid:0) n

e

ilt
n

15.4 asymmetric stable distributions

235

and

(cid:0)i

lim
n!¥

¶c( t
n )n
¶t

?
?
?
?

t=0

= l

a
a (cid:0) 1

(15.10)

a
a(cid:0)1 , which does not depends on n.

Thus we can see how the converging asymptotic distribution for the average will have for
mean the scale times
Let cS(t) be the characteristic function of the corresponding stable distribution Sa,b,m,s,
from the distribution of an inﬁnitely summed copies of Y. By the Lévy continuity theorem,
we have
(cid:15) 1
S
n
tion

D(cid:0)! S, with distribution Sa,b,m,s, where

D(cid:0)! denotes convergence in distribu-

i(cid:20)nYi

and

(cid:15) cS(t) = limn!¥ c(t=n)n

are equivalent.
So we are dealing with the standard result [193],[145], for exact Pareto sums [191], replac-
ing the conventional m with the mean from above:

(

(

cS(t) = exp

i

l

at
a (cid:0) 1

+ jtja

(

b tan

)

( pa
2

)))

sgn(t) + i

.

15.4 asymmetric stable distributions

We can verify by symmetry that, effectively, ﬂipping the distribution in subclasses P1 and
P2 around y0 to make it negative yields a negative value of the mean d higher moments,
hence degradation from stochastic a.
The central question becomes:

Remark 15.1 (Preservation of Asymmetry)
A normalized sum in P1 one-tailed distribution with expectation that depends on a of the form in
Eq. 15.8 will necessarily converge in distribution to an asymmetric stable distribution Sa,b,m,1, with
b ̸= 0.

Remark 15.2
′
Let Y
sgn(b).

be Y under mean-preserving stochastic a. The convexity effect, or sgn (E(Y

′

) (cid:0) E(Y)) =

The sketch of the proof is as follows. Consider two slowly varying functions as in 15.1,
each on one side of the tails. We have L(y) = 1y<yq L
8
><

L+(y), L : [yq, +¥],

(y) + 1y(cid:21)yq L+(y):

limy!¥ L+(y) = c

(cid:0)

>:

(cid:0)

L

(y), L : [(cid:0)¥, yq],

limy!(cid:0)¥ L

(cid:0)

(y) = d.

From [145],
8
><

P(X > x) (cid:24) cx

(cid:0)a

, x ! +¥

if

>:

P(X < x) (cid:24) djxj(cid:0)a

, x ! +¥,

coefﬁcient b = c(cid:0)d
c+d .

then Y converges in distribution to Sa,b,m,1 with the

236

stochastic tail exponent for asymmetric power laws †

We can show that the mean can be written as (l+ (cid:0) l(cid:0))

a
a(cid:0)1 where:

l+ (cid:21) l(cid:0) if

∫ ¥

yq

L+(y)dy, (cid:21)

∫

yq

(cid:0)¥

(cid:0)

L

(y)dy

15.5 pareto distribution with lognormally distributed a

Now assume a is following a shifted Lognormal distribution with mean a
0 and minimum
value b, that is, a (cid:0) b follows a Lognormal LN
. The parameter b allows
us to work with a lower bound on the tail exponent in order to satisfy ﬁnite expectation.
We know that the tail exponent will eventually converge to b but the process may be quite
slow.

0) (cid:0) s2

log(a

2 , s

(

)

Proposition 15.3
Assuming ﬁnite expectation for X’ and for exponent the lognormally distributed shifted variable
a (cid:0) b with law LN

, b (cid:21) 1 mininum value for a, and scale l:

log(a

(

)

0) (cid:0) s2

2 , s

′

E(Y

s2 (cid:0) b)
) = E(Y) + l (e
(cid:0) b
a

0

(15.11)

We need b (cid:21) 1 to avoid problems of inﬁnite expectation.
Let ϕ(y, a) be the density with stochastic tail exponent. With a > 0, a
0, Y (cid:21) l > 0 ,

0 > b, b (cid:21) 1, s >

E(Y) =

=

∫ ¥

b
∫ ¥

b

0

∫ ¥

L

yϕ(y; a) dy da
a
a (cid:0) 1
(

1p

2ps(a (cid:0) b)

l

log(a (cid:0) b) (cid:0) log(a

(cid:0) b) +

0

B
@(cid:0)

exp

(

l

a

=

0 + e
a

s2 (cid:0) b
(cid:0) b

0

2s2

)

.

1

)

2

C
A da

s2
2

(15.12)

Approximation of the density

With b = 1 (which is the lower bound for b),we get the density with stochastic a:

ϕ(y; a

0, s) = lim
k!¥

1
Y2

k
(cid:229)
i=0

1
i!

L(a

0

(cid:0) 1)ie

1

2 i(i(cid:0)1)s2

(log(l) (cid:0) log(y))i(cid:0)1(i + log(l) (cid:0) log(y))

(15.13)

This result is obtained by expanding a around its lower bound b (which we simpliﬁed to
b = 1) and integrating each summand.

15.6 pareto distribution with gamma distributed alpha

237

15.6 pareto distribution with gamma distributed alpha

Proposition 15.4
Assuming ﬁnite expectation for X
a (cid:0) 1 with law φ(.), mean a

′

scale l, and for exponent a gamma distributed shifted variable

0 and variance s2, all values for a greater than 1:

′

E(X

) = E(X

′

) +

(a

0

(cid:0) 1)(a

0

s2
(cid:0) s (cid:0) 1)(a

0 + s (cid:0) 1)

(15.14)

Proof.

φ(a) =

(

(cid:0)1)

(cid:0) (a(cid:0)1)(a
s2

0

e

)

(cid:0) (a

(cid:0)1)2
0
s2

s2
(a(cid:0)1)(a
(
(a

)

(cid:0)1)
0
(cid:0)1)2
0
s2

(a(cid:0)1)G

∫ ¥

1

ala

(cid:0)a(cid:0)1 φ(a) da

x

, a > 1

(15.15)

(15.16)

0

a

@e

(cid:0) (a(cid:0)1)(a
s2

0

(cid:0)1)

(

(a (cid:0) 1)
(

(

s2
(a(cid:0)1)(a
0
(

(a (cid:0) 1)G

)(cid:0) (a

(cid:0)1)2
0
s2

1

A

(cid:0)1)

(a

0

(cid:0)1)2
s2

))

da

)

∫ ¥

=

1

=

1
2

1
0 + s (cid:0) 1

a

+

a

0

1
(cid:0) s (cid:0) 1

+ 2

15.7 the bounded power law in cirillo and taleb (2016)

In [34] and [33], the studies make use of bounded power laws, applied to violence and
operational risk, respectively. Although with a < 1 the variable Z has ﬁnite expectations
owing to the upper bound.

The methods offered were a smooth transformation of the variable as follows: we start
with z 2 [L, H), L > 0 and transform it into x 2 [L, ¥), the latter legitimately being Power
Law distributed.

So the smooth logarithmic transformation):

and

x = φ(z) = L (cid:0) H log

(

)

,

H (cid:0) z
H (cid:0) L

)(cid:0)a(cid:0)1

(

x(cid:0)L
as + 1
s

.

f (x) =

We thus get the distribution of Z which will have a ﬁnite expectation for all positive values
of a.

238

stochastic tail exponent for asymmetric power laws †

¶2E(Z)

¶a2 =

(

(

1

H3 (H (cid:0) L)

as
H

e

2H3G4,0
3,4

(

as

H

j

)

a + 1, a + 1, a + 1
1, a, a, a
(
as

j

H

a + 1, a + 1
1, a, a
)

)

(15.17)

(

+ s

as2 + (a + 1)H2 + 2aHs

Ea

(cid:0) Hs(H + s)

(cid:0) 2H2(H + s)G3,0
2,3
))

)

( as
H

1
which appears to be positive in the range of numerical perturbations in [34].
level of a, around 1
extremely pronounced.

At such a low
2 , the expectation is extremely convex and the bias will be accordingly

This convexity has the following practical implication. Historical data on violence over
the past two millennia, is fundamentally unreliable [34]. Hence an imprecision about the
tail exponent , from errors embedded in the data, need to be present in the computations.
The above shows that uncertainty about a, is more likely to make the "true" statistical
mean (that is the mean of the process as opposed to sample mean) higher than lower,
hence supports the statement that more uncertainty increases the estimation of violence.

15.8 additional comments

The bias in the estimation of the mean and shortfalls from uncertainty in the tail exponent
can be added to analyses where data is insufﬁcient, unreliable, or simply prone to forgeries.

In additional to statistical inference, these result can extend to processes, whether a com-
pound Poisson process with power laws subordination [151] (i.e. a Poisson arrival time
and a jump that is Power Law distributed) or a Lévy process. The latter can be analyzed by
considering successive "slice distributions" or discretization of the process [37]. Since the
expectation of a sum of jumps is the sum of expectation, the same convexity will appear
as the one we got from Eq. 15.8.

15.9 acknowledgments

Marco Avellaneda, Robert Frey, Raphael Douady, Pasquale Cirillo.

(

1 G4,0
3,4

j

as
H

a + 1, a + 1, a + 1
1, a, a, a

)

is the Meijer G function.

Part VII

TA I L S F O R B O U N D E D R A N D O M V A R I A B L E S

16 T H E M E TA - D I S T R I B U T I O N O F

S TA N D A R D P - V A L U E S ‡

We present an exact probability distribution (meta-distribution) for p-values

across ensembles of statistically identical phenomena, as well as the
distribution of the minimum p-value among m independents tests. We
(cid:3) (cid:25) 30 as well as
derive the distribution for small samples 2 < n (cid:20) n
the limiting one as the sample size n becomes large. We also look at the
properties of the "power" of a test through the distribution of its inverse for a given
p-value and parametrization.

P-values are shown to be extremely skewed and volatile, regardless of the sample
size n, and vary greatly across repetitions of exactly same protocols under identical
stochastic copies of the phenomenon; such volatility makes the minimum p value
diverge signiﬁcantly from the "true" one. Setting the power is shown to offer little
remedy unless sample size is increased markedly or the p-value is lowered by at least
one order of magnitude.

The formulas allow the investigation of the stability of the reproduction of results

and "p-hacking" and other aspects of meta-analysis.
From a probabilistic standpoint, neither a p-value of .05 nor a "power" at .9 appear
to make the slightest sense.

(cid:229)(cid:20)m pi

P(cid:0)! ps (where

Assume that we know the "true" p-value, ps, what would its realizations look like across
various attempts on statistically identical copies of the phenomena? By true value ps, we
mean its expected value by the law of large numbers across an m ensemble of possible
P(cid:0)! denotes
samples for the phenomenon under scrutiny, that is 1
m
convergence in probability). A similar convergence argument can be also made for the
corresponding "true median" pM. The main result of the paper is that the the distribution
of n small samples can be made explicit (albeit with special inverse functions), as well as
its parsimonious limiting one for n large, with no other parameter than the median value
pM. We were unable to get an explicit form for ps but we go around it with the use of the
median. Finally, the distribution of the minimum p-value under can be made explicit, in a
parsimonious formula allowing for the understanding of biases in scientiﬁc studies.
It turned out, as we can see in Fig. 16.2 the distribution is extremely asymmetric (right-
skewed), to the point where 75% of the realizations of a "true" p-value of .05 will be <.05 (a
borderline situation is 3(cid:2) as likely to pass than fail a given protocol), and, what is worse,
60% of the true p-value of .12 will be below .05.

241

242

the meta-distribution of standard p-values ‡

Figure 16.1: The different values for Equ. 16.1 showing convergence to the limiting distribution.

Although with compact support, the distribution exhibits the attributes of extreme
fat-tailedness. For an observed p-value of, say, .02, the "true" p-value is likely to be
>.1 (and very possibly close to .2), with a standard deviation >.2 (sic) and a mean
deviation of around .35 (sic, sic). Because of the excessive skewness, measures of
dispersion in L1 and L2 (and higher norms) vary hardly with ps, so the standard
deviation is not proportional, meaning an in-sample .01 p-value has a signiﬁcant
probability of having a true value > .3.

So clearly we don’t know what we are talking about when we talk about p-values.

Earlier attempts for an explicit meta-distribution in the literature were found in [91] and
[144], though for situations of Gaussian subordination and less parsimonious parametriza-
tion. The severity of the problem of signiﬁcance of the so-called "statistically signiﬁcant" has
been discussed in [75] and offered a remedy via Bayesian methods in [95], which in fact
recommends the same tightening of standards to p-values (cid:25) .01. But the gravity of the
extreme skewness of the distribution of p-values is only apparent when one looks at the
meta-distribution.

For notation, we use n for the sample size of a given study and m the number of trials

leading to a p-value.

16.1 proofs and derivations

Proposition 16.1
Let P be a random variable 2 [0, 1]) corresponding to the sample-derived one-tailed p-value from

n=5n=10n=15n=20n=250.000.050.100.150.20p246810PDF16.1 proofs and derivations

243

2 [0, 1] derived from
the paired T-test statistic (unknown variance) with median value M(P) = pM
a sample of n size. The distribution across the ensemble of statistically identical copies of the sample
has for PDF

{

φ(p; pM) =

φ(p; pM)L
for p < 1
2
φ(p; pM)H for p > 1
2

φ(p; pM)L = l

1

2 ((cid:0)n(cid:0)1)
p v
u
u
t(cid:0)

(

)

lp (cid:0) 1

lpM

(cid:0) 2

(

lpM
lp
√(
1 (cid:0) lp

)
√(

(cid:0) 1
)
lp
0

)

1 (cid:0) lpM

lpM + 1

B
B
@

1
lp

p
p

(cid:0) 2

p

1
lpM
1(cid:0)lpM

1(cid:0)lp
p
lp

+

1
1(cid:0)lpM

(cid:0) 1

n=2

1

C
C
A

φ(p; pM)H =

)

(

1 (cid:0) l′
p

1

2 ((cid:0)n(cid:0)1)
0

B
B
@

(

l′
p

(cid:0)lpM

(

)

(cid:0)1
where lp = I
2p
beta regularized function.

n
2 , 1
2

, lpM = I

(cid:0)1
1(cid:0)2pM

)

(

) (

(

l′
p
√(

(cid:0) 1

)

lpM
)

(cid:0) 1
√(

1 (cid:0) l′
p

l′
p

(

, l′

p = I

(cid:0)1
2p(cid:0)1

2 , n
1
2

+ 2

)

2 , n
1
2

1 (cid:0) lpM
)

1

C
C
A

n+1
2

(16.1)

)

lpM + 1

, and I

(cid:0)1
(.) (., .) is the inverse

Remark 16.1
For p= 1
2 the distribution doesn’t exist in theory, but does in practice and we can work around it
with the sequence pmk = 1
k , as in the graph showing a convergence to the Uniform distribution
2
on [0, 1] in Figure 16.3. Also note that what is called the "null" hypothesis is effectively a set of
measure 0.

(cid:6) 1

Proof. Let Z be a random normalized variable with realizations z, from a vector ⃗v of n
realizations, with sample mean mv, and sample standard deviation sv, z = mv(cid:0)mh
(where
mh is the level it is tested against), hence assumed to s Student T with n degrees of
freedom, and, crucially, supposed to deliver a mean of ¯z,
) n+1
2
)

svp
n

(

(

f (z; ¯z) =

n
( ¯z(cid:0)z)2+n
p
n
2 , 1
2

nB

where B(.,.) is the standard beta function. Let g(.) be the one-tailed survival function of the
Student T distribution with zero mean and n degrees of freedom:

g(z) = P(Z > z) =

8
>><

1
2 I n
(
z2+n

(

)

n
2 , 1
2
(

)

)

>>:

1
2

I z2
z2+n

2 , n
1
2

+ 1

z (cid:21) 0

z < 0

244

the meta-distribution of standard p-values ‡

where I(.,.) is the incomplete Beta function.
We now look for the distribution of g ◦ f (z). Given that g(.)
is a legit Borel function,
and naming p the probability as a random variable, we have by a standard result for the
transformation:

(

)

φ(p, ¯z) =

f
jg′

g((cid:0)1)(p)
(
g((cid:0)1)(p)

)

j

We can convert ¯z into the corresponding median survival probability because of symme-
try of Z. Since one half the observations fall on either side of ¯z, we can ascertain that the
transformation is median preserving: g( ¯z) = 1
2 . Hence we end up hav-
g (negative

2 , hence φ(pM, .) = 1
(
2 , n
1
2

g (positive case) and f ¯z : 1
2

I z2
z2+n
case). Replacing we get Eq.16.1 and Proposition 16.1 is done.

ing f ¯z : 1

2 I n
¯z2+n

= pM

= pM

n
2 , 1
2

+ 1

)

(

)

)

(

We note that n does not increase signiﬁcance, since p-values are computed from nor-
malized variables (hence the universality of the meta-distribution); a high n corresponds
to an increased convergence to the Gaussian. For large n, we can prove the following
proposition:

Proposition 16.2
Under the same assumptions as above, the limiting distribution for φ(.):

lim
n!¥

φ(p; pM) = e

(cid:0)erfc

(cid:0)1(2pM)(erfc

(cid:0)1(2pM)(cid:0)2erfc

(cid:0)1(2p))

where erfc(.) is the complementary error function and er f c(.)
The limiting CDF F(.)

(cid:0)1 its inverse.

F(k; pM) =

(

1
2

erfc

(cid:0)1(1 (cid:0) 2k) (cid:0) erf

erf

(cid:0)1(1 (cid:0) 2pM)

)

(16.2)

(16.3)

Proof. For large n, the distribution of Z = mv
svp
n

(

)

tailed survival function g(.) = 1

2 erfc

p

, z(p) !

2erfc

(cid:0)1(p).

zp
2

becomes that of a Gaussian, and the one-

This limiting distribution applies for paired tests with known or assumed sample vari-
ance since the test becomes a Gaussian variable, equivalent to the convergence of the T-test
(Student T) to the Gaussian when n is large.

Remark 16.2
For values of p close to 0, φ in Equ. 16.2 can be usefully calculated as:

φ(p; pM) =

v
u
u
t

log

p

2p pM

√

(

(

)

1
2p p2
M
(

))

√

(

(

))

(cid:0) log

2p log

1
2p p2

(cid:0)2 log(p)

(cid:0) log

2p log

1
2p p2
M

(cid:0)2 log(pM)

e

+ O(p2).

(16.4)

16.1 proofs and derivations

245

Figure 16.2: The probability distribution of a one-tailed p-value with expected value .11 generated by Monte
Carlo (histogram) as well as analytically with φ(.) (the solid line). We draw all possible subsamples from an
ensemble with given properties. The excessive skewness of the distribution makes the average value considerably
higher than most observations, hence causing illusions of "statistical signiﬁcance".

The approximation works more precisely for the band of relevant values 0 < p < 1

2p .

From this we can get numerical results for convolutions of φ using the Fourier Transform
or similar methods.

We can and get the distribution of the minimum p-value per m trials across statistically
identical situations thus get an idea of "p-hacking", deﬁned as attempts by researchers
to get the lowest p-values of many experiments, or try until one of the tests produces
statistical signiﬁcance.

Proposition 16.3
The distribution of the minimum of m observations of statistically identical p-values becomes (under
the limiting distribution of proposition 16.2):

φm(p; pM) = m eerfc

(cid:0)1(2pM)(2erfc

(cid:0)1(2pM))
(cid:0)1(2p)(cid:0)erfc
(

(

erfc

erfc

(cid:0)1(2p) (cid:0) erfc

(cid:0)1(2pM)

))

m(cid:0)1

(16.5)

1 (cid:0) 1
2

Proof. P (p1 > p, p2 > p, . . . , pm > p) =
get the result.

∩

n
i=1

F(pi) = ¯F(p)m. Taking the ﬁrst derivative we

Outside the limiting distribution: we integrate numerically for different values of m as
shown in ﬁgure 16.4. So, more precisely, for m trials, the expectation is calculated as:

E(pmin) =

∫

1

0

(cid:0)m φ(p; pM)

(∫

p

0

)

m(cid:0)1

dp

φ(u, .) du

p-value(true mean)5% cutpointMedian∼ 53% of realizations <.05∼25% of realizations <.010.050.100.150.20p0.000.050.100.15PDF/Frequ.246

the meta-distribution of standard p-values ‡

Figure 16.3: The probability distribution of p at different values of pM. We observe how pM = 1
uniform distribution.

2 leads to a

Figure 16.4: The "p-hacking" value across m trials for pM = .15 and ps = .22.

.025.1.150.50.00.20.40.60.81.0p12345φn=5n=152468101214mtrials0.020.040.060.080.100.12Expectedminp-val16.2 inverse power of test

247

16.2 inverse power of test

Let b be the power of a test for a given p-value p, for random draws X from unobserved
parameter q and a sample size of n. To gauge the reliability of b as a true measure of
power, we perform an inverse problem:

Xq,p,n

b

∆

b(cid:0)1(X)

Proposition 16.4
Let bc be the projection of the power of the test from the realizations assumed to be student T
distributed and evaluated under the parameter q. We have

{

F(bc) =

for bc < 1
F(bc)L
2
F(bc)H for bc > 1
2

where

F(bc)L =

√

1 (cid:0) g
1

g

(cid:0) n
2
1 (

(cid:0)

√

2

p

(cid:0)(g
1

(cid:0)1

1
g
3

(cid:0)1)g
(cid:0)2
1
√

p

g
1
(cid:0)(g
1
(cid:0) (g
1

(cid:0)1)g
1+g
1
(cid:0) 1) g
1

)

(

√

2

1
g
3

(cid:0)1(cid:0) 1
g
3

n+1
2

)

(cid:0)1

(16.6)

F(bc)H =

p

2 (1 (cid:0) g
g

2)

(cid:0) n

2 B

(

0

B
@

)

1
2

,

n
2

(p

(cid:0)2

(cid:0)(g
2

1

C
A n+1
2

(16.7)

(cid:0)1)g

2+g
2
√

)√

√

1g
3

1
1g
(cid:0)1+2
3
(cid:0)1
g
2
(cid:0) 1) g

(cid:0) (g
2

p

(cid:0)1+2
(

(cid:0)1

(cid:0)(g
(cid:0)1)g
2
2
)

+ 1
g
3

2B
(

n
2 , 1
2
)

n
2 , 1
2

.

where g

1 = I

(cid:0)1
2bc

(

)

n
2 , 1
2

, g

2 = I

(cid:0)1
2bc(cid:0)1

(

)

2 , n
1
2

, and g

3 = I

(cid:0)1
(1,2ps(cid:0)1)

16.3 application and conclusion

(cid:15) One can safely see that under such stochasticity for the realizations of p-values and
the distribution of its minimum, to get what people mean by 5% conﬁdence (and the
inferences they get from it), they need a p-value of at least one order of magnitude
smaller.

(cid:15) Attempts at replicating papers, such as the open science project [36], should consider
a margin of error in its own procedure and a pronounced bias towards favorable re-
sults (Type-I error). There should be no surprise that a previously deemed signiﬁcant
test fails during replication –in fact it is the replication of results deemed signiﬁcant
at a close margin that should be surprising.

248

the meta-distribution of standard p-values ‡

(cid:15) The "power" of a test has the same problem unless one either lowers p-values or sets

the test at higher levels, such at .99.

acknowledgment

Marco Avellaneda, Pasquale Cirillo, Yaneer Bar-Yam, friendly people on twitter ...

17 E L E C T I O N P R E D I C T I O N S A S

M A R T I N G A L E S : A N A R B I T R A G E
A P P R O A C H ‡

We examine the effect of uncertainty on binary outcomes, with application

to elections. A standard result in quantitative ﬁnance is that when the
volatility of the underlying security increases, arbitrage pressures push
the corresponding binary option to trade closer to 50%, and become less
variable over the remaining time to expiration. Counterintuitively, the
higher the uncertainty of the underlying security, the lower the volatility of the binary
option. This effect should hold in all domains where a binary price is produced – yet
we observe severe violations of these principles in many areas where binary forecasts
are made, in particular those concerning the U.S. presidential election of 2016. We
observe stark errors among political scientists and forecasters, for instance with 1)
assessors giving the candidate D. Trump between 0.1% and 3% chances of success ,
2) jumps in the revisions of forecasts from 48% to 15%, both made while invoking
uncertainty.

Conventionally, the quality of election forecasting has been assessed statically by De Finetti’s
method, which consists in minimizing the Brier score , a metric of divergence from the ﬁnal
outcome (the standard for tracking the accuracy of probability assessors across domains,
from elections to weather). No intertemporal evaluations of changes in estimates appear to
have been imposed outside the quantitative ﬁnance practice and literature. Yet De Finetti’s
own principle is that a probability should be treated like a two-way "choice" price, which
is thus violated by conventional practice.

In this paper we take a dynamic, continuous-time approach based on the principles of
quantitative ﬁnance and argue that a probabilistic estimate of an election outcome by a
given "assessor" needs be treated like a tradable price, that is, as a binary option value
subjected to arbitrage boundaries (particularly since binary options are actually used in
betting markets). Future revised estimates need to be compatible with martingale pricing,
otherwise intertemporal arbitrage is created, by "buying" and "selling" from the assessor.

A mathematical complication arises as we move to continuous time and apply the stan-
dard martingale approach: namely that as a probability forecast, the underlying security
lives in [0, 1]. Our approach is to create a dual (or "shadow") martingale process Y, in
an interval [L, H] from an arithmetic Brownian motion, X in ((cid:0)¥, ¥) and price elections
accordingly. The dual process Y can for example represent the numerical votes needed
for success. A complication is that, because of the transformation from X to Y, if Y is a
martingale, X cannot be a martingale (and vice-versa).

The process for Y allows us to build an arbitrage relationship between the volatility of a
probability estimate and that of the underlying variable, e.g. the vote number. Thus we are

249

250

election predictions as martingales: an arbitrage approach ‡

Figure 17.1: Election arbitrage "estimation" (i.e., valuation) at different expected proportional votes Y 2
[0, 1], with s the expected volatility of Y between present and election results. We can see that under higher
uncertainty, the estimation of the result gets closer to 0.5, and becomes insensitive to estimated electoral
margin.

Figure 17.2: X is an open non observable random variable (a shadow variable of sorts) on R, Y, its mapping
into "votes" or "electoral votes" via a sigmoidal function S(.), which maps one-to-one, and the binary as the
expected value of either using the proper corresponding distribution.

able to show that when there is a high uncertainty about the ﬁnal outcome, 1) indeed, the

0.420.440.460.480.50.040.060.080.100.12s0.20.30.40.5EstimatorBt0∈[0,1]Y∈[L,H]X∈(-∞,∞)B= ℙ(XT>l)B= ℙ(YT>S(l))Y=S(X)election predictions as martingales: an arbitrage approach ‡

251

arbitrage value of the forecast (as a binary option) gets closer to 50% and 2) the estimate
1
should not undergo large changes even if polls or other bases show signiﬁcant variations.
The pricing links are between 1) the binary option value (that is, the forecast probability),
2) the estimation of Y and 3) the volatility of the estimation of Y over the remaining time
to expiration (see Figures 17.1 and 17.2 ).

17.0.1 Main results

For convenience, we start with our notation.

Notation

Y0

T

t0

s

B(.)

the observed estimated proportion of votes expressed in [0, 1] at time t0.
These can be either popular or electoral votes, so long as one treats them
with consistency.
period when the irrevocable ﬁnal election outcome YT is revealed, or
expiration.
present evaluation period, hence T (cid:0) t0 is the time until the ﬁnal election,
expressed in years.
annualized volatility of Y, or uncertainty attending outcomes for Y in
the remaining time until expiration. We assume s is constant without
any loss of generality –but it could be time dependent.
"forecast probability", or estimated continuous-time arbitrage evaluation
of the election results, establishing arbitrage bounds between B(.), Y0
and the volatility s.

Main results

where

B(Y0, s, t0, T) =

(

1
2

erfc

(cid:0)1(2Y0
l (cid:0) erf
√
e2s2(T(cid:0)t0) (cid:0) 1

(cid:0) 1)e

s2(T(cid:0)t0)

)

,

(17.1)

√

(

log

s (cid:25)

2ps2e2erf
p

p

)

(cid:0)1(2Y0

(cid:0)1)2 + 1

,

(17.2)

2
l is the threshold needed (defaults to .5), and erfc(.) is the standard complementary error
function, 1-erf(.), with erf(z) = 2p
p

(cid:0)t2

dt.

∫

z
0 e

T (cid:0) t0

We ﬁnd it appropriate here to answer the usual comment by statisticians and people op-
erating outside of mathematical ﬁnance: "why not simply use a Beta-style distribution for
Y?". The answer is that 1) the main purpose of the paper is establishing (arbitrage-free)
time consistency in binary forecasts, and 2) we are not aware of a continuous time stochas-
tic process that accommodates a beta distribution or a similarly bounded conventional
one.

1 A central property of our model is that it prevents B(.) from varying more than the estimated Y: in a two candidate
contest, it will be capped (ﬂoored) at Y if lower (higher) than .5. In practice, we can observe probabilities of
winning of 98% vs. 02% from a narrower spread of estimated votes of 47% vs. 53%; our approach prevents, under
high uncertainty, the probabilities from diverging away from the estimated votes. But it remains conservative
enough to not give a higher proportion.

252

election predictions as martingales: an arbitrage approach ‡

17.0.2 Organization

The remaining parts of the paper are organized as follows. First, we show the process for
Y and the needed transformations from a speciﬁc Brownian motion. Second, we derive
the arbitrage relationship used to obtain equation (17.1). Finally, we discuss De Finetti’s
approach and show how a martingale valuation relates to minimizing the conventional
standard in the forecasting industry, namely the Brier Score.

A comment on absence of closed form solutions for s We note that for Y we lack a
ds,
closed form solution for the integral reﬂecting the total variation:
though the corresponding one for X is computable. Accordingly, we have relied on propa-
gation of uncertainty methods to obtain a closed form solution for the probability density
of Y, though not explicitly its moments as the logistic normal integral does not lend itself
to simple expansions [135].

(cid:0)1(2ys(cid:0)1)2

(cid:0)erf

p e

T
t0

sp

∫

Time slice distributions for X and Y The time slice distribution is the probability density
function of Y from time t, that is the one-period representation, starting at t with y0 =
1
2 + 1
2 erf(x0). Inversely, for X given y0, the corresponding x0, X may be found to be normally
distributed for the period T (cid:0) t0 with

E(X, T) = X0e

V(X, T) =

s2(T(cid:0)t0),
e2s2(T(cid:0)t0) (cid:0) 1
2

and a kurtosis of 3. By probability transformation we obtain φ, the corresponding distri-
bution of Y with initial value y0 is given by

φ(y; y0, T) =

1√

e2s2(t(cid:0)t0) (cid:0) 1

{

exp

(cid:0)1(2y (cid:0) 1)2

erf

(

(

)

) (

coth

s2t

(cid:0) 1

(cid:0) 1
2

(cid:0)1(2y (cid:0) 1) (cid:0) erf

erf

(cid:0)1(2y0

(cid:0) 1)e

s2(t(cid:0)t0)

(17.3)

}

)

2

and we have E(Yt) = Y0.
As to the variance, E(Y2), as mentioned above, does not lend itself to a closed-form
solution derived from φ(.), nor from the stochastic integral; but it can be easily estimated
from the closed form distribution of X using methods of propagation of uncertainty for
the ﬁrst two moments (the delta method).

Since the variance of a function f of a ﬁnite moment random variable X can be approxi-

mated as V ( f (X)) = f

′ (E(X))2 V(X):

(cid:12)
(cid:12)
(cid:12)
(cid:12)

¶S

(cid:0)1(y)
¶y

y=Y0

s2 (cid:25) e2s2(T(cid:0)t0) (cid:0) 1

2

v
u
u
t e(cid:0)2erf

(

(cid:0)1(2Y0

(cid:0)1)2

e2s2(T(cid:0)t0) (cid:0) 1

2p

)

.

s (cid:25)

(17.4)

Likewise for calculations in the opposite direction, we ﬁnd

17.1 the bachelier-style valuation

253

√

(

log

s (cid:25)

(cid:0)1(2Y0

(cid:0)1)2 + 1

2ps2e2erf
p

p

)

,

T (cid:0) t0
which is (17.2) in the presentation of the main result.

2

Note that expansions including higher moments do not bring a material increase in preci-
sion – although s is highly nonlinear around the center, the range of values for the volatility
of the total or, say, the electoral college is too low to affect higher order terms in a signiﬁ-
cant way, in addition to the boundedness of the sigmoid-style transformations.

17.0.3 A Discussion on Risk Neutrality

We apply risk neutral valuation, for lack of conviction regarding another way, as a default
option. Although Y may not necessarily be tradable, adding a risk premium for the process
involved in determining the arbitrage valuation would necessarily imply a negative one
for the other candidate(s), which is hard to justify. Further, option values or binary bets,
need to satisfy a no Dutch Book argument (the De Finetti form of no-arbitrage) (see [?
]), i.e. properly priced binary options interpreted as probability forecasts give no betting
"edge" in all outcomes without loss. Finally, any departure from risk neutrality would
degrade the Brier score (about which, below) as it would represent a diversion from the
ﬁnal forecast.

Also note the absence of the assumptions of ﬁnancing rate usually present in ﬁnancial

discussions.

17.1 the bachelier-style valuation

Let F(.) be a function of a variable X satisfying

dXt = s2 Xtdt + s dWt.

(17.5)

We wish to show that X has a simple Bachelier option price B(.). The idea of no arbitrage

is that a continuously made forecast must itself be a martingale.
Applying Itô’s Lemma to F ≜ B for X satisfying (17.5) yields

[

]

dF =

s2 X

1
2
≜ 0, F must satisfy the partial differential equation

dt + s F
X

F
X

dW

F
t

2F
X2 +

s2

+

so that, since F
t

s2

1
2

2F
X2 + s2 X

F
X

+

F
t

= 0,

(17.6)

which is the driftless condition that makes B a martingale.

254

election predictions as martingales: an arbitrage approach ‡

Figure 17.3: Theoretical approach (top) vs practice (bottom). Shows how the estimation process cannot be in
sync with the volatility of the estimation of (electoral or other) votes as it violates arbitrage boundaries

For a binary (call) option, we have for terminal conditions B(X, t) ≜ F, FT = q(x (cid:0) l), where
q(.) is the Heaviside theta function and l is the threshold:

q(x) :=

{

1,

0,

x (cid:21) l
x < l

ELECTIONDAY538Rigorousupdating204060801000.50.60.70.80.91.017.2 bounded dual martingale process

255

Figure 17.4: Process and Dual Process

with initial condition x0 at time t0 and terminal condition at T given by:

)

(

1
2

erfc

x0e
√

s2t (cid:0) l
e2s2t (cid:0) 1

which is, simply, the survival function of the Normal distribution parametrized under the
process for X.

Likewise we note from the earlier argument of one-to one (one can use Borel set argu-

ments ) that

{

q(y) :=

1,

0,

y (cid:21) S(l)
y < S(l),

so we can price the alternative process B(Y, t) = P(Y > 1
threshold l, by pricing

2 ) (or any other similarly obtained

B(Y0, t0) = P(x > S

(cid:0)1(l)).

The pricing from the proportion of votes is given by:

B(Y0, s, t0, T) =

(

1
2

erfc

(cid:0)1(2Y0
l (cid:0) erf
√
e2s2(T(cid:0)t0) (cid:0) 1

(cid:0) 1)e

s2(T(cid:0)t0)

)

,

the main equation (17.1), which can also be expressed less conveniently as

B(y0, s, t0, T) =

1√

(

exp

(cid:0)1(2y (cid:0) 1)2

erf

∫

1

l

e2s2t (cid:0) 1
(
(cid:0) 1
2

(

)

) (

coth

s2t

(cid:0) 1

(cid:0)1(2y (cid:0) 1) (cid:0) erf

erf

(cid:0)1(2y0

(cid:0) 1)e

s2t

)

)

2

dy

17.2 bounded dual martingale process

YT is the terminal value of a process on election day. It lives in [0, 1] but can be generalized
to the broader [L, H], L, H 2 [0, ¥). The threshold for a given candidate to win is ﬁxed at
l. Y can correspond to raw votes, electoral votes, or any other metric. We assume that Yt
is an intermediate realization of the process at t, either produced synthetically from polls
(corrected estimates) or other such systems.

XY2004006008001000t-1.5-1.0-0.50.5X,Y256

election predictions as martingales: an arbitrage approach ‡

Next, we create, for an unbounded arithmetic stochastic process, a bounded "dual"
stochastic process using a sigmoidal transformation. It can be helpful to map processes
such as a bounded electoral process to a Brownian motion, or to map a bounded payoff to
an unbounded one, see Figure 17.2.

Proposition 17.1
Under sigmoidal style transformations S : x 7! y, R ! [0, 1] of the form a) 1
1+exp((cid:0)x) , if X is a martingale, Y is only a martingale for Y0 = 1
only a martingale for X0 = 0 .

2 + 1
2 erf(x), or b)
2 , and if Y is a martingale, X is

1

(

)

Proof. The proof is sketched as follows. From Itô’s lemma, the drift term for dXt becomes 1)
s2X(t), or 2) 1
, where s denotes the volatility, respectively with transforma-
2
tions of the forms a) of Xt and b) of Xt under a martingale for Y. The drift for dYt becomes:
1)

s2Y(Y (cid:0) 1)(2Y (cid:0) 1) under a martingale for X.

s2Tanh

(2Y(cid:0)1)

X(t)
2

(cid:0)erf

s2e

(cid:0)1

(cid:0)1

(2Y(cid:0)1)2erf
p
p

or 2) 1
2

We therefore select the case of Y being a martingale and present the details of the trans-
formation a). The properties of the process have been developed by Carr [24]. Let X be the
arithmetic Brownian motion (17.5), with X-dependent drift and constant scale s:

dXt = s2Xtdt + sdWt, 0 < t < T < +¥.

We note that this has similarities with the Ornstein-Uhlenbeck process normally written
dXt = q(m (cid:0) Xt)dt + sdW, except that we have m = 0 and violate the rules by using a
negative mean reversion coefﬁcient, rather more adequately described as "mean repelling",
q = (cid:0)s2.
We map from X 2 ((cid:0)¥, ¥) to its dual process Y as follows. With S : R ! [0, 1], Y = S(x),

S(x) =

1
2

+

1
2

erf(x)

the dual process (by unique transformation since S is one to one, becomes, for y ≜ S(x),
using Ito’s lemma (since S(.) is twice differentiable and S=t = 0):
(

)

dS =

s2

1
2

¶2S
¶x2 + Xs2

¶S
¶x

dt + s

dW

¶S
¶x

which with zero drift can be written as a process

for all t > t, E(YtjYt) = Yt. and scale

dYt = s(Y)dWt,

s(Y) =

s
p
p e

(cid:0)erf

(cid:0)1(2y(cid:0)1)2

which as we can see in Figure 17.5, s(y) can be approximated by the quadratic function
y(1 (cid:0) y) times a constant.
(cid:0)1(2y (cid:0) 1), and again
We can recover equation (17.5) by inverting, namely S
applying Itô’s Lemma. As a consequence of gauge invariance option prices are identical
whether priced on X or Y, even if one process has a drift while the other is a martingale .
In other words, one may apply one’s estimation to the electoral threshold, or to the more

(cid:0)1(y) = erf

17.3 relation to de finetti’s probability assessor

257

Figure 17.5: The instantaneous volatility
of Y as a function of the level of Y for two
different methods of transformations of X,
which appear to not be substantially dif-
ferent. We compare to the quadratic form
y (cid:0) y2 scaled by a constant
. The

√

3

1
8p
2

volatility declines as we move away from
1
2 and collapses at the edges, thus main-
taining Y in (0, 1). For simplicity we as-
sumed s = t = 1.

complicated X with the same results. And, to summarize our method, pricing an option
on X is familiar, as it is exactly a Bachelier-style option price.

17.3 relation to de finetti’s probability assessor

Figure 17.6: Bruno de Finetti
(1906-
1985). A probabilist, philosopher, and
insurance mathematician, he formulated
the Brier score for probabilistic assessment
which we show is compatible dynamically
with a martingale. Source: DeFinetti.org

This section provides a brief background for the conventional approach to probability
assessment. The great De Finetti [44] has shown that the "assessment" of the "probabil-
ity" of the realization of a random variable in f0, 1g requires a nonlinear loss function –
which makes his deﬁnition of probabilistic assessment differ from that of the P/L of a trader
engaging in binary bets.

ⅇ-erf-1(-1+2y)2π8π23y(1-y)0.20.40.60.81.0Yt0.050.100.150.200.25s258

election predictions as martingales: an arbitrage approach ‡

Assume that a betting agent in an n-repeated two period model, t0 and t1, produces a
2 [0, 1] indexed by i = 1, 2, . . . , n, with the realization of the binary

t1,i. If we take the absolute variation of his P/L over n bets, it will be

strategy S of bets b0,i
r.v. 1

L1(S) =

(cid:12)
(cid:12)1

1
n

n
(cid:229)
i=1

(cid:0) bt0,i

t1,i

(cid:12)
(cid:12)

.

2 , produces a loss
2 in expectation, which is the same as betting either 0 or 1 – hence not favoring the

2 . Betting on the probability, here 1

For example, assume that E(1t1) = 1
of 1
agent to bet on the exact probability.

If we work with the same random variable and non-time-varying probabilities, the L1

metric would be appropriate:

L1(S) =

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

1

t1,i

(cid:0)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) .

n
(cid:229)
i=1

bt0,i

De Finetti proposed a "Brier score" type function, a quadratic loss function in L2:

L2(S) =

1
n

n
(cid:229)
i=1

(1

t1,i

(cid:0) bt0,i)2,

the minimum of which is reached for bt0,i = E(1t1).
In our world of continuous time derivative valuation, where, in place of a two period
lattice model, we are interested, for the same ﬁnal outcome at t1, in the stochastic process
(cid:21) t (cid:21) t1, the arbitrage "value" of a bet on a binary outcome needs to match the expec-
bt, t0
tation, hence, again, we map to the Brier score – by an arbitrage argument. Although there
is no quadratic loss function involved, the fact that the bet is a function of a martingale,
which is required to be itself a martingale, i.e. that the conditional expectation remains
invariant to time, does not allow an arbitrage to take place. A "high" price can be "shorted"
by the arbitrageur, a "low" price can be "bought", and so on repeatedly. The consistency
between bets at period t and other periods t + ∆t enforces the probabilistic discipline. In
other words, someone can "buy" from the forecaster then "sell" back to him, generating a
positive expected "return" if the forecaster is out of line with martingale valuation.

As to the current practice by forecasters, although some election forecasters appear to be
aware of the need to minimize their Brier score , the idea that the revisions of estimates
should also be subjected to martingale valuation is not well established.

17.4 conclusion and comments

As can be seen in Figure 17.1, a binary option reveals more about uncertainty than about
the true estimation, a result well known to traders, see [156].
In the presence of more than 2 candidates, the process can be generalized with the fol-
lowing heuristic approximation. Establish the stochastic process for Y1,t, and just as Y1,t is
a process in [0, 1], Y2,t is a process 2 (Y1,t, 1], with Y3,t the residual 1 (cid:0) Y2,t
(cid:0) Y1,t, and more
generally Yn(cid:0)1,t
i=1 Yi,t. For n candidates, the
nth is the residual.

2 (Yn2,t, 1] and Yn,t is the residual Yn = 1 (cid:0) (cid:229)n(cid:0)1

17.5 acknowledgements

259

17.5 acknowledgements

The author thanks Dhruv Madeka and Raphael Douady for detailed and extensive dis-
cussions of the paper as well as thorough auditing of the proofs across the various iter-
ations, and, worse, the numerous changes of notation. Peter Carr helped with discus-
sions on the properties of a bounded martingale and the transformations. I thank David
Shimko,Andrew Lesniewski, and Andrew Papanicolaou for comments. I thank Arthur Bre-
itman for guidance with the literature for numerical approximations of the various logistic-
normal integrals. I thank participants of the Tandon School of Engineering and Bloomberg
I also thank Bruno Dupire,MikeLawler, theEditors-In-
Quantitative Finance Seminars.
Chief, and various friendly people on social media. DhruvMadeka fromBloomberg, while
working on a similar problem, independently came up with the same relationships be-
tween the volatility of an estimate and its bounds and the same arbitrage bounds. All
errors are mine.

Dhruv Madeka, then at Bloomberg, while working on a similar problem, independently
came up with the same relationships between the volatility of an estimate and its bounds
and the same arbitrage bounds. All errors are mine.

Part VIII

O P T I O N T R A D I N G A N D P R I C I N G U N D E R FAT TA I L S

18 F I N A N C I A L T H E O R Y ’ S FA I L U R E S I N

O P T I O N P R I C I N G

†

Let us discuss why option theory as according to the so-called "neoclassical

economics" fails in the real world. How does ﬁnancial theory price ﬁnancial
products? The principal difference in paradigm between the one presented
by Bachelier in 1900, [4] and the modern ﬁnance one known as Black-Scholes-
Merton [16] and [116] lies in the following.

Figure 18.1: The hedging er-
rors for an option portfolio (un-
der a daily revision regime)
over 3000 days, under a con-
stant volatility Student T with
tail exponent a = 3. Techni-
cally the errors should not con-
verge in ﬁnite time as their dis-
tribution has inﬁnite variance.

Figure 18.2: Hedging errors
for an option portfolio (daily
revision) under
an equiva-
lent (rather ﬁctional) "Black-
Scholes" world.

263

0.0000.0020.0040.0060.0080.0100.0120.0000.0020.0040.0060.0080.0100.012264

financial theory’s failures in option pricing

Figure 18.3: Portfolio Hedg-
ing errors including the stock
market crash of 1987.

Bachelier’s model is based on an actuarial expectation of ﬁnal payoffs –not dynamic
hedging.
It means you can use any distribution! A more formal proof using measure
theory is provided in Chapter 19 so for now let us just get the intuition without too much
mathematics.
The same method was later used by a series of researchers, such as Sprenkle [150] in
1964, Boness, [18] in 1964, Kassouf and Thorp, [178] in 1967, Thorp, [175] (only published
in 1973).
They all encountered the following problem: how to produce a risk parameter –a risky
asset discount rate – to make it compatible with portfolio theory? The Capital Asset
Pricing Model requires that securities command an expected rate of return in propor-
tion to their riskiness. In the Black-Scholes-Merton approach, an option price is derived
from continuous-time dynamic hedging, and only in properties obtained from continuous
time dynamic hedging –we will describe dynamic hedging in some details further down.
Thanks to such a method, an option collapses into a deterministic payoff and provides
returns independent of the market; hence it does not require any risk premium.

The problem we have with the Black-Scholes-Merton approach is that the requirements
for dynamic hedging are extremely idealized, requiring the following strict conditions.
The operator is assumed to be able to buy and sell in a frictionless market, incurring no
transaction costs. The procedure does not allow for the price impact of the order ﬂow –if
an operator sells a quantity of shares, it should not have consequences on the subsequent
price. The operator knows the probability distribution, which is the Gaussian, with ﬁxed
and constant parameters through time (all parameters do not change). Finally, the most
signiﬁcant restriction: no scalable jumps. In a subsequent revision [Merton, 1976] allows
for jumps but these are deemed to be Poisson arrival time, and ﬁxed or, at the worst,
Gaussian. The framework does not allow the use of power laws both in practice and
mathematically. Let us examine the mathematics behind the stream of dynamic hedges in
the Black-Scholes-Merton equation.
Assume the risk-free interest rate r=0 with no loss of generality. The canonical Black-
Scholes-Merton model consists in selling a call and purchasing shares of stock that provide
a hedge against instantaneous moves in the security. Thus the portfolio p locally “hedged”
against exposure to the ﬁrst moment of the distribution is the following:

where C is the call price, and S the underlying security.

p = (cid:0)C +

¶C
¶S

S

(18.1)

0.000.010.020.030.040.05financial theory’s failures in option pricing

265

Take the change in the values of the portfolio

∆p = (cid:0)∆C +

¶C
¶S

∆S

(18.2)

By expanding around the initial values of S, we have the changes in the portfolio in discrete
time. Conventional option theory applies to the Gaussian in which all orders higher than
(∆S)2 and ∆t disappears rapidly.

(

)

¶C
¶t

¶2C
¶S2

∆S2 + O

∆p = (cid:0)

∆t (cid:0) 1
2
Taking expectations on both sides, we can see from (3) very strict requirements on moment
ﬁniteness: all moments need to converge. If we include another term, (cid:0) 1
∆S3, it may be
6
of signiﬁcance in a probability distribution with signiﬁcant cubic or quartic terms. Indeed,
although the nth derivative with respect to S can decline very sharply, for options that have
a strike K away from the center ot the distribution, it remains that the moments are rising
disproportionately fast for that to carry a mitigating effect.

(18.3)

¶3C
¶S3

∆S3

So here we mean all moments need to be ﬁnite and losing in impact –no approximation.
Note here that the jump diffusion model (Merton,1976) does not cause much trouble since
it has all the moments. And the annoyance is that a power law will have every moment
higher than a inﬁnite, causing the equation of the Black-Scholes-Merton portfolio to fail.
As we said, the logic of the Black-Scholes-Merton so-called solution thanks to Ito’s lemma
was that the portfolio collapses into a deterministic payoff. But let us see how quickly or
effectively this works in practice.

18.0.1 The actual replication process:

The payoff of a call should be replicated with the following stream of dynamic hedges, the
limit of which can be seen here, between t and T

(

n=T=∆t
(cid:229)
i=1

¶C
¶S

j

Lim
∆t!0

S=St+(i(cid:0)1)∆t,t=t+(i(cid:0)1)∆t,

(

St+i∆t

(cid:0) St+(i(cid:0)1)∆t

)

)

(18.4)

¶C
We break up the period into n increments ∆t. Here the hedge ratio
¶S is computed as of
time t +(i-1) ∆t, but we get the nonanticipating difference between the price at the time
the hedge was initiatied and the resulting price at t+ i ∆t.
This is supposed to make the payoff deterministic at the limit of ∆t ! 0. In the Gaussian
world, this would be an Ito-McKean integral.

18.0.2

Failure:How hedging errors can be prohibitive.

As a consequence of the mathematical property seen above, hedging errors in an cubic a
appear to be indistinguishable from those from an inﬁnite variance process. Furthermore
such error has a disproportionaly large effect on strikes away from the money.

In short: dynamic hedging in a power law world removes no risk.

The next chapter will use measure theory to show why options can still be risk-neutral.

19 U N I Q U E O P T I O N P R I C I N G

M E A S U R E W I T H N E I T H E R D Y N A M I C
H E D G I N G N O R C O M P L E T E
M A R K E T S ‡

We present the proof that under simple assumptions, such as constraints

of Put-Call Parity, the probability measure for the valuation of a Euro-
pean option has the mean derived from the forward price which can,
but does not have to be the risk-neutral one, under any general proba-
bility distribution, bypassing the Black-Scholes-Merton dynamic hedg-
ing argument, and without the requirement of complete markets and other strong
assumptions. We conﬁrm that the heuristics used by traders for centuries are both
more robust, more consistent, and more rigorous than held in the economics litera-
ture. We also show that options can be priced using inﬁnite variance (ﬁnite mean)
distributions.

19.1 background

Option valuations methodologies have been used by traders for centuries, in an effective
way (Haug and Taleb,

There have been a couple of predecessors to the present thesis that Put-Call parity is
sufﬁcient constraint to enforce some structure at the level of the mean of the underlying
distribution, such as Derman and Taleb (2005), Haug and Taleb (2010). These approaches
were heuristic, robust though deemed hand-waving (Rufﬁno and Treussard, [143]).
In
addition they showed that operators need to use the risk-neutral mean. What this paper
does is

(cid:15) It goes beyond the "handwaving" with formal proofs.
(cid:15) It uses a completely distribution-free, expectation-based approach and proves the
risk-neutral argument without dynamic hedging, and without any distributional as-
sumption.

(cid:15) Beyond risk-neutrality, it establishes the case of a unique pricing distribution for
option prices in the absence of such argument. The forward (or future) price can
embed expectations and deviate from the arbitrage price (owing to, say, regulatory
or other limitations) yet the options can still be priced at a distibution corresponding
to the mean of such a forward.

(cid:15) It shows how one can practically have an option market without "completeness" and

without having the theorems of ﬁnancial economics hold.

267

268

unique option pricing measure with neither dynamic hedging nor complete markets ‡

These are done with solely two constraints: "horizontal", i.e. put-call parity, and "vertical",
the different valuations across strike prices deliver a probability measure which is
i.e.
shown to be unique. The only economic assumption made here is that the forward exits,
is tradable — in the absence of such unique forward price it is futile to discuss standard
option pricing. We also require the probability measures to correspond to distributions
with ﬁnite ﬁrst moment.

.

1

Preceding works in that direction are as follows. Breeden and Litzenberger [22] and
Dupire [54], show how option spreads deliver a unique probability measure; there are
papers establishing broader set of arbitrage relations between options such as Carr and
Madan [26]
However 1) none of these papers made the bridge between calls and puts via the forward,
thus translating the relationships from arbitrage relations between options delivering a
probability distribution into the necessity of lining up to the mean of the distribution of
the forward, hence the risk-neutral one (in case the forward is arbitraged.) 2) Nor did any
paper show that in the absence of second moment (say, inﬁnite variance), we can price
options very easily. Our methodology and proofs make no use of the variance. 3) Our
method is vastly simpler, more direct, and robust to changes in assumptions.

We make no assumption of general market completeness. Options are not redundant
securities and remain so. Table 1 summarizes the gist of the paper.

2 3

19.2 proof

Deﬁne C(St0 , K, t) and P(St0 , K, t) as European-style call and put with strike price K, respec-
tively, with expiration t, and S0 as an underlying security at times t0, t (cid:21) t0, and St the
possible value of the underlying security at time t.

19.2.1 Case 1: Forward as risk-neutral measure

∫

t
t0

rsds, the return of a risk-free money market fund and d = 1
t(cid:0)t0

Deﬁne r = 1
t(cid:0)t0
the payout of the asset (continuous dividend for a stock, foreign interest for a currency).
We have the arbitrage forward price FQ
t
(1 + r)(t(cid:0)t0)
(1 + d)(t(cid:0)t0)

t S0 e(r(cid:0)d)(t(cid:0)t0)

FQ
t = S0

:

(19.1)

∫

t
t0

dsds

1 See also Green and Jarrow [83] and Nachman [119]. We have known about the possibility of risk neutral pric-
ing without dynamic hedging since Harrison and Kreps [88] but the theory necessitates extremely strong –and
severely unrealistic –assumptions, such as strictly complete markets and a multiperiod pricing kernel

2 The famed Hakkanson paradox is as follows: if markets are complete and options are redudant, why would
someone need them? If markets are incomplete, we may need options but how can we price them? This
discussion may have provided a solution to the paradox: markets are incomplete and we can price options.

3 Option prices are not unique in the absolute sense: the premium over intrinsic can take an entire spectrum of
values; it is just that the put-call parity constraints forces the measures used for puts and the calls to be the same
and to have the same expectation as the forward. As far as securities go, options are securities on their own; they
just have a strong link to the forward.

19.2 proof

269

Table 19.1: Main practical differences between the dynamic hedging argument and the static Put-Call parity
with spreading across strikes.

Type

Limit

Black-Scholes Merton

Put-Call Parity with Spreading

Continuous rebalancing.

Interpolative static hedge.

Law of large numbers in time
(horizontal).

Law of
strikes (vertical).

large numbers across

Market Assump-
tions

1) Continuous Markets, no gaps,
no jumps.

2) Ability to borrow and lend un-
derlying asset for all dates.

1) Gaps and jumps acceptable.
Possibility of continuous Strikes,
or acceptable number of strikes.

2) Ability to borrow and lend un-
derlying asset for single forward
date.

3) No transaction costs in trading
asset.

3) Low transaction costs in trad-
ing options.

Probability Dis-
tribution

Requires all moments to be ﬁnite.
Excludes the class of slowly vary-
ing distributions

Requires ﬁnite 1st moment (inﬁ-
nite variance is acceptable).

Market
pleteness

Com-

Achieved through dynamic com-
pleteness

Not required (in the traditional
sense)

Realism of As-
sumptions

Low

High

Convergence

Uncertain;
changes expectation

one

large

jump

Robust

Fitness to Reality Only used after "fudging" stan-

dard deviations per strike.

Portmanteau, using speciﬁc dis-
tribution adapted to reality

by arbitrage, see Keynes 1924. We thus call FQ
t
arbitrage, at the risk-neutral rate. Let FP
return" m, with expected forward price:

the future (or forward) price obtained by
t be the future requiring a risk-associated "expected

t = S0(1 + m)(t(cid:0)t0) t S0 em (t(cid:0)t0).
FP

(19.2)

Remark: By arbitrage, all tradable values of the forward price given St0 need to be equal to FQ
t
"Tradable" here does not mean "traded", only subject to arbitrage replication by "cash and
carry", that is, borrowing cash and owning the secutity yielding d if the embedded forward
return diverges from r.

.

270

unique option pricing measure with neither dynamic hedging nor complete markets ‡

19.2.2 Derivations

In the following we take F as having dynamics on its own –irrelevant to whether we are
in case 1 or 2 –hence a unique probability measure Q.
[ Ac
K = (K, ¥).
Deﬁne Ω = [0, ¥) = AK
Consider a class of standard (simpliﬁed) probability spaces (Ω, m
is a probability measure, i.e., satisfying

K where AK = [0, K] and Ac

i) indexed by i, where m

∫

i

Ω dm

i = 1.

Theorem 19.1
For a given maturity T, there is a unique measure m
expectation of terminal payoff.

Q that prices European puts and calls by

This measure can be risk-neutral in the sense that it prices the forward FQ
t
have to be and imparts rate of return to the stock embedded in the forward.

, but does not

Lemma 19.1
For a given maturity T, there exist two measures m
2 for European calls and puts of the same
maturity and same underlying security associated with the valuation by expectation of terminal
payoff, which are unique such that, for any call and put of strike K, we have:

1 and m

and

∫

∫

Ω

Ω

C =

P =

fC dm

1 ,

fP dm

2 ,

(19.3)

(19.4)

respectively, and where fC and fP are (St (cid:0) K)+ and (K (cid:0) St)+ respectively.

Proof. For clarity, set r and d to 0 without a loss of generality. By Put-Call Parity Arbitrage,
a positive holding of a call ("long") and negative one of a put ("short") replicates a tradable
forward; because of P/L variations, using positive sign for long and negative sign for short:

C(St0 , K, t) (cid:0) P(St0 , K, t) + K = FP
t

necessarily since FP
t

is tradable.

Put-Call Parity holds for all strikes, so:

C(St0 , K + ∆K, t) (cid:0) P(St0 , K + ∆K, t) + K + ∆K = FP
t

for all K 2 Ω
Now a Call spread in quantities 1

∆K , expressed as

C(St0 , K, t) (cid:0) C(St0 , K + ∆K, t),

(19.5)

(19.6)

delivers $1 if St > K + ∆K (that is, corresponds to the indicator function 1S>K+∆K), 0 if
St (cid:20) K (or 1S>K), and the quantity times St (cid:0) K if K < St (cid:20) K + ∆K, that is, between 0 and
$1 (see Breeden and Litzenberger, 1978[22]). Likewise, consider the converse argument for
a put, with ∆K < St.
At the limit, for ∆K ! 0

¶C(St0 , K, t)
¶K

= (cid:0)P(St > K) = (cid:0)

∫

dm

1.

Ac
K

(19.7)

19.2 proof

271

By the same argument:

¶P(St0 , K, t)
¶K

=

∫

AK

dm

2 = 1 (cid:0)

∫

dm

2.

Ac
K

(19.8)

As semi-closed intervals generate the whole Borel s-algebra on Ω, this shows that m
2 are unique.

m

1and

Lemma 19.2
The probability measures of puts and calls are the same, namely for each Borel set A in Ω, m
m
2(A).

1(A) =

Proof. Combining Equations 19.5 and 19.6, dividing by 1

∆K and taking ∆K ! 0:

for all values of K, so

(cid:0)

¶C(St0 , K, t)
¶K

+

¶P(St0 , K, t)
¶K

= 1

∫

Ac
K

∫

dm

1 =

dm

2,

Ac
K

(19.9)

(19.10)

hence m
interval, it extends to any Borel set.

1(AK) = m

2(AK) for all K 2 [0, ¥). This equality being true for any semi-closed

Lemma 19.3
Puts and calls are required, by static arbitrage, to be evaluated at same as risk-neutral measure m
as the tradable forward.

Q

Proof.

from Equation 19.5

∫

FP
t =

Ft dm

Q;

Ω

∫

Ω

fC(K) dm

1

(cid:0)

∫

Ω

fP(K) dm

1 =

∫

Ω

Ft dm

Q

(cid:0) K

(19.11)

(19.12)

Taking derivatives on both sides, and since fC

(cid:0) fP = S0 + K, we get the Radon-Nikodym

derivative:

for all values of K.

dm
Q
dm

1

= 1

(19.13)

272

unique option pricing measure with neither dynamic hedging nor complete markets ‡

19.3 case where the forward is not risk neutral

Consider the case where Ft is observable, tradable, and use it solely as an underlying secu-
rity with dynamics on its own. In such a case we can completely ignore the dynamics of
the nominal underlying S, or use a non-risk neutral "implied" rate linking cash to forward,

(

)

(cid:3)

log

F
S0
t(cid:0)t0

=

m
regulatory impediments to borrowing, with no effect on the ﬁnal result.

. the rate m can embed risk premium, difﬁculties in ﬁnancing, structural or

In that situation, it can be shown that the exact same results as before apply, by remplac-
ing the measure m

Q(cid:3) . Option prices remain unique

Q by another measure m

4

.

19.4 comment

We have replaced the complexity and intractability of dynamic hedging with a simple,
more benign interpolation problem, and explained the performance of pre-Black-Scholes
option operators using simple heuristics and rules, bypassing the structure of the theorems
of ﬁnancial economics.

Options can remain non-redundant and markets incomplete: we are just arguing here
for a form of arbitrage pricing (which includes risk-neutral pricing at the level of the
expectation of the probability measure), nothing more. But this is sufﬁcient for us to use
any probability distribution with ﬁnite ﬁrst moment, which includes the Lognormal, which
recovers Black Scholes.

A ﬁnal comparison. In dynamic hedging, missing a single hedge, or encountering a single
gap (a tail event) can be disastrous —as we mentioned, it requires a series of assumptions
beyond the mathematical, in addition to severe and highly unrealistic constraints on the
mathematical. Under the class of fat tailed distributions, increasing the frequency of the
hedges does not guarantee reduction of risk. Further, the standard dynamic hedging argu-
ment requires the exact speciﬁcation of the risk-neutral stochastic process between t0 and t,
something econometrically unwieldy, and which is generally reverse engineered from the
price of options, as an arbitrage-oriented interpolation tool rather than as a representation
of the process.

Here, in our Put-Call Parity based methodology, our ability to track the risk neutral
distribution is guaranteed by adding strike prices, and since probabilities add up to 1, the
degrees of freedom that the recovered measure m
Q has in the gap area between a strike
price K and the next strike up, K + ∆K, are severely reduced, since the measure in the
∫
dm. In other words, no single gap
interval is constrained by the difference
between strikes can signiﬁcantly affect the probability measure, even less the ﬁrst moment,
unlike with dynamic hedging. In fact it is no different from standard kernel smoothing
methods for statistical samples, but applied to the distribution across strikes.

c
AK+∆K

dm (cid:0)

c
AK

∫

5

The assumption about the presence of strike prices constitutes a natural condition: condi-
tional on having a practical discussion about options, options strikes need to exist. Further,
as it is the experience of the author, market-makers can add over-the-counter strikes at will,
should they need to do so.

4 We assumed 0 discount rate for the proofs; in case of nonzero rate, premia are discounted at the rate of the

arbitrage operator

5 For methods of interpolation of implied probability distribution between strikes, see Avellaneda et al.[3].

acknowledgment

Peter Carr, Marco Avellaneda, Hélyette Geman, Raphael Douady, Gur Huberman, Espen
Haug, and Hossein Kazemi.

19.4 comment

273

20 O P T I O N T R A D E R S N E V E R U S E T H E

B L A C K - S C H O L E S - M E R T O N
F O R M U L A (cid:3),‡

Option traders use a heuristically derived pricing formula which they

adapt by fudging and changing the tails and skewness by varying one
parameter, the standard deviation of a Gaussian. Such formula is pop-
ularly called BlackScholesMerton owing to an attributed eponymous
discovery (though changing the standard deviation parameter is in con-
tradiction with it). However, we have historical evidence that: (1) the said Black, Sc-
holes and Merton did not invent any formula, just found an argument to make a
well known (and used) formula compatible with the economics establishment, by re-
moving the risk parameter through dynamic hedging, (2) option traders use (and evi-
dently have used since 1902) sophisticated heuristics and tricks more compatible with
the previous versions of the formula of Louis Bachelier and Edward O. Thorp (that
allow a broad choice of probability distributions) and removed the risk parameter
using put-call parity, (3) option traders did not use the BlackScholesMerton formula
or similar formulas after 1973 but continued their bottom-up heuristics more robust
to the high impact rare event. The chapter draws on historical trading methods and
19th and early 20th century references ignored by the ﬁnance literature. It is time to
stop using the wrong designation for option pricing.

20.1 breaking the chain of transmission

1

For us, practitioners, theories should arise from practice
. This explains our concern
with the scientiﬁc notion that practice should ﬁt theory. Option hedging, pricing, and
trading is neither philosophy nor mathematics. It is a rich craft with traders learning from
traders (or traders copying other traders) and tricks developing under evolution pressures,
in a bottom-up manner.
It is technë, not ëpistemë. Had it been a science it would not
have survived for the empirical and scientiﬁc ﬁtness of the pricing and hedging theories
offered are, we will see, at best, defective and unscientiﬁc (and, at the worst, the hedging
methods create more risks than they reduce). Our approach in this paper is to ferret out
historical evidence of technë showing how option traders went about their business in the
past.

Options, we will show, have been extremely active in the pre-modern ﬁnance world.
Tricks and heuristically derived methodologies in option trading and risk management of
derivatives books have been developed over the past century, and used quite effectively by

1 For us, in this discussion, a practitioner is deemed to be someone involved in repeated decisions about option

hedging, not a support quant who writes pricing software or an academic who provides consulting advice.

275

276

option traders never use the black-scholes-merton formula (cid:3),‡

operators. In parallel, many derivations were produced by mathematical researchers. The
economics literature, however, did not recognize these contributions, substituting the re-
discoveries or subsequent reformulations done by (some) economists. There is evidence of
an attribution problem with Black-Scholes-Merton option formula, which was developed,
used, and adapted in a robust way by a long tradition of researchers and used heuristi-
cally by option book runners. Furthermore, in a case of scientiﬁc puzzle, the exact formula
called Black-Sholes-Merton was written down (and used) by Edward Thorp which, para-
doxically, while being robust and realistic, has been considered unrigorous. This raises
the following: 1) The Black-Scholes-Merton innovation was just a neoclassical ﬁnance ar-
2
, 2) We are not aware of traders using their
gument, no more than a thought experiment
argument or their version of the formula.

It is high time to give credit where it belongs.

20.2 black-scholes was an argument

Option traders call the formula they use the Black-Scholes-Merton formula without being
aware that by some irony, of all the possible options formulas that have been produced in
the past century, what is called the Black-Scholes-Merton formula (after Black and Scholes,
1973, and Merton, 1973) is the one the furthest away from what they are using. In fact of
the formulas written down in a long history it is the only formula that is fragile to jumps
and tail events.
First, something seems to have been lost in translation: Black and Scholes [17] and Merton
[117] actually never came up with a new option formula, but only an theoretical economic
argument built on a new way of deriving, rather re-deriving, an already existing and well
known formula. The argument, we will see, is extremely fragile to assumptions. The
foundations of option hedging and pricing were already far more ﬁrmly laid down before
them. The Black-Scholes-Merton argument, simply, is that an option can be hedged using
a certain methodology called dynamic hedging and then turned into a risk-free instrument,
as the portfolio would no longer be stochastic. Indeed what Black, Scholes and Merton did
was marketing, ﬁnding a way to make a well-known formula palatable to the economics
establishment of the time, little else, and in fact distorting its essence.

3

Such argument requires strange far-fetched assumptions: some liquidity at the level
of transactions, knowledge of the probabilities of future events (in a neoclassical Arrow-
Debreu style) , and, more critically, a certain mathematical structure that requires thin-tails,
. The entire argument is indeed, quite strange and
or mild randomness, on which, later
rather inapplicable for someone clinically and observation-driven standing outside con-
ventional neoclassical economics. Simply, the dynamic hedging argument is dangerous in
practice as it subjects you to blowups; it makes no sense unless you are concerned with
neoclassical economic theory. The Black-Scholes-Merton argument and equation ﬂow a
top-down general equilibrium theory, built upon the assumptions of operators working in
full knowledge of the probability distribution of future outcomes in addition to a collection
of assumptions that, we will see, are highly invalid mathematically, the main one being the

2 Here we question the notion of confusing thought experiments in a hypothetical world, of no predictive power,
with either science or practice. The fact that the Black-Scholes-Merton argument works in a Platonic world and
appears to be elegant does not mean anything since one can always produce a Platonic world in which a certain
equation works, or in which a rigorous proof can be provided, a process called reverse-engineering.

3 Of all the misplaced assumptions of Black Scholes that cause it to be a mere thought experiment, though an
extremely elegant one, a ﬂaw shared with modern portfolio theory, is the certain knowledge of future delivered
variance for the random variable (or, equivalently, all the future probabilities). This is what makes it clash with
practice the rectiﬁcation by the market fattening the tails is a negation of the Black-Scholes thought experiment.

20.2 black-scholes was an argument

277

ability to cut the risks using continuous trading which only works in the very narrowly
special case of thin-tailed distributions. But it is not just these ﬂaws that make it inapplica-
ble: option traders do not buy theories, particularly speculative general equilibrium ones,
which they ﬁnd too risky for them and extremely lacking in standards of reliability. A
normative theory is, simply, not good for decision-making under uncertainty (particularly
if it is in chronic disagreement with empirical evidence). People may take decisions based
on speculative theories, but avoid the fragility of theories in running their risks.

Yet professional traders, including the authors (and, alas, the Swedish Academy of Sci-
ence) have operated under the illusion that it was the Black-Scholes-Merton formula they
actually used we were told so. This myth has been progressively reinforced in the liter-
ature and in business schools, as the original sources have been lost or frowned upon as
anecdotal (Merton [118]).

The

These are

reduction"

Figure 20.1:
typical
"risk
performed
by the Black-Scholes-Merton
the
argument.
variations of a dynamically
hedged portfolio (and a quite
standard one). BSM indeed
"smoothes" out variations but
exposes the operator to massive
tail events reminiscent of such
blowups
as LTCM. Other
option formulas are robust to
the rare event and make no
such claims.

This discussion will present our real-world, ecological understanding of option pricing
and hedging based on what option traders actually do and did for more than a hundred
years.

This is a very general problem. As we said, option traders develop a chain of transmis-
sion of technë, like many professions. But the problem is that the chain is often broken
as universities do not store the acquired skills by operators. Effectively plenty of robust
heuristically derived implementations have been developed over the years, but the eco-
nomics establishment has refused to quote them or acknowledge them. This makes traders
need to relearn matters periodically. Failure of dynamic hedging in 1987, by such ﬁrm as
Leland OBrien Rubinstein, for instance, does not seem to appear in the academic litera-
ture published after the event (Merton, [118], Rubinstein,[142], Ross [141]); to the contrary
dynamic hedging is held to be a standard operation

4

.

There are central elements of the real world that can escape them academic research
without feedback from practice (in a practical and applied ﬁeld) can cause the diversions
we witness between laboratory and ecological frameworks. This explains why some many
ﬁnance academics have had the tendency to make smooth returns, then blow up using
. We started the other way around, ﬁrst by years of option trading doing
their own theories

5

4 For instance how mistakes never resurface into the consciousness, Mark Rubinstein was awarded in 1995 the
Financial Engineer of the Year award by the International Association of Financial Engineers. There was no
mention of portfolio insurance and the failure of dynamic hedging.

5 For a standard reaction to a rare event, see the following: "Wednesday is the type of day people will remember in
quant-land for a very long time," said Mr. Rothman, a University of Chicago Ph.D. who ran a quantitative fund

278

option traders never use the black-scholes-merton formula (cid:3),‡

million of hedges and thousands of option trades. This in combination with investigating
the forgotten and ignored ancient knowledge in option pricing and trading we will explain
some common myths about option pricing and hedging. There are indeed two myths:

(cid:15) That we had to wait for the Black-Scholes-Merton options formula to trade the prod-
uct, price options, and manage option books. In fact the introduction of the Black,
Scholes and Merton argument increased our risks and set us back in risk manage-
ment. More generally, it is a myth that traders rely on theories, even less a general
equilibrium theory, to price options.

(cid:15) That we use the Black-Scholes-Merton options pricing formula. We, simply dont.
In our discussion of these myth we will focus on the bottom-up literature on option
theory that has been hidden in the dark recesses of libraries. And that addresses only
recorded matters not the actual practice of option trading that has been lost.

20.3 myth 1: traders did not "price" options before black-scholes

It is assumed that the Black-Scholes-Merton theory is what made it possible for option
traders to calculate their delta hedge (against the underlying) and to price options. This
argument is highly debatable, both historically and analytically.
Options were actively trading at least already in the 1600 as described by Joseph De
La Vega implying some form of technë, a heuristic method to price them and deal with
their exposure. De La Vega describes option trading in the Netherlands, indicating that
operators had some expertise in option pricing and hedging. He diffusely points to the
put-call parity, and his book was not even meant to teach people about the technicalities
in option trading. Our insistence on the use of Put-Call parity is critical for the following
reason: The Black-Scholes-Mertons claim to fame is removing the necessity of a risk-based
drift from the underlying security to make the trade risk-neutral. But one does not need
dynamic hedging for that: simple put call parity can sufﬁce (Derman and Taleb, 2005), as
we will discuss later. And it is this central removal of the risk-premium that apparently
was behind the decision by the Nobel committee to grant Merton and Scholes the (then
called) Bank of Sweden Prize in Honor of Alfred Nobel: Black, Merton and Scholes made
a vital contribution by showing that it is in fact not necessary to use any risk premium
when valuing an option. This does not mean that the risk premium disappears; instead it
is already included in the stock price. It is for having removed the effect of the drift on
the value of the option, using a thought experiment, that their work was originally cited,
something that was mechanically present by any form of trading and converting using far
simpler techniques.

Options have a much richer history than shown in the conventional literature. Forward
contracts seems to date all the way back to Mesopotamian clay tablets dating all the way
back to 1750 B.C. Gelderblom and Jonker [74] show that Amsterdam grain dealers had
used options and forwards already in 1550.
In the late 1800 and the early 1900 there were active option markets in London and
New York as well as in Paris and several other European exchanges. Markets it seems,
were active and extremely sophisticated option markets in 1870. Kairys and Valerio (1997)

before joining Lehman Brothers. "Events that models only predicted would happen once in 10,000 years happened
every day for three days." One ’Quant’ Sees Shakeout For the Ages – ’10,000 Years’ By Kaja Whitehouse,Wall Street
Journal August 11, 2007; Page B3.

20.3 myth 1: traders did not "price" options before black-scholes

279

discuss the market for equity options in USA in the 1870s, indirectly showing that traders
6
were sophisticated enough to price for tail events

.

There was even active option arbitrage trading taking place between some of these mar-
kets. There is a long list of missing treatises on option trading: we traced at least ten Ger-
7
man treatises on options written between the late 1800s and the hyperinﬂation episode

6 The historical description of the market is informative until Kairys and Valerio [96] try to gauge whether options
in the 1870s were underpriced or overpriced (using Black-Scholes-Merton style methods). There was one tail-
event in this period, the great panic of September 1873. Kairys and Valerio ﬁnd that holding puts was proﬁtable,
but deem that the market panic was just a one-time event : However, the put contracts beneﬁt from the ﬁnancial
panic that hit the market in September, 1873. Viewing this as a one-time event, we repeat the analysis for puts
excluding any unexpired contracts written before the stock market panic. Using references to the economic
literature that also conclude that options in general were overpriced in the 1950s 1960s and 1970s they conclude:
"Our analysis shows that option contracts were generally overpriced and were unattractive for retail investors to
purchase. They add: Empirically we ﬁnd that both put and call options were regularly overpriced relative to a
theoretical valuation model." These results are contradicted by the practitioner Nelson (1904): the majority of the
great option dealers who have found by experience that it is the givers, and not the takers, of option money who
have gained the advantage in the long run.

21 F O U R L E S S O N S F R O M J E F F

H O L M A N ’ S M I S TA K E S I N T H E
D I S C U S S I O N O F A N T I F R A G I L E (cid:3),‡

We discuss Jeff Holman’s comments in Quantitative Finance to illustrate four critical errors
students should learn to avoid: 1) Mistaking tails (4th moment) for volatility (2nd moment),
2) Missing Jensen’s Inequality, 3) Analyzing the hedging wihout the underlying, 4) The
necessity of a numéraire in ﬁnance.
The review of Antifragile by Mr Holman (dec 4, 2013) is replete with factual, logical, and
analytical errors. We will only list here the critical ones, and ones with generality to the risk
management and quantitative ﬁnance communities; these should be taught to students in
quantitative ﬁnance as central mistakes to avoid, so beginner quants and risk managers
can learn from these fallacies.

21.1 conflation of second and fourth moments

It is critical for beginners not to fall for the following elementary mistake. Mr Holman
gets the relation of the VIX (volatility contract) to betting on "tail events" backwards. Let
us restate the notion of "tail events" in the Incerto (that is the four books on uncertainty
of which Antifragile is the last installment): it means a disproportionate role of the tails in
deﬁning the properties of distribution, which, mathematically, means a smaller one for the
1
"body".

Mr Holman seems to get the latter part of the attributes of fattailedness in reverse. It is
an error to mistake the VIX for tail events. The VIX is mostly affected by at-the-money
options which corresponds to the center of the distribution, closer to the second moment
not the fourth (at-the-money options are actually linear in their payoff and correspond to
the conditional ﬁrst moment). As explained about seventeen years ago in Dynamic Hedging
(Taleb, 1997) (see appendix), in the discussion on such tail bets, or "fourth moment bets",
betting on the disproportionate role of tail events of fattailedness is done by selling the
around-the-money-options (the VIX) and purchasing options in the tails, in order to extract
the second moment and achieve neutrality to it (sort of becoming "market neutral"). Such
a neutrality requires some type of "short volatility" in the body because higher kurtosis
means lower action in the center of the distribution.

A more mathematical formulation is in the technical version of the Incerto : fat tails means
"higher peaks" for the distribution as, the fatter the tails, the more markets spend time be-
p
tween m (cid:0)
s where s is the standard deviation and

s and m +

5 (cid:0)

5 (cid:0)

√

17

√

17

p

(

)

(

)

1
2

1
2

1 The point is staring at every user of spreadsheets: kurtosis, or scaled fourth moment, the standard measure of

fattailedness, entails normalizing the fourth moment by the square of the variance.

281

282

four lessons from jeff holman’s mistakes in the discussion of antifragile (cid:3),‡

m the mean of the distribution (we used the Gaussian here as a base for ease of presentation
but the argument applies to all unimodal distributions with "bell-shape" curves, known as
semiconcave). And "higher peaks" means less variations that are not tail events, more quiet
times, not less. For the consequence on option pricing, the reader might be interested in
a quiz I routinely give students after the ﬁrst lecture on derivatives: "What happens to
at-the-money options when one fattens the tails?", the answer being that they should drop
in value.
Effectively, but in a deeper argument, in the QF paper (Taleb and Douady 2013), our mea-
sure of fragility has an opposite sensitivity to events around the center of the distribution,
since, by an argument of survival probability, what is fragile is sensitive to tail shocks and,
critically, should not vary in the body (otherwise it would be broken).

2

21.2 missing jensen’s inequality in analyzing option returns

Here is an error to avoid at all costs in discussions of volatility strategies or, for that matter,
anything in ﬁnance. Mr Holman seems to miss the existence of Jensen’s inequality, which
is the entire point of owning an option, a point that has been belabored in Antifragile. One
manifestation of missing the convexity effect is a critical miscalculation in the way one can
naively assume options respond to the VIX.

"A $1 investment on January 1, 2007 in a strategy of buying and rolling short-term VIX
futures would have peaked at $4.84 on November 20, 2008 -and then subsequently lost
99% of its value over the next four and a half years, ﬁnishing under $0.05 as of May 31,
2013."

3

This mistake in the example given underestimates option returns by up to... several or-
ders of magnitude. Mr Holman analyzes the performance a tail strategy using investments
in ﬁnancial options by using the VIX (or VIX futures) as proxy, which is mathematically
erroneous owing to second- order effects, as the link is tenuous (it would be like evaluating
investments in ski resorts by analyzing temperature futures). Assume a periodic rolling
of an option strategy: an option 5 STD away from the money
gains 16 times in value if
its implied volatility goes up by 4, but only loses its value if volatility goes to 0. For a 10
STD it is 144 times. And, to show the acceleration, assuming these are traded, a 20 STD
5
options by around 210,000 times
. There is a second critical mistake in the discussion: Mr
Holman’s calculations here exclude the payoff from actual in-the-moneyness.

4

One should remember that the VIX is not a price, but an inverse function, an index de-
rived from a price: one does not buy "volatility" like one can buy a tomato; operators buy
options correponding to such inverse function and there are severe, very severe nonlinear-
ities in the effect. Although more linear than tail options, the VIX is still convex to actual
market volatility, somewhere between variance and standard deviation, since a strip of

p

√

2 Technical Point: Where Does the Tail Start? As we saw in 3.3, for a general class of symmetric distributions
s

with power laws, the tail starts at: (cid:6)
, with a inﬁnite in the stochastic volatility Gaussian case
and s the standard deviation. The "tail" is located between around 2 and 3 standard deviations. This ﬂows from
the heuristic deﬁnition of fragility as second order effect: the part of the distribution is convex to errors in the
estimation of the scale. But in practice, because historical measurements of STD will be biased lower because of
small sample effects (as we repeat fat tails accentuate small sample effects), the deviations will be > 2-3 STDs.
3 In the above discussion Mr Holman also shows evidence of dismal returns on index puts which, as we said

(a+1)(17a+1)+1

a(cid:0)1
p
2

5a+

before, respond to volatility not tail events. These are called, in the lingo, "sucker puts".

4 We are using implied volatility as a benchmark for its STD.
5 An event this author witnessed, in the liquidation of Victor Niederhoffer, options sold for $.05 were purchased
back at up to $38, which bankrupted Refco, and, which is remarkable, without the options getting close to the
money: it was just a panic rise in implied volatility.

21.3 the inseparability of insurance and insured

283

options spanning all strikes should deliver the variance (Gatheral,2006). The reader can
go through a simple exercise. Let’s say that the VIX is "bought" at 10% -that is, the com-
ponent options are purchased at a combination of volatilities that corresponds to a VIX
at that level. Assume returns are in squares. Because of nonlinearity, the package could
beneﬁt from an episode of 4% volatility followed by an episode of 15%, for an average
of 9.5%; Mr Holman believes or wants the reader to believe that this 0.5 percentage point
should be treated as a loss when in fact second order un-evenness in volatility changes are
more relevant than the ﬁrst order effect.

21.3 the inseparability of insurance and insured

One should never calculate the cost of insurance without offsetting it with returns gener-
ated from packages than one would not have purchased otherwise.

Even had he gotten the sign right on the volatility, Mr Holman in the example above an-
alyzes the performance of a strategy buying options to protect a tail event without adding
the performance of the portfolio itself, like counting the cost side of the insurance without
the performance of what one is insuring that would not have been bought otherwise. Over
the same period he discusses the market rose more than 100%: a healthy approach would
be to compare dollar-for-dollar what an investor would have done (and, of course, getting
rid of this "VIX" business and focusing on very small dollars invested in tail options that
would allow such an aggressive stance). Many investors (such as this author) would have
stayed out of the market, or would not have added funds to the market, without such an
insurance.

21.4 the necessity of a numéraire in finance

There is a deeper analytical error.

A barbell is deﬁned as a bimodal investment strategy, presented as investing a portion of
your portfolio in what is explicitly deﬁned as a "numéraire repository of value" (Antifragile),
and the rest in risky securities (Antifragile indicates that such numéraire would be, among
other things, inﬂation protected). Mr Holman goes on and on in a nihilistic discourse on
the absence of such riskless numéraire (of the sort that can lead to such sophistry as "he is
saying one is safer on terra ﬁrma than at sea, but what if there is an earthquake?").

The familiar Black and Scholes derivation uses a riskless asset as a baseline; but the
literature since around 1977 has substituted the notion of "cash" with that of a numéraire
, along with the notion that one can have different currencies, which technically allows
for changes of probability measure. A numéraire is deﬁned as the unit to which all other
units relate. ( Practically, the numéraire is a basket the variations of which do not affect the
welfare of the investor.) Alas, without numéraire, there is no probability measure, and no
quantitative in quantitative ﬁnance, as one needs a unit to which everything else is brought
back to. In this (emotional) discourse, Mr Holton is not just rejecting the barbell per se,
but any use of the expectation operator with any economic variable, meaning he should
go attack the tens of thousand research papers and the existence of the journal Quantitative
Finance itself.

Clearly, there is a high density of other mistakes or incoherent statements in the outpour
of rage in Mr Holman’s review; but I have no doubt these have been detected by the

284

four lessons from jeff holman’s mistakes in the discussion of antifragile (cid:3),‡

Quantitative Finance reader and, as we said, the object of this discussion is the prevention
of analytical mistakes in quantitative ﬁnance.

To conclude, this author welcomes criticism from the ﬁnance community that are not
straw man arguments, or, as in the case of Mr Holmam, violate the foundations of the ﬁeld
itself.

Figure 21.1: First Method to
Extract the Fourth Moment,
from Dynamic Hedging, 1997.

Figure 21.2: Second Method
to Extract the Fourth Moment ,
from Dynamic Hedging, 1997.

21.5 appendix (betting on tails of distribution)

285

21.5 appendix (betting on tails of distribution)

From Dynamic Hedging, pages 264-265:

A fourth moment bet is long or short the volatility of volatility. It could be achieved either with
out-of-the-money options or with calendars. Example: A ratio "backspread" or reverse spread is
a method that includes the buying of out-of-the-money options in large amounts and the selling
of smaller amounts of at-the-money but making sure the trade satisﬁes the "credit" rule (i.e., the
trade initially generates a positive cash ﬂow). The credit rule is more difﬁcult to interpret when
one uses in-the-money options. In that case, one should deduct the present value of the intrinsic
part of every option using the put-call parity rule to equate them with out-of-the-money.

The trade shown in Figure 1 was accomplished with the purchase of both out-of-the-money puts
and out-of-the-money calls and the selling of smaller amounts of at-the-money straddles of the
same maturity.

Figure 2 shows the second method, which entails the buying of 60- day options in some amount
and selling 20-day options on 80% of the amount. Both trades show the position beneﬁting from
the fat tails and the high peaks. Both trades, however, will have different vega sensitivities, but
close to ﬂat modiﬁed vega.

See The Body, The Shoulders, and The Tails from section 3.3 where we assume tails start
at the level of convexity of the segment of the probability distribution to the scale of the
distribution.

22 TA I L R I S K C O N S T R A I N T S A N D

M A X I M U M E N T R O P Y ( W I T H D .
G E M A N A N D H . G E M A N ) ‡

Portfolio selection in the ﬁnancial literature has essentially been ana-

lyzed under two central assumptions: full knowledge of the joint prob-
ability distribution of the returns of the securities that will comprise
the target portfolio; and investors’ preferences are expressed through a
utility function. In the real world, operators build portfolios under risk
constraints which are expressed both by their clients and regulators and which bear
on the maximal loss that may be generated over a given time period at a given conﬁ-
dence level (the so-called Value at Risk of the position). Interestingly, in the ﬁnance
literature, a serious discussion of how much or little is known from a probabilistic
standpoint about the multi-dimensional density of the assets’ returns seems to be of
limited relevance.

Our approach in contrast is to highlight these issues and then adopt throughout
a framework of entropy maximization to represent the real world ignorance of the
“true” probability distributions, both univariate and multivariate, of traded securities’
returns. In this setting, we identify the optimal portfolio under a number of downside
risk constraints. Two interesting results are exhibited: (i) the left- tail constraints are
sufﬁciently powerful to override all other considerations in the conventional theory;
(ii) the “barbell portfolio” (maximal certainty/ low risk in one set of holdings, maxi-
mal uncertainty in another), which is quite familiar to traders, naturally emerges in
our construction.

22.1 left tail risk as the central portfolio constraint

Customarily, when working in an institutional framework, operators and risk takers prin-
cipally use regulatorily mandated tail-loss limits to set risk levels in their portfolios (obli-
gatorily for banks since Basel II). They rely on stress tests, stop-losses, value at risk (VaR),
expected shortfall (–i.e., the expected loss conditional on the loss exceeding VaR, also
known as CVaR), and similar loss curtailment methods, rather than utility. In particular,
the margining of ﬁnancial transactions is calibrated by clearing ﬁrms and exchanges on
tail losses, seen both probabilistically and through stress testing. (In the risk-taking termi-
nology, a stop loss is a mandatory order that attempts terminates all or a portion of the
exposure upon a trigger, a certain pre-deﬁned nominal loss. Basel II is a generally used
name for recommendations on banking laws and regulations issued by the Basel Commit-
tee on Banking Supervision. The Value-at-risk, VaR, is deﬁned as a threshold loss value K
such that the probability that the loss on the portfolio over the given time horizon exceeds

287

288

tail risk constraints and maximum entropy (with d. geman and h. geman) ‡

this value is ϵ. A stress test is an examination of the performance upon an arbitrarily set
deviation in the underlying variables.) The information embedded in the choice of the
constraint is, to say the least, a meaningful statistic about the appetite for risk and the
shape of the desired distribution.

Operators are less concerned with portfolio variations than with the drawdown they may
face over a time window. Further, they are in ignorance of the joint probability distribu-
tion of the components in their portfolio (except for a vague notion of association and
hedges), but can control losses organically with allocation methods based on maximum
risk. (The idea of substituting variance for risk can appear very strange to practitioners of
risk-taking. The aim by Modern Portfolio Theory at lowering variance is inconsistent with
the preferences of a rational investor, regardless of his risk aversion, since it also minimizes
the variability in the proﬁt domain –except in the very narrow situation of certainty about
the future mean return, and in the far-fetched case where the investor can only invest in
variables having a symmetric probability distribution, and/or only have a symmetric pay-
off. Stop losses and tail risk controls violate such symmetry.) The conventional notions of
utility and variance may be used, but not directly as information about them is embedded
in the tail loss constaint.

Since the stop loss, the VaR (and expected shortfall) approaches and other risk-control
methods concern only one segment of the distribution, the negative side of the loss domain,
we can get a dual approach akin to a portfolio separation, or “barbell-style” construction,
as the investor can have opposite stances on different parts of the return distribution.
Our deﬁnition of barbell here is the mixing of two extreme properties in a portfolio such
as a linear combination of maximal conservatism for a fraction w of the portfolio, with
w 2 (0, 1), on one hand and maximal (or high) risk on the (1 (cid:0) w) remaining fraction.
Historically, ﬁnance theory has had a preference for parametric, less robust, methods.
The idea that a decision-maker has clear and error-free knowledge about the distribution
of future payoffs has survived in spite of its lack of practical and theoretical validity –for
instance, correlations are too unstable to yield precise measurements. It is an approach
that is based on distributional and parametric certainties, one that may be useful for re-
search but does not accommodate responsible risk taking. (Correlations are unstable in an
unstable way, as joint returns for assets are not elliptical, see Bouchaud and Chicheportiche
(2012) [30].)
There are roughly two traditions: one based on highly parametric decision-making by
the economics establishment (largely represented by Markowitz [112]) and the other based
on somewhat sparse assumptions and known as the Kelly criterion (Kelly, 1956 [97], see
Bell and Cover, 1980 [11].) (In contrast to the minimum-variance approach, Kelly’s method,
developed around the same period as Markowitz, requires no joint distribution or utility
function. In practice one needs the ratio of expected proﬁt to worst-case return dynamically
adjusted to avoid ruin. Obviously, model error is of smaller consequence under the Kelly
criterion: Thorp (1969) [176], Haigh (2000) [86], Mac Lean, Ziemba and Blazenko [107].
For a discussion of the differences between the two approaches, see Samuelson’s objection
to the Kelly criterion and logarithmic sizing in Thorp 2010 [177].) Kelly’s method is also
related to left- tail control due to proportional investment, which automatically reduces
the portfolio in the event of losses; but the original method requires a hard, nonparametric
worst-case scenario, that is, securities that have a lower bound in their variations, akin to a
gamble in a casino, which is something that, in ﬁnance, can only be accomplished through
binary options. The Kelly criterion, in addition, requires some precise knowledge of future
returns such as the mean. Our approach goes beyond the latter method in accommodating
more uncertainty about the returns, whereby an operator can only control his left-tail via
derivatives and other forms of insurance or dynamic portfolio construction based on stop-

22.1 left tail risk as the central portfolio constraint

289

losses. (Xu, Wu, Jiang, and Song (2014) [189] contrast mean variance to maximum entropy
and uses entropy to construct robust portfolios.)

In a nutshell, we hardwire the curtailments on loss but otherwise assume maximal un-
certainty about the returns. More precisely, we equate the return distribution with the
maximum entropy extension of constraints expressed as statistical expectations on the left-
tail behavior as well as on the expectation of the return or log-return in the non-danger
zone. (Note that we use Shannon entropy throughout. There are other information mea-
sures, such as Tsallis entropy [181] , a generalization of Shannon entropy, and Renyi en-
tropy, [94] , some of which may be more convenient computationally in special cases.
However, Shannon entropy is the best known and has a well-developed maximization
framework. )

Here, the “left-tail behavior” refers to the hard, explicit, institutional constraints dis-
cussed above. We describe the shape and investigate other properties of the resulting
so-called maxent distribution. In addition to a mathematical result revealing the link be-
tween acceptable tail loss (VaR) and the expected return in the Gaussian mean-variance
framework, our contribution is then twofold: 1) an investigation of the shape of the dis-
tribution of returns from portfolio construction under more natural constraints than those
imposed in the mean-variance method, and 2) the use of stochastic entropy to represent
residual uncertainty.

VaR and CVaR methods are not error free –parametric VaR is known to be ineffective
as a risk control method on its own. However, these methods can be made robust using
constructions that, upon paying an insurance price, no longer depend on parametric as-
sumptions. This can be done using derivative contracts or by organic construction (clearly
if someone has 80% of his portfolio in numéraire securities, the risk of losing more than
20% is zero independent from all possible models of returns, as the ﬂuctuations in the
numéraire are not considered risky). We use “pure robustness” or both VaR and zero
shortfall via the “hard stop” or insurance, which is the special case in our paper of what
we called earlier a “barbell” construction.

It is worth mentioning that it is an old idea in economics that an investor can build a port-
folio based on two distinct risk categories, see Hicks (1939) [? ]. Modern Portfolio Theory
proposes the mutual fund theorem or “separation” theorem, namely that all investors can
obtain their desired portfolio by mixing two mutual funds, one being the riskfree asset and
one representing the optimal mean-variance portfolio that is tangent to their constraints;
see Tobin (1958) [179], Markowitz (1959) [113], and the variations in Merton (1972) [115],
Ross (1978) [140].
In our case a riskless asset is the part of the tail where risk is set to
exactly zero. Note that the risky part of the portfolio needs to be minimum variance in
traditional ﬁnancial economics; for our method the exact opposite representation is taken
for the risky one.

22.1.1 The Barbell as seen by E.T. Jaynes

Our approach to constrain only what can be constrained (in a robust manner) and to
maximize entropy elsewhere echoes a remarkable insight by E.T. Jaynes in “How should
we use entropy in economics?” [92]:

“It may be that a macroeconomic system does not move in response to (or
at least not solely in response to) the forces that are supposed to exist in cur-
rent theories; it may simply move in the direction of increasing entropy as
constrained by the conservation laws imposed by Nature and Government.”

290

tail risk constraints and maximum entropy (with d. geman and h. geman) ‡

22.2 revisiting the mean variance setting

Let ⃗X = (X1, ..., Xm) denote m asset returns over a given single period with joint density
j, 1 (cid:20)
1, ..., mm) and m (cid:2) m covariance matrix S: S
g(⃗x), mean returns ⃗m = (m
i, j (cid:20) m. Assume that ⃗m and S can be reliably estimated from data.
The return on the portolio with weights ⃗w = (w1, ..., wm) is then

ij = E(XiXj) (cid:0) m

m

i

X =

m
(cid:229)
i=1

wiXi,

which has mean and variance

E(X) = ⃗w⃗mT, V(X) = ⃗wS⃗wT.

In standard portfolio theory one minimizes V(X) over all ⃗w subject to E(X) = m for a ﬁxed
desired average return m. Equivalently, one maximizes the expected return E(X) subject to
a ﬁxed variance V(X). In this framework variance is taken as a substitute for risk.

To draw connections with our entropy-centered approach, we consider two standard

cases:
(1) Normal World: The joint distribution g(⃗x) of asset returns is multivariate Gaussian
N(⃗m, S). Assuming normality is equivalent to assuming g(⃗x) has maximum (Shannon)
entropy among all multivariate distributions with the given ﬁrst- and second-order
statistics ⃗m and S. Moreover, for a ﬁxed mean E(X), minimizing the variance V(X)
is equivalent to minimizing the entropy (uncertainty) of X.
(This is true since joint
normality implies that X is univariate normal for any choice of weights and the entropy
of a N (m, s2) variable is H = 1
2 (1 + log(2ps2)).) This is natural in a world with complete
information. ( The idea of entropy as mean uncertainty is in Philippatos and Wilson
(1972) [128]; see Zhou –et al. (2013) [192] for a review of entropy in ﬁnancial economics
and Georgescu-Roegen (1971) [77] for economics in general.)

(2) Unknown Multivariate Distribution: Since we assume we can estimate the second-
order structure, we can still carry out the Markowitz program, –i.e., choose the portfo-
lio weights to ﬁnd an optimal mean-variance performance, which determines E(X) = m
and V(X) = s2. However, we do not know the distribution of the return X. Observe
that assuming X is normally distributed N (m, s2) is equivalent to assuming the entropy
of X is maximized since, again, the normal maximizes entropy at a given mean and
variance, see [128].

Our strategy is to generalize the second scenario by replacing the variance s2 by two left-
tail value-at-risk constraints and to model the portfolio return as the maximum entropy
extension of these constraints together with a constraint on the overall performance or on
the growth of the portfolio in the non-danger zone.

22.2.1 Analyzing the Constraints

Let X have probability density f (x). In everything that follows, let K < 0 be a normalizing
constant chosen to be consistent with the risk-taker’s wealth. For any ϵ > 0 and n(cid:0) < K,
the value-at-risk constraints are:
(1) Tail probability:

P(X (cid:20) K) =

∫

K

(cid:0)¥

f (x) dx = ϵ.

22.3 revisiting the gaussian case

291

(2) Expected shortfall (CVaR):

E(XjX (cid:20) K) = n(cid:0).

Assuming (1) holds, constraint (2) is equivalent to

E(XI(X(cid:20)K)) =

∫

K

(cid:0)¥

x f (x) dx = ϵn(cid:0).

Given the value-at-risk parameters q = (K, ϵ, n(cid:0)), let Ωvar(q) denote the set of probability
2 Ωvar(q)
densities f satisfying the two constraints. Notice that Ωvar(q) is convex:
2 Ωvar(q). Later we will add another constraint involving the overall
implies a f1 + (1 (cid:0) a) f2
mean.

f1, f2

22.3 revisiting the gaussian case

Suppose we assume X is Gaussian with mean m and variance s2. In principle it should
be possible to satisfy the VaR constraints since we have two free parameters. Indeed, as
shown below, the left-tail constraints determine the mean and variance; see Figure 22.1.
However, satisfying the VaR constraints imposes interesting restrictions on m and s and
leads to a natural inequality of a “no free lunch” style.

Figure 22.1: By setting K (the value at risk), the probability ϵ of exceeding it, and the shortfall when doing so,
there is no wiggle room left under a Gaussian distribution: s and m are determined, which makes construction
according to portfolio theory less relevant.

Let h(ϵ) be the ϵ-quantile of the standard normal distribution, –i.e., h(ϵ) = F(cid:0)1(ϵ), where
F is the c.d.f. of the standard normal density ϕ(x). In addition, set

B(ϵ) =

1
ϵh(ϵ)

ϕ(h(ϵ)) =

1p

2pϵh(ϵ)

expf(cid:0)

h(ϵ)2
2

g.

Proposition 22.1
If X (cid:24) N(m, s2) and satisﬁes the two VaR constraints, then the mean and variance are given by:

m =

n(cid:0) + KB(ϵ)
1 + B(ϵ)

, s =

K (cid:0) n(cid:0)
h(ϵ)(1 + B(ϵ))

.

Moreover, B(ϵ) < (cid:0)1 and limϵ#0 B(ϵ) = (cid:0)1.

Area ϵν_K-4-224Returns0.10.20.30.4292

tail risk constraints and maximum entropy (with d. geman and h. geman) ‡

The proof is in the Appendix. The VaR constraints lead directly to two linear equations
in m and s:

m + h(ϵ)s = K, m (cid:0) h(ϵ)B(ϵ)s = n(cid:0).

Consider the conditions under which the VaR constraints allow a positive mean return
m = E(X) > 0. First, from the above linear equation in m and s in terms of h(ϵ) and K,
we see that s increases as ϵ increases for any ﬁxed mean m, and that m > 0 if and only
if s > K
h(ϵ) , –i.e., we must accept a lower bound on the variance which increases with ϵ,
which is a reasonable property. Second, from the expression for m in Proposition 1, we
have

m > 0 () jn(cid:0)j> KB(ϵ).

Consequently, the only way to have a positive expected return is to accommodate a sufﬁ-
ciently large risk expressed by the various tradeoffs among the risk parameters q satisfying
the inequality above. (This type of restriction also applies more generally to symmetric
distributions since the left tail constraints impose a structure on the location and scale.
For instance, in the case of a Student T distribution with scale s, location m, and tail
exponent a, the same linear relation between s and m applies: s = (K (cid:0) m)k(a), where
k(a) = (cid:0) i
(cid:0)1 is the inverse of the regularized incomplete beta func-

√

p

√

(cid:0)1
2ϵ ( a
I
(cid:0)1
2ϵ ( a
I
tion I, and s the solution of ϵ = 1
2 I

2 )
2 , 1
2 )(cid:0)1
2 , 1

, where I

a

(

)

. )

a
2 , 1
2

as2
(k(cid:0)m)2+as2

22.3.1 A Mixture of Two Normals

In many applied sciences, a mixture of two normals provides a useful and natural exten-
sion of the Gaussian itself; in ﬁnance, the Mixture Distribution Hypothesis (denoted as
MDH in the literature) refers to a mixture of two normals and has been very widely inves-
tigated (see for instance Richardson and Smith (1995) [138]). H. Geman and T. Ané (1996)
[2] exhibit how an inﬁnite mixture of normal distributions for stock returns arises from
the introduction of a "stochastic clock" accounting for the uneven arrival rate of informa-
tion ﬂow in the ﬁnancial markets. In addition, option traders have long used mixtures to
account for fat tails, and to examine the sensitivity of a portfolio to an increase in kurtosis
("DvegaDvol"); see Taleb (1997) [156]. Finally, Brigo and Mercurio (2002) [23] use a mixture
of two normals to calibrate the skew in equity options.

Consider the mixture

f (x) = lN(m

1, s2

1 ) + (1 (cid:0) l)N(m

2, s2
2 ).

1 = n(cid:0), in which case m

An intuitively simple and appealing case is to ﬁx the overall mean m, and take l = ϵ
m(cid:0)ϵn(cid:0)
and m
1(cid:0)ϵ . It then follows that the left-tail
(cid:25)
constraints are approximately satisﬁed for s
1, s
0, the density is effectively composed of two spikes (small variance normals) with the left
m(cid:0)ϵn(cid:0)
one centered at n(cid:0) and the right one centered at at
1(cid:0)ϵ . The extreme case is a Dirac
function on the left, as we see next.

2 sufﬁciently small. Indeed, when s

2 is constrained to be

1 = s
2

Dynamic Stop Loss, A Brief Comment One can set a level K below which there is no
mass, with results that depend on accuracy of the execution of such a stop. The distribution
to the right of the stop-loss no longer looks like the standard Gaussian, as it builds positive

22.4 maximum entropy

293

skewness in accordance to the distance of the stop from the mean. We limit any further
discussion to the illustrations in Figure ??.

22.4 maximum entropy

From the comments and analysis above, it is clear that, in practice, the density f of the
return X is unknown; in particular, no theory provides it. Assume we can adjust the
portfolio parameters to satisfy the VaR constraints, and perhaps another constraint on the
expected value of some function of X (e.g., the overall mean). We then wish to compute
probabilities and expectations of interest, for example P(X > 0) or the probability of
losing more than 2K, or the expected return given X > 0. One strategy is to make such
estimates and predictions under the most unpredictable circumstances consistent with the
constraints. That is, use the maximum entropy extension (MEE) of the constraints as a
model for f (x).
The “differential entropy” of f is h( f ) = (cid:0)
f (x) ln f (x) dx. (In general, the integral may
not exist.) Entropy is concave on the space of densities for which it is deﬁned. In general,
the MEE is deﬁned as

∫

f MEE = arg max
f 2Ω

h( f )

where Ω is the space of densities which satisfy a set of constraints of the form Eϕ
j(X) =
cj, j = 1, ..., J. Assuming Ω is non-empty, it is well-known that f MEE is unique and (away
from the boundary of feasibility) is an exponential distribution in the constraint functions,
–i.e., is of the form

(

)

f MEE(x) = C

(cid:0)1 exp

ϕ

l

j

j(x)

(cid:229)
j

1, ..., l

where C = C(l
M) is the normalizing constant. (This form comes from differentiating
an appropriate functional J( f ) based on entropy, and forcing the integral to be unity and
imposing the constraints with Lagrange mult1ipliers.) In the special cases below we use
this characterization to ﬁnd the MEE for our constraints.

In our case we want to maximize entropy subject to the VaR constraints together with any
others we might impose. Indeed, the VaR constraints alone do not admit an MEE since
they do not restrict the density f (x) for x > K. The entropy can be made arbitrarily large
by allowing f to be identically C = 1(cid:0)ϵ
N(cid:0)K over K < x < N and letting N ! ¥. Suppose,
however, that we have adjoined one or more constraints on the behavior of f which are
compatible with the VaR constraints in the sense that the set of densities Ω satisfying all
the constraints is non-empty. Here Ω would depend on the VaR parameters q = (K, ϵ, n(cid:0))
together with those parameters associated with the additional constraints.

22.4.1 Case A: Constraining the Global Mean

The simplest case is to add a constraint on the mean return, –i.e., ﬁx E(X) = m. Since
E(X) = P(X (cid:20) K)E(XjX (cid:20) K) + P(X > K)E(XjX > K), adding the mean constraint is
equivalent to adding the constraint

where n+ satisﬁes ϵn(cid:0) + (1 (cid:0) ϵ)n+ = m.

E(XjX > K) = n+

294

tail risk constraints and maximum entropy (with d. geman and h. geman) ‡

Deﬁne

and

{

1

(K(cid:0)n(cid:0)) exp
0

]

[

(cid:0) K(cid:0)x
K(cid:0)n(cid:0)

if x < K,
if x (cid:21) K.

f(cid:0)(x) =

{

1

(n+(cid:0)K) exp
0

]

[

(cid:0) x(cid:0)K
n+(cid:0)K

if x > K,
if x (cid:20) K.

f+(x) =

It is easy to check that both f(cid:0) and f+ integrate to one. Then

f MEE(x) = ϵ f(cid:0)(x) + (1 (cid:0) ϵ) f+(x)

is the MEE of the three constraints. First, evidently

∫

1.

2.
3.

∫

K
(cid:0)¥ f MEE(x) dx = ϵ;
K
(cid:0)¥ x f MEE(x) dx = ϵn(cid:0);
∫ ¥
K x f MEE(x) dx = (1 (cid:0) ϵ)n+.

Hence the constraints are satisﬁed. Second, f MEE has an exponential form in our constraint
functions:

]

f MEE(x) = C

[

(cid:0)1 exp

(cid:0)(l

1x + l

2 I(x(cid:20)K) + l

3xI(x(cid:20)K))

.

The shape of f(cid:0) depends on the relationship between K and the expected shortfall n(cid:0).
The closer n(cid:0) is to K, the more rapidly the tail falls off. As n(cid:0) ! K, f(cid:0) converges to a unit
spike at x = K (Figures 22.3 and 22.4).

Figure 22.3: Case A: Effect of different values of ϵ on the shape of the distribution.

0.0.10.250.5-20-1010200.10.20.30.4Perturbatingϵ22.4 maximum entropy

295

Figure 22.4: Case A: Effect of different values of n(cid:0) on the shape of the distribution.

22.4.2 Case B: Constraining the Absolute Mean

If instead we constrain the absolute mean, namely

∫

EjXj=

jxj f (x) dx = m,

then the MEE is somewhat less apparent but can still be found. Deﬁne f(cid:0)(x) as above, and
let

{

1

l
2(cid:0)exp(l
0

f+(x) =

1K) exp((cid:0)l

1

jxj)

if x (cid:21) K,
if x < K.

Then l

1 can be chosen such that

ϵn(cid:0) + (1 (cid:0) ϵ)

∫ ¥

K

jxj f+(x) dx = m.

22.4.3 Case C: Power Laws for the Right Tail

If we believe that actual returns have “fat tails,” in particular that the right tail decays as
a Power Law rather than exponentially (as with a normal or exponential density), than
we can add this constraint to the VaR constraints instead of working with the mean or
absolute mean. In view of the exponential form of the MEE, the density f+(x) will have a
power law, namely

1
C(a)
for a > 0 if the constraint is of the form

f+(x) =

(1 + jxj)

(cid:0)(1+a), x (cid:21) K,

E (log(1 + jXj)jX > K) = A.

-10-55100.10.20.30.40.5Perturbatingν-296

tail risk constraints and maximum entropy (with d. geman and h. geman) ‡

Moreover, again from the MEE theory, we know that the parameter is obtained by mini-
mizing the logarithm of the normalizing function. In this case, it is easy to show that

C(a) =

∫ ¥

K

(1 + jxj)

(cid:0)(1+a) dx =

1
a (2 (cid:0) (1 (cid:0) K)

(cid:0)a

).

It follows that A and a satisfy the equation

A =

1
a

(cid:0) log(1 (cid:0) K)
2(1 (cid:0) K)a (cid:0) 1

.

We can think of this equation as determining the decay rate a for a given A or, alternatively,
as determining the constraint value A necessary to obtain a particular Power Law a.
The ﬁnal MEE extension of the VaR constraints together with the constraint on the log of

the return is then:

f MEE(x)

=

ϵI(x(cid:20)K)

1
(K (cid:0) n(cid:0))

exp

]

[

(cid:0) K (cid:0) x
K (cid:0) n(cid:0)

+ (1 (cid:0) ϵ)I(x>K)

(cid:0)(1+a)

(1 + jxj)
C(a)

,

(see Figures 22.5 and 22.6).

Figure 22.5: Case C: Effect of different values of on the shape of the fat-tailed maximum entropy distribution.

1322523-2-11230.51.01.5Perturbatingα22.4 maximum entropy

297

Figure 22.6: Case C: Effect of different values of on the shape of the fat-tailed maximum entropy distribution
(closer K).

22.4.4 Extension to a Multi-Period Setting: A Comment

Consider the behavior in multi-periods. Using a naive approach, we sum up the perfor-
mance as if there was no response to previous returns. We can see how Case A approaches
the regular Gaussian, but not Case C (Figure 22.7).
For case A the characteristic functioncan be written:

YA(t) =

eiKt(t(K (cid:0) n(cid:0)ϵ + n+(ϵ (cid:0) 1)) (cid:0) i)
(Kt (cid:0) n(cid:0)t (cid:0) i)((cid:0)1 (cid:0) it(K (cid:0) n+))

So we can derive from convolutions that the function YA(t)n converges to that of an
n-summed Gaussian. Further, the characteristic function of the limit of the average of
strategies, namely

YA(t=n)n = eit(n++ϵ(n(cid:0)(cid:0)n+)),

lim
n!¥

(22.1)

is the characteristic function of the Dirac delta, visibly the effect of the law of large numbers
delivering the same result as the Gaussian with mean n+ + ϵ(n(cid:0) (cid:0) n+) .
As to the Power Law in Case C, convergence to Gaussian only takes place for a (cid:21) 2, and
rather slowly.

1322523-2-11230.51.01.5Perturbatingα298

tail risk constraints and maximum entropy (with d. geman and h. geman) ‡

Figure 22.7: Average return for multiperiod naive strategy for Case A, that is, assuming independence of
“sizing”, as position size does not depend on past performance. They aggregate nicely to a standard Gaussian,
and (as shown in Equation (22.1)), shrink to a Dirac at the mean value.

22.5 comments and conclusion

We note that the stop loss plays a larger role in determining the stochastic properties than
the portfolio composition. Simply, the stop is not triggered by individual components,
but by variations in the total portfolio. This frees the analysis from focusing on individual
portfolio components when the tail –via derivatives or organic construction– is all we know
and can control.

To conclude, most papers dealing with entropy in the mathematical ﬁnance literature
have used minimization of entropy as an optimization criterion. For instance, Fritelli (2000)
[71] exhibits the unicity of a "minimal entropy martingale measure" under some conditions
and shows that minimization of entropy is equivalent to maximizing the expected exponen-
tial utility of terminal wealth. We have, instead, and outside any utility criterion, proposed
entropy maximization as the recognition of the uncertainty of asset distributions. Under
VaR and Expected Shortfall constraints, we obtain in full generality a "barbell portfolio"
as the optimal solution, extending to a very general setting the approach of the two-fund
separation theorem.
Appendix A
Proof of Proposition 1: Since X (cid:24) N(m, s2), the tail probability constraint is

ϵ = P(X < K) = P(Z <

K (cid:0) m

s ) = F(

K (cid:0) m

s ).

By deﬁnition, F(h(ϵ)) = ϵ. Hence,

K = m + h(ϵ)s

(22.2)

-4-22468100.10.20.30.40.5For the shortfall constraint,

E(X; X < k) =

∫

K

(cid:0)¥

22.5 comments and conclusion

299

exp (cid:0) (x (cid:0) m)2
2s2

dx

xp

2ps
∫

= mϵ + s

= mϵ (cid:0)

(K(cid:0)m)=s)

xϕ(x) dx
exp (cid:0) (K (cid:0) m)2

2s2

(cid:0)¥
s
p
2p

Since, E(X; X < K) = ϵn(cid:0), and from the deﬁnition of B(ϵ), we obtain

n(cid:0) = m (cid:0) h(ϵ)B(ϵ)s

(22.3)

Solving (22.2) and (22.3) for m and s2 gives the expressions in Proposition 1.
Finally, by symmetry to the “upper tail inequality” of the standard normal, we have, for
x < 0, F(x) (cid:20) ϕ(x)
(cid:0)x . Choosing x = h(ϵ) = F(cid:0)1(ϵ) yields ϵ = P(X < h(ϵ)) (cid:20) (cid:0)ϵB(ϵ) or
1 + B(ϵ) (cid:20) 0. Since the upper tail inequality is asymptotically exact as x ! ¥ we have
B(0) = (cid:0)1, which concludes the proof.

Part IX

B I B L I O G R A P H Y A N D I N D E X

B I B L I O G R A P H Y

[1] Inmaculada B Aban, Mark M Meerschaert, and Anna K Panorska. Parameter estima-
tion for the truncated pareto distribution. Journal of the American Statistical Association,
101(473):270–277, 2006.

[2] Thierry Ané and Hélyette Geman. Order ﬂow, transaction clock, and normality of

asset returns. The Journal of Finance, 55(5):2259–2284, 2000.

[3] Marco Avellaneda, Craig Friedman, Richard Holmes, and Dominick Samperi. Cal-
ibrating volatility surfaces via relative-entropy minimization. Applied Mathematical
Finance, 4(1):37–64, 1997.

[4] Louis Bachelier. Théorie de la spéculation. Gauthier-Villars, 1900.

[5] Kevin P Balanda and HL MacGillivray. Kurtosis: a critical review. The American

Statistician, 42(2):111–119, 1988.

[6] August A Balkema and Laurens De Haan. Residual life time at great age. The Annals

of probability, pages 792–804, 1974.

[7] August A Balkema and Laurens De Haan. Limit distributions for order statistics. i.

Theory of Probability & Its Applications, 23(1):77–92, 1978.

[8] August A Balkema and Laurens de Haan. Limit distributions for order statistics. ii.

Theory of Probability & Its Applications, 23(2):341–358, 1979.

[9] Shaul K Bar-Lev, Idit Lavi, and Benjamin Reiser. Bayesian inference for the power
law process. Annals of the Institute of Statistical Mathematics, 44(4):623–639, 1992.

[10] Norman C Beaulieu, Adnan A Abu-Dayya, and Peter J McLane. Estimating the
distribution of a sum of independent lognormal random variables. Communications,
IEEE Transactions on, 43(12):2869, 1995.

[11] Robert M Bell and Thomas M Cover. Competitive optimality of logarithmic invest-

ment. Mathematics of Operations Research, 5(2):161–166, 1980.

[12] Shlomo Benartzi and Richard Thaler. Heuristics and biases in retirement savings

behavior. Journal of Economic perspectives, 21(3):81–104, 2007.

[13] Shlomo Benartzi and Richard H Thaler. Naive diversiﬁcation strategies in deﬁned

contribution saving plans. American economic review, 91(1):79–98, 2001.

[14] Nicholas H Bingham, Charles M Goldie, and Jef L Teugels. Regular variation, vol-

ume 27. Cambridge university press, 1989.

303

304

Bibliography

[15] Giulio Biroli, J-P Bouchaud, and Marc Potters. On the top eigenvalue of heavy-tailed

random matrices. EPL (Europhysics Letters), 78(1):10001, 2007.

[16] Fischer Black and Myron Scholes. The pricing of options and corporate liabilities.

81:637–654, May–June 1973.

[17] Fischer Black and Myron Scholes. The pricing of options and corporate liabilities.

The journal of political economy, pages 637–654, 1973.

[18] A.J. Boness. Elements of a theory of stock-option value. 72:163–175, 1964.

[19] Jean-Philippe Bouchaud, Marc Mézard, Marc Potters, et al. Statistical properties of
stock order books: empirical results and models. Quantitative Finance, 2(4):251–256,
2002.

[20] Jean-Philippe Bouchaud and Marc Potters. Theory of ﬁnancial risk and derivative pricing:

from statistical physics to risk management. Cambridge University Press, 2003.

[21] Olivier Bousquet, Stéphane Boucheron, and Gábor Lugosi. Introduction to statistical
In Advanced lectures on machine learning, pages 169–207. Springer,

learning theory.
2004.

[22] D. T. Breeden and R. H. Litzenberger. Price of state-contigent claimes implicit in

option prices. 51:621–651, 1978.

[23] Damiano Brigo and Fabio Mercurio. Lognormal-mixture dynamics and calibration
International Journal of Theoretical and Applied Finance,

to market volatility smiles.
5(04):427–446, 2002.

[24] Peter Carr. Bounded brownian motion. NYU Tandon School of Engineering, 2017.

[25] Peter Carr, Hélyette Geman, Dilip B Madan, and Marc Yor. Stochastic volatility for

lévy processes. Mathematical ﬁnance, 13(3):345–382, 2003.

[26] Peter Carr and Dilip Madan. Optimal positioning in derivative securities. 2001.

[27] Lars-Erik Cederman. Modeling the size of wars: from billiard balls to sandpiles.

American Political Science Review, 97(01):135–150, 2003.

[28] Bikas K Chakrabarti, Anirban Chakraborti, Satya R Chakravarty, and Arnab Chat-
terjee. Econophysics of income and wealth distributions. Cambridge University Press,
2013.

[29] Shaohua Chen, Hong Nie, and Benjamin Ayers-Glassey. Lognormal sum approxi-
mation with a variant of type iv pearson distribution. IEEE Communications Letters,
12(9), 2008.

[30] Rémy Chicheportiche and Jean-Philippe Bouchaud. The joint distribution of stock
returns is not elliptical. International Journal of Theoretical and Applied Finance, 15(03),
2012.

[31] VP Chistyakov. A theorem on sums of independent positive random variables and
its applications to branching random processes. Theory of Probability & Its Applications,
9(4):640–648, 1964.

[32] Pasquale Cirillo. Are your data really pareto distributed?
Mechanics and its Applications, 392(23):5947–5962, 2013.

Physica A: Statistical

Bibliography

305

[33] Pasquale Cirillo and Nassim Nicholas Taleb. Expected shortfall estimation for ap-
parently inﬁnite-mean models of operational risk. Quantitative Finance, pages 1–10,
2016.

[34] Pasquale Cirillo and Nassim Nicholas Taleb. On the statistical properties and tail
risk of violent conﬂicts. Physica A: Statistical Mechanics and its Applications, 452:29–45,
2016.

[35] Pasquale Cirillo and Nassim Nicholas Taleb. What are the chances of war? Signiﬁ-

cance, 13(2):44–45, 2016.

[36] Open Science Collaboration et al. Estimating the reproducibility of psychological

science. Science, 349(6251):aac4716, 2015.

[37] Rama Cont and Peter Tankov. Financial modelling with jump processes, volume 2. CRC

press, 2003.

[38] Harald Cramér. On the mathematical theory of risk. Centraltryckeriet, 1930.

[39] George Cybenko. Approximation by superpositions of a sigmoidal function. Mathe-

matics of control, signals and systems, 2(4):303–314, 1989.

[40] Camilo Dagum. Inequality measures between income distributions with applications.

Econometrica, 48(7):1791–1803, 1980.

[41] Camilo Dagum. Income distribution models. Wiley Online Library, 1983.

[42] Anirban DasGupta. Probability for statistics and machine learning:

advanced topics. Springer Science & Business Media, 2011.

fundamentals and

[43] Herbert A David and Haikady N Nagaraja. Order statistics. 2003.

[44] Bruno De Finetti. Philosophical Lectures on Probability: collected, edited, and annotated by

Alberto Mura, volume 340. Springer Science & Business Media, 2008.

[45] Amir Dembo and Ofer Zeitouni. Large deviations techniques and applications, volume 38.

Springer Science & Business Media, 2009.

[46] Victor DeMiguel, Lorenzo Garlappi, and Raman Uppal. Optimal versus naive diver-
siﬁcation: How inefﬁcient is the 1/n portfolio strategy? The review of Financial studies,
22(5):1915–1953, 2007.

[47] Marco Di Renzo, Fabio Graziosi, and Fortunato Santucci. Further results on the
approximation of log-normal power sum via pearson type iv distribution: a general
formula for log-moments computation. IEEE Transactions on Communications, 57(4),
2009.

[48] Persi Diaconis and David Freedman. On the consistency of bayes estimates. The

Annals of Statistics, pages 1–26, 1986.

[49] Persi Diaconis and Sandy Zabell. Closed form summation for classical distributions:

variations on a theme of de moivre. Statistical Science, pages 284–302, 1991.

[50] Cornelius Frank Dietrich. Uncertainty, calibration and probability: the statistics of scien-

tiﬁc and industrial measurement. Routledge, 2017.

[51] NIST Digital Library of Mathematical Functions. http://dlmf.nist.gov/, Release 1.0.19
of 2018-06-22. F. W. J. Olver, A. B. Olde Daalhuis, D. W. Lozier, B. I. Schneider, R. F.
Boisvert, C. W. Clark, B. R. Miller and B. V. Saunders, eds.

306

Bibliography

[52] Daniel Dufresne. Sums of lognormals.
conference. University of Regina, 2008.

In Proceedings of the 43rd actuarial research

[53] Daniel Dufresne et al. The log-normal approximation in ﬁnancial and other compu-

tations. Advances in Applied Probability, 36(3):747–773, 2004.

[54] Bruno Dupire. Pricing with a smile. 7(1), 1994.

[55] Iddo Eliazar. Inequality spectra. Physica A: Statistical Mechanics and its Applications,

469:824–847, 2017.

[56] Iddo Eliazar and Morrel H Cohen. On social inequality: Analyzing the rich–poor
disparity. Physica A: Statistical Mechanics and its Applications, 401:148–158, 2014.

[57] Iddo Eliazar and Igor M Sokolov. Maximization of statistical heterogeneity: From
shannon’s entropy to gini’s index. Physica A: Statistical Mechanics and its Applications,
389(16):3023–3038, 2010.

[58] Iddo I Eliazar and Igor M Sokolov. Gini characterization of extreme-value statistics.

Physica A: Statistical Mechanics and its Applications, 389(21):4462–4472, 2010.

[59] Iddo I Eliazar and Igor M Sokolov. Measuring statistical evenness: A panoramic
overview. Physica A: Statistical Mechanics and its Applications, 391(4):1323–1353, 2012.

[60] Paul Embrechts. Modelling extremal events:

Springer, 1997.

for insurance and ﬁnance, volume 33.

[61] Paul Embrechts and Charles M Goldie. On convolution tails. Stochastic Processes and

their Applications, 13(3):263–278, 1982.

[62] Paul Embrechts, Charles M Goldie, and Noël Veraverbeke. Subexponentiality and

inﬁnite divisibility. Probability Theory and Related Fields, 49(3):335–347, 1979.

[63] M Émile Borel. Les probabilités dénombrables et leurs applications arithmétiques.

Rendiconti del Circolo Matematico di Palermo (1884-1940), 27(1):247–271, 1909.

[64] Michael Falk et al. On testing the extreme value index via the pot-method. The

Annals of Statistics, 23(6):2013–2035, 1995.

[65] Michael Falk, Jürg Hüsler, and Rolf-Dieter Reiss. Laws of small numbers: extremes and

rare events. Springer Science & Business Media, 2010.

[66] Kai-Tai Fang. Elliptically contoured distributions. Encyclopedia of Statistical Sciences,

2006.

[67] Doyne James Farmer and John Geanakoplos. Hyperbolic discounting is rational:

Valuing the far future with uncertain discount rates. 2009.

[68] William Feller. 1971an introduction to probability theory and its applications, vol. 2.

[69] Andrea Fontanari, Pasquale Cirillo, and Cornelis W Oosterlee. From concentration
proﬁles to concentration maps. new tools for the study of loss distributions. Insur-
ance: Mathematics and Economics, 78:13–29, 2018.

[70] Shane Frederick, George Loewenstein, and Ted O’donoghue. Time discounting and

time preference: A critical review. Journal of economic literature, 40(2):351–401, 2002.

[71] Marco Frittelli. The minimal entropy martingale measure and the valuation problem

in incomplete markets. Mathematical ﬁnance, 10(1):39–52, 2000.

Bibliography

307

[72] Xavier Gabaix. Power laws in economics and ﬁnance. Technical report, National

Bureau of Economic Research, 2008.

[73] Jim Gatheral. The Volatility Surface: a Practitioner’s Guide. John Wiley & Sons, 2006.

[74] Oscar Gelderblom and Joost Jonker. Amsterdam as the cradle of modern futures and
options trading, 1550-1650. William Goetzmann and K. Geert Rouwenhorst, 2005.

[75] Andrew Gelman and Hal Stern. The difference between “signiﬁcant” and “not sig-
niﬁcant” is not itself statistically signiﬁcant. The American Statistician, 60(4):328–331,
2006.

[76] Donald Geman, Hélyette Geman, and Nassim Nicholas Taleb. Tail risk constraints

and maximum entropy. Entropy, 17(6):3724, 2015.

[77] Nicholas Georgescu-Roegen. The entropy law and the economic process, 1971. Cam-

bridge, Mass, 1971.

[78] Gerd Gigerenzer and Peter M Todd. Simple heuristics that make us smart. Oxford

University Press, New York, 1999.

[79] Corrado Gini. Variabilità e mutabilità. Reprinted in Memorie di metodologica statistica

(Ed. Pizetti E, Salvemini, T). Rome: Libreria Eredi Virgilio Veschi, 1912.

[80] BV Gnedenko and AN Kolmogorov. Limit Distributions for Sums of Independent Ran-

dom Variables (1954).

[81] Charles M Goldie. Subexponential distributions and dominated-variation tails. Jour-

nal of Applied Probability, pages 440–442, 1978.

[82] Daniel Goldstein and Nassim Taleb. We don’t quite know what we are talking about

when we talk about volatility. Journal of Portfolio Management, 33(4), 2007.

[83] Richard C Green, Robert A Jarrow, et al. Spanning and completeness in markets

with contingent claims. Journal of Economic Theory, 41(1):202–210, 1987.

[84] Emil Julius Gümbel. Statistics of extremes. 1958.

[85] Laurens Haan and Ana Ferreira. Extreme value theory: An introduction. Springer

Series in Operations Research and Financial Engineering (, 2006.

[86] John Haigh. The kelly criterion and bet comparisons in spread betting. Journal of the

Royal Statistical Society: Series D (The Statistician), 49(4):531–539, 2000.

[87] Godfrey Harold Hardy, John Edensor Littlewood, and George Pólya.

Cambridge university press, 1952.

Inequalities.

[88] J Michael Harrison and David M Kreps. Martingales and arbitrage in multiperiod

securities markets. Journal of Economic theory, 20(3):381–408, 1979.

[89] Espen Gaarder Haug and Nassim Nicholas Taleb. Option traders use (very) so-
phisticated heuristics, never the black–scholes–merton formula. Journal of Economic
Behavior & Organization, 77(2):97–106, 2011.

[90] P. J. Huber. Robust Statistics. Wiley, New York, 1981.

[91] HM James Hung, Robert T O’Neill, Peter Bauer, and Karl Kohne. The behavior of
the p-value when the alternative hypothesis is true. Biometrics, pages 11–22, 1997.

308

Bibliography

[92] E.T. Jaynes. How should we use entropy in economics? 1991.

[93] Hedegaard Anders Jessen and Thomas Mikosch. Regularly varying functions. Publi-

cations de l’Institut Mathematique, 80(94):171–192, 2006.

[94] Petr Jizba, Hagen Kleinert, and Mohammad Shefaat. Rényi’s information transfer
between ﬁnancial time series. Physica A: Statistical Mechanics and its Applications,
391(10):2971–2989, 2012.

[95] Valen E Johnson. Revised standards for statistical evidence. Proceedings of the National

Academy of Sciences, 110(48):19313–19317, 2013.

[96] Joseph P Kairys Jr and NICHOLAS VALERIO III. The market for equity options in

the 1870s. The Journal of Finance, 52(4):1707–1723, 1997.

[97] John L Kelly. A new interpretation of information rate.

Transactions on, 2(3):185–189, 1956.

Information Theory, IRE

[98] Gideon Keren. Calibration and probability judgements: Conceptual and method-

ological issues. Acta Psychologica, 77(3):217–273, 1991.

[99] Christian Kleiber and Samuel Kotz. Statistical size distributions in economics and actu-

arial sciences, volume 470. John Wiley & Sons, 2003.

[100] Andrey Kolmogorov. Sulla determinazione empirica di una lgge di distribuzione.

Inst. Ital. Attuari, Giorn., 4:83–91, 1933.

[101] Samuel Kotz and Norman Johnson. Encyclopedia of Statistical Sciences. Wiley, 2004.

[102] Jean Laherrere and Didier Sornette. Stretched exponential distributions in nature
and economy:“fat tails” with characteristic scales. The European Physical Journal B-
Condensed Matter and Complex Systems, 2(4):525–539, 1998.

[103] David Laibson. Golden eggs and hyperbolic discounting. The Quarterly Journal of

Economics, 112(2):443–478, 1997.

[104] Deli Li, M Bhaskara Rao, and RJ Tomkins. The law of the iterated logarithm and cen-
tral limit theorem for l-statistics. Technical report, PENNSYLVANIA STATE UNIV
UNIVERSITY PARK CENTER FOR MULTIVARIATE ANALYSIS, 1997.

[105] Filip Lundberg.

I. Approximerad framställning af sannolikhetsfunktionen. II. Åter-
försäkring af kollektivrisker. Akademisk afhandling... af Filip Lundberg,... Almqvist och
Wiksells boktryckeri, 1903.

[106] HL MacGillivray and Kevin P Balanda. Mixtures, myths and kurtosis. Communica-

tions in Statistics-Simulation and Computation, 17(3):789–802, 1988.

[107] LC MacLean, William T Ziemba, and George Blazenko. Growth versus security in

dynamic investment analysis. Management Science, 38(11):1562–1585, 1992.

[108] Spyros Makridakis and Nassim Taleb. Decision making and planning under low

levels of predictability, 2009.

[109] Benoit Mandelbrot. The pareto-levy law and the distribution of income. International

Economic Review, 1(2):79–106, 1960.

[110] Benoit Mandelbrot. The stable paretian income distribution when the apparent ex-

ponent is near two. International Economic Review, 4(1):111–115, 1963.

Bibliography

309

[111] Benoît B Mandelbrot and Nassim Nicholas Taleb. Random jump, not random walk,

2010.

[112] Harry Markowitz. Portfolio selection*. The journal of ﬁnance, 7(1):77–91, 1952.

[113] Harry M Markowitz. Portfolio selection: efﬁcient diversiﬁcation of investments, vol-

ume 16. Wiley, 1959.

[114] RARD Maronna, Douglas Martin, and Victor Yohai. Robust statistics. John Wiley &

Sons, Chichester. ISBN, 2006.

[115] Robert C Merton. An analytic derivation of the efﬁcient portfolio frontier. Journal of

ﬁnancial and quantitative analysis, 7(4):1851–1872, 1972.

[116] Robert C. Merton. The relationship between put and call prices: Comment. 28(1):183–

184, 1973.

[117] Robert C. Merton. Theory of rational option pricing. 4:141–183, Spring 1973.

[118] Robert C Merton and Paul Anthony Samuelson. Continuous-time ﬁnance. 1992.

[119] David C Nachman. Spanning and completeness with options. The review of ﬁnancial

studies, 1(3):311–328, 1988.

[120] Hansjörg Neth and Gerd Gigerenzer. Heuristics: Tools for an uncertain world. Emerg-
ing trends in the social and behavioral sciences: An Interdisciplinary, Searchable, and Link-
able Resource, 2015.

[121] Donald J Newman. A problem seminar. Springer Science & Business Media, 2012.

[122] Hong Nie and Shaohua Chen. Lognormal sum approximation with type iv pearson

distribution. IEEE Communications Letters, 11(10), 2007.

[123] John P Nolan. Parameterizations and modes of stable distributions. Statistics &

probability letters, 38(2):187–195, 1998.

[124] T. Mikosch P. Embrechts, C. Kluppelberg. Modelling Extremal Events. Springer, 2003.

[125] Vilfredo Pareto. La courbe des revenus. Travaux de Sciences Sociales, pages 299–345,

1896 (1964).

[126] O. Peters and M. Gell-Mann. Evaluating gambles using dynamics. Chaos, 26(2), 2016.

[127] T Pham-Gia and TL Hung. The mean and median absolute deviations. Mathematical

and Computer Modelling, 34(7-8):921–936, 2001.

[128] George C Philippatos and Charles J Wilson. Entropy, market risk, and the selection

of efﬁcient portfolios. Applied Economics, 4(3):209–220, 1972.

[129] Charles Phillips and Alan Axelrod. Encyclopedia of Wars:(3-Volume Set). Infobase Pub.,

2004.

[130] James Pickands III. Statistical inference using extreme order statistics. the Annals of

Statistics, pages 119–131, 1975.

[131] Thomas Piketty. Capital in the 21st century, 2014.

[132] Thomas Piketty and Emmanuel Saez. The evolution of top incomes: a historical and
international perspective. Technical report, National Bureau of Economic Research,
2006.

310

Bibliography

[133] Iosif Pinelis. Characteristic function of the positive part of a random variable and

related results, with applications. Statistics & Probability Letters, 106:281–286, 2015.

[134] Steven Pinker. The better angels of our nature: Why violence has declined. Penguin, 2011.

[135] Dan Pirjol. The logistic-normal integral and its generalizations. Journal of Computa-

tional and Applied Mathematics, 237(1):460–469, 2013.

[136] EJG Pitman. Subexponential distribution functions.

29(3):337–347, 1980.

J. Austral. Math. Soc. Ser. A,

[137] Svetlozar T Rachev, Young Shin Kim, Michele L Bianchi, and Frank J Fabozzi. Finan-
cial models with Lévy processes and volatility clustering, volume 187. John Wiley & Sons,
2011.

[138] Matthew Richardson and Tom Smith. A direct test of the mixture of distributions
hypothesis: Measuring the daily ﬂow of information. Journal of Financial and Quanti-
tative Analysis, 29(01):101–116, 1994.

[139] Christian Robert and George Casella. Monte Carlo statistical methods. Springer Science

& Business Media, 2013.

[140] Stephen A Ross. Mutual fund separation in ﬁnancial theory—the separating distri-

butions. Journal of Economic Theory, 17(2):254–286, 1978.

[141] Stephen A Ross. Neoclassical ﬁnance. Princeton University Press, 2009.

[142] Mark Rubinstein. Rubinstein on derivatives. Risk Books, 1999.

[143] Doriana Rufﬁno and Jonathan Treussard. Derman and taleb’s ‘the illusions of dy-

namic replication’: a comment. Quantitative Finance, 6(5):365–367, 2006.

[144] Harold Sackrowitz and Ester Samuel-Cahn. P values as random variables—expected

p values. The American Statistician, 53(4):326–331, 1999.

[145] Gennady Samorodnitsky and Murad S Taqqu. Stable non-Gaussian random processes:

stochastic models with inﬁnite variance, volume 1. CRC Press, 1994.

[146] D Schleher. Generalized gram-charlier series with application to the sum of log-
IEEE Transactions on Information Theory, 23(2):275–280,

normal variates (corresp.).
1977.

[147] Jun Shao. Mathematical Statistics. Springer, 2003.

[148] SK Singh and GS Maddala. A function for size distribution of incomes: reply. Econo-

metrica, 46(2), 1978.

[149] Didier Sornette. Critical phenomena in natural sciences: chaos, fractals, selforganization,

and disorder: concepts and tools. Springer, 2004.

[150] C.M. Sprenkle. Warrant Prices as Indicators of Expectations and Preferences: in P. Cootner,
ed., 1964, The Random Character of Stock Market Prices,. MIT Press, Cambridge, Mass,
1964.

[151] AJ Stam. Regular variation of the tail of a subordinated probability distribution.

Advances in Applied Probability, pages 308–327, 1973.

[152] Giitiro Suzuki. A consistent estimator for the mean deviation of the pearson type
distribution. Annals of the Institute of Statistical Mathematics, 17(1):271–285, 1965.

Bibliography

311

[153] N N Taleb and R Douady. Mathematical deﬁnition, mapping, and detection of (anti)

fragility. Quantitative Finance, 2013.

[154] Nassim N Taleb and Daniel G Goldstein. The problem is beyond psychology: The
real world is more random than regression analyses. International Journal of Forecast-
ing, 28(3):715–716, 2012.

[155] Nassim N Taleb and G Martin. The illusion of thin tails under aggregation (a reply

to jack treynor). Journal of Investment Management, 2012.

[156] Nassim Nicholas Taleb. Dynamic Hedging: Managing Vanilla and Exotic Options. John

Wiley & Sons (Wiley Series in Financial Engineering), 1997.

[157] Nassim Nicholas Taleb. Incerto: Antifragile, The Black Swan , Fooled by Randomness, the

Bed of Procrustes, Skin in the Game. Random House and Penguin, 2001-2018.

[158] Nassim Nicholas Taleb. Black swans and the domains of statistics. The American

Statistician, 61(3):198–200, 2007.

[159] Nassim Nicholas Taleb. Errors, robustness, and the fourth quadrant. International

Journal of Forecasting, 25(4):744–759, 2009.

[160] Nassim Nicholas Taleb. Finiteness of variance is irrelevant in the practice of quanti-

tative ﬁnance. Complexity, 14(3):66–76, 2009.

[161] Nassim Nicholas Taleb. Antifragile: things that gain from disorder. Random House and

Penguin, 2012.

[162] Nassim Nicholas Taleb. Four points beginner risk managers should learn from jeff
holman’s mistakes in the discussion of antifragile. arXiv preprint arXiv:1401.2524,
2014.

[163] Nassim Nicholas Taleb. The meta-distribution of standard p-values. arXiv preprint

arXiv:1603.07532, 2016.

[164] Nassim Nicholas Taleb. Stochastic tail exponent for asymmetric power laws. arXiv

preprint arXiv:1609.02369, 2016.

[165] Nassim Nicholas Taleb. Election predictions as martingales: an arbitrage approach.

Quantitative Finance, 18(1):1–5, 2018.

[166] Nassim Nicholas Taleb. How much data do you need?

an operational, pre-

asymptotic metric for fat-tailedness. International Journal of Forecasting, 2018.

[167] Nassim Nicholas Taleb. Skin in the Game: Hidden Asymmetries in Daily Life. Penguin

(London) and Random House (N.Y.), 2018.

[168] Nassim Nicholas Taleb. Technical Incerto Vol 1: The Statistical Consequences of Fat Tails,

Papers and Commentaries. Monograph, 2018.

[169] Nassim Nicholas Taleb, Elie Canetti, Tidiane Kinda, Elena Loukoianova, and Chris-
tian Schmieder. A new heuristic measure of fragility and tail risks: application to
stress testing. International Monetary Fund, 2018.

[170] Nassim Nicholas Taleb and Raphael Douady. On the super-additivity and estimation
biases of quantile contributions. Physica A: Statistical Mechanics and its Applications,
429:252–260, 2015.

312

Bibliography

[171] Nassim Nicholas Taleb and George A Martin. How to prevent other ﬁnancial crises.

SAIS Review of International Affairs, 32(1):49–60, 2012.

[172] Nassim Nicholas Taleb and Avital Pilpel.

I problemi epistemologici del risk man-
agement. Daniele Pace (a cura di)" Economia del rischio. Antologia di scritti su rischio e
decisione economica", Giuffrè, Milano, 2004.

[173] Nassim Nicholas Taleb and Constantine Sandis. The skin in the game heuristic for

protection against tail events. Review of Behavioral Economics, 1:1–21, 2014.

[174] Jozef L Teugels. The class of subexponential distributions. The Annals of Probability,

3(6):1000–1011, 1975.

[175] Edward Thorp. A corrected derivation of the black-scholes option model. Based on
private conversation with Edward Thorp and a copy of a 7 page paper Thorp wrote around
1973, with disclaimer that I understood Ed. Thorp correctly., 1973.

[176] Edward O Thorp. Optimal gambling systems for favorable games. Revue de l’Institut

International de Statistique, pages 273–293, 1969.

[177] Edward O Thorp. Understanding the kelly criterion. The Kelly Capital Growth Invest-

ment Criterion: Theory and Practice’, World Scientiﬁc Press, Singapore, 2010.

[178] Edward O. Thorp and S. T. Kassouf. Beat the Market. New York: Random House,

1967.

[179] James Tobin. Liquidity preference as behavior towards risk. The review of economic

studies, pages 65–86, 1958.

[180] Jack L Treynor. Insights-what can taleb learn from markowitz? Journal of Investment

Management, 9(4):5, 2011.

[181] Constantino Tsallis, Celia Anteneodo, Lisa Borland, and Roberto Osorio. Nonex-
tensive statistical mechanics and economics. Physica A: Statistical Mechanics and its
Applications, 324(1):89–100, 2003.

[182] Vladimir V Uchaikin and Vladimir M Zolotarev. Chance and stability: stable distribu-

tions and their applications. Walter de Gruyter, 1999.

[183] Aad W Van Der Vaart and Jon A Wellner. Weak convergence. In Weak convergence

and empirical processes, pages 16–28. Springer, 1996.

[184] Willem Rutger van Zwet. Convex transformations of random variables, volume 7. Math-

ematisch centrum, 1964.

[185] SR Srinivasa Varadhan. Large deviations and applications, volume 46. SIAM, 1984.

[186] José A Villaseñor-Alva and Elizabeth González-Estrada. A bootstrap goodness of ﬁt
test for the generalized pareto distribution. Computational Statistics & Data Analysis,
53(11):3835–3841, 2009.

[187] Eric Weisstein. Wolfram MathWorld. Wolfram Research www.wolfram.com, 2017.

[188] Heath Windcliff and Phelim P Boyle. The 1/n pension investment puzzle. North

American Actuarial Journal, 8(3):32–45, 2004.

[189] Yingying Xu, Zhuwu Wu, Long Jiang, and Xuefeng Song. A maximum entropy

method for a robust portfolio problem. Entropy, 16(6):3401–3415, 2014.

Bibliography

313

[190] Yingying Yang, Shuhe Hu, and Tao Wu. The tail probability of the product of de-
pendent random variables from max-domains of attraction. Statistics & Probability
Letters, 81(12):1876–1882, 2011.

[191] IV Zaliapin, Yan Y Kagan, and Federic P Schoenberg. Approximating the distribu-

tion of pareto sums. Pure and Applied geophysics, 162(6-7):1187–1228, 2005.

[192] Rongxi Zhou, Ru Cai, and Guanqun Tong. Applications of entropy in ﬁnance: A

review. Entropy, 15(11):4909–4931, 2013.

[193] VM Zolotarev. On a new viewpoint of limit theorems taking into account large
deviationsr. Selected Translations in Mathematical Statistics and Probability, 9:153, 1971.

I N D E X

k metric, 114

Antifragility, 54, 281

Bayesian methods, 27
Beta (ﬁnance), 14
Black Swan, 19, 24, 28, 41, 77, 205, 221
Black-Scholes, 267, 276, 283
Brier score, 249, 253, 258

Central limit theorem, 99
Central limit theorem (CLT), 12, 119, 122,

128, 129, 131, 138, 204, 221

Characteristic function, 37, 38, 46, 61, 80,
100, 101, 104, 106, 109, 116, 122,
134, 159, 234, 235, 297

Chernoff Bound, 91
Concavity/Convexity, 28–30, 41, 43, 54
Convolution, 109
Cramer condition, 17

De Finetti, 257
Degenerate distribution, 10, 19, 51, 81, 106

Econometrics, 73
Ellipticality, 62
Ellipticality of distributions, 61
Empirical distribution, 14, 94

Fréchet class, 190
Fragility, 30
Fughedaboudit, 9

Gamma variance, 38
GARCH econometric models, 1, 22, 77,

127

Gauss-Markov theorem, 14
Generalized central limit theorem (GCLT),

99, 160, 171

Gini coefﬁcient, 15

Heteroscedasticity, 35
Hidden Properties, 14
Higher Dimensions (Fat Tailedness), 59,

62

Karamata Representation Theorem, 233
Kurtosis, 1, 37, 80, 105, 113, 116, 119, 126,

137, 139, 142, 281

Large deviation principle, 91
Large deviation theory, 15
Law of Large Numbers, 9, 88, 106
Law of large numbers (LLN), 129
Lindy Effect, 53
Location-scale family, 35, 165
Lucretius (fallacy), 69

Machine Learning, 93
Mandelbrot, Benoit , 50, 112, 123, 198, 221
Marchenko-Pastur distribution, 15
Martingale, 249, 252, 256
Mediocristan vs. Extremistan, 7, 17, 19,

32, 51, 52

Medium numbers, 99
Method of Moments, 15
Modern Portfolio Theory (Markowitz), 120,

137, 288, 289

Multivariate stable distribution, 64
Myopic loss aversion, 88

Naive empiricism, 18
Numéraire, 283

Paretian, Paretianity, 54, 65, 71, 100, 103,
131, 132, 134, 157, 158, 163, 165,
166, 190, 193, 200, 204, 233

Pareto, Vilfredo, 175
Paul Levy, 99
Peso problem, 77
Poisson jumps, 77

315

316

Index

Power Law, 15, 35, 49, 51, 54, 62, 105, 111,
113, 116, 119, 127, 128, 132, 136,
139, 175, 197, 199, 202, 227, 232,
295–297

Principal component analysis, 15

Robust statistics, 14

Sharpe ratio (coefﬁcient of variation), 1,

14
Sigmoid, 93
Skin in the game, 78
Slowly Varying Function, 132, 190
Slowly varying function, 115, 116, 132, 177,

200, 233

Stable (Lévy Stable) distribution, 62
Stochastic volatility, 37, 40, 43, 50, 85, 105,

119, 127, 137, 140, 231, 282

Stochasticization (of variance), 37

Tail exponent, 10, 14, 27, 42, 49, 55, 65,
113, 117, 118, 134, 143, 156, 157,
183, 186, 199, 203, 204, 227, 231,
232, 234, 236, 238, 292

Universal approximation theorem, 93

Value at Risk, 77

