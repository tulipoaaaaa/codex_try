WILEY SERIES in PROBABILITY and STATISTICS 

Analysis of 
FINANCIAL 
TIME SERIES 

THIRD EDITION 

RUEY S. TSAY 

©WILEY 

WWW 

r 

Analysis of Financial Time Series 

WILEY SERIES IN PROBABILITY AND STATISTICS 

Established by WALTER A. SHEWHART and SAMUEL S. WILKS 

Editors: David J. Balding, Noel A. C. Cressie, Garrett M. Fitzmaurice, 
Iain M. Johnstone, Geert Molenberghs, David W. Scott, Adrian F. M. Smith, 
Ruey S. Tsay, Sanford Weisberg 

Editors Emeriti: Vic Barnett, J. Stuart Hunter, JozefL. Teugels 

A complete list of the titles in this series appears at the end of this volume. 

Analysis of Financial Time Series 

Third Edition 

RUEY S. TSAY 

The University of Chicago 
Booth School of Business 
Chicago, IL 

national institutes of hfai tl, 
NIH LI9HAHV HEALTH 

SEP 1 8 2010 

bethesda. 

©WILEY 

A JOHN WILEY & SONS, INC., PUBLICATION 

Copyright © 2010 by John Wiley & Sons, Inc. All rights reserved. 

Published by John Wiley & Sons, Inc., Hoboken, New Jersey. 

Published simultaneously in Canada. 

No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form 

or by any means, electronic, mechanical, photocopying, recording, scanning, or otherwise, except as 

permitted under Section 107 or 108 of the 1976 United States Copyright Act, without either the prior 

written permission of the Publisher, or authorization through payment of the appropriate per-copy fee 

to the Copyright Clearance Center, Inc., 222 Rosewood Drive, Danvers, MA 01923, (978) 750-8400, 

fax (978) 750-4744. Requests to the Publisher for permission should be addressed to the Permissions 

Department, John Wiley & Sons, Inc., Ill River Street, Hoboken, NJ 07030, (201) 748-6011, fax (201) 
748-6008, or online at http://www.wiley.com/go/permission. 

Limit of Liability/Disclaimer of Warranty: While the publisher and author have used their best efforts 
in preparing this book, they make no representations or warranties with respect to the accuracy or com¬ 

pleteness of the contents of this book and specifically disclaim any implied warranties of merchantability 

or fitness for a particular purpose. No warranty may be created or extended by sales representatives 

or written sales materials. The advice and strategies contained herein may not be suitable for your 

situation. You should consult with a professional where appropriate. Neither the publisher nor author 

shall be liable for any loss of profit or any other commercial damages, including but not limited to 
special, incidental, consequential, or other damages. 

For general information on our other products and services or for technical support, please contact our 

Customer Care Department within the United States at (800) 762-2974, outside the United States at 
(317) 572-3993 or fax (317) 572-4002. 

Wiley also publishes its books in a variety of electronic formats. Some content that appears in print 
may not be available in electronic formats. For more information about Wiley products, visit our web 
site at www.wiley.com. 

Library of Congress Cataloging-in-Publication Data: 
Tsay, Ruey S., 1951 — 

Analysis of financial time series / Ruey S. Tsay. - 3rd ed. 

p. cm. - (Wiley series in probability and statistics) 

Includes bibliographical references and index. 
ISBN 978-0-470-41435-4 (cloth) 

1. Time-series analysis. 2. Econometrics. 3. Risk management I Title 
HA30.3T76 2010 
332.0U51955- dc22 

2010005151 

Printed in the United States of America 

10 987654321 

To Teresa and my father, and in memory of my mother 

Contents 

Preface 
Preface to the Second Edition 
Preface to the First Edition 

1 Financial Time Series and Their Characteristics 

1.1 Asset Returns, 2 

1.2 Distributional Properties of Returns, 7 

xvii 
xix 
xxi 

1 

1.2.1 Review of Statistical Distributions and Their Moments, 7 

1.2.2 Distributions of Returns, 14 

1.2.3 Multivariate Returns, 18 

1.2.4 Likelihood Function of Returns, 19 

1.2.5 Empirical Properties of Returns, 19 

1.3 Processes Considered, 22 

Appendix: R Packages, 24 

Exercises, 25 

References, 27 

2 Linear Time Series Analysis and Its Applications 29 

2.1 Stationarity, 30 

2.2 Correlation and Autocorrelation Function, 30 

2.3 White Noise and Linear Time Series, 36 

2.4 Simple AR Models, 37 

2.4.1 Properties of AR Models, 38 

2.4.2 Identifying AR Models in Practice, 46 

2.4.3 Goodness of Fit, 53 

2.4.4 Forecasting, 54 

vii 

Vlll 

CONTENTS 

2.5 Simple MA Models, 57 

2.5.1 Properties of MA Models, 59 

2.5.2 Identifying MA Order, 60 

2.5.3 Estimation, 61 

2.5.4 Forecasting Using MA Models, 62 

2.6 Simple ARMA Models, 64 

2.6.1 Properties of ARMA(1,1) Models, 64 

2.6.2 General ARMA Models, 66 

2.6.3 Identifying ARMA Models, 66 

2.6.4 Forecasting Using an ARMA Model, 68 

2.6.5 Three Model Representations for an ARMA Model, 69 

2.7 Unit-Root Nonstationarity, 71 

2.7.1 Random Walk, 72 

2.7.2 Random Walk with Drift, 73 

2.7.3 Trend-Stationary Time Series, 75 

2.7.4 General Unit-Root Nonstationary Models, 75 

2.7.5 Unit-Root Test, 76 

2.8 Seasonal Models, 81 

2.8.1 Seasonal Differencing, 82 

2.8.2 Multiplicative Seasonal Models, 84 

2.9 Regression Models with Time Series Errors, 90 

2.10 Consistent Covariance Matrix Estimation, 97 

2.11 Long-Memory Models, 101 

Appendix: Some SCA Commands, 103 

Exercises, 104 

References, 107 

3 Conditional Heteroscedastic Models ] 

3.1 Characteristics of Volatility, 110 

3.2 Structure of a Model, 111 

3.3 Model Building, 113 

3.3.1 Testing for ARCH Effect, 114 

3.4 The ARCH Model, 115 

3.4.1 Properties of ARCH Models, 117 

3.4.2 Weaknesses of ARCH Models, 119 

3.4.3 Building an ARCH Model, 119 

3.4.4 Some Examples, 123 

3.5 The GARCH Model, 131 

3.5.1 An Illustrative Example, 134 

CONTENTS 

ix 

3.5.2 Forecasting Evaluation, 139 

3.5.3 A Two-Pass Estimation Method, 140 

3.6 The Integrated GARCH Model, 140 

3.7 The GARCH-M Model, 142 

3.8 The Exponential GARCH Model, 143 

3.8.1 Alternative Model Form, 144 

3.8.2 Illustrative Example, 145 

3.8.3 Second Example, 145 

3.8.4 Forecasting Using an EG ARCH Model, 147 

3.9 The Threshold GARCH Model, 149 

3.10 The CHARMA Model, 150 

3.10.1 Effects of Explanatory Variables, 152 

3.11 Random Coefficient Autoregressive Models, 152 

3.12 Stochastic Volatility Model, 153 

3.13 Long-Memory Stochastic Volatility Model, 154 

3.14 Application, 155 

3.15 Alternative Approaches, 159 

3.15.1 Use of High-Frequency Data, 159 

3.15.2 Use of Daily Open, High, Low, and Close Prices, 162 

3.16 Kurtosis of GARCH Models, 165 

Appendix: Some RATS Programs for Estimating Volatility Models, 167 

Exercises, 168 

References, 171 

4 Nonlinear Models and Their Applications 

175 

4.1 Nonlinear Models, 177 

4.1.1 Bilinear Model, 177 

4.1.2 Threshold Autoregressive (TAR) Model, 179 

4.1.3 Smooth Transition AR (STAR) Model, 184 

4.1.4 Markov Switching Model, 186 

4.1.5 Nonparametric Methods, 189 

4.1.6 Functional Coefficient AR Model, 198 

4.1.7 Nonlinear Additive AR Model, 198 

4.1.8 Nonlinear State-Space Model, 199 

4.1.9 Neural Networks, 199 

4.2 Nonlinearity Tests, 205 

4.2.1 Nonparametric Tests, 206 

4.2.2 Parametric Tests, 209 

4.2.3 Applications, 213 

X 

CONTENTS 

4.3 Modeling, 214 

4.4 Forecasting, 215 

4.4.1 Parametric Bootstrap, 215 

4.4.2 Forecasting Evaluation, 215 

4.5 Application, 218 

Appendix A: Some RATS Programs for Nonlinear Volatility Models, 222 

Appendix B: R and S-Plus Commands for Neural Network, 223 

Exercises, 224 

References, 226 

High-Frequency Data Analysis and Market Microstructure 

231 

5.1 Nonsynchronous Trading, 232 
5.2 

Bid-Ask Spread, 235 

5.3 

5.4 

Empirical Characteristics of Transactions Data, 237 

Models for Price Changes, 244 

5.4.1 Ordered Probit Model, 245 

5.4.2 Decomposition Model, 248 

5.5 

Duration Models, 253 

5.5.1 The ACD Model, 255 

5.5.2 Simulation, 257 

5.5.3 Estimation, 260 

Nonlinear Duration Models, 264 

Bivariate Models for Price Change and Duration, 265 

Application, 270 

5.6 

5.7 

5.8 

Appendix A: Review of Some Probability Distributions, 276 

Appendix B: Hazard Function, 279 

Appendix C: Some RATS Programs for Duration Models, 280 
Exercises, 282 

References, 284 

Continuous-Time Models and Their Applications 

287 

6.1 Options, 288 
6.2 

Some Continuous-Time Stochastic Processes, 288 

6.2.1 Wiener Process, 289 

6.2.2 Generalized Wiener Process, 291 

6.2.3 Ito Process, 292 

6.3 

Ito’s Lemma, 292 

6.3.1 Review of Differentiation, 292 

CONTENTS 

XI 

6.3.2 Stochastic Differentiation, 293 

6.3.3 An Application, 294 

6.3.4 Estimation of fi and a, 295 

6.4 Distributions of Stock Prices and Log Returns, 297 

6.5 Derivation of Black-Scholes Differential Equation, 298 

6.6 Black-Scholes Pricing Formulas, 300 

6.6.1 Risk-Neutral World, 300 

6.6.2 Formulas, 300 

6.6.3 Lower Bounds of European Options, 304 

6.6.4 Discussion, 305 

6.7 Extension of Ito’s Lemma, 309 

6.8 Stochastic Integral, 310 

6.9 Jump Diffusion Models, 311 

6.9.1 Option Pricing under Jump Diffusion, 315 

6.10 Estimation of Continuous-Time Models, 318 

Appendix A: Integration of Black-Scholes Formula, 319 

Appendix B: Approximation to Standard Normal Probability, 320 

Exercises, 321 

References, 322 

7 Extreme Values, Quantiles, and Value at Risk 

325 

7.1 Value at Risk, 326 

7.2 RiskMetrics, 328 

7.2.1 Discussion, 331 

7.2.2 Multiple Positions, 332 

7.2.3 Expected Shortfall, 332 

7.3 Econometric Approach to VaR Calculation, 333 

7.3.1 Multiple Periods, 336 

7.3.2 Expected Shortfall under Conditional Normality, 338 

7.4 Quantile Estimation, 338 

7.4.1 Quantile and Order Statistics, 338 

7.4.2 Quantile Regression, 341 

7.5 Extreme Value Theory, 342 

7.5.1 Review of Extreme Value Theory, 342 

7.5.2 Empirical Estimation, 345 

7.5.3 Application to Stock Returns, 348 

7.6 Extreme Value Approach to VaR, 353 

7.6.1 Discussion, 356 

Xll 

CONTENTS 

7.6.2 Multiperiod VaR, 357 

7.6.3 Return Level, 358 

7.7 New Approach Based on the Extreme Value Theory, 359 

7.7.1 Statistical Theory, 360 

7.7.2 Mean Excess Function, 361 

7.7.3 New Approach to Modeling Extreme Values, 363 

7.7.4 VaR Calculation Based on the New Approach, 365 

7.7.5 Alternative Parameterization, 367 

7.7.6 Use of Explanatory Variables, 371 

7.7.7 Model Checking, 372 

7.7.8 An Illustration, 373 

7.8 The Extremal Index, 377 

7.8.1 The D(un) Condition, 378 

7.8.2 Estimation of the Extremal Index, 381 

7.8.3 Value at Risk for a Stationary Time Series, 384 

Exercises, 384 

References, 387 

8  Multivariate Time Series Analysis and Its Applications 

389 

8.1 

Weak Stationarity and Cross-Correlation Matrices, 390 

8.1.1 Cross-Correlation Matrices, 390 

8.1.2 Linear Dependence, 392 

8.1.3 Sample Cross-Correlation Matrices, 392 

8.1.4 Multivariate Portmanteau Tests, 397 

8.2 

Vector Autoregressive Models, 399 

8.2.1 Reduced and Structural Forms, 399 

8.2.2 Stationarity Condition and Moments of a VAR(l) 

Model, 401 

8.2.3 Vector AR(p) Models, 403 

8.2.4 Building a VAR(p) Model, 405 

8.2.5 Impulse Response Function, 413 

Vector Moving-Average Models, 417 

Vector ARMA Models, 422 

8.4.1 Marginal Models of Components, 427 

Unit-Root Nonstationarity and Cointegration, 428 

8.5.1 An Error Correction Form, 431 

Cointegrated VAR Models, 432 

8.6.1 Specification of the Deterministic Function, 434 

8.3 

8.4 

8.5 

8.6 

CONTENTS 

xiii 

8.6.2 Maximum-Likelihood Estimation, 435 

8.6.3 Cointegration Test, 436 

8.6.4 Forecasting of Cointegrated VAR Models, 437 

8.6.5 An Example, 438 

8.7 Threshold Cointegration and Arbitrage, 442 

8.7.1 Multivariate Threshold Model, 444 

8.7.2 The Data, 445 

8.7.3 Estimation, 445 

8.8 Pairs Trading, 446 

8.8.1 Theoretical Framework, 446 

8.8.2 Trading Strategy, 448 

8.8.3 Simple Illustration, 449 

Appendix A: Review of Vectors and Matrices, 456 

Appendix B: Multivariate Normal Distributions, 460 

Appendix C: Some SCA Commands, 461 

Exercises, 462 

References, 464 

9 Principal Component Analysis and Factor Models 

467 

9.1 A Factor Model, 468 

9.2 Macroeconometric Factor Models, 470 

9.2.1 Single-Factor Model, 470 

9.2.2 Multifactor Models, 474 

9.3 Fundamental Factor Models, 476 

9.3.1 BARRA Factor Model, 477 

9.3.2 Fama-French Approach, 482 

9.4 Principal Component Analysis, 483 

9.4.1 Theory of PCA, 483 

9.4.2 Empirical PCA, 485 

9.5 Statistical Factor Analysis, 489 

9.5.1 Estimation, 490 

9.5.2 Factor Rotation, 492 

9.5.3 Applications, 492 

9.6 Asymptotic Principal Component Analysis, 498 

9.6.1 Selecting the Number of Factors, 499 

9.6.2 An Example, 500 

Exercises, 501 

References, 503 

xiv 

CONTENTS 

10 Multivariate Volatility Models and Their Applications 

10.1 Exponentially Weighted Estimate, 506 

10.2 Some Multivariate GARCH Models, 510 

10.2.1 Diagonal Vectorization (VEC) Model, 510 

10.2.2 BEKK Model, 513 

10.3 Reparameterization, 516 

10.3.1 Use of Correlations, 516 

10.3.2 Cholesky Decomposition, 517 

10.4 GARCH Models for Bivariate Returns, 521 

10.4.1 Constant-Correlation Models, 521 

10.4.2 Time-Varying Correlation Models, 525 

10.4.3 Dynamic Correlation Models, 531 

10.5 Higher Dimensional Volatility Models, 537 

10.6 Factor-Volatility Models, 543 

10.7 Application, 546 

10.8 Multivariate t Distribution, 548 

Appendix: Some Remarks on Estimation, 549 

Exercises, 554 

References, 555 

11 State-Space Models and Kalman Filter 

11.1 Local Trend Model, 558 

11.1.1 Statistical Inference, 561 

11.1.2 Kalman Filter, 562 

11.1.3 Properties of Forecast Error, 564 

11.1.4 State Smoothing, 566 

11.1.5 Missing Values, 570 

11.1.6 Effect of Initialization, 571 

11.1.7 Estimation, 572 

11.1.8 S-Plus Commands Used, 572 

11.2 Linear State-Space Models, 576 

11.3 Model Transformation, 577 

11.3.1 CAPM with Time-Varying Coefficients, 577 
11.3.2 ARMA Models, 580 

11.3.3 Linear Regression Model, 586 

11.3.4 Linear Regression Models with ARMA Errors, 588 

11.3.5 Scalar Unobserved Component Model, 590 

CONTENTS 

XV 

11.4 Kalman Filter and Smoothing, 591 

11.4.1 Kalman Filter, 591 

11.4.2 State Estimation Error and Forecast Error, 594 

11.4.3 State Smoothing, 595 

11.4.4 Disturbance Smoothing, 597 

11.5 Missing Values, 600 

11.6 Forecasting, 601 

11.7 Application, 602 

Exercises, 609 

References, 611 

12 Markov Chain Monte Carlo Methods with Applications 

613 

12.1 Markov Chain Simulation, 614 

12.2 Gibbs Sampling, 615 

12.3 Bayesian Inference, 617 

12.3.1 Posterior Distributions, 617 

12.3.2 Conjugate Prior Distributions, 618 

12.4 Alternative Algorithms, 622 

12.4.1 Metropolis Algorithm, 622 

12.4.2 Metropolis-Hasting Algorithm, 623 

12.4.3 Griddy Gibbs, 623 

12.5 Linear Regression with Time Series Errors, 624 

12.6 Missing Values and Outliers, 628 

12.6.1 Missing Values, 629 

12.6.2 Outlier Detection, 632 

12.7 Stochastic Volatility Models, 636 

12.7.1 Estimation of Univariate Models, 637 

12.7.2 Multivariate Stochastic Volatility Models, 643 

12.8 New Approach to SV Estimation, 649 

12.9 Markov Switching Models, 660 

12.10 Forecasting, 666 

12.11 Other Applications, 669 

Exercises, 670 

References, 671 

Index 

673 

Preface 

As many countries struggle to recover from the recent global financial crisis, one 
thing clear is that we do not want to suffer another crisis like this in the future. 
We must study the past in order to prevent future financial crisis. Financial data 
of the past few years thus become important in empirical study. The primary 
objective of the revision is to update the data used and to reanalyze the examples 
so that one can better understand the properties of asset returns. At the same time, 
we also witness many new developments in financial econometrics and financial 
software packages. In particular, the Rmetrics now has many packages for analyzing 
financial time series. The second goal of the revision is to include R commands and 
demonstrations, making it possible and easier for readers to reproduce the results 
shown in the book. 

Collapses of big financial institutions during the crisis show that extreme events 
occur in clusters; they are not independent. To deal with dependence in extremes, 
I include the extremal index in Chapter 7 and discuss its impact on value at risk. 
I also rewrite Chapter 7 to make it easier to understand and more complete. It 
now contains the expected shortfall, or conditional value at risk, for measuring 
finanical risk. 

Substantial efforts are made to draw a balance between the length and cover¬ 
age of the book. I do not include credit risk or operational risk in this revision 
for three reasons. First, effective methods for assessing credit risk require further 
study. Second, the data are not widely available. Third, the length of the book is 
approaching my limit. 

A brief summary of the added material in the third edition is: 

1. To update the data used throughout the book. 

2. To provide R commands and demonstrations. In some cases, R programs are 

given. 

3. To reanalyze many examples with updated observations. 

4. To introduce skew distributions for volatility modeling in Chapter 3. 

5. To investigate properties of recent high-frequency trading data and to add 

applications of nonlinear duration models in Chapter 5. 

XVII 

xviii 

PREFACE 

6. To provide a unified approach to value at risk (VaR) via loss function, to 
discuss expected shortfall (ES), or equivalently the conditional value at risk 
(CVaR), and to introduce extremal index for dependence data in Chapter 7. 

7. To discuss application of cointegration to pairs trading in Chapter 8. 

8. To study applications of dynamic correlation models in Chapter 10. 

I benefit greatly from constructive comments of many readers of the second 
edition, including students, colleagues, and friends. I am indebted to them all. In 
particular, I like to express my sincere thanks to Spencer Graves for creating the 
FinTS package for R and Tom Doan of ESTIMA and Eugene Gath for careful 
reading of the text. I also thank Kam Hamidieh for suggestions concerning new 
topics for the revision. I also like to thank colleagues at Wiley, especially Jackie 
Palmieri and Stephen Quigley, for their support. As always, the revision would not 
be possible without the constant encouragement and unconditional love of my wife 
and children. They are my motivation and source of energy. Part of my research 
is supported by the Booth School of Business, University of Chicago. 

Finally, the website for the book is: 

http://faculty.chicagobooth.edu/ruey.tsaY/teaching/fts3. 

Ruey S. Tsay 

Booth School of Business, University of Chicago 

Chicago, Illinois 

Preface to the Second Edition 

The subject of financial time series analysis has attracted substantial attention in 
recent years, especially with the 2003 Nobel awards to Professors Robert Engle and 
Clive Granger. At the same time, the field of financial econometrics has undergone 
various new developments, especially in high-frequency finance, stochastic volatil¬ 
ity, and software availability. There is a need to make the material more complete 
and accessible for advanced undergraduate and graduate students, practitioners, and 
researchers. The main goals in preparing this second edition have been to bring the 
book up to date both in new developments and empirical analysis, and to enlarge 
the core material of the book by including consistent covariance estimation under 
heteroscedasticity and serial correlation, alternative approaches to volatility mod¬ 
eling, financial factor models, state-space models, Kalman filtering, and estimation 
of stochastic diffusion models. 

The book therefore has been extended to 12 chapters and substantially revised 
to include S-Plus commands and illustrations. Many empirical demonstrations and 
exercises are updated so that they include the most recent data. 

The two new chapters are Chapter 9, Principal Component Analysis and Factor 
Models, and Chapter 11, State-Space Models and Kalman Filter. The factor mod¬ 
els discussed include macroeconomic, fundamental, and statistical factor models. 
They are simple and powerful tools for analyzing high-dimensional financial data 
such as portfolio returns. Empirical examples are used to demonstrate the appli¬ 
cations. The state-space model and Kalman filter are added to demonstrate their 
applicability in finance and ease in computation. They are used in Chapter 12 to 
estimate stochastic volatility models under the general Markov chain Monte Carlo 
(MCMC) framework. The estimation also uses the technique of forward filtering 
and backward sampling to gain computational efficiency. 

A brief summary of the added material in the second edition is: 

1. To update the data used throughout the book. 

2. To provide S-Plus commands and demonstrations. 

xix 

XX 

PREFACE TO THE SECOND EDITION 

3. To consider unit-root tests and methods for consistent estimation of the 
covariance matrix in the presence of conditional heteroscedasticity and serial 
correlation in Chapter 2. 

4. To describe alternative approaches to volatility modeling, including use of 
high-frequency transactions data and daily high and low prices of an asset in 
Chapter 3. 

5. To give more applications of nonlinear models and methods in Chapter 4. 

6. To introduce additional concepts and applications of value at risk in 

Chapter 7. 

7. To discuss cointegrated vector AR models in Chapter 8. 

8. To cover various multivariate volatility models in Chapter 10. 

9. To add an effective MCMC method for estimating stochastic volatility models 

in Chapter 12. 

The revision benefits greatly from constructive comments of colleagues, friends, 
and many readers of the first edition. I am indebted to them all. In particular, I 
thank J. C. Artigas, Spencer Graves, Chung-Ming Kuan, Henry Lin, Daniel Pena, 
Jeff Russell, Michael Steele, George Tiao, Mark Wohar, Eric Zivot, and students 
of my MBA classes on financial time series for their comments and discussions 
and Rosalyn Farkas for editorial assistance. I also thank my wife and children for 
their unconditional support and encouragements. Part of my research in financial 
econometrics is supported by the National Science Foundation, the High-Frequency 
Finance Project of the Institute of Economics, Academia Sinica, and the Graduate 
School of Business, University of Chicago. 
Finally, the website for the book is: 

gsbwww.uchicago.edu/fac/ruey.tsay/teaching/fts2. 

Ruey S. Tsay 

University of Chicago 

Chicago, Illinois 

Preface to the First Edition 

This book grew out of an MBA course in analysis of financial time series that I have 

been teaching at the University of Chicago since 1999. It also covers materials of 

Ph.D. courses in time series analysis that I taught over the years. It is an introductory 

book intended to provide a comprehensive and systematic account of financial 

econometric models and their application to modeling and prediction of financial 

time series data. The goals are to learn basic characteristics of financial data, 

understand the application of financial econometric models, and gain experience in 
analyzing financial time series. 

The book will be useful as a text of time series analysis for MBA students with 

finance concentration or senior undergraduate and graduate students in business, 

economics, mathematics, and statistics who are interested in financial econometrics. 

The book is also a useful reference for researchers and practitioners in business, 

finance, and insurance facing value at risk calculation, volatility modeling, and 

analysis of serially correlated data. 

The distinctive features of this book include the combination of recent devel¬ 

opments in financial econometrics in the econometric and statistical literature. The 

developments discussed include the timely topics of value at risk (VaR), high- 

frequency data analysis, and Markov chain Monte Carlo (MCMC) methods. In 

particular, the book covers some recent results that are yet to appear in academic 

journals; see Chapter 6 on derivative pricing using jump diffusion with closed-form 

formulas, Chapter 7 on value at risk calculation using extreme value theory based on 

a nonhomogeneous two-dimensional Poisson process, and Chapter 9 on multivariate 

volatility models with time-varying correlations. MCMC methods are introduced 

because they are powerful and widely applicable in financial econometrics. These 

methods will be used extensively in the future. 

Another distinctive feature of this book is the emphasis on real examples and 

data analysis. Real financial data are used throughout the book to demonstrate 

applications of the models and methods discussed. The analysis is carried out by 

using several computer packages; the SCA (the Scientific Computing Associates) 

for building linear time series models, the RATS (regression analysis for time series) 

xxi 

xxii 

PREFACE TO THE FIRST EDITION 

for estimating volatility models, and the S-Plus for implementing neural networks 
and obtaining postscript plots. Some commands required to run these packages 
are given in appendixes of appropriate chapters. In particular, complicated RATS 
programs used to estimate multivariate volatility models are shown in Appendix 
A of Chapter 9. Some Fortran programs written by myself and others are used 
to price simple options, estimate extreme value models, calculate VaR, and carry 
out Bayesian analysis. Some data sets and programs are accessible from the World 
Wide Web at http://www.gsb.uchicago.edu/fac/ruey.tsay/teaching/fts. 

The book begins with some basic characteristics of financial time series data in 
Chapter 1. The other chapters are divided into three parts. The first part, consisting 
of Chapters 2 to 7, focuses on analysis and application of univariate financial time 
series. The second part of the book covers Chapters 8 and 9 and is concerned with 
the return series of multiple assets. The final part of the book is Chapter 10, which 
introduces Bayesian inference in finance via MCMC methods. 

A knowledge of basic statistical concepts is needed to fully understand the book. 
Throughout the chapters, I have provided a brief review of the necessary statistical 
concepts when they first appear. Even so, a prerequisite in statistics or business 
statistics that includes probability distributions and linear regression analysis is 
highly recommended. A knowledge of finance will be helpful in understanding the 
applications discussed throughout the book. However, readers with advanced back¬ 
ground in econometrics and statistics can find interesting and challenging topics in 
many areas of the book. 

An MBA course may consist of Chapters 2 and 3 as a core component, followed 
by some nonlinear methods (e.g., the neural network of Chapter 4 and the applica¬ 
tions discussed in Chapters 5-7 and 10). Readers who are interested in Bayesian 
inference may start with the first five sections of Chapter 10. 

Research in financial time series evolves rapidly and new results continue to 
appear regularly. Although I have attempted to provide broad coverage, there are 
many subjects that I do not cover or can only mention in passing. 

I sincerely thank my teacher and dear friend, George C. Tiao, for his guid¬ 
ance, encouragement, and deep conviction regarding statistical applications over the 
years. I am grateful to Steve Quigley, Heather Haselkorn, Leslie Galen, Danielle 
LaCouriere, and Amy Hendrickson for making the publication of this book pos¬ 
sible, to Richard Smith for sending me the estimation program of extreme value 
theory, to Bonnie K. Ray for helpful comments on several chapters, to Steve Kou 
for sending me his preprint on jump diffusion models, to Robert E. McCulloch for 
many years of collaboration on MCMC methods, to many students in my courses 
on analysis of financial time series for their feedback and inputs, and to Jeffrey 
Russell and Michael Zhang for insightful discussions concerning analysis of high- 
frequency financial data. To all these wonderful people I owe a deep sense of 
gratitude. I am also grateful for the support of the Graduate School of Business, 
University of Chicago and the National Science Foundation. Finally, my heartfelt 

PREFACE TO THE FIRST EDITION 

xxiii 

thanks to my wife, Teresa, for her continuous support, encouragement, and under¬ 
standing; to Julie, Richard, and Vicki for bringing me joy and inspirations; and to 
my parents for their love and care. 

R. S. T. 

Chicago, Illinois 

. 

CHAPTER 1 

Financial Time Series 
and Their Characteristics 

Financial time series analysis is concerned with the theory and practice of asset 
valuation over time. It is a highly empirical discipline, but like other scientific 
fields theory forms the foundation for making inference. There is, however, a 
key feature that distinguishes financial time series analysis from other time series 
analysis. Both financial theory and its empirical time series contain an element of 
uncertainty. For example, there are various definitions of asset volatility, and for a 
stock return series, the volatility is not directly observable. As a result of the added 
uncertainty, statistical theory and methods play an important role in financial time 
series analysis. 

The objective of this book is to provide some knowledge of financial time 
series, introduce some statistical tools useful for analyzing these series, and gain 
experience in financial applications of various econometric methods. We begin 
with the basic concepts of asset returns and a brief introduction to the processes 
to be discussed throughout the book. Chapter 2 reviews basic concepts of linear 
time series analysis such as stationarity and autocorrelation function, introduces 
simple linear models for handling serial dependence of the series, and discusses 
regression models with time series errors, seasonality, unit-root nonstationarity, and 
long-memory processes. The chapter also provides methods for consistent estima¬ 
tion of the covariance matrix in the presence of conditional heteroscedasticity and 
serial correlations. Chapter 3 focuses on modeling conditional heteroscedasticity 
(i.e., the conditional variance of an asset return). It discusses various econometric 
models developed recently to describe the evolution of volatility of an asset return 
over time. The chapter also discusses alternative methods to volatility modeling, 
including use of high-frequency transactions data and daily high and low prices of 
an asset. In Chapter 4, we address nonlinearity in financial time series, introduce 
test statistics that can discriminate nonlinear series from linear ones, and discuss 
several nonlinear models. The chapter also introduces nonparametric estimation 

Analysis of Financial Time Series, Third Edition, By Ruey S. Tsay 
Copyright © 2010 John Wiley & Sons, Inc. 

1 

2 

FINANCIAL TIME SERIES AND THEIR CHARACTERISTICS 

methods and neural networks and shows various applications of nonlinear models 
in finance. Chapter 5 is concerned with analysis of high-frequency financial data, the 
effects of market microstructure, and some applications of high-frequency finance. 
It shows that nonsynchronous trading and bid-ask bounce can introduce serial cor¬ 
relations in a stock return. It also studies the dynamic of time duration between 
trades and some econometric models for analyzing transactions data. In Chapter 6, 
we introduce continuous-time diffusion models and Ito’s lemma. Black-Scholes 
option pricing formulas are derived, and a simple jump diffusion model is used 
to capture some characteristics commonly observed in options markets. Chapter 7 
discusses extreme value theory, heavy-tailed distributions, and their application to 
financial risk management. In particular, it discusses various methods for calculat¬ 
ing value at risk and expected shortfall of a financial position. Chapter 8 focuses 
on multivariate time series analysis and simple multivariate models with empha¬ 
sis on the lead-lag relationship between time series. The chapter also introduces 
cointegration, some cointegration tests, and threshold cointegration and applies the 
concept of cointegration to investigate arbitrage opportunity in financial markets, 
including pairs trading. Chapter 9 discusses ways to simplify the dynamic struc¬ 
ture of a multivariate series and methods to reduce the dimension. It introduces 
and demonstrates three types of factor model to analyze returns of multiple assets. 
In Chapter 10, we introduce multivariate volatility models, including those with 
time-varying correlations, and discuss methods that can be used to reparameterize 
a conditional covariance matrix to satisfy the positiveness constraint and reduce the 
complexity in volatility modeling. Chapter 11 introduces state-space models and 
the Kalman filter and discusses the relationship between state-space models and 
other econometric models discussed in the book. It also gives several examples 
of financial applications. Finally, in Chapter 12, we introduce some Markov chain 
Monte Carlo (MCMC) methods developed in the statistical literature and apply 
these methods to various financial research problems, such as the estimation of 
stochastic volatility and Markov switching models. 

The book places great emphasis on application and empirical data analysis. 
Every chapter contains real examples and, in many occasions, empirical character¬ 
istics of financial time series are used to motivate the development of econometric 
models. Computer programs and commands used in data analysis are provided 
when needed. In some cases, the programs are given in an appendix. Many real 
data sets are also used in the exercises of each chapter. 

1.1 ASSET RETURNS 

Most financial studies involve returns, instead of prices, of assets. Campbell, Lo, 
and MacKinlay (1997) give two main reasons for using returns. First, for average 
investors, return of an asset is a complete and scale-free summary of the investment 
opportunity. Second, return series are easier to handle than price series because 
the former have more attractive statistical properties. There are, however, several 
definitions of an asset return. 

ASSET RETURNS 

3 

Let P, be the price of an asset at time index t. We discuss some definitions of 
returns that are used throughout the book. Assume for the moment that the asset 
pays no dividends. 

One-Period Simple Return 
Holding the asset for one period from date T — 1 to date t would result in a simple 
gross return: 

1 + Rt 

Pt 

P,-1 

or Pt = Pf_!(l + Rt). 

(1.1) 

The corresponding one-period simple net return or simple return is 

Rt = 

P, ~ Pt-1 

Pt-1 

(L2) 

Multiperiod Simple Return 
Holding the asset for k periods between dates t — k and t gives a k-period simple 
gross return: 

1 + Rt[k] = 

Pt 

Pt 

Pt-k 

Pt-1 

x 

Pt-1 

Pt-2 

X - - - X 

Pt-k+l 

Pt-k 

k-1 

7=0 

Thus, the Auperiod simple gross return is just the product of the k one-period simple 
gross returns involved. This is called a compound return. The k-period simple net 

return is /?,[£] = (Pt - Pt-k)/Pt-k- 

In practice, the actual time interval is important in discussing and comparing 
returns (e.g., monthly return or annual return). If the time interval is not given, 
then it is implicitly assumed to be one year. If the asset was held for k years, then 
the annualized (average) return is defined as 

Annualized {/?, [k]} 

k-1 
IT1+ *<-,) 

7=0 

1 /k 

- 1. 

This is a geometric mean of the k one-period simple gross returns involved and 
can be computed by 

Annualized {/?f[k]} = exp 

1 

k 

k-1 

X>0 + *<-;) 
7=0 

1 

4 

FINANCIAL TIME SERIES AND THEIR CHARACTERISTICS 

where exp(x) denotes the exponential function and ln(x) is the natural logarithm 
of the positive number x. Because it is easier to compute arithmetic average than 
geometric mean and the one-period returns tend to be small, one can use a first-order 
Taylor expansion to approximate the annualized return and obtain 

Annualized {/?,[£]} ~ - £*<-;• <>-3) 

l k~l 

/v 

7=0 

Accuracy of the approximation in Eq. (1.3) may not be sufficient in some applica¬ 
tions, however. 

Continuous Compounding 
Before introducing continuously compounded return, we discuss the effect of com¬ 
pounding. Assume that the interest rate of a bank deposit is 10% per annum and 
the initial deposit is $1.00. If the bank pays interest once a year, then the net value 
of the deposit becomes $1(1 + 0.1) = $1.1 one year later. If the bank pays inter¬ 
est semiannually, the 6-month interest rate is 10%/2 = 5% and the net value is 
$1(1 + 0.1/2)2 = $1.1025 after the first year. In general, if the bank pays interest 
m times a year, then the interest rate for each payment is 10%/m and the net value 
of the deposit becomes $1(1 + 0.1/m)m one year later. Table 1.1 gives the results 
for some commonly used time intervals on a deposit of $ 1.00 with interest rate of 
10% per annum. In particular, the net value approaches $1.1052, which is obtained 
by exp(O.l) and referred to as the result of continuous compounding. The effect of 
compounding is clearly seen. 

In general, the net asset value A of continuous compounding is 

A — C exp(r x n), (1.4) 

where r is the interest rate per annum, C is the initial capital, and n is the number 
of years. From Eq. (1.4), we have 

C = A exp(-r x n), (1.5) 

TABLE 1.1 Illustration of Effects of Compounding: Time Interval Is 1 Year and 
Interest Rate Is 10% per Annum 

Type 

Number of Payments 

Interest Rate per Period 

Net Value 

Annual 
Semiannual 
Quarterly 
Monthly 
Weekly 
Daily 
Continuously 

1 
2 
4 
12 
52 
365 
oo 

0.1 
0.05 
0.025 
0.0083 
0.1/52 
0.1/365 

$1.10000 
$1.10250 
$1.10381 
$1.10471 
$1.10506 
$1.10516 
$1.10517 

ASSET RETURNS 

5 

which is referred to as the present value of an asset that is worth A dollars n 
years from now, assuming that the continuously compounded interest rate is r per 
annum. 

Continuously Compounded Return 
The natural logarithm of the simple gross return of an asset is called the continu¬ 
ously compounded return or log return: 

rt = ln(l + Rt) = In -p- = pt - pt-U (1.6) 

n-i 

where pt = ln(P;). Continuously compounded returns rt enjoy some advantages 
over the simple net returns Rt. First, consider multiperiod returns. We have 

rt[k] = ln(l + Rt[k]) = ln[(l + Rt)(l + Rt^) • • •(1 + Rt-k+1)] 

= ln(l + Rt) + ln(l + Rt-i) + • • • + ln(l + Rt-k+1) 

= rt + rt-1 + • • • + rt-k+1- 

Thus, the continuously compounded multiperiod return is simply the sum of con¬ 
tinuously compounded one-period returns involved. Second, statistical properties 
of log returns are more tractable. 

Portfolio Return 
The simple net return of a portfolio consisting of N assets is a weighted average 
of the simple net returns of the assets involved, where the weight on each asset is 
the percentage of the portfolio’s value invested in that asset. Let p be a portfolio 
that places weight u;, on asset i. Then the simple return of p at time t is Rp t = 

XlfLi wiRit> where Ru is the simple return of asset i. 

The continuously compounded returns of a portfolio, however, do not have the 
above convenient property. If the simple returns Rit are all small in magnitude, then 
we have rPit & wiriti where rp t is the continuously compounded return of 
the portfolio at time t. This approximation is often used to study portfolio returns. 

Dividend Payment 
If an asset pays dividends periodically, we must modify the definitions of asset 
returns. Let Dt be the dividend payment of an asset between dates t — 1 and t and Pt 
be the price of the asset at the end of period t. Thus, dividend is not included in P,. 
Then the simple net return and continuously compounded return at time t become 

Rt = P-p+ Dt~ - 1, rt = In(Pt + Dt) - ln(Pf_i). 

Pt-1 

Excess Return 
Excess return of an asset at time t is the difference between the asset’s return and 
the return on some reference asset. The reference asset is often taken to be riskless 

6 

FINANCIAL TIME SERIES AND THEIR CHARACTERISTICS 

such as a short-term U.S. Treasury bill return. The simple excess return and log 
excess return of an asset are then defined as 

Z, = R, - R0t, Zt — rt — rot, (1-7) 

where Rot and rot are the simple and log returns of the reference asset, respectively. 
In the finance literature, the excess return is thought of as the payoff on an arbitrage 
portfolio that goes long in an asset and short in the reference asset with no net 
initial investment. 

Remark. A long financial position means owning the asset. A short position 
involves selling an asset one does not own. This is accomplished by borrowing the 
asset from an investor who has purchased it. At some subsequent date, the short 
seller is obligated to buy exactly the same number of shares borrowed to pay back 
the lender. Because the repayment requires equal shares rather than equal dollars, 
the short seller benefits from a decline in the price of the asset. If cash dividends are 
paid on the asset while a short position is maintained, these are paid to the buyer 
of the short sale. The short seller must also compensate the lender by matching 
the cash dividends from his own resources. In other words, the short seller is also 
obligated to pay cash dividends on the borrowed asset to the lender. □ 

Summary of Relationship 
The relationships between simple return Rt and continuously compounded (or log) 
return rt are 

rt — ln(l + Rt), R, = en - 1. 

If the returns R, and rt are in percentages, then 

rt = 100 In (l + ^ , Rt = 100 (er'/10° - l) . 

Temporal aggregation of the returns produces 

1 + Rt[k] — (1 + Rt)( 1 + Rt-1) •••(! + Rt-k+l), 

rt[k] — rt + r;_i 4- • • • + rt-k+\- 

If the continuously compounded interest rate is r per annum, then the relationship 
between present and future values of an asset is 

A — C exp(r x n), C = A exp(—r x n). 

Example 1.1. If the monthly log return of an asset is 4.46%, then the corre¬ 
sponding monthly simple return is 100[exp(4.46/100) - 1] = 4.56%. Also, if the 
monthly log returns of the asset within a quarter are 4.46%, -7.34%, and 10.77%, 
respectively, then the quarterly log return of the asset is (4.46 - 7.34 + 10.77)% = 
7.89%. 

DISTRIBUTIONAL PROPERTIES OF RETURNS 

7 

1.2 DISTRIBUTIONAL PROPERTIES OF RETURNS 

To study asset returns, it is best to begin with their distributional properties. 
The objective here is to understand the behavior of the returns across assets 
and over time. Consider a collection of N assets held for T time periods, say, 
t = 1,..., T. For each asset i, let r,-? be its log return at time t. The log returns 
under study are {r,,; i = 1,..., N\ t = 1,..., T). One can also consider the sim¬ 
ple returns i = 1,,N\t = 1,..., T} and the log excess returns {z,r; i = 

1.2.1 Review of Statistical Distributions and Their Moments 

We briefly review some basic properties of statistical distributions and the 
moment equations of a random variable. Let Rk be the ^-dimensional Euclidean 
space. A point in Rk is denoted by jc e Rk. Consider two random vectors 
X = (Xu ..., XkY and Y = (Yu Yq)'. Let P(X e A, Y e B) be the proba¬ 
bility that X is in the subspace A C Rk and Y is in the subspace B C Rq. For 
most of the cases considered in this book, both random vectors are assumed to be 
continuous. 

Joint Distribution 
The function 

Fx,y(x, y; 0) = P(X <x,Y < y, 0), 

where x e Rp, y e Rq, and the inequality < is a component-by-component oper¬ 
ation, is a joint distribution function of X and Y with parameter 0. Behavior of X 
and Y is characterized by Fxj(x, y; 0). If the joint probability density function 
fx,y{x, y \ 0) of X and Y exists, then 

Fx,y(x, y; 0) = f f fx,y(w, z; 0) dz dw. 

J—oo J — oo 

In this case, X and Y are continuous random vectors. 

Marginal Distribution 
The marginal distribution of X is given by 

Fx(x; 0) = FXj(x, oo, • • •, oo; 0). 

Thus, the marginal distribution of X is obtained by integrating out Y. A similar 
definition applies to the marginal distribution of Y. 

If k = 1, X is a scalar random variable and the distribution function becomes 

Fx(x) = P(X <x-0), 

8 

FINANCIAL TIME SERIES AND THEIR CHARACTERISTICS 

which is known as the cumulative distribution function (CDF) of X. The CDF of a 
random variable is nondecreasing [i.e., Fx(x\) < Fx(x2) if x\ < X2] and satisfies 
Fx(—oo) = 0 and F*(oo) = 1. For a given probability p, the smallest real number 
xp such that p < Fx(xp) is called the lOOpth quantile of the random variable X. 

More specifically, 

xp = inf {x\p < Fx(a)}. 

X 

We use the CDF to compute the p value of a test statistic in the book. 

Conditional Distribution 
The conditional distribution of X given Y < y is given by 

Fx\Y<y(x\ 9) 

P(X < x,Y < y, 9) 

P( Y < y, 0 ) 

If the probability density functions involved exist, then the conditional density of 
X given Y — y is 

fx\y(x- 9) 

fx,y(x, y; 9) 

fy(y\°) 

(1.8) 

where the marginal density function fy(y, 9) is obtained by 

/OO 

fx,y(x, y\9)dx. 

From Eq. (1.8), the relation among joint, marginal, and conditional distributions is 

-OO 

fx,y(x, y; 9) = fx|y(x; 9) x fy(y\9). (1.9) 

This identity is used extensively in time series analysis (e.g., in maximum- 
likelihood estimation). Finally, X and Y are independent random vectors if and 
only if fx\y{x\ 9) = fx(x\ 9). In this case, fx,y(x, y; 9) = fx(x; 9)fy(y, 9). 

Moments of a Random Variable 
The fth moment of a continuous random variable X is defined as 

/OO 

xef(x)dx, 

-OO 

where E stands for expectation and f(x) is the probability density function of X. 
The first moment is called the mean or expectation of X. It measures the central 
location of the distribution. We denote the mean of X by /it. The fth central 
moment of X is defined as 

mt = E[(X - nxY] = 

(x - px)1 f (x) dx 

DISTRIBUTIONAL PROPERTIES OF RETURNS 

9 

provided that the integral exists. The second central moment, denoted by cr^, mea¬ 
sures the variability of X and is called the variance of X. The positive square root, 
0x, of variance is the standard deviation of X. The first two moments of a random 
variable uniquely determine a normal distribution. For other distributions, higher 
order moments are also of interest. 

The third central moment measures the symmetry of X with respect to its mean, 
whereas the fourth central moment measures the tail behavior of X. In statistics, 
skewness and kurtosis, which are normalized third and fourth central moments 
of X, are often used to summarize the extent of asymmetry and tail thickness. 
Specifically, the skewness and kurtosis of X are defined as 

S(x) = E 

(X-fxx) 3l 

Or 

K(x) - E 

(X - nx) 4 ~l 

cr: 

The quantity K(x) — 3 is called the excess kurtosis because K(x) = 3 for a nor¬ 
mal distribution. Thus, the excess kurtosis of a normal random variable is zero. 
A distribution with positive excess kurtosis is said to have heavy tails, implying 
that the distribution puts more mass on the tails of its support than a normal distri¬ 
bution does. In practice, this means that a random sample from such a distribution 
tends to contain more extreme values. Such a distribution is said to be leptokur- 
tic. On the other hand, a distribution with negative excess kurtosis has short tails 
(e.g., a uniform distribution over a finite interval). Such a distribution is said to be 
platykurtic. 

In application, skewness and kurtosis can be estimated by their sample counter¬ 
parts. Let {x\,..., Xj) be a random sample of X with T observations. The sample 
mean is 

the sample variance is 

i=l 

-2 
°x  = yz\ - ^)2> 
j=i 

the sample skewness is 

S{x) = 

(T - l)ffr3 
v ' x r=i 

~ Ax)3, 

and the sample kurtosis is 

K(x) = 

1 

(T - 1)5- ■4 £<*< - v,)1 

i=i 

(1.10) 

(1.11) 

(1.12) 

(1.13) 

10 

FINANCIAL TIME SERIES AND THEIR CHARACTERISTICS 

Under the normality assumption, S(x) and K(x) — 3 are distributed asymptoti¬ 
cally as normal with zero mean and variances 6/T and 24/ T, respectively; see 
Snedecor and Cochran (1980, p. 78). These asymptotic properties can be used to 
test the normality of asset returns. Given an asset return series {r\, ...,rr), to 
test the skewness of the returns, we consider the null hypothesis Ho : S(r) = 0 
versus the alternative hypothesis Ha : S(r) ^ 0. The t-ratio statistic of the sample 
skewness in Eq. (1.12) is 

f_ Sjr) 

vw 

The decision rule is as follows. Reject the null hypothesis at the a significance 
level, if |t| > Za/2, where Za/2 is the upper 100(a/2)th quantile of the standard 
normal distribution. Alternatively, one can compute the p value of the test statistic 
t and reject Hq if and only if the p value is less than a. 

Similarly, one can test the excess kurtosis of the return series using the hypothe¬ 

ses Hq : K (r) — 3 = 0 versus Ha : K(r) — 3^0. The test statistic is 

Kir)-3 

J24/T ' 

which is asymptotically a standard normal random variable. The decision rule is to 
reject Hq if and only if the p value of the test statistic is less than the significance 
level a. Jarque and Bera (1987) (JB) combine the two prior tests and use the test 
statistic 

jB = 

6/T + 24/ T ' 

which is asymptotically distributed as a chi-squared random variable with 2 degrees 
of freedom, to test for the normality of rt. One rejects H0 of normality if the p 
value of the JB statistic is less than the significance level. 

Example 1.2. Consider the daily simple returns of the International Business 
Machines (IBM) stock used in Table 1.2. The sample skewness and kurtosis of 
the returns are parts of the descriptive (or summary) statistics that can be obtained 
easily using various statistical software packages. Both R and S-Plus are used in 
the demonstration, where d-ibm3dx7008.txt is the data file name. Note that in 
R the kurtosis denotes excess kurtosis. From the output, the excess kurtosis is high, 
indicating that the daily simple returns of IBM stock have heavy tails. To test the 
symmetry of return distribution, we use the test statistic 

0.0614 0.0614 

t - — =-= 2.49 

V6/9845 0.0247 

which gives a p value of about 0.013, indicating that the daily simple returns of 
IBM stock are significantly skewed to the right at the 5% level. 

DISTRIBUTIONAL PROPERTIES OF RETURNS 

11 

TABLE 1.2 Descriptive Statistics for Daily and Monthly Simple and Log Returns of 
Selected Indexes and Stocks'2 

Security 

Start 

Size  Mean 

Standard 
Deviation  Skewness 

Excess 
Kurtosis  Minimum  Maximum 

Daily Simple Returns (%) 

SP 
70/01/02  9845 
VW 
70/01/02  9845  0.040 
EW 
70/01/02  9845  0.076 
IBM 
70/01/02  9845  0.040 
Intel 
72/12/15  9096  0.108 
3M 
670/01/02  9845  0.045 
Microsoft  86/03/14  5752  0.123 
Citi-Grp 
86/10/30  5592  0.067 

0.029 1.056 -0.73 
-0.62 
1.004 
0.814 
-0.77 
1.693 
0.06 
-0.15 
2.891 
1.482 
-0.36 
-0.13 
2.359 
1.80 
2.602 

Daily Log Returns (%) 

SP 
70/01/02  9845 
VW 
70/01/02  9845  0.035 
EW 
70/01/02  9845  0.072 
IBM 
70/01/02  9845  0.026 
72/12/15  9096  0.066 
Intel 
3M 
70/01/02  9845  0.034 
Microsoft  86/03/14  5752  0.095 
86/10/30  5592  0.033 
Citi-Grp 

0.023 1.062 -1.17 
-0.94 
1.008 
-1.00 
0.816 
1.694 
-0.27 
-0.54 
2.905 
1.488 
-0.78 
-0.63 
2.369 
0.22 
2.575 

Monthly Simple Returns (%) 

SP 
VW 
EW 
IBM 
Intel 
3M 
Microsoft 
Citi-Grp 

SP 
VW 
EW 
IBM 
Intel 
3M 
Microsoft 
Citi-Grp 

26/01 
26/01 
26/01 
26/01 
73/01 
46/02 
86/04 
86/11 

26/01 
26/01 
26/01 
26/01 
73/01 
46/02 
86/04 
86/11 

996 
996  0.89 
996  1.22 
996  1.35 
432  2.21 
755  1.24 
273  2.62 
266  1.17 

0.58 5.53 0.32 
0.15 
1.52 
0.44 
0.32 
0.22 
0.66 
-0.47 

5.43 
7.40 
7.15 
12.85 
6.45 
11.08 
9.75 

Monthly Log Returns (%) 

996 
996  0.74 
996  0.96 
996  1.09 
432  1.39 
755  1.03 
273  2.01 
266  0.68 

0.43 5.54 -0.52 
-0.58 
0.25 
-0.07 
-0.55 
-0.08 
0.10 
-1.09 

5.43 
7.14 
7.03 
12.80 
6.37 
10.66 
10.09 

22.81 
18.02 
17.08 
9.92 
6.13 
13.34 
9.92 
55.25 

30.20 
21.56 
17.76 
12.17 
7.81 
20.57 
14.23 
33.19 

9.47 
7.69 
14.94 
3.43 
2.70 
0.98 
1.96 
1.77 

7.93 
6.85 
8.55 
2.62 
3.06 
1.25 
1.59 
3.76 

-20.47 
-17.13 
-10.39 
-22.96 
-29.57 
-25.98 
-30.12 
-26.41 

-22.90 
-18.80 
-10.97 
-26.09 
-35.06 
-30.08 
-35.83 
-30.66 

-29.94 
-29.01 
-31.28 
-26.19 
-44.87 
-27.83 
-34.35 
-39.27 

-35.58 
-34.22 
-37.51 
-30.37 
-59.54 
-32.61 
-42.09 
-49.87 

11.58 
11.52 
10.74 
13.16 
26.38 
11.54 
19.57 
57.82 

10.96 
10.90 
10.20 
12.37 
23.41 
10.92 
17.87 
45.63 

42.22 
38.37 
66.59 
47.06 
62.50 
25.80 
51.55 
26.08 

35.22 
32.47 
51.04 
38.57 
48.55 
22.95 
41.58 
23.18 

“Returns are in percentages and the sample period ends on December 31, 2008. The statistics are defined 

in eqs. (1.10)—(1.13), and VW, EW and SP denote value-weighted, equal-weighted, and S&P composite 

index. 

12 

FINANCIAL TIME SERIES AND THEIR CHARACTERISTICS 

R Demonstration 

In the following program code > is the prompt character and % denotes explana¬ 

tion: 

> library(fBasics) % Load the package fBasics. 
> da=read.table("d-ibm3dx7008.txt",header=T) % Load the data. 
% header=T means 1st row of the data file contains 
% variable names. The default is header=F, i.e., no names. 

> dim(da) % Find size of the data: 9845 rows and 5 columns. 
[1] 9845 5 
> da[l,] % See the first row of the data 

Date rtn vwretd ewretd sprtrn % column names 

1 19700102 0.000686 0.012137 0.03345 0.010211 

> ibm=da[,2] % Obtain IBM simple returns 
> sibm=ibm*100 % Percentage simple returns 

> basicStats(sibm) % Compute the summary statistics 

nobs 
NAs 
Minimum 
Maximum 
1. Quartile 
3. Quartile 
Mean 
Median 
Sum 
SE Mean 
LCL Mean 

sibm 

9845.000000  %  Sample size 

0.000000  %  Number of missing values 

-22.963000 
13.163600 
-0.857100  %  25th percentile 
0.883300  %  75th percentile 
0.040161  %  Sample mean 
0.000000  %  Sample median 

395.387600  %  Sum of the percentage simple returns 
0.017058  %  Standard error of the sample mean 
0.006724  %  Lower bound of 95% conf. 

UCL Mean 

0.073599  %  Upper bound of 95% conf. 

%  interval for mean 

Variance 
St dev 
Skewness 
Kurtosis 

%  interval for mean 

2.864705  %  Sample variance 
1.692544  %  Sample standard error 
0.061399  %  Sample skewness 
9.916359  %  Sample excess kurtosis. 

% Alternatively, one can use individual commands as follows: 
> mean(sibm) 
[1] 0.04016126 
> var(sibm) 
[1] 2.864705 

> sqrt(var(sibm)) % Standard deviation 
[1] 1.692544 
> skewness(sibm) 
[1] 0.06139878 
attr(,"method") 

DISTRIBUTIONAL PROPERTIES OF RETURNS 

13 

[1] "moment" 
> kurtosis(sibm) 
[1] 9.91636 
attr(,"method") 
[1] "excess" 

% Simple tests 
> sl=skewness(sibm) 

> tl=sl/sqrt(6/9845) % Compute test statistic 
> tl 
[1] 2.487093 

> pv=2*(l-pnorm(tl)) % Compute p-value. 
> pv 
[1] 0.01287919 

% Turn to log returns in percentages 
> libm=log(ibm+1)*100 
> t.test(libm) % Test mean being zero. 

One Sample t-test 

data: libm 
t = 1.5126, df = 9844, p-value = 0.1304 
alternative hypothesis: true mean is not equal to 0 
95 percent confidence interval: 

-0.007641473 0.059290531 

% The result shows that the hypothesis of zero expected return 
% cannot be rejected at the 5% or 10% level. 

> normalTest(libm,method='jb') % Normality test 
Title: 

Jarque - Bera Normality Test 

Test Results: 
STATISTIC: 

X-squared: 60921.9343 

P VALUE: 

Asymptotic p Value: < 2.2e-16 

% The result shows the normality for log-return is rejected. 

S-Plus Demonstration 
In the following program code > is the prompt character and % marks explanation: 

> module(finmetrics) % Load the Finmetrics module. 
> da=read.table("d-ibm3dx7008.txt",header=T) % Load data. 
> dim(da) % Obtain the size of the data set. 

[1] 9845 

> da[1,] 

5 
% See the first row of the data 

Date 
1 19700102 

rtn vwretd ewretd sprtrn 
0.000686 0.012137 0.03345 0.010211 

14 

FINANCIAL TIME SERIES AND THEIR CHARACTERISTICS 

> sibm=da[,2]*100 % Obtain percentage simple returns of 

> summaryStats(sibm) % Obtain summary statistics 

% IBM stock. 

Sample Quantiles: 

min IQ median 3Q max 
-22.96 -0.8571 0 0.8833 13.16 

Sample Moments: 

mean std skewness kurtosis 
0.04016 1.693 0.06141 12.92 

Number of Observations: 9845 
% simple tests 
> sl=skewness(sibm) % Compute skewness 
> t=sl/sqrt(6/9845) % Perform test of skewness 
> t 
[1] 2.487851 
> pv=2*(l-pnorm(t)) % Calculate p-value. 
> pv 
[1] 0.01285177 

> libm=log(da[,2]+1)*100 % Turn to log-return 
> t.test(libm) % Test expected return being zero. 

One-sample t-Test 

data: libm 
t = 1.5126, df = 9844, p-value = 0.1304 
alternative hypothesis: mean is not equal to 0 
95 percent confidence interval: 

-0.007641473 0.059290531 

> normalTest(libm,method^'jb') % Normality test 
Test for Normality: Jarque-Bera 
Null Hypothesis: data is normally distributed 

Test Stat 60921.93 
p.value 0.00 

Dist. under Null: chi-square with 2 degrees of freedom 

Total Observ.: 9845 

Remark. In S-Plus, kurtosis is the regular kurtosis, not excess kurtosis. That 
is, S-Plus does not subtract 3 from the sample kurtosis. Also, in many cases R and 
S-Plus use the same commands. □ 

1.2.2 Distributions of Returns 

The most general model for the log returns {rit; i = 1,..., N; t = 1,..., T} is its 
joint distribution function: 

Fr(rn,...,rm-,ri2,..., ry 2', ■ ■ ■; Hr, • • • - rNT', Y;0), 

(1.14) 

DISTRIBUTIONAL PROPERTIES OF RETURNS 

15 

where Y is a state vector consisting of variables that summarize the environment 
in which asset returns are determined and 0 is a vector of parameters that uniquely 
determines the distribution function Fr(-). The probability distribution />(•) gov¬ 
erns the stochastic behavior of the returns rif and Y. In many financial studies, the 
state vector Y is treated as given and the main concern is the conditional distri¬ 
bution of {r,-,} given Y. Empirical analysis of asset returns is then to estimate the 
unknown parameter 0 and to draw statistical inference about the behavior of [rlt} 
given some past log returns. 

The model in Eq. (1.14) is too general to be of practical value. However, it 
provides a general framework with respect to which an econometric model for 
asset returns r;-f can be put in a proper perspective. 

Some financial theories such as the capital asset pricing model (CAPM) of 
Sharpe (1964) focus on the joint distribution of N returns at a single time index 
t (i.e., the distribution of {r\t,..., r^}). Other theories emphasize the dynamic 
structure of individual asset returns (i.e., the distribution of {r£ i,..., rjT} for a 
given asset i). In this book, we focus on both. In the univariate analysis of Chapters 
2-7, our main concern is the joint distribution of {rn}J=l for asset i. To this end, 
it is useful to partition the joint distribution as 

F(n i, ..., riT; 0) = F(rn)F(ri2\rn) ■ ■ ■ F(riT\ritT-i,rn) 

T 

= F(rn) ]~[ F(rit\ri>t-\, ..., rn), (1.15) 

t—i 

where, for simplicity, the parameter 0 is omitted. This partition highlights the 
temporal dependencies of the log return r,f. The main issue then is the specification 
of the conditional distribution F(r;',|r; r_i, •), in particular, how the conditional 
distribution evolves over time. In finance, different distributional specifications 
lead to different theories. For instance, one version of the random-walk hypothesis 
is that the conditional distribution F(ru..., ra) is equal to the marginal 
distribution F(rit). In this case, returns are temporally independent and, hence, not 
predictable. 

It is customary to treat asset returns as continuous random variables, especially 
for index returns or stock returns calculated at a low frequency, and use their 
probability density functions. In this case, using the identity in Eq. (1.9), we can 
write the partition in Eq. (1.15) as 

f(rn, ...,riT\0) = f(rn]0)Y\f(rit\rij-i, ...,rn;0). 

(1.16) 

t=2 

For high-frequency asset returns, discreteness becomes an issue. For example, stock 
prices change in multiples of a tick size on the New York Stock Exchange (NYSE). 
The tick size was | of a dollar before July 1997 and was of a dollar from July 
1997 to January 2001. Therefore, the tick-by-tick return of an individual stock listed 

16 

FINANCIAL TIME SERIES AND THEIR CHARACTERISTICS 

on the NYSE is not continuous. We discuss high-frequency stock price changes 
and time durations between price changes later in Chapter 5. 

Remark. On August 28, 2000, the NYSE began a pilot program with 7 stocks 
priced in decimals and the American Stock Exchange (AMEX) began a pilot pro¬ 
gram with 6 stocks and two options classes. The NYSE added 57 stocks and 94 
stocks to the program on September 25 and December 4, 2000, respectively. All 
NYSE and AMEX stocks started trading in decimals on January 29, 2001. □ 

Equation (1.16) suggests that conditional distributions are more relevant than 
marginal distributions in studying asset returns. However, the marginal distributions 
may still be of some interest. In particular, it is easier to estimate marginal distribu¬ 
tions than conditional distributions using past returns. In addition, in some cases, 
asset returns have weak empirical serial correlations, and, hence, their marginal 
distributions are close to their conditional distributions. 

Several statistical distributions have been proposed in the literature for the 
marginal distributions of asset returns, including normal distribution, lognormal dis¬ 
tribution, stable distribution, and scale mixture of normal distributions. We briefly 
discuss these distributions. 

Normal Distribution 

A traditional assumption made in financial study is that the simple returns {/?,7|t = 
1, • • •, T) are independently and identically distributed as normal with fixed mean 
and variance. This assumption makes statistical properties of asset returns tractable. 
But it encounters several difficulties. First, the lower bound of a simple return is 
— 1. Yet the normal distribution may assume any value in the real line and, hence, 
has no lower bound. Second, if Rit is normally distributed, then the multiperiod 
simple return Rit[k] is not normally distributed because it is a product of one-period 
returns. Third, the normality assumption is not supported by many empirical asset 
returns, which tend to have a positive excess kurtosis. 

Lognormal Distribution 

Another commonly used assumption is that the log returns rt of an asset are inde¬ 
pendent and identically distributed (iid) as normal with mean /x and variance a2. 
The simple returns are then iid lognormal random variables with mean and variance 
given by 

Var(Rt) = exp(2;U + a2)[exp(a2) - 1], (1.17) 

These two equations are useful in studying asset returns (e.g., in forecasting using 
models built for log returns). Alternatively, let m\ and m2 be the mean and variance 
of the simple return Rt, which is lognormally distributed. Then the mean and 

DISTRIBUTIONAL PROPERTIES OF RETURNS 

17 

variance of the corresponding log return rt are 

E(rt) = In 

m i + 1 

-y/l + m2/( 1 + mi)2 

Var(r/) = In 

m2 

(1 + mi)2 

Because the sum of a finite number of iid normal random variables is normal, 
rt[k] is also normally distributed under the normal assumption for {rt}. In addition, 
there is no lower bound for rt, and the lower bound for Rr is satisfied using 
1 + Rt = exp(r,). However, the lognormal assumption is not consistent with all 
the properties of historical stock returns. In particular, many stock returns exhibit 
a positive excess kurtosis. 

Stable Distribution 
The stable distributions are a natural generalization of normal in that they are sta¬ 
ble under addition, which meets the need of continuously compounded returns rt. 
Furthermore, stable distributions are capable of capturing excess kurtosis shown 
by historical stock returns. However, nonnormal stable distributions do not have 
a finite variance, which is in conflict with most finance theories. In addition, sta¬ 
tistical modeling using nonnormal stable distributions is difficult. An example of 
nonnormal stable distributions is the Cauchy distribution, which is symmetric with 
respect to its median but has infinite variance. 

Scale Mixture of Normal Distributions 
Recent studies of stock returns tend to use scale mixture or finite mixture of normal 
distributions. Under the assumption of scale mixture of normal distributions, the log 
return rt is normally distributed with mean /x and variance a2 [i.e., rt ~ N(/i, cr2)]. 
However, a2 is a random variable that follows a positive distribution (e.g., o~2 
follows a gamma distribution). An example of finite mixture of normal distribu¬ 
tions is 

A ~ (1 - X)N(fJi, a2) + XN(vl, or2), 

where X is a Bernoulli random variable such that P{X — 1) = a and P(X = 0) = 
1 — a with 0 < a < 1, af is small, and ct22 is relatively large. For instance, with 
a = 0.05, the finite mixture says that 95% of the returns follow N(fi, oj2) and 5% 
follow N (ji, o|). The large value of o\ enables the mixture to put more mass at the 
tails of its distribution. The low percentage of returns that are from A(/x, cr2) says 
that the majority of the returns follow a simple normal distribution. Advantages 
of mixtures of normal include that they maintain the tractability of normal, have 
finite higher order moments, and can capture the excess kurtosis. Yet it is hard to 
estimate the mixture parameters (e.g., the a in the finite-mixture case). 

Figure 1.1 shows the probability density functions of a finite mixture of normal, 
Cauchy, and standard normal random variable. The finite mixture of normal is 
(1 — X)N(0, 1) + X x A(0, 16) with X being Bernoulli such that P(X = 1) = 

18 

N" 
o 

FINANCIAL TIME SERIES AND THEIR CHARACTERISTICS 

o 
o 

-4 

-2 

0 
x 

2 

4 

Figure 1.1 Comparison of finite mixture, stable, and standard normal density functions. 

0.05, and the density function of Cauchy is 

—OO < X < oo. 

It is seen that the Cauchy distribution has fatter tails than the finite mixture of 
normal, which, in turn, has fatter tails than the standard normal. 

1.2.3 Multivariate Returns 

Let r, = (r\t, ..., rNt)' be the log returns of N assets at time t. The multivariate 
analyses of Chapters 8 and 10 are concerned with the joint distribution of {rt}J_v 
This joint distribution can be partitioned in the same way as that of Eq. (1.15). 
The analysis is then focused on the specification of the conditional distribution 
function F(rt\rt-i, ...,r\,0). In particular, how the conditional expectation and 
conditional covariance matrix of r, evolve over time constitute the main subjects 
of Chapters 8 and 10. 

The mean vector and covariance matrix of a random vector X — (Xj, ..., Xp) 

are defined as 

E(X) = nx = [E(Xl),...,E(Xp)]f, 

Cov(X) = Ex = E[(X - ilx){X - ilx)% 

DISTRIBUTIONAL PROPERTIES OF RETURNS 

19 

provided that the expectations involved exist. When the data {jci,..., xt} of X 
are available, the sample mean and covariance matrix are defined as 

These sample statistics are consistent estimates of their theoretical counterparts pro¬ 
vided that the covariance matrix of X exists. In the finance literature, multivariate 
normal distribution is often used for the log return rt. 

1.2.4 Likelihood Function of Returns 

The partition of Eq. (1.15) can be used to obtain the likelihood function of the 
log returns {ri,..., rj) of an asset, where for ease in notation the subscript i is 
omitted from the log return. If the conditional distribution /(rt|r,_i, ..., r\, 0) is 
normal with mean pt and variance cr2, then 0 consists of the parameters in pt and 
cr2, and the likelihood function of the data is 

(1.18) 

where f(r\\ 0) is the marginal density function of the first observation r\. The value 
of 0 that maximizes this likelihood function is the maximum-likelihood estimate 
(MLE) of 0. Since the log function is monotone, the MLE can be obtained by 
maximizing the log-likelihood function, 

(rt - it)2 

which is easier to handle in practice. The log-likelihood function of the data can 
be obtained in a similar manner if the conditional distribution /(r,|rf_i,..., r\\ 0) 
is not normal. 

1.2.5 Empirical Properties of Returns 

The data used in this section are obtained from the Center for Research in Secu¬ 
rity Prices (CRSP) of the University of Chicago. Dividend payments, if any, are 
included in the returns. Figure 1.2 shows the time plots of monthly simple returns 
and log returns of IBM stock from January 1926 to December 2008. A time plot 
shows the data against the time index. The upper plot is for the simple returns. 
Figure 1.3 shows the same plots for the monthly returns of value-weighted market 
index. As expected, the plots show that the basic patterns of simple and log returns 

are similar. 

20 

FINANCIAL TIME SERIES AND THEIR CHARACTERISTICS 

Figure 1.2 Time plots of monthly returns of IBM stock from January 1926 to December 2008. Upper 
panel is for simple returns, and lower panel is for log returns. 

1940 

1960 

1980 

2000 

Year 

1940 

1960 

1980 

2000 

Year 

Figure 1.3 Time plots of monthly returns of value-weighted index from January 1926 to December 
2008. Upper panel is for simple returns, and lower panel is for log returns. 

DISTRIBUTIONAL PROPERTIES OF RETURNS 

21 

Table 1.2 provides some descriptive statistics of simple and log returns for 
selected U.S. market indexes and individual stocks. The returns are for daily and 
monthly sample intervals and are in percentages. The data spans and sample sizes 
are also given in Table 1.2. From the table, we make the following observations, 
(a) Daily returns of the market indexes and individual stocks tend to have high 
excess kurtoses. For monthly series, the returns of market indexes have higher 
excess kurtoses than individual stocks, (b) The mean of a daily return series is close 
to zero, whereas that of a monthly return series is slightly larger, (c) Monthly returns 
have higher standard deviations than daily returns, (d) Among the daily returns, 
market indexes have smaller standard deviations than individual stocks. This is in 
agreement with common sense, (e) The skewness is not a serious problem for both 
daily and monthly returns, (f) The descriptive statistics show that the difference 
between simple and log returns is not substantial. 

Figure 1.4 shows the empirical density functions of monthly simple and log 
returns of IBM stock from 1926 to 2008. Also shown, by a dashed line, in each 
graph is the normal probability density function evaluated by using the sample 
mean and standard deviation of IBM returns given in Table 1.2. The plots indicate 
that the normality assumption is questionable for monthly IBM stock returns. The 
empirical density function has a higher peak around its mean, but fatter tails than 
that of the corresponding normal distribution. In other words, the empirical density 

Figure 1.4 Comparison of empirical and normal densities for monthly simple and log returns of IBM 

stock. Sample period is from January 1926 to December 2008. Left plot is for simple returns and right 

plot for log returns. Normal density, shown by the dashed line, uses sample mean and standard deviation 

given in Table 1.2. 

22 

FINANCIAL TIME SERIES AND THEIR CHARACTERISTICS 

function is taller and skinnier, but with a wider support than the corresponding 
normal density. 

1.3 PROCESSES CONSIDERED 

Besides the return series, we also consider the volatility process and the behavior of 
extreme returns of an asset. The volatility process is concerned with the evolution 
of conditional variance of the return over time. This is a topic of interest because, as 
shown in Figures 1.2 and 1.3, the variabilities of returns vary over time and appear 
in clusters. In application, volatility plays an important role in pricing options and 
risk management. By extremes of a return series, we mean the large positive or 
negative returns. Table 1.2 shows that the minimum and maximum of a return series 
can be substantial. The negative extreme returns are important in risk management, 
whereas positive extreme returns are critical to holding a short position. We study 
properties and applications of extreme returns, such as the frequency of occurrence, 
the size of an extreme, and the impacts of economic variables on the extremes, in 
Chapter 7. 

Other financial time series considered in the book include interest rates, exchange 
rates, bond yields, and quarterly earning per share of a company. Figure 1.5 shows 
the time plots of two U.S. monthly interest rates. They are the 10-year and 1-year 

(a) 

(b) 

Figure 1.5 Time plots of monthly U.S. interest rates from April 1953 to February 2009: (a) 10-year 
Treasury constant maturity rate and (b) 1-year maturity rate. 

PROCESSES CONSIDERED 

23 

Year 

(a) 

<D O) 
c 
03 

O 

C\J 

o 

CM 
I 

I 

2000 

2002 

2004 

2006 

2008 

Year 

(b) 

Figure 1.6 Time plot of daily exchange rate between U.S. dollar and Japanese yen from January 4, 

2000, to March 27, 2009: (a) exchange rate and (b) changes in exchange rate. 

Treasury constant maturity rates from April 1954 to February 2009. As expected, 
the two interest rates moved in unison, but the 1-year rates appear to be more 
volatile. Figure 1.6 shows the daily exchange rate between the U.S. dollar and the 
Japanese yen from January 4, 2000, to March 27, 2009. From the plot, the exchange 
rate encountered occasional big changes in the sampling period. Table 1.3 provides 
some descriptive statistics for selected U.S. financial time series. The monthly bond 
returns obtained from CRSP are Fama bond portfolio returns from January 1952 to 
December 2008. The interest rates are obtained from the Federal Reserve Bank of 
St. Louis. The weekly 3-month Treasury bill rate started on January 8, 1954, and 
the 6-month rate started on December 12, 1958. Both series ended on March 27, 
2009. For the interest rate series, the sample means are proportional to the time to 
maturity, but the sample standard deviations are inversely proportional to the time 
to maturity. For the bond returns, the sample standard deviations are positively 
related to the time to maturity, whereas the sample means remain stable for all 
maturities. Most of the series considered have positive excess kurtoses. 

With respect to the empirical characteristics of returns shown in Table 1.2, 
Chapters 2-4 focus on the first four moments of a return series and Chapter 7 on 
the behavior of minimum and maximum returns. Chapters 8 and 10 are concerned 
with moments of and the relationships between multiple asset returns, and Chapter 5 
addresses properties of asset returns when the time interval is small. An introduction 
to mathematical finance is given in Chapter 6. 

24 

FINANCIAL TIME SERIES AND THEIR CHARACTERISTICS 

TABLE 1.3 Descriptive Statistics of Selected U.S. Financial Time Series" 

Maturity 

Mean 

Standard 
Deviation 

Skewness 

Excess 
Kurtosis  Minimum  Maximum 

Monthly Bond Returns: Jan. 1952 to Dec. 2008, T  = 684 

1-12 months 
12-24 months 
24-36 months 
48-60 months 
61-120 months 

0.45 
0.49 
0.52 
0.53 
0.55 

0.35 
0.67 
0.98 
1.40 
1.69 

2.47 
1.88 
1.37 
0.60 
0.65 

13.14 
15.44 
12.92 
4.83 
4.79 

-0.40 
-2.94 
-4.90 
-5.78 
-7.35 

Monthly Treasury Rates: April 1953 to February 2009, T — 671 

1 year 
3 years 
5 years 
10 years 

5.59 
5.98 
6.19 
6.40 

2.98 
2.85 
2.77 
2.69 

1.02 
0.95 
0.97 
0.95 

1.32 
0.95 
0.82 
0.61 

0.44 
1.07 
1.52 
2.29 

Weekly Treasury Bill Rates: End on March 27, 2009. 

3 months 
6 months 

5.07 
5.52 

2.82 
2.73 

1.08 
0.99 

1.80 
1.53 

0.02 
0.20 

3.52 
6.85 
9.33 
10.06 
10.92 

16.72 
16.22 
15.93 
15.32 

16.76 
15.76 

“The data are in percentages. The weekly 3-month Treasury bill rate started from January 8, 1954, and 

the 6-month rate started from December 12, 1958. The sample sizes for Treasury bill rates are 2882 
and 2625, respectively. Data sources are given in the text. 

APPENDIX: R PACKAGES 

R is a free software available from http://www.r-project.org. One can click CRAN 
on its Web page to select a nearby CRAN Mirror to download and install the 
software and selected packages. For financial time series analysis, the Rmetrics of 
Diethelm Wuertz and his associates have produced many useful packages, including 
fBasics, timeSeries, fGarch, etc. We use many functions of these packages in 
this book. Further information concerning installing R and the commands used can 
be found either on the Web page of this book or on the author’s teaching Web page. 
R and S-Plus are objective-oriented software. They enable users to create many 
objects. For instance, one can use the command ts to create a time series object. 
Treating time series data as a time series object in R has some advantages, but 
it requires some learning to get used to it. It is, however, not necessary to create 
a time series object in R to perform the analyses discussed in this book. As an 
illustration, consider the monthly simple returns to the General Motors stock from 
January 1975 to December 2008; see Exercise 1.2. The data have 408 observations. 
The following R commands are used to illustrate the points: 

> da=read.table("m-gm3dx7508.txt",header=T) % Load data 
> gm=da[,2] % Column 2 contains GM stock returns 
> gml=ts(gm,frequency=12,start=c(1975,1)) 
% Creates a ts object. 

> par(mfcol=c(2,1)) % Put two plots on a page. 

EXERCISES 

25 

Figure 1.7 Time plots of monthly simple returns to General Motors stock from January 1975 to 

December 2008: (a) and (b) are without and with time series object, respectively. 

Time 

(b) 

> plot(gm,type='1') 
> plot(gml,type='1') 
> acf(gm,lag=24) 
> acf(gml,lag=24) 

In the ts command, frequency = 12 says that the time unit is year and there 
are 12 equally spaced observations in each time unit, and start = c( 1975,1) means 
the starting time is January 1975. Frequency and start are the two basic arguments 
needed in R to create a time series object. For further details, please use help (ts) 
in R to obtain details of the command. Here gml is a time series object in R, but 
gm is not. Figures 1.7 and 1.8 show, respectively, the time plot and autocorrelation 
function (ACF) of the returns of GM stock. In each figure, the upper plot is pro¬ 
duced without using time series object, whereas the lower plot is produced by a 
time series object. The upper and lower plots are identical except for the horizontal 
label. For the time plot, the time series object uses calendar time to label the x 
axis, which is preferred. On the other hand, for the ACF plot, the time series object 
uses fractions of time unit in the label, not the commonly used time lags. 

EXERCISES 

1.1. Consider the daily stock returns of American Express (AXP), Caterpillar 
(CAT), and Starbucks (SBUX) from January 1999 to December 2008. The 

26 

FINANCIAL TIME SERIES AND THEIR CHARACTERISTICS 

Series : gm 

CO 
o 

LL 
O ^ 
< o 

0 5 10 15 20 

Lag 

(a) 

0.0 0.5 1.0 1.5 2.0 

Lag 

(b) 

Figure 1.8 Sample ACFs of the monthly simple returns to General Motors stock from January 1975 

to December 2008: (a) and (b) are without and with time series object, respectively. 

data are simple returns given in the file d-3stocks9908 . txt (date, axp, cat, 
sbux). 

(a) Express the simple returns in percentages. Compute the sample mean, 
standard deviation, skewness, excess kurtosis, minimum, and maximum 
of the percentage simple returns. 

(b) Transform the simple returns to log returns. 

(c) Express the log returns in percentages. Compute the sample mean, stan¬ 
dard deviation, skewness, excess kurtosis, minimum, and maximum of the 
percentage log returns. 

(d) Test the null hypothesis that the mean of the log returns of each stock is 
zero. That is, perform three separate tests. Use 5% significance level to 
draw your conclusion. 

1.2. Answer the same questions as in Exercise 1.1 but using monthly stock returns 
for General Motors (GM), CRSP value-weighted index (VW), CRSP equal- 
weighted index (EW), and S&P composite index from January 1975 to Decem¬ 
ber 2008. The returns of the indexes include dividend distributions. Data file 
is m-gm3dx7508.txt (date, gm, vw, ew, sp). 

1.3. Consider the monthly stock returns of S&P composite index from January 
1975 to December 2008 in Exercise 1.2. Answer the following questions: 

(a) What is the average annual log return over the data span? 

REFERENCES 

27 

(b) Assume that there were no transaction costs. If one invested $1.00 on the 
S&P composite index at the beginning of 1975, what was the value of the 
investment at the end of 2008? 

1.4. Consider the daily log returns of American Express stock from January 1999 
to December 2008 as in Exercise 1.1. Use the 5% significance level to perform 
the following tests: (a) Test the null hypothesis that the skewness measure of 
the returns is zero, (b) Test the null hypothesis that the excess kurtosis of the 
returns is zero. 

1.5. Daily foreign exchange rates (spot rates) can be obtained from the Federal 
Reserve Bank in Chicago. The data are the noon buying rates in New York City 
certified by the Federal Reserve Bank of New York. Consider the exchange 
rates between the U.S. dollar and the Canadian dollar, euro, U.K. pound, and 
the Japanese yen from January 4, 2000, to March 27, 2009. The data are 
also on the Web. (a) Compute the daily log return of each exchange rate, 
(b) Compute the sample mean, standard deviation, skewness, excess kurtosis, 
minimum, and maximum of the log returns of each exchange rate, (c) Discuss 
the empirical characteristics of the log returns of exchange rates, (d) Obtain a 
density plot of the daily long returns of dollar-euro exchange rate. 

REFERENCES 

Campbell, J. Y., Lo, A. W., and MacKinlay, A. C. (1997). The Econometrics of Financial 

Markets. Princeton University Press, Princeton, NJ. 

Jarque, C. M. and Bera, A. K. (1987). A test of normality of observations and regression 

residuals. International Statistical Review 55: 163-172. 

Sharpe, W. (1964). Capital asset prices: A theory of market equilibrium under conditions of 

risk. Journal of Finance 19: 425-442. 

Snedecor, G. W. and Cochran, W. G. (1980). Statistical Methods, 7th ed. Iowa State Uni¬ 

versity Press, Ames, IA. 

■ 

■ 

CHAPTER 2 

Linear Time Series Analysis 
and Its Applications 

In this chapter, we discuss basic theories of linear time series analysis, introduce 
some simple econometric models useful for analyzing financial data, and apply the 
models to financial time series such as asset returns. Discussions of the concepts are 
brief with emphasis on those relevant to financial applications. Understanding the 
simple time series models introduced here will go a long way to better appreciate 
the more sophisticated financial econometric models of the later chapters. There 
are many time series textbooks available. For basic concepts of linear time series 
analysis, see Box, Jenkins, and Reinsel (1994, Chapters 2 and 3) and Brockwell 
and Davis (1996, Chapters 1-3). 

Treating an asset return (e.g., log return r, of a stock) as a collection of random 
variables over time, we have a time series {rt}. Linear time series analysis provides 
a natural framework to study the dynamic structure of such a series. The theories 
of linear time series discussed include stationarity, dynamic dependence, autocor¬ 
relation function, modeling, and forecasting. The econometric models introduced 
include (a) simple autoregressive (AR) models, (b) simple moving-average (MA) 
models, (b) mixed autoregressive moving-average (ARMA) models, (c) seasonal 
models, (d) unit-root nonstationarity, (e) regression models with time series errors, 
and (f) fractionally differenced models for long-range dependence. For an asset 
return rt, simple models attempt to capture the linear relationship between rt and 
information available prior to time t. The information may contain the historical 
values of rt and the random vector Y in Eq. (1.14), which describes the eco¬ 
nomic environment under which the asset price is determined. As such, correlation 
plays an important role in understanding these models. In particular, correlations 
between the variable of interest and its past values become the focus of linear 
time series analysis. These correlations are referred to as serial correlations or 
autocorrelations. They are the basic tool for studying a stationary time series. 

Analysis of Financial Time Series, Third Edition, By Ruey S. Tsay 
Copyright © 2010 John Wiley & Sons, Inc. 

29 

30 

LINEAR TIME SERIES ANALYSIS AND ITS APPLICATIONS 

2.1 STATIONARITY 

The foundation of time series analysis is stationarity. A time series {rt} is said to 
be strictly stationary if the joint distribution of (rtl,,rtk) is identical to that of 
0>I+f, ..., rtk+t) for all t, where k is an arbitrary positive integer and (t\, ..., 4) is 
a collection of k positive integers. In other words, strict stationarity requires that the 
joint distribution of (r,,,..., rtk) is invariant under time shift. This is a very strong 
condition that is hard to verify empirically. A weaker version of stationarity is often 
assumed. A time series {rr} is weakly stationary if both the mean of rt and the 
covariance between r, and r,_^ are time invariant, where i is an arbitrary integer. 
More specifically, {4} is weakly stationary if (a) E(rt) = 4, which is a constant, 
and (b) Cov(rt, rt-{) — yt, which only depends on l. In practice, suppose that we 
have observed T data points [rt\t = 1,..., T). The weak stationarity implies that 
the time plot of the data would show that the T values fluctuate with constant 
variation around a fixed level. In applications, weak stationarity enables one to 
make inference concerning future observations (e.g., prediction). 

Implicitly, in the condition of weak stationarity, we assume that the first two 
moments of rt are finite. From the definitions, if rt is strictly stationary and its 
first two moments are finite, then rt is also weakly stationary. The converse is 
not true in general. However, if the time series rt is normally distributed, then 
weak stationarity is equivalent to strict stationarity. In this book, we are mainly 
concerned with weakly stationary series. 

The covariance yi = Cov(rt, rt_() is called the lag-f autocovariance of rt. It has 
two important properties: (a) yo = Var(r,) and (b) y-t = yt. The second property 
holds because Cov(r?, r,_(_*)) = Cov(r,_(_*), rt) = Cov(rt+i, rt) = Cov(rri, r(1_*), 
where t\ = t + i. 

In the finance literature, it is common to assume that an asset return series is 
weakly stationary. This assumption can be checked empirically provided that a 
sufficient number of historical returns are available. For example, one can divide 
the data into subsamples and check the consistency of the results obtained across 
the subsamples. 

2.2 CORRELATION AND AUTOCORRELATION FUNCTION 

The correlation coefficient between two random variables X and Y is defined as 

= Cov(X, Y) _ E[(X - tix)(Y - fiy)] 

Px’y VVar(A)Var(T) JE(X - nxpE(Y - fly)2' 

where jix and fiy are the mean of X and Y, respectively, and it is assumed that the 
variances exist. This coefficient measures the strength of linear dependence between 
X and Y, and it can be shown that -1 < px<y < 1 and px>y = py x. The two random 
variables are uncorrelated if px<y = 0. In addition, if both X and Y are normal 
random variables, then px%y = 0 if and only if X and Y are independent. When the 

CORRELATION AND AUTOCORRELATION FUNCTION 

31 

sample {(xt, yt)}J=i is available, the correlation can be consistently estimated by 
its sample counterpart 

5 _ Eli (xt-x)(yt-y) 

PX,y — i =U 

V Eli te - x)2 Elite - y)2 

where x = Eli Xt/T and y = Eli Yt/T are the sample mean of X and Y, 
respectively. 

Autocorrelation Function (ACF) 
Consider a weakly stationary return series rt. When the linear dependence between 
rt and its past values rr_,- is of interest, the concept of correlation is generalized 
to autocorrelation. The correlation coefficient between rt and rt-i is called the 
lag-f autocorrelation of rt and is commonly denoted by pi, which under the weak 
stationary assumption is a function of i only. Specifically, we define 

Cov(r„r,_*) Cov(rt, rt_i) yt 

Pi = —, - = - = —, (2.1) 

yVar(r,)Var(rf_U Var(r?) y0 

where the property Var(rt) = Var(rt-e) for a weakly stationary series is used. From 
the definition, we have Po = 1, pt = p~i, and — 1 < pt < 1. In addition, a weakly 
stationary series rt is not serially correlated if and only if pt = 0 for all l > 0. 

For a given sample of returns {rt}J=l, let r be the sample mean (i.e., r = 

Eli rt/T). Then the lag-1 sample autocorrelation of rt is 

Pi 

E^te - r)(rt-i - r) 
Elite ~r)2 

Under some general conditions, pi is a consistent estimate of p\. For example, if 
{rt} is an independent and identically distributed (iid) sequence and E(r2) < oo, 
then pi is asymptotically normal with mean zero and variance 1 /T; see Brockwell 
and Davis (1991, Theorem 7.2.2). This result can be used in practice to test the 
null hypothesis Hq : pi = 0 versus the alternative hypothesis Ha\ p\ ^ 0. The test 
statistic is the usual t ratio, which is -JTp\ and follows asymptotically the standard 
normal distribution. The null hypothesis Hq is rejected if the t ratio is large in 
magnitude or, equivalently, the p value of the t ratio is small, say less than 0.05. 
In general, the lag-f sample autocorrelation of rt is defined as 

Ht=i+ite 

r)(rt-i ~ r) 

Pi 

E*=ite -r)7 

0<£<T-1. 

(2.2) 

If {rt} is an iid sequence satisfying E(rj) < oo, then pi is asymptotically normal 
with mean zero and variance l/T for any fixed positive integer l. More generally, 
if rt is a weakly stationary time series satisfying rt = p, + Elo where 

32 

LINEAR TIME SERIES ANALYSIS AND ITS APPLICATIONS 

T/r0 = 1 and {aj} is a sequence of iid random variables with mean zero, then pi is 
asymptotically normal with mean zero and variance (1+2 Y^i=i P,2)/T for t > q. 
This is referred to as Bartlett’s formula in the time series literature; see Box, 
Jenkins, and Reinsel (1994). For more information about the asymptotic distribution 
of sample autocorrelations, see Fuller (1976, Chapter 6) and Brockwell and Davis 
(1991, Chapter 7). 

Testing Individual ACF 
For a given positive integer f, the previous result can be used to test Ho : pi = 0 
vs. Ha\ pi ^ 0. The test statistic is 

t ratio = 

Pt 

If {rt} is a stationary Gaussian series satisfying pj = 0 for j > i, the t ratio 
is asymptotically distributed as a standard normal random variable. Hence, the 
decision rule of the test is to reject Hq if \t ratio| > Za/2, where Za/2 is the 
100(1 — a/2)th percentile of the standard normal distribution. For simplicity, many 
software packages use 1 / T as the asymptotic variance of pi for all l ^ 0. They 
essentially assume that the underlying time series is an iid sequence. 

In finite samples, pi is a biased estimator of pi. The bias is in the order of 
\/T, which can be substantial when the sample size T is small. In most financial 
applications, T is relatively large so that the bias is not serious. 

Portmanteau Test 
Financial applications often require to test jointly that several autocorrelations of 
r, are zero. Box and Pierce (1970) propose the Portmanteau statistic 

m 

Q*(m) = TY/P2e 

as a test statistic for the null hypothesis Ho : p\ = ■ • ■ = pm — 0 against the alter¬ 
native hypothesis Ha : p, ^ 0 for some i e {1,..., m}. Under the assumption that 
{rt} is an iid sequence with certain moment conditions, Q*(m) is asymptotically a 
chi-squared random variable with m degrees of freedom. 

Ljung and Box (1978) modify the Q*(m) statistic as below to increase the power 

of the test in finite samples. 

(2.3) 

The decision rule is to reject H0 if Q(m) > xl, where xl denotes the 100(1 - a)th 
percentile of a chi-squared distribution with m degrees of freedom. Most software 

CORRELATION AND AUTOCORRELATION FUNCTION 

33 

packages will provide the p value of Q(m). The decision rule is then to reject Hq 
if the p value is less than or equal to a, the significance level. 

In practice, the choice of m may affect the performance of the Q(m) statistic. 
Several values of m are often used. Simulation studies suggest that the choice of 
m ~ ln(T) provides better power performance. This general rule needs modification 
in analysis of seasonal time series for which autocorrelations with lags at multiples 
of the seasonality are more important. 

The statistics p\, P2, ... defined in Eq. (2.2) is called the sample autocorrela¬ 
tion function (ACF) of rt. It plays an important role in linear time series analysis. 
As a matter of fact, a linear time series model can be characterized by its ACF, 
and linear time series modeling makes use of the sample ACF to capture the lin¬ 
ear dynamic of the data. Figure 2.1 shows the sample autocorrelation functions 
of monthly simple and log returns of IBM stock from January 1926 to Decem¬ 
ber 2008. The two sample ACFs are very close to each other, and they suggest 
that the serial correlations of monthly IBM stock returns are very small, if any. 
The sample ACFs are all within their two standard error limits, indicating that 
they are not significantly different from zero at the 5% level. In addition, for the 
simple returns, the Ljung-Box statistics give Q(5) = 3.37 and Q( 10) = 13.99, 
which correspond to p values of 0.64 and 0.17, respectively, based on chi-squared 

(a) 

Lag 

(b) 

Figure 2.1 Sample autocorrelation functions of monthly (a) simple returns and (b) log returns of 
IBM stock from January 1926 to December 2008. In each plot, two horizontal dashed lines denote two 

standard error limits of sample ACF. 

34 

LINEAR TIME SERIES ANALYSIS AND ITS APPLICATIONS 

(a) 

(b) 

Figure 2.2 Sample autocorrelation functions of monthly (a) simple returns and (b) log returns of value- 

weighted index of U.S. markets from January 1926 to December 2008. In each plot, two horizontal 

dashed lines denote two standard error limits of sample ACF. 

distributions with 5 and 10 degrees of freedom. For the log returns, we have 
Q(5) = 3.52 and (2(10) = 13.39 with p values 0.62 and 0.20, respectively. The 
joint tests confirm that monthly IBM stock returns have no significant serial corre¬ 
lations. Figure 2.2 shows the same for the monthly returns of the value-weighted 
index from the Center for Research in Security Prices (CRSP), at the University 
of Chicago. There are some significant serial correlations at the 5% level for both 
return series. The Ljung-Box statistics give (9(5) = 29.71 and (9(10) = 39.55 for 
the simple returns and (9(5) = 28.38 and (9(10) = 36.16 for the log returns. The 
p values of these four test statistics are all less than 0.0001, suggesting that monthly 
returns of the value-weighted index are serially correlated. Thus, the monthly mar¬ 
ket index return seems to have stronger serial dependence than individual stock 
returns. 

In the finance literature, a version of the capital asset pricing model (CAPM) 
theory is that the return {r,} of an asset is not predictable and should have no auto¬ 
correlations. Testing for zero autocorrelations has been used as a tool to check the 
efficient market assumption. However, the way by which stock prices are deter¬ 
mined and index returns are calculated might introduce autocorrelations in the 
observed return series. This is particularly so in analysis of high-frequency financial 
data. We discuss some of these issues, such as bid-ask bounce and nonsynchronous 
trading, in Chapter 5. 

CORRELATION AND AUTOCORRELATION FUNCTION 

35 

R Demonstration 

The following output has been edited and % denotes explanation: 

> da=read.table("m-ibm3dx2608.txt",header=T) % Load data 

> da[l,] % Check the 1st row of the data 

date rtn vwrtn ewrtn sprtn 

1 19260130 -0.010381 0.000724 0.023174 0.022472 

> sibm=da[,2] % Get the IBM simple returns 

> Box.test(sibm,lag=5,type='Ljung') % Ljung-Box statistic Q(5) 

Box-Ljung test 

data: sibm 

X-squared = 3.3682, df = 5, p-value = 0.6434 

> libm=log(sibm+1) % Log IBM returns 

> Box.test(libm,lag=5,type='Ljung') 

Box-Ljung test 

data: libm 

X-squared = 3.5236, df = 5, p-value = 0.6198 

S-Plus Demonstration 
Output edited. 

> module(finmetrics) 

> da=read.table("m-ibm3dx2608.txt",header=T) % Load data 

> da[l,] % Check the 1st row of the data 

date rtn vwrtn ewrtn sprtn 

1 19260130 -0.010381 0.000724 0.023174 0.022472 

> sibm=da[,2] % Get IBM simple returns 

> autocorTest(sibm,lag=5) % Ljung-Box Q(5) test 

Test for Autocorrelation: Ljung-Box 

Null Hypothesis: no autocorrelation 

Test Statistics: 

Test Stat 3.3682 

p.value 0.6434 

Dist. under Null: chi-square with 5 degrees of freedom 

Total Observ.: 996 

> libm=log(sibm+1) % IBM log returns 

> autocorTest(libm,lag=5) 

Test for Autocorrelation: Ljung-Box 

Null Hypothesis: no autocorrelation 

Test Statistics: 

Test Stat 3.5236 

p.value 0.6198 

36 

LINEAR TIME SERIES ANALYSIS AND ITS APPLICATIONS 

2.3 WHITE NOISE AND LINEAR TIME SERIES 

White Noise 
A time series rt is called a white noise if {rt} is a sequence of independent and 
identically distributed random variables with finite mean and variance. In particular, 
if r, is normally distributed with mean zero and variance o2, the series is called a 
Gaussian white noise. For a white noise series, all the ACFs are zero. In practice, 
if all sample ACFs are close to zero, then the series is a white noise series. Based 
on Figures 2.1 and 2.2, the monthly returns of IBM stock are close to white noise, 
whereas those of the value-weighted index are not. 

The behavior of sample autocorrelations of the value-weighted index returns 
indicates that for some asset returns it is necessary to model the serial dependence 
before further analysis can be made. In what follows, we discuss some simple time 
series models that are useful in modeling the dynamic structure of a time series. 
The concepts presented are also useful later in modeling volatility of asset returns. 

Linear Time Series 
A time series rt is said to be linear if it can be written as 

OO 

rt = + (2.4) 

i=0 

where /x is the mean of rt, xj/o = 1, and {a,} is a sequence of iid random variables 
with mean zero and a well-defined distribution (i.e., {at} is a white noise series). It 
will be seen later that at denotes the new information at time t of the time series 
and is often referred to as the innovation or shock at time t. In this book, we are 
mainly concerned with the case where a, is a continuous random variable. Not 
all financial time series are linear, however. We study nonlinearity and nonlinear 
models in Chapter 4. 

For a linear time series in Eq. (2.4), the dynamic structure of r, is governed by 
the coefficients xf/j, which are called the x/s weights of r, in the time series literature. 
If rt is weakly stationary, we can obtain its mean and variance easily by using the 
independence of {at} as 

E{rt) = /x, Var(rt) = o2 xjs2, (2.5) 

OO 

i=0 

where a2 is the variance of at. Because Var(rr) < oo, {xj/2} must be a convergent 
sequence, that is, xj/2 -* 0 as i -» oo. Consequently, for a stationary series, impact 
of the remote shock a,-i on the return rt vanishes as i increases. 

The lag-f autocovariance of rt is 

Yt = Cov(r,, rt_£) = E 

SIMPLE AR MODELS 

= E ( Y1 fifjat-idt~l-j 

\ij=0 

7=0 7=0 

37 

(2.6) 

Consequently, the i/r weights are related to the autocorrelations of rt as follows: 

n _ E”o fifj+e 
Y0~ 1 + E“i^2’ 

f > 0, 

(2.7) 

where \j/0 = \. Linear time series models are econometric and statistical models 
used to describe the pattern of the \js weights of rt. For a weakly stationary time 
series, -* 0 as i —>• oo and, hence, pi converges to zero as l increases. For asset 
returns, this means that, as expected, the linear dependence of current return rt on 
the remote past return rt-i diminishes for large t. 

2.4 SIMPLE AR MODELS 

The fact that the monthly return rt of CRSP value-weighted index has a statistically 
significant lag-1 autocorrelation indicates that the lagged return rt-\ might be useful 
in predicting rt. A simple model that makes use of such predictive power is 

rt = 0o + 0iav_i +at, (2.8) 

where {at} is assumed to be a white noise series with mean zero and variance 
<7q. This model is in the same form as the well-known simple linear regression 
model in which rt is the dependent variable and rt-\ is the explanatory variable. 
In the time series literature, model (2.8) is referred to as an autoregressive (AR) 
model of order 1 or simply an AR(1) model. This simple model is also widely 
used in stochastic volatility modeling when rt is replaced by its log volatility; see 
Chapters 3 and 12. 

The AR(1) model in Eq. (2.8) has several properties similar to those of the 
simple linear regression model. However, there are some significant differences 
between the two models, which we discuss later. Here it suffices to note that an 
AR(1) model implies that, conditional on the past return rf_i, we have 

E(rt\rt-i) =(po + 4>\rt-\, Var(rf |r,_i) = Var(a,) = o2a. 

That is, given the past return rt-i, the current return is centered around <po + <p\rt~\ 
with standard deviation oa. This is a Markov property such that conditional on rt-\, 
the return rt is not correlated with r;_,- for i > 1. Obviously, there are situations 
in which rt-\ alone cannot determine the conditional expectation of r, and a more 

38 

LINEAR TIME SERIES ANALYSIS AND ITS APPLICATIONS 

flexible model must be sought. A straightforward generalization of the AR(1) model 
is the AR(p) model: 

rt = </>0 + 01A-1 + • • • + 4>prt-p + at> 

(2.9) 

where p is a nonnegative integer and {a,} is defined in Eq. (2.8). This model 
says that the past p variables r,-, (i = 1,..., p) jointly determine the conditional 
expectation of rt given the past data. The AR(p) model is in the same form as 
a multiple linear regression model with lagged values serving as the explanatory 
variables. 

2.4.1 Properties of AR Models 

For effective use of AR models, it pays to study their basic properties. We discuss 
properties of AR(1) and AR(2) models in detail and give the results for the general 
AR(p) model. 

AR(1) Model 
We begin with the sufficient and necessary condition for weak stationarity of the 
AR(1) model in Eq. (2.8). Assuming that the series is weakly stationary, we have 
E(rt) = /x, VarOv) = yo, and Cov(r,, rt-j) = yj, where p and yo are constant and 
Yj is a function of j, not t. We can easily obtain the mean, variance, and autocor¬ 
relations of the series as follows. Taking the expectation of Eq. (2.8) and because 
E(at) = 0, we obtain 

E(rt) = </>o + <f>iE(rt-i). 

Under the stationarity condition, E(rt) = E(rt-\) = p and hence 

p — 0o + 4>\p or 

This result has two implications for rt. First, the mean of rt exists if 0^1. 
Second, the mean of rt is zero if and only if 0O = 0. Thus, for a stationary AR(1) 
process, the constant term 0o is related to the mean of r, via 0o = (1 — <f>i)p and 
0o = 0 implies that E(rt) = 0. 

Next, using 0O = (1 — 0i )p, the AR(1) model can be rewritten as 

n- P = <t>- p) + at. 

(2.10) 

By repeated substitutions, the prior equation implies that 

r, - p = a, + 0ia,_i + 0i<3,_2 H- 

OO 

i=0 

(2.11) 

SIMPLE AR MODELS 

39 

This equation expresses an AR(1) model in the form of Eq. (2.4) with xfr{ = 0j. 
Thus, rt — /x is a linear function of for i > 0. Using this property and the 
independence of the series {at}, we obtain E[(rt — /x)at+\] = 0. By the stationarity 
assumption, we have Cov(r,_i, at) = E[(rt^\ — /x)at] = 0. This latter result can 
also be seen from the fact that rt_\ occurred before time t and at does not depend 
on any past information. Taking the square, then the expectation of Eq. (2.10), we 
obtain 

Var(r,) = 0^Var(rf_i) + <r2, 

where cr2 is the variance of at, and we make use of the fact that the covariance 
between rf_i and at is zero. Under the stationarity assumption, Var(rf) = Var(r,_i), 
so that 

Var(r,) - —^ 
1 -0f 

provided that 02 < 1. The requirement of 02 < 1 results from the fact that the 
variance of a random variable is bounded and nonnegative. Consequently, the weak 
stationarity of an AR(1) model implies that —1 < 0i < 1, that is, |0i| < 1. Yet if 
|0i | < 1, then by Eq. (2.11) and the independence of the {at} series, we can show 
that the mean and variance of rt are finite and time invariant; see Eq. (2.5). In 
addition, by Eq. (2.6), all the autocovariances of rt are finite. Therefore, the AR(1) 
model is weakly stationary. In summary, the necessary and sufficient condition for 
the AR(1) model in Eq. (2.8) to be weakly stationary is |0i| < 1. 

Using 0o = (1 — 0i )/x, one can rewrite a stationary AR(1) model as 

rt = (1 - 0i)M + 01U-1 + at. 

This model is often used in the finance literature with 0i measuring the persistence 
of the dynamic dependence of an AR(1) time series. 

Autocorrelation Function of an AR(1) Model 
Multiplying Eq. (2.10) by at, using the independence between at and rt-\, and 
taking expectation, we obtain 

E[at{rt - fi)] = 0i£[a?(r,_i - /x)] + E(aj) = E(aj) = o], 

where cr2 is the variance of at. Multiplying Eq. (2.10) by rt— /x, taking expec¬ 
tation, and using the prior result, we have 

Yi = 

0iyi+cr2 if 1=0 

0m-1 if ■£ > 0, 

40 

LINEAR TIME SERIES ANALYSIS AND ITS APPLICATIONS 

where we use yi = y_^. Consequently, for a weakly stationary AR(1) model in 

Eq. (2.8), we have 

Var(rf) = y0 - -9 and ye = 0i^-i, for i > 0. 

1 -0f 

From the latter equation, the ACF of r, satisfies 

^=0iP£_i, for £ > 0. 

Because po = 1, we have pi = 0[. This result says that the ACF of a weakly 
stationary AR(1) series decays exponentially with rate <p\ and starting value po = 1. 
For a positive <p\, the plot of ACF of an AR(1) model shows a nice exponential 
decay. For a negative 0i, the plot consists of two alternating exponential decays 
with rate (p\. Figure 2.3 shows the ACF of two AR(1) models with (p\ = 0.8 and 
01 = -0.8. 

AR(2) Model 
An AR(2) model assumes the form 

A = 00 + 0irf_i + 02*7-2 + a,. 

(2.12) 

Lag 

(a) 

Lag 

(b) 

Figure 2.3 Autocorrelation function of an AR(1) model: (a) for (j>{ = 0.8 and (b) for (j)\ = —0.8. 

SIMPLE AR MODELS 

41 

Using the same technique as that of the AR(1) case, we obtain 

E(rt) = /x = 

00 

1—01—02 

provided that 0i + 02 f 1. Using 0o = (1 — 0i — 02)ii, we can rewrite the AR(2) 
model as 

(rt - /x) = 0i(rf_i - jtt) + 02(r,_2 -fi) + at. 

Multiplying the prior equation by (/■>_£ — /x), we have 

iTt-i ~ M)(A - fi) = 4>i(rt-i - aO(A-1 - jtt) 

+ 02ft—l - A007-2 - AO + ft-t ~ A0<2f 

Taking expectation and using E[(rt-e - ix)at] = 0 for i > 0, we obtain 

Yi = <t>iYi-i + 02W-2, for ^ > 0. 

This result is referred to as the moment equation of a stationary AR(2) model. 
Dividing the above equation by Yo, we have the property 

pt = 4>\pi-\ +(p2Pt-2, for l>0, (2.13) 

for the ACF of rt. In particular, the lag-1 ACF satisfies 

Pi = 01P0 + 02P—1 = 01 + 02Pi • 

Therefore, for a stationary AR(2) series rt, we have p0 = 1, 

_ 01 

P1 _ 1 - 02 

P£ = 01 Pt— 1 + 02P£—2) f > 2. 

The result of Eq. (2.13) says that the ACF of a stationary AR(2) series satisfies the 
second-order difference equation 

(1— <\>XB- 0252)^=O, 

where 5 is called the back-shift operator such that Bpn — pi-\. This difference 
equation determines the properties of the ACF of a stationary AR(2) time series. 
It also determines the behavior of the forecasts of rt. In the time series literature, 
some people use the notation L instead of B for the back-shift operator. Here 
L stands for lag operator. For instance, Lrt = rt_\ and Lfk = 0>-i• 

42 

LINEAR TIME SERIES ANALYSIS AND ITS APPLICATIONS 

Corresponding to the prior difference equation, there is a second-order polyno¬ 

mial equation: 

1 — 0i Jt — 02 A = 0. 

(2.14) 

Solutions of this equation are 

01 ± J<t>\ + 402 
x =-. 

-202 

In the time series literature, inverses of the two solutions are referred to as the 
characteristic roots of the AR(2) model. Denote the two characteristic roots by 
co\ and (02. If both co,- are real valued, then the second-order difference equation 
of the model can be factored as (1— co\B){\ — (02B) and the AR(2) model can 
be regarded as an AR(1) model operates on top of another AR(1) model. The 
ACF of r, is then a mixture of two exponential decays. If 0^ + 402 < 0, then co 1 
and (x>2 are complex numbers (called a complex-conjugate pair), and the plot of 
ACF of rt would show a picture of damping sine and cosine waves. In business and 
economic applications, complex characteristic roots are important. They give rise to 
the behavior of business cycles. It is then common for economic time series models 
to have complex-valued characteristic roots. For an AR(2) model in Eq. (2.12) with 
a pair of complex characteristic roots, the average length of the stochastic cycles is 

2n 

k== cos-i[01/(2v^)r 

where the cosine inverse is stated in radians. If one writes the complex solutions 
as a ± bi, where i = V—I, then we have 01 = 2a, 02 = — (a2 + b2), and 

k = —-.. .... 

2n 

cos-1 (a/Va2 + b2) 

where Va2 + b2 is the absolute value of a ± bi. See Example 2.1 for an illustration. 
Figure 2.4 shows the ACF of four stationary AR(2) models. Part (b) is the ACF 
of the AR(2) model (1 — 0.6B + 0.4B2)rt = at. Because 0^ + 402 = 0.36 + 4 x 
(—0.4) = —1.24 < 0, this particular AR(2) model contains two complex charac¬ 
teristic roots, and hence its ACF exhibits damping sine and cosine waves. The 
other three AR(2) models have real-valued characteristic roots. Their ACFs decay 
exponentially. 

Example 2.1. As an illustration, consider the quarterly growth rate of U.S. 
real gross national product (GNP), seasonally adjusted, from the second quarter 
of 1947 to the first quarter of 1991. This series shown in Figure 2.5 is also used 
in Chapter 4 as an example of nonlinear economic time series. Here we simply 
employ an AR(3) model for the data. Denoting the growth rate by rt, we can use 

SIMPLE AR MODELS 

43 

Lag 

(a) 

Lag 

(b) 

Lag 

(c) 

Lag 

(d) 

Figure 2.4 Autocorrelation function of an AR(2) model: (a) (pi = 1.2 and (p2 = -0.35, (b) cp\ = 0.6 
and <p2 = —0.4, (c) cp\ = 0.2 and (p2 — 0.35, and (d) (p\ = —0.2 and cpi = 0.35. 

the model building procedure of the next subsection to estimate the model. The 
fitted model is 

rt = 0.0047 + 0.348r,_i + 0.179rf_2 — 0.142rf_3 + at, aa = 0.0097. (2.15) 

Rewriting the model as 

r, -0.348r,_! - 0.179rr_2 + 0.142rf_3 = 0.0047 + at, 

we obtain a corresponding third-order difference equation 

1 — 0.3485 — 0.17952 + 0.14153 = 0, 

which can be factored approximately as 

(1 + 0.521R)(1 - 0.869R + 0.274R2) = 0. 

The first factor (1 -f 0.521 B) shows an exponentially decaying feature of the GNP 
growth rate. Focusing on the second-order factor 1 — 0.8695 — (—0.274)B2 = 0, 
we have </>2 + 402 = 0.8692 + 4(—0.274) = —0.341 < 0. Therefore, the second 
factor of the AR(3) model confirms the existence of stochastic business cycles 

44 

LINEAR TIME SERIES ANALYSIS AND ITS APPLICATIONS 

Figure 2.5 Time plot of growth rate of U.S. quarterly real GNP from 1947.11 to 1991.1. Data are 

seasonally adjusted and in percentages. 

in the quarterly growth rate of U.S. real GNP. This is reasonable as the U.S. 
economy went through expansion and contraction periods. The average length of 
the stochastic cycles is approximately 

2(3.14159) 

k =---- = 10.62 quarters, 

cos-i[01/(2V=^)] 

which is about 3 years. If one uses a nonlinear model to separate U.S. economy 
into “expansion” and “contraction” periods, the data show that the average duration 
of contraction periods is about three quarters and that of expansion periods is about 
3 years; see the analysis in Chapter 4. The average duration of 10.62 quarters is 
a compromise between the two separate durations. The periodic feature obtained 
here is common among growth rates of national economies. For example, similar 
features can be found for many OECD (Organization for Economic Cooperation 
and Development) countries. 

R Demonstration 
The R demonstration for Example 2.1, where % denotes explanation, follows: 

> gnp=scan(file='dgnp82.txt') % Load data 

% To create a time-series object 

> gnpl=ts(gnp,frequency=4,start=c(1947, 2) ) 

> plot(gnpl) 

SIMPLE AR MODELS 

> points(gnpl,pch='*') 

45 

> ml=ar(gnp,method=''mle'') % Find the AR order 

> ml$order % An AR(3) is selected based on AIC 

[1] 3 

> m2=arima(gnp,order=c(3,0,0)) % Estimation 

> m2 

Call: 

arima(x = gnp, order = c(3, 0, 0)) 

Coefficients: 

arl ar2 ar3 intercept 

0.3480 0.1793 -0.1423 0.0077 

s.e. 0.0745 0.0778 0.0745 0.0012 

sigma~2 estimated as 9.427e-05: log likelihood=565.84, 

aic=-1121.68 

% In R, ''intercept'' denotes the mean of the series. 

% Therefore, the constant term is obtained below: 

> (1-.348-.1793+.1423)*0.0077 

[1] 0.0047355 

> sqrt(m2$sigma2) % Residual standard error 

[1] 0.009709322 

> pl=c(1,-m2$coef[1:3]) % Characteristic equation 

> roots=polyroot(pi) % Find solutions 

> roots 

[1] 1.590253+1.063882i -1.920152+0.OOOOOOi 1.590253-1.063882i 

> Mod(roots) % Compute the absolute values of the solutions 

[1] 1.913308 1.920152 1.913308 

% To compute average length of business cycles: 

> k=2*pi/acos(l.590253/1.913308) 
> k 
[1] 10.65638 

Stationarity 
The stationarity condition of an AR(2) time series is that the absolute values of 
its two characteristic roots are less than 1, that is, its two characteristic roots 
are less than 1 in modulus. Equivalently, the two solutions of the characteristic 
equation are greater than 1 in modulus. Under such a condition, the recursive 
equation in (2.13) ensures that the ACF of the model converges to 0 as the 
lag t increases. This convergence property is a necessary condition for a sta¬ 
tionary time series. In fact, the condition also applies to the AR(1) model where 
the polynomial equation is 1 — <p\x = 0. The characteristic root is w = l/x = 0i, 
which must be less than 1 in modulus for rt to be stationary. As shown before, 
pi — for a stationary AR(1) model. The condition implies that pi -> 0 as 
l —> oo. 

46 

LINEAR TIME SERIES ANALYSIS AND ITS APPLICATIONS 

AR(p) Model 
The results of the AR(1) and AR(2) models can readily be generalized to the 
general AR(/?) model in Eq. (2.9). The mean of a stationary series is 

E(rt) = 

_00_ 
1 0] — • • • (f)p 

provided that the denominator is not zero. The associated characteristic equation 
of the model is 

1 — (p\X — <p2X2 — • • • — (f)pXP — 0. 

If all the solutions of this equation are greater than 1 in modulus, then the series 
rt is stationary. Again, inverses of the solutions are the characteristic roots of the 
model. Thus, stationarity requires that all characteristic roots are less than 1 in 
modulus. For a stationary AR(p) series, the ACF satisfies the difference equation 

(1 — (p\B — 0252-(/)pBp)pi=0, for l> 0. 

The plot of ACF of a stationary AR(p) model would then show a mixture of 
damping sine and cosine patterns and exponential decays depending on the nature 
of its characteristic roots. 

2.4.2 Identifying AR Models in Practice 

In application, the order p of an AR time series is unknown. It must be specified 
empirically. This is referred to as the order determination (or order specification) 
of AR models, and it has been extensively studied in the time series literature. Two 
general approaches are available for determining the value of p. The first approach 
is to use the partial autocorrelation function, and the second approach uses some 
information criteria. 

Partial Autocorrelation Function (PACF) 
The PACF of a stationary time series is a function of its ACF and is a useful 
tool for determining the order p of an AR model. A simple, yet effective way to 
introduce PACF is to consider the following AR models in consecutive orders: 

rt — 0o,l + 0i,i*7-i + ^11, 

*7 — 0o,2 + 01,2*7-1 + 02,2*7-2 + ?2t, 

*7 = 00,3 + 01,3*" t — \ + 02,3*" f—2 + 03,3*7—3 + ^31, 

*7 = 00,4 + 01,4*7-1 + 02,4*7-2 + 03,4*7-3 + 04,4*7-4 + e41, 

SIMPLE AR MODELS 

47 

where 0oj, 0,-j, and are, respectively, the constant term, the coefficient of 
rt~i, and the error term of an AR(/) model. These models are in the form of a 
multiple linear regression and can be estimated by the least-squares method. As a 
matter of fact, they are arranged in a sequential order that enables us to apply the 
idea of partial F test in multiple linear regression analysis. The estimate 0u of 
the first equation is called the lag-1 sample PACF of rt. The estimate 02.2 of the 
second equation is the lag-2 sample PACF of rt. The estimate 03 3 of the third 
equation is the lag-3 sample PACF of rt, and so on. 

From the definition, the lag-2 PACF 02;2 shows the added contribution of r?_2 
to rt over the AR(1) model rt = 0o + 0irf_i +e\t. The lag-3 PACF shows the 
added contribution of r,_3 to rt over an AR(2) model, and so on. Therefore, for 
an AR(p) model, the lag-p sample PACF should not be zero, but 4>jj should be 
close to zero for all j > p. We make use of this property to determine the order 
p. For a stationary Gaussian AR(p) model, it can be shown that the sample PACF 
has the following properties: 

• 4>p<p converges to (pp as the sample size T goes to infinity. 

/V 

• <piti converges to zero for all i > p. 

• The asymptotic variance of 0^ is \/T for l > p. 

These results say that, for an AR(p) series, the sample PACF cuts off at 
lag p. 

As an example, consider the monthly simple returns of CRSP value-weighted 
index from January 1926 to December 2008. Table 2.1 gives the first 12 lags of 
sample PACF of the series. With T = 996, the asymptotic standard error of the 
sample PACF is approximately 0.032. Therefore, using the 5% significant level, 
we identify an AR(3) or AR(9) model for the data (i.e., p = 3 or 9). If the 1% 
significant level is used, we specify an AR(3) model. 

As another example, Figure 2.6 shows the PACF of the GNP growth rate series 
of Example 2.1. The two dotted lines of the plot denote the approximate two 
standard error limits ±(2/Vl76). The plot suggests an AR(3) model for the data 
because the first three lags of sample PACF appear to be large. 

TABLE 2.1 Sample Partial Autocorrelation Function and Some Information 
Criteria for the Monthly Simple Returns of CRSP Value-Weighted Index from 
January 1926 to December 2008 

p 
PACF 
AIC 
BIC 

P 
PACF 
AIC 
BIC 

1 

2 

0.115 
-5.838 
-5.833 
7 
0.031 
-5.846 
-5.812 

-0.030 
-5.837 
-5.827 

8 

0.052 
-5.847 
-5.807 

3 
-0.102 
-5.846 
-5.831 
9 
0.063 
-5.849 
-5.805 

4 
0.033 
-5.845 
-5.825 

5 
0.062 
-5.847 
-5.822 

6 

-0.050 
-5.847 
-5.818 

10 

11 

12 

0.005 
-5.847 
-5.798 

-0.005 
-5.845 
-5.791 

0.011 

-5.843 
-5.784 

48 

LINEAR TIME SERIES ANALYSIS AND ITS APPLICATIONS 

2 4 6 8 10 12 

Lag 

Figure 2.6 Sample partial autocorrelation function of U.S. quarterly real GNP growth rate from 1947.13 

to 1991.1. Dotted lines give approximate pointwise 95% confidence interval. 

Information Criteria 
There are several information criteria available to determine the order p of an AR 
process. All of them are likelihood based. For example, the well-known Akaike 
information criterion (AIC) (Akaike, 1973) is defined as 

-2 2 

AIC = — In (likelihood) + — x (number of parameters), (2.16) 

where the likelihood function is evaluated at the maximum-likelihood estimates 
and T is the sample size. For a Gaussian AR(f) model, AIC reduces to 

2 f 
AIC(f) = In (of) + - 

where of is the maximum-likelihood estimate of erf, which is the variance of at, 
and T is the sample size; see Eq. (1.18). The first term of the AIC in Eq. (2.16) 
measures the goodness of fit of the AR(£) model to the data, whereas the second 
term is called the penalty function of the criterion because it penalizes a candidate 
model by the number of parameters used. Different penalty functions result in 
different information criteria. 

Another commonly used criterion function is the Schwarz-Bayesian information 

criterion (BIC). For a Gaussian AR(f) model, the criterion is 

BIC(f) = In (erf) + 

fin (T) 

T 

SIMPLE AR MODELS 

49 

The penalty for each parameter used is 2 for AIC and ln(T) for BIC. Thus, com¬ 
pared with AIC, BIC tends to select a lower AR model when the sample size is 
moderate or large. 

Selection Rule 

To use AIC to select an AR model in practice, one computes AIC(T) for i = 
0, ..., P, where p is a prespecified positive integer and selects the order k that has 
the minimum AIC value. The same rule applies to BIC. 

Table 2.1 also gives the AIC and BIC for p = 1, ..., 12. The AIC values are 
close to each other with minimum —5.849 occurring at p = 9, suggesting that an 
AR(9) model is preferred by the criterion. The BIC, on the other hand, attains 
its minimum value —5.833 at p = 1 with —5.831 as a close second at p = 3. 
Thus, the BIC selects an AR(1) model for the value-weighted return series. This 
example shows that different approaches or criteria to order determination may 
result in different choices of p. There is no evidence to suggest that one approach 
outperforms the other in a real application. Substantive information of the problem 
under study and simplicity are two factors that also play an important role in 
choosing an AR model for a given time series. 

Again, consider the growth rate series of U.S. quarterly real GNP of 
Example 2.1. The AIC obtained from R also identifies an AR(3) model. Note that 
the AIC value of the ar command in R has been adjusted so that the minimum 
AIC is zero. 

> gnp=scan(file='q-gnp4791.txt') 
> ord=ar(gnp,method=''mle'') 
> ord$aic 

[1] 27.847 2.742 1.603 0.000 0.323 2.243 
[7] 4.052 6.025 5.905 7.572 7.895 9.679 

> ord$order 

[1] 3 

Parameter Estimation 
For a specified AR(p) model in Eq. (2.9), the conditional least-squares method, 
which starts with the (p + l)th observation, is often used to estimate the parameters. 
Specifically, conditioning on the first p observations, we have 

rt — 00 + 0177-1 + • • • + 4>prt-p + <27, t = p -\- 1, ..., T, 

which is in the form of a multiple linear regression and can be estimated by the 
least-squares method. Denote the estimate of 0, by 0,. The fitted model is 

/V /V /V 

ft = 00 + 0177-1 + • • • + 4>prt-p, 

and the associated residual is 

50 

LINEAR TIME SERIES ANALYSIS AND ITS APPLICATIONS 

The series {at} is called the residual series, from which we obtain 

a2 
/—it=p+l ut 

_ t -2P- r 

If the conditional-likelihood method is used, the estimates of 0, remain unchanged, 
but the estimate of a2 becomes d2 = a2 x (T — 2p — 1 )/{T — p). In some pack¬ 
ages, a2 is defined as d2 x (T — 2p — l)/T. For illustration, consider an AR(3) 
model for the monthly simple returns of the value-weighted index in Table 2.1. 

The fitted model is 

rt =0.0091 +0.116rf_, - 0.019r,_2 - 0.104rf_3 + at, da = 0.054. 

The standard errors of the coefficients are 0.002, 0.032, 0.032, and 0.032, respec¬ 
tively. Except for the lag-2 coefficient, all parameters are statistically significant at 

the 1% level. 

For this example, the AR coefficients of the fitted model are small, indicating that 
the serial dependence of the series is weak, even though it is statistically significant 
at the 1% level. The significance of 0o of the entertained model implies that the 
expected mean return of the series is positive. In fact, p — 0.0091/(1 — 0.116 + 
0.019 + 0.104) = 0.009, which is small but has an important long-term implication. 
It implies that the long-term return of the index can be substantial. Using the 
multiperiod simple return defined in Chapter 1, the average annual simple gross 

return is [nf=l(1 + Rr)]12/996 — 1 ~ 0.093. In other words, the monthly simple 
returns of the CRSP value-weighted index grew about 9.3% per annum from 1926 
to 2008, supporting the common belief that equity market performs well in the 
long term. A one-dollar investment at the beginning of 1926 would be worth about 
$1593 at the end of 2008. 

> vw=read.table('m-ibm3dx.txtheader=T)[,3] 

> tl=prod(vw+1) 
> tl 
[1] 1592.953 
> tl7'(12/996)-1 
[1] 0.0929 

Model Checking 
A fitted model must be examined carefully to check for possible model inadequacy. 
If the model is adequate, then the residual series should behave as a white noise. 
The ACF and the Ljung-Box statistics in Eq. (2.3) of the residuals can be used to 
check the closeness of dt to a white noise. For an AR(p) model, the Ljung-Box 
statistic Q{m) follows asymptotically a chi-squared distribution with m — g degrees 
of freedom, where g denotes the number of AR coefficients used in the model. The 
adjustment in the degrees of freedom is made based on the number of constraints 
added to the residuals at from fitting the AR(p) to an AR(0) model. If a fitted 

SIMPLE AR MODELS 

51 

model is found to be inadequate, it must be refined. For instance, if some of the 
estimated AR coefficients are not significantly different from zero, then the model 
should be simplified by trying to remove those insignificant parameters. If residual 
ACF shows additional serial correlations, then the model should be extended to 
take care of those correlations. 

Remark. Most time series packages do not adjust the degrees of freedom when 
applying the Ljung-Box statistics Q{m) to a residual series. This is understandable 
when m <g- □ 

Consider the residual series of the fitted AR(3) model for the monthly value- 
weighted simple returns. We have 2(12) = 16.35 with a p value 0.060 based on 
its asymptotic chi-squared distribution with 9 degrees of freedom. Thus, the null 
hypothesis of no residual serial correlation in the first 12 lags is barely not rejected 
at the 5% level. However, since the lag-2 AR coefficient is not significant at the 
5% level, one can refine the model as 

rt = 0.0088 + 0.114r,_i - 0.106rr_3 + at, oa = 0.0536, 

where all the estimates are now significant at the 1% level. The residual series 
gives 2(12) = 16.83 with a p value 0.078 (based on /j20). The model is adequate 
in modeling the dynamic linear dependence of the data. 

R Demonstration 
In the following R demonstration, % denotes an explanation: 

> vw=read.table('m-ibm3dx2608.txtheader=T)[,3] 

> m3=arima(vw,order=c(3,0,0)) 

> m3 

Call: 

arimafx = vw, order = c(3, 0, 0)) 

Coefficients: 

arl ar2 ar3 intercept 

0.1158 -0.0187 -0.1042 0.0089 

s.e. 0.0315 0.0317 0.0317 0.0017 

sigmaA,2 estimated as 0.002875: log likelihood=1500.86, 

aic=-2991.73 

> (1-.1158+.0187+.1042)*mean(vw) % Compute 

the intercept phi(0). 

[1] 0.00896761 

> sqrt(m3$sigma2) % Compute standard error of residuals 

[1] 0.0536189 

> Box.test(m3$residuals,lag=12,type='Ljung') 

52 

LINEAR TIME SERIES ANALYSIS AND ITS APPLICATIONS 

Box-Ljung test 

data: m3$residuals % R uses 12 degrees of freedom 

X-squared = 16.3525, df = 12, p-value = 0.1756 

> pv=l-pchisq(16.35,9) % Compute p-value using 9 degrees 

of freedom 

> pv 

[1] 0.05992276 

% To fix the AR(2) coef to zero: 

> m3=arima(vw,order=c(3,0,0),fixed=c(NA,0,NA,NA)) 

% The subcommand 'fixed' is used to fix parameter values, 

% where NA denotes estimation and 0 means fixing the 

parameter to 0. 

% The ordering of the parameters can be found using m3$coef. 

> m3 

Call: 

arima(x = vw, order = c(3, 0, 0), fixed = c(NA, 0, NA, NA)) 

Coefficients: 

arl ar2 ar3 intercept 

0.1136 0 -0.1063 0.0089 

s.e. 0.0313 0 0.0315 0.0017 

sigma/'2 estimated as 0.002876: log likelihood=1500.69, 

aic=-2993.38 

> (1-.1136+.1063)*.0089 % Compute phi(0) 

[1] 0.00883503 

> sqrt(m3$sigma2) % Compute residual standard error 

[1] 0.05362832 

> Box.test(m3$residuals,lag=12,type='Ljung' ) 

Box-Ljung test 

data: m3$residuals 

X-squared = 16.8276, df = 12, p-value = 0.1562 

> pv=l-pchisq(16.83,10) 

> pv 

[1] 0.0782113 

S-Plus Demonstration 
The following S-Plus output has been edited: 

> vw=read.table('m-ibm3dx2608.txt',header=T)[,3] 

> ar3=OLS(vw ar(3)) 

53 

SIMPLE AR MODELS 

> summary(ar3) 

Call: 

OLS(formula = vw ~ ar(3)) 

Residuals: 

Min IQ Median 3Q Max 

-0.2863 -0.0263 0.0034 0.0297 0.3689 

Coefficients: 

(Intercept) 

0.0091 

0.0018 5.1653 

0.0000 

Value 

Std. Error t value 

Pr (> 111) 

lagl 

lag2 

0.1148  0.0316 

3.6333 

0.0003 

-0.0188  0.0318 

-0.5894  0.5557 

lag3 

-0.1043 

0.0318 

-3.2763  0.0011 

Regression Diagnostics: 

R-Squared 0.0246 

Adjusted R-Squared 0.0216 

Durbin-Watson Stat 1.9913 

Residual Diagnostics: 

Stat P-Value 

Jarque-Bera 1656.3928 0.0000 

Ljung-Box 50.1279 0.0087 

Residual standard error: 0.05375 on 989 degrees of freedom 

> autocorTest(ar3$residuals,lag=12) 

Test for Autocorrelation: Ljung-Box 

Null Hypothesis: no autocorrelation 

Test Statistics: 

Test Stat 16.5668 

p.value 0.1666 % S-Plus uses 12 degrees of freedom 

Dist. under Null: chi-square with 12 degrees of freedom 

Total Observ.: 993 

> l-pchisq(16.57,9) % Compute p-value with 9 degrees 

of freedom 

[1] 0.05589128 

2.4.3 Goodness of Fit 

A commonly used statistic to measure goodness of fit of a stationary model is the 
R square (R2) defined as 

^ ^ residual sum of squares 

total sum of squares 

54 

LINEAR TIME SERIES ANALYSIS AND ITS APPLICATIONS 

For a stationary AR(p) time series model with T observations {rt\t = 1, ..., T}, 

the measure becomes 

R2 = 1- f'=P+\a!—, 
ELp+i(A - r)2 

a2 

where r — Ylt=p+1 rt/(T — p). It is easy to show that 0 < R2 < 1. Typically, a 
larger R2 indicates that the model provides a closer fit to the data. However, this is 
only true for a stationary time series. For the unit-root nonstationary series discussed 
later in this chapter, R2 of an AR(1) fit converges to one when the sample size 
increases to infinity, regardless of the true underlying model of rt. 

For a given data set, it is well known that R2 is a nondecreasing function of 
the number of parameters used. To overcome this weakness, an adjusted R2 is 
proposed, which is defined as 

r* variance of residuals 
Adj - R2 = 1-:-7- 

variance of rt 

where a2 is the sample variance of r,. This new measure takes into account the 
number of parameters used in the fitted model. However, it is no longer between 
0 and 1. 

2.4.4 Forecasting 

Forecasting is an important application of time series analysis. For the AR(p) 
model in Eq. (2.9), suppose that we are at the time index h and are interested 
in forecasting r^+i, where i > 1. The time index h is called the forecast origin 
and the positive integer l is the forecast horizon. Let ?h(l) be the forecast of r^+z 
using the minimum squared error loss function. In other words, the forecast ?k{l) 
is chosen such that 

E{[rh+z-rh(l)]2\Fh} < min E[(rh+t - g)2\Fh], 

g 

where g is a function of the information available at time h (inclusive), that is, 
a function of F/,. We referred to r/,(f) as the f-step ahead forecast of rt at the 
forecast origin h. Let F/, be the collection of information available at the forecast 
origin h. 

1-Step-Ahead Forecast 
From the AR(p) model, we have 

rh+\ = <^0 + 01 rh + • • • + 0/T/i + l-p + Qh+1- 

SIMPLE AR MODELS 

55 

Under the minimum squared error loss function, the point forecast of r/,+1 given 
Fh is the conditional expectation 

p 

i=l 

and the associated forecast error is 

0,( 1) = 0,+1 - r*(l) = ah+i. 

Consequently, the variance of the 1-step-ahead forecast error is Var[<?/,(1)] = 
Var(a/2+i) = cr2. If a, is normally distributed, then a 95% 1-step-ahead interval 
forecast of r^+i is r/,(1) ± 1.96 x oa. For the linear model in Eq. (2.4), at+\ is also 
the 1-step-ahead forecast error at the forecast origin t. In the econometric literature, 
at+1 is referred to as the shock to the series at time t + 1. 

In practice, estimated parameters are often used to compute point and interval 
forecasts. This results in a conditional forecast because such a forecast does not 
take into consideration the uncertainty in the parameter estimates. In theory, one 
can consider parameter uncertainty in forecasting, but it is much more involved. A 
natural way to consider parameter and model uncertainty in forecasting is Bayesian 
forecasting with Markov chan Monte Carlo (MCMC) methods. See Chapter 12 for 
further discussion. For simplicity, we assume that the model is given in this chapter. 
When the sample size used in estimation is sufficiently large, then the conditional 
forecast is close to the unconditional one. 

2-Step-Ahead Forecast 
Next consider the forecast of rh+i at the forecast origin h. From the AR(p) model, 
we have 

Ph+2 — 00 + 010,+t + ' ' ' + <PpPh+2-p + ah + 2- 

Taking conditional expectation, we have 

hi2) = E(rh+2\Fh) = 0O + 010,(1) + 02O, H-h (pprh+2-p 

and the associated forecast error 

0,(2) = Oz+2 — 0,(2) = </>l[Oi+l ~ Phi 1)] + ah+ 2 = <*h+2 + + 

The variance of the forecast error is Var[c/,(2)J = (1 + 0^)oa2. Interval forecasts 
of rh+2 can be computed in the same way as those for o,+i- It is interesting to 
see that Var[e^(2)] > Var[o(l)], meaning that as the forecast horizon increases 
the uncertainty in forecast also increases. This is in agreement with common sense 

56 

LINEAR TIME SERIES ANALYSIS AND ITS APPLICATIONS 

that we are more uncertain about rh+2 than r^+\ at the time index h for a linear 

time series. 

Multistep-Ahead Forecast 
In general, we have 

fh+e = 00 + fafh+i-l + • • • + 4>prh+l-p + O-h+t- 

The f-step-ahead forecast based on the minimum squared error loss function is the 
conditional expectation of rh+i given Ff,, which can be obtained as 

p 

W) =<p0 + ^2 fan~ 0, 
i=1 

where it is understood that (i) = r^+i if i < 0. This forecast can be computed 
recursively using forecasts r^ (i) for i = 1.i — 1. The i-step-ahead forecast 
error is e^(f) = rh+i — hif)- It can be shown that for a stationary AR(p) model, 
?h{i) converges to E(rt) as l -» 00, meaning that for such a series long-term point 
forecast approaches its unconditional mean. This property is referred to as the mean 
reversion in the finance literature. For an AR(1) model, the speed of mean reversion 
is measured by the half-life defined as l = ln(0.5)/ln(|0i |). The variance of the 
forecast error then approaches the unconditional variance of r,. Note that for an 
AR(1) model in (2.8), let xt = rt — E(rt) be the mean-adjusted series. It is easy to 
see that the i-step-ahead forecast of Xh+i at the forecast orign h is Xh(i) = f[xh. 
The half-life is the forecast horizon such that Xf,(£) = jXj,. That is, = \. Thus, 
l = ln(O.5)/ln(|01|). 

Table 2.2 contains the 1-step- to 12-step ahead forecasts and the standard errors 
of the associated forecast errors at the forecast origin 984 for the monthly simple 
return of the value-weighted index using an AR(3) model that was reestimated 
using the first 984 observations. The fitted model is 

rt = 0.0098 + 0.1024r?_! - 0.0201r,_2 - 0.1090rr_3 + at. 

where cra = 0.054. The actual returns of 2008 are also given in Table 2.2. Because 
of the weak serial dependence in the series, the forecasts and standard deviations 
of forecast errors converge to the sample mean and standard deviation of the data 
quickly. For the first 984 observations, the sample mean and standard error are 
0.0095 and 0.0540, respectively. 

Figure 2.7 shows the corresponding out-of-sample prediction plot for the 
monthly simple return series of the value-weighted index. The forecast origin 
t — 984 corresponds to December 2007. The prediction plot includes the two 
standard error limits of the forecasts and the actual observed returns for 2008. 
The forecasts and actual returns are marked by ° and •, respectively. From the 
plot, except for the return of October 2008, all actual returns are within the 95% 
prediction intervals. 

SIMPLE MA MODELS 

TABLE 2.2 Multistep Ahead Forecasts of an AR(3) Model for Monthly Simple 
Returns of CRSP Value-Weighted Index 
Step 

1 

2 

4 

3 

5 

57 

6 

Forecast 
Std. Error 
Actual 

Step 

Forecast 
Std. Error 
Actual 

0.0076 
0.0534 
-0.0623 

7 

0.0095 
0.0540 
-0.0132 

0.0161 
0.0537 
-0.0220 

8 

0.0097 
0.0540 
0.0110 

0.0118 
0.0537 
-0.0105 

9 

0.0096 
0.0540 
-0.0981 

0.0099 
0.0540 
0.0511 

10 

0.0096 
0.0540 
-0.1847 

0.0089 
0.0540 
0.0238 

11 

0.0096 
0.0540 
-0.0852 

0.0093 
0.0540 
-0.0786 

12 

0.0096 
0.0540 
0.0215 

“The forecast origin is h = 984. 

Figure 2.7 Plot of 1- to 12-step-ahead out-of-sample forecasts for monthly simple returns of CRSP 

value-weighted index. Forecast origin is t = 984, which is December 2007. Forecasts are denoted by 

“o” and actual observations by Two dashed lines denote two standard error limits of the forecasts. 

Time 

2.5 SIMPLE MA MODELS 

We now turn to another class of simple models that are also useful in model¬ 
ing return series in finance. These models are the moving-average (MA) models. 
As is shown in Chapter 5, the bid-ask bounce in stock trading may introduce 
an MA(1) structure in a return series. There are several ways to introduce MA 
models. One approach is to treat the model as a simple extension of white noise 
series. Another approach is to treat the model as an infinite-order AR model with 
some parameter constraints. We adopt the second approach. 

58 

LINEAR TIME SERIES ANALYSIS AND ITS APPLICATIONS 

There is no particular reason, but simplicity, to assume a priori that the order 
of an AR model is finite. We may entertain, at least in theory, an AR model with 

infinite order as 

rt = 00 + <P\rt-\ + <t>2rt-2 + • ■ ■ + df 

However, such an AR model is not realistic because it has infinite many parameters. 
One way to make the model practical is to assume that the coefficients </>,•’s satisfy 
some constraints so that they are determined by a finite number of parameters. A 

special case of this idea is 

rt = 0o ~ ~ 6\rt-2 ~ #i3A-3 — ■ ■ ■ +at, (2.17) 

where the coefficients depend on a single parameter 6\ via 0,- = — 9[ for i > 1. For 
the model in Eq. (2.17) to be stationary, 6\ must be less than 1 in absolute value; 
otherwise, 0[ and the series will explode. Because |6fi| < 1, we have 0[ —»■ 0 as 
i —> oo. Thus, the contribution of to r, decays exponentially as i increases. 
This is reasonable as the dependence of a stationary series rt on its lagged value 
rt-i, if any, should decay over time. 

The model in Eq. (2.17) can be rewritten in a rather compact form. To see this, 

rewrite the model as 

rt + 0\rt-\ + &\rt-2 + • • • — 0o + dt. (2.18) 

The model for r,_ 1 is then 

rt-1 + 0\rt-2 + Q\rt-3 + • • • — 0o + at-1- (2.19) 

Multiplying Eq. (2.19) by and subtracting the result from Eq. (2.18), we obtain 

r, = 0O(1 -9i) + at -6\at-\, 

which says that except for the constant term rt is a weighted average of shocks a, 
and a,[—\. Therefore, the model is called an MA model of order 1 or MA(1) model 
for short. The general form of an MA(1) model is 

rt = c0 + at - Q\at-\ or r, = c0 + (1 - 0\B)at, (2.20) 

where Co is a constant and {a,} is a white noise series. Similarly, an MA(2) model 
is in the form 

r, = c0 + a, - 0\at-.\ - 92at-2, 

(2.21) 

and an MA(g) model is 

T/ — Cq T dt 0\d(—\ • • ■ OqUf—q, 

(2.22) 

or r, = co + (1 — 9\B — ■ ■ ■ — OqBq)at, where q > 0. 

SIMPLE MA MODELS 

59 

2.5.1 Properties of MA Models 

Again, we focus on the simple MA(1) and MA(2) models. The results of MA(g) 
models can easily be obtained by the same techniques. 

Stationarity 

Moving-average models are always weakly stationary because they are finite linear 
combinations of a white noise sequence for which the first two moments are time 
invariant. For example, consider the MA(1) model in Eq. (2.20). Taking expectation 
of the model, we have 

E(rt) = c0, 

which is time invariant. Taking the variance of Eq. (2.20), we have 

Var(r;) = o2a + Pfcr2 = (1 + P2)<r2, 

where we use the fact that at and at-\ are uncorrelated. Again, Var(rr) is time 
invariant. The prior discussion applies to general MA(g) models, and we obtain 
two general properties. First, the constant term of an MA model is the mean of the 
series [i.e., E(rt) = Co]. Second, the variance of an MA(g) model is 

Var (rf) = (1 + P2 + P2 + • • • + P2),o-2. 

Autocorrelation Function 
Assume for simplicity that cq = 0 for an MA(1) model. Multiplying the model by 
rt-i, we have 

rt-irt = rt-iat - 6\rt-iat-\. 

Taking expectation, we obtain 

Yi = —9\crl and yi = 0, for l > 1. 

Using the prior result and the fact that Var(r,) = (1 + we have 

Po = 1, 

Pi 

-01 

1+0?’ 

Pi = 0, for i > 1. 

Thus, for an MA(1) model, the lag-1 ACF is not zero, but all higher order ACFs 
are zero. In other words, the ACF of an MA(1) model cuts off at lag 1. For the 
MA(2) model in Eq. (2.21), the autocorrelation coefficients are 

Pi 

—6\ + 0\ $2 
r+pf+pf’ 

P2 

~ P2 

T+pf+pf’ 

Pi = 0, for t > 2. 

Here the ACF cuts off at lag 2. This property generalizes to other MA models. For 
an MA(g) model, the lag-q ACF is not zero, but pi = 0 for t > q. Consequently, 

60 

LINEAR TIME SERIES ANALYSIS AND ITS APPLICATIONS 

an MA(g) series is only linearly related to its first g-lagged values and hence is a 

“finite-memory” model. 

Invertibility 
Rewriting a zero-mean MA(1) model as a, = rt + 9\at-\, one can use repeated 

substitutions to obtain 

at — rt + Q\rt~\ + d\rt-2 + 9\rt-3 + • • * • 

This equation expresses the current shock a, as a linear combination of the present 
and past returns. Intuitively, Q{ should go to zero as j increases because the remote 
return rt_y should have very little impact on the current shock, if any. Consequently, 
for an MA(1) model to be plausible, we require \0\\ < 1. Such an MA(1) model 
is said to be invertible. If \6\ \ = 1, then the MA(1) model is noninvertible. See 
Section 2.6.5 for further discussion on invertibility. 

2.5.2 Identifying MA Order 

The ACF is useful in identifying the order of an MA model. For a time series rt 
with ACF pi, if pq ^ 0, but pi = 0 for l > q, then rt follows an MA(<20 model. 

Figure 2.8 shows the time plot of monthly simple returns of the CRSP equal- 
weighted index from January 1926 to December 2008 and the sample ACF of the 
series. The two dashed lines shown on the ACF plot denote the two standard error 

Figure 2.8 Time plot and sample autocorrelation function of monthly simple returns of CRSP equal- 
weighted index from January 1926 to December 2008. 

SIMPLE MA MODELS 

61 

limits. It is seen that the series has significant ACF at lags 1, 3, and 9. There are 
some marginally significant ACF at higher lags, but we do not consider them here. 
Based on the sample ACF, the following MA(9) model 

rt — co + — 0\at-.\ — d^at-j — dgat-9 

is identified for the series. Note that, unlike the sample PACF, sample ACF provides 
information on the nonzero MA lags of the model. 

2.5.3 Estimation 

Maximum-likelihood estimation is commonly used to estimate MA models. There 
are two approaches for evaluating the likelihood function of an MA model. The 
first approach assumes that the initial shocks (i.e., at for t < 0) are zero. As such, 
the shocks needed in likelihood function calculation are obtained recursively from 
the model, starting with a\=r\ — cq and a2 = r2 — Co + 6\a\. This approach is 
referred to as the conditional-likelihood method and the resulting estimates the 
conditional maximum-likelihood estimates. The second approach treats the initial 
shocks at, t < 0, as additional parameters of the model and estimate them jointly 
with other parameters. This approach is referred to as the exact-likelihood method. 
The exact-likelihood estimates are preferred over the conditional ones, especially 
when the MA model is close to being noninvertible. The exact method, however, 
requires more intensive computation. If the sample size is large, then the two types 
of maximum-likelihood estimates are close to each other. For details of conditional- 
and exact-likelihood estimates of MA models, readers are referred to Box, Jenkins, 
and Reinsel (1994) or Chapter 8. 

For illustration, consider the monthly simple return series of the CRSP equal- 
weighted index and the specified MA(9) model. The conditional maximum- 
likelihood method produces the fitted model 

rr = 0.012 + a,+0.189flf_!-0.121flr_3+0.122af_9, oa = 0.0714, (2.23) 

where standard errors of the coefficient estimates are 0.003, 0.031, 0.031, and 
0.031, respectively. The Ljung-Box statistics of the residuals give <2(12) = 17.5 
with a p value 0.041, which is based on an asymptotic chi-squared distribution 
with 9 degrees of freedom. The model needs some refinements in modeling the 
linear dynamic dependence of the data. The p value would be 0.132 if 12 degrees 
of freedom are used. The exact maximum-likelihood method produces the fitted 

model 

rt = 0.012 + at +0.191af_i -0.120a,_3 +0.123a,_9, oa = 0.0714, (2.24) 

where standard errors of the estimates are 0.003, 0.031, 0.031, and 0.031, respec¬ 
tively. The Ljung-Box statistics of the residuals give <2(12) = 17.6. The corre¬ 
sponding p values are 0.040 and 0.128, respectively, when the degrees of freedom 

62 

LINEAR TIME SERIES ANALYSIS AND ITS APPLICATIONS 

are 9 and 12. Again, this fitted model is only marginally adequate. Comparing 
models (2.23) and (2.24), we see that for this particular instance, the difference 
between the conditional- and exact-likelihood methods is negligible. 

2.5.4 Forecasting Using MA Models 

Forecasts of an MA model can easily be obtained. Because the model has finite 
memory, its point forecasts go to the mean of the series quickly. To see this, assume 
that the forecast origin is h and let Fh denote the information available at time h. 
For the 1-step-ahead forecast of an MA(1) process, the model says 

rh+1 = cq + fl/H-1 - 010/1- 

Taking the conditional expectation, we have 

h,(1) = E(rh+i\Fh) = c0 - 010/,, 

c/,(l) = rh+\ - o,(l) = ah+i. 

The variance of the 1-step-ahead forecast error is Var|>*,(l)] = cr2. In practice, 
the quantity a/, can be obtained in several ways. For instance, assume that ao = 0, 
then a\ = r\ — co, and we can compute at for 2 < / < h recursively by using at = 
r, — co + 010,-1. Alternatively, it can be computed by using the AR representation 
of the MA(1) model; see Section 2.6.5. Of course, at is the residual series of a 
fitted MA(1) model. Thus, a/, is readily available from the estimation. 

For the 2-step-ahead forecast from the equation 

C/,+2 = Co + 0/,+2 ~ 010/1+1, 

we have 

0,(2) = E(rh+2\Fh) = c0, 

C/,(2) = Oi+ 2 — C/,(2) = 0/,+2 — 0i0/,+i. 

The variance of the forecast error is Var[c/,(2)] = (1 + 02)ct2, which is the variance 
of the model and is greater than or equal to that of the 1-step-ahead forecast error. 
The prior result shows that for an MA(1) model the 2-step-ahead forecast of the 
series is simply the unconditional mean of the model. This is true for any forecast 
origin h. More generally, o,(£) — cq for l > 2. In summary, for an MA(1) model, 
the 1-step-ahead point forecast at the forecast origin h is co — 0i0/, and the multistep 
ahead forecasts are co, which is the unconditional mean of the model. If we plot 
the forecasts r/,(€) versus i, we see that the forecasts form a horizontal line after 
one step. Thus, for MA(1) models, mean reverting only takes one time period. 

Similarly, for an MA(2) model, we have 

rh+e — Co + 0/1+^ — 010/1+£_1 — 020/i+£—2, 

SIMPLE MA MODELS 

from which we obtain 

63 

hi 1) = c0 - 9iah - 62ah-i, 

hi2) = c0 - 92ah, 

h (f-) =— coi for i > 2. 

Thus, the multistep-ahead forecasts of an MA(2) model go to the mean of the series 
after two steps. The variances of forecast errors go to the variance of the series 
after two steps. In general, for an MA(g) model, multistep-ahead forecasts go to 
the mean after the first q steps. 

Table 2.3 gives some out-of-sample forecasts of an MA(9) model in the form of 
Eq. (2.24) for the monthly simple returns of the equal-weighted index at the forecast 
origin h = 986 (February 2008). The model parameters are reestimated using 
the first 986 observations. The sample mean and standard error of the estimation 
subsample are 0.0128 and 0.0736, respectively. As expected, the table shows that 
(a) the 10-step-ahead forecast is the sample mean, and (b) the standard deviations 
of the forecast errors converge to the standard deviation of the series as the forecast 
horizon increases. In this particular case, the point forecasts deviate substantially 
from the observed returns because of the worldwide financial crisis caused by the 
subprime mortgage problem and the collapse of Lehman Brothers. 

Summary 
A brief summary of AR and MA models is in order. We have discussed the fol¬ 
lowing properties: 

• For MA models, ACF is useful in specifying the order because ACF cuts off 

at lag q for an MA(^) series. 

• For AR models, PACF is useful in order determination because PACF cuts 

off at lag p for an AR(p) process. 

• An MA series is always stationary, but for an AR series to be stationary, all 

of its characteristic roots must be less than 1 in modulus. 

TABLE 2.3 Out-of-Sample Forecast Performance of an MA(9) Model for Monthly 

Simple Returns of CRSP Equal-Weighted Index” 

Step 

Forecast 
Std. Error 
Actual 

Step 

Forecast 
Std. Error 
Actual 

1 

0.0043 
0.0712 
-0.0260 

6 

0.0019 
0.0729 
0.0141 

2 

0.0136 
0.0724 
0.0312 

7 

0.0122 
0.0729 
-0.1209 

3 

0.0150 
0.0729 
0.0322 

8 

0.0056 
0.0729 
-0.2060 

4 

0.0144 

0.0729 
-0.0871 

9 

0.0085 
0.0729 
-0.1366 

5 

0.0120 
0.0729 
-0.0010 

10 

0.0128 
0.0734 
0.0431 

“The forecast origin is February 2008 With h = 986. The model is estimated by the exact maximum- 

likelihood method. 

64 

LINEAR TIME SERIES ANALYSIS AND ITS APPLICATIONS 

• For a stationary series, the multistep-ahead forecasts converge to the mean of 
the series, and the variances of forecast errors converge to the variance of the 
series as the forecast horizon increases. 

2.6 SIMPLE ARMA MODELS 

In some applications, the AR or MA models discussed in the previous sections 
become cumbersome because one may need a high-order model with many param¬ 
eters to adequately describe the dynamic structure of the data. To overcome this 
difficulty, the autoregressive moving-average (ARMA) models are introduced; see 
Box, Jenkins, and Reinsel (1994). Basically, an ARMA model combines the ideas 
of AR and MA models into a compact form so that the number of parameters used 
is kept small, achieving parsimony in parameterization. For the return series in 
finance, the chance of using ARMA models is low. However, the concept of ARMA 
models is highly relevant in volatility modeling. As a matter of fact, the generalized 
autoregressive conditional heteroscedastic (GARCH) model can be regarded as an 
ARMA model, albeit nonstandard, for the aj series; see Chapter 3 for details. In 
this section, we study the simplest ARMA( 1,1) model. 

A time series rt follows an ARMA( 1,1) model if it satisfies 

rt - 0ir,_i = 0o + at - 0iaf_i, (2.25) 

where {a,} is a white noise series. The left-hand side of the Eq. (2.25) is the AR 
component of the model and the right-hand side gives the MA component. The 
constant term is 0o- For this model to be meaningful, we need 0i ^ 9\ \ otherwise, 
there is a cancellation in the equation and the process reduces to a white noise series. 

2.6.1 Properties of ARMA(1,1) Models 

Properties of ARMA( 1,1) models are generalizations of those of AR(1) models 
with some minor modifications to handle the impact of the MA(1) component. We 
start with the stationarity condition. Taking expectation of Eq. (2.25), we have 

E(rt) - 0iE(rt-i) = 0o + E(at) - 0i£(af_i). 

Because E{a{) = 0 for all i, the mean of r, is 

EV \ 
E{rt) = ii = --— 
1 - 01 

provided that the series is weakly stationary. This result is exactly the same as that 
of the AR(1) model in Eq. (2.8). 

Next, assuming for simplicity that 0O = 0, we consider the autocovariance func¬ 

tion of ti. First, multiplying the model by at and taking expectation, we have 

E(rtat) = E(af) - Q\E(atat-\) = E(a.2) = a 

(2.26) 

SIMPLE ARMA MODELS 

Rewriting the model as 

65 

rt = 01^-t +at - 9iat-i 

and taking the variance of the prior equation, we have 

Var(rt) = 0?Var(rf_i) + o2a + 9\ol - 20i0lJE(rr_iar_1). 

Here we make use of the fact that rt-\ and at are uncorrelated. Using Eq. (2.26), 
we obtain 

Var(r,) - 0j!Var(r,_i) = (1 - 20^ + Q\)o2a. 

Therefore, if the series r, is weakly stationary, then Var(rt) = Var(r;_i) and we 
have 

Var(rr) = 

(1 

2016*1 + e\)ol 

1 0i 

Because the variance is positive, we need 0j < 1 (i.e., |0i| < 1). Again, this is 
precisely the same stationarity condition as that of the AR(1) model. 

To obtain the autocovariance function of rt, we assume 0o = 0 and multiply the 

model in Eq. (2.25) by rr_£ to obtain 

rtrt_n - (\)\rt-\rt-i — atrt-t - 9\at-\rt-i. 

For l — 1, taking expectation and using Eq. (2.26) for t — 1, we have 

7i - 01 Ko = -9io2a, 

where yi = Cov(rr, rt-i). This result is different from that of the AR(1) case for 
which yi — 0i yo = 0. However, for £ = 2 and taking expectation, we have 

n - 0m = o, 

which is identical to that of the AR(1) case. In fact, the same technique yields 

Yt ~ — 0, for l> 1. (2.27) 

In terms of ACF, the previous results show that for a stationary ARMA( 1,1) model 

P\ = 01 

01 °l 
Yo 

pe = (pipe-i, for i> 1. 

66 

LINEAR TIME SERIES ANALYSIS AND ITS APPLICATIONS 

Thus, the ACF of an ARMA(1,1) model behaves very much like that of an AR(1) 
model except that the exponential decay starts with lag 2. Consequently, the ACF 
of an ARMA(1,1) model does not cut off at any finite lag. 

Turning to PACF, one can show that the PACF of an ARMA(1,1) model does 
not cut off at any finite lag either. It behaves very much like that of an MA(1) 
model except that the exponential decay starts with lag 2 instead of lag 1. 

In summary, the stationarity condition of an ARMA(1,1) model is the same as 
that of an AR(1) model, and the ACF of an ARMA( 1,1) exhibits a similar pattern 
like that of an AR(1) model except that the pattern starts at lag 2. 

2.6.2 General ARMA Models 

A general ARMA(p, q) model is in the form 

p <? 

A = 0o + 01^t—i + at — 6jat-i, 

i=l i=l 

where {a,} is a white noise series and p and q are nonnegative integers. The AR 
and MA models are special cases of the ARMA(p, q) model. Using the back-shift 
operator, the model can be written as 

(1 - 0iR-(pPBp)rt = 0o + (1 - 6XB-0qBq)at. (2.28) 

The polynomial \ — (p\B — ■ ■ ■ — (ppBp is the AR polynomial of the model. Sim¬ 
ilarly, 1 — 0\ B — • • • — QqBq is the MA polynomial. We require that there are no 
common factors between the AR and MA polynomials; otherwise the order (p, q) 
of the model can be reduced. Like a pure AR model, the AR polynomial intro¬ 
duces the characteristic equation of an ARMA model. If all of the solutions of the 
characteristic equation are less than 1 in absolute value, then the ARMA model is 
weakly stationary. In this case, the unconditional mean of the model is E(rt) = 

0o/(l — 0i-<Pp)- 

2.6.3 Identifying ARMA Models 

The ACF and PACF are not informative in determining the order of an ARMA 
model. Tsay and Tiao (1984) propose a new approach that uses the extended auto¬ 
correlation function (EACF) to specify the order of an ARMA process. The basic 
idea of EACF is relatively simple. If we can obtain a consistent estimate of the AR 
component of an ARMA model, then we can derive the MA component. From the 
derived MA series, we can use ACF to identify the order of the MA component. 

The derivation of EACF is relatively involved; see Tsay and Tiao (1984) for 
details. Yet the function is easy to use. The output of EACF is a two-way table, 
where the rows correspond to AR order p and the columns to MA order q. The 
theoretical version of EACF for an ARMA(1,1) model is given in Table 2.4. The 

SIMPLE ARMA MODELS 

67 

TABLE 2.4 Theoretical EACF Table for an ARMA(1,1) Model, Where X Denotes 
Nonzero, O Denotes Zero, and * Denotes Either Zero or Nonzero0 

AR 

0 
1 
2 
3 
4 
5 

0 

X 
X 
* 
* 

* 
* 

1 

X 
0 
X 
* 

* 
* 

2 

X 
o 
0 
X 
* 
* 

MA 

3 

X 
O 
0 
0 
X 
* 

4 

X 
0 
0 
0 
0 
X 

5 

X 
o 
o 
o 
o 
o 

6 

X 
0 
0 
0 
0 
0 

7 

X 
0 
0 
0 
0 
o 

“This latter category does not play any role in identifying the order (1,1). 

key feature of the table is that it contains a triangle of O with the upper left vertex 
located at the order (1,1). This is the characteristic we use to identify the order of 
an ARMA process. In general, for an ARMA(/l q) model, the triangle of O will 
have its upper left vertex at the (p, q) position. 

For illustration, consider the monthly log stock returns of the 3M Company from 
February 1946 to December 2008. There are 755 observations. The return series 
and its sample ACF are shown in Figure 2.9. The ACF indicates that there are 
no significant serial correlations in the data at the 1% level. Table 2.5 shows the 
sample EACF and a corresponding simplified table for the series. The simplified 
table is constructed by using the following notation: 

1. X denotes that the absolute value of the corresponding EACF is greater than 
or equal to 2/VT, which is twice of the asymptotic standard error of the 
EACF. 

2. O denotes that the corresponding EACF is less than 2/VT in modulus. 

The simplified table exhibits a triangular pattern of O with its upper left vertex 
at the order (p, q) = (0, 0). A few exceptions of X appear when q = 2, 5, 9, and 
11. However, the EACF table shows that the values of sample ACF corresponding 
to those X are around 0.08 or 0.09. These ACFs are only slightly greater than 
2/V755 = 0.073. Indeed, if 1% critical value is used, those X would become O in 
the simplified EACF table. Consequently, the EACF suggests that the monthly log 
returns of 3M stock follow an ARMA(0,0) model (i.e., a white noise series). This 
is in agreement with the result suggested by the sample ACF in Figure 2.9. 

The information criteria discussed earlier can also be used to select ARMA(p, q) 
models. Typically, for some prespecified positive integers P and Q, one computes 
AIC (or BIC) for ARMA(p, q) models, where 0 < p < P and 0 < q < Q, and 
selects the model that gives the minimum AIC (or BIC). This approach requires 
maximum-likelihood estimation of many models and in some cases may encounter 

the difficulty of overfitting in estimation. 

68 

LINEAR TIME SERIES ANALYSIS AND ITS APPLICATIONS 

o 
co 
o 
co 
LL O 
O -d- 
< o 
CM 
o 
o 
o 

-j.r 

i-'-1-r- 

10 

15 

20 

Lag 

Figure 2.9 Time plot and sample autocorrelation function of monthly log stock returns of 3M Company 

from February 1946 to December 2008. 

Once an ARMA(/?, q) model is specified, its parameters can be estimated by 
either the conditional or exact-likelihood method. In addition, the Ljung-Box statis¬ 
tics of the residuals can be used to check the adequacy of a fitted model. If the 
model is correctly specified, then Q{m) follows asymptotically a chi-squared dis¬ 
tribution with m — g degrees of freedom, where g denotes the number of AR or 
MA coefficients fitted in the model. 

2.6.4 Forecasting Using an ARM A Model 

Like the behavior of ACF, forecasts of an ARMA(p, q) model have similar char¬ 
acteristics as those of an AR(p) model after adjusting for the impacts of the MA 
component on the lower horizon forecasts. Denote the forecast origin by h and 
the available information by F/,. The 1-step-ahead forecast of r^+i can be easily 
obtained from the model as 

W) = E(rh+i\Fh) = 00 + ]p0iT71+1_i - ^2eiah+l-i, 

i'=l (=1 

p <? 

and the associated forecast error is e/,( 1) = rh+\ - rh{\) = ah+\. The variance of 
1-step-ahead forecast error is Var[e/,(1)] = aa2. For the i-step-ahead forecast, we 
have 

rh(t) = E(rh+e\Fh) = 0O + ^0,r/!(£ - i) - ^diah(l - i), 

p ? 

i=i l=l 

SIMPLE ARMA MODELS 

69 

TABLE 2.5 Sample Extended Autocorrelation Function and a Simplified Table for 
the Monthly Log Returns of 3M Stock from February 1946 to December 2008 

Sample Extended Autocorrelation Function 

MA Order: q 

P 

0 

1 

2 

3 

4 

5 

6 

7 

8 

9 

10 

11 

12 

0.01 

0  -0.06 - -0.04 - -0.08 - -0.00  0.02  0.08 
0.01 - -0.03 - -0.08  0.05  0.09 - -0.01 
1  -0.47  0.01 - -0.07 - -0.02  0.00  0.08  -0.03  0.00 - -0.01 - -0.07  0.04  0.09 - -0.02 
2  -0.38 - -0.35 ■ -0.07  0.02 - -0.01 
0.00 - -0.03  0.02  0.04  0.04 
3  -0.18  0.14  0.38 - -0.02  0.00  0.04  -0.02  0.02- -0.00 - -0.03  0.02  0.01 
0.04 
0.00  0.02 - -0.00  0.01 
0.45 - -0.01  0.00  0.00  -0.01  0.03 
0.42  0.03 
4 
0.03 
5  -0.11 
0.21 
0.45  0.01 
0.04 
6  -0.21 - -0.25  0.24  0.31 

0.20 - -0.01  -0.00  0.04 - -0.01 - -0.01  0.03  0.01 
0.01 
0.17 - -0.04  -0.00  0.04 - -0.01 - -0.03  0.01 

0.08  0.03  0.01 

0.01 

Simplified EACF Table 

MA Order: q 

P 

0 
1 
2 
3 
4 
5 
6 

0 

O 
X 
X 
X 
X 
X 
X 

1 

0 
0 
X 
X 
0 
X 
X 

2 

X 
0 
0 
X 
X 
X 
X 

3 

o 
o 
o 
0 
o 
o 
X 

4 

0 
0 
0 
0 
0 
X 
X 

5 

X 
X 
X 
0 
o 
o 
0 

6 

o 
0 
0 
0 
0 
0 
0 

7 

o 
o 
o 
o 
o 
o 
0 

8 

0 
0 
0 
0 
0 
0 
0 

9 

X 
0 
0 
0 
0 
0 
0 

10 

0 
0 
o 
o 
o 
o 
0 

11 

X 
X 
0 
0 
0 
0 
o 

12 

o 
o 
0 
0 
0 
0 
o 

where it is understood that fh(£ — i) = rh+i-i if l — i < 0 and cih(l — i) = 0 if 
t — i> 0 and «/,(£ — /) = ah+t-i if t — i <0. Thus, the multistep-ahead forecasts 
of an ARMA model can be computed recursively. The associated forecast error is 

eh(£) = rh+t - W), 

which can be computed easily via a formula to be given in Eq. (2.34). 

2.6.5 Three Model Representations for an ARMA Model 

In this section, we briefly discuss three model representations for a stationary 
ARMA(p, q) model. The three representations serve three different purposes. 
Knowing these representations can lead to a better understanding of the model. The 
first representation is the ARMA(p, q) model in Eq. (2.28). This representation 
is compact and useful in parameter estimation. It is also useful in computing 
recursively multistep-ahead forecasts of rt\ see the discussion in the last section. 

70 

LINEAR TIME SERIES ANALYSIS AND ITS APPLICATIONS 

For the other two representations, we use long division of two polynomials. 

Given two polynomials 0(5) = 1 — J]f=i 0!-®1 anc^ 9(B) = 1 — X!f=i ®iBl, we 
can obtain, by long division, that 

^ = 1 + fiB + f2B2 + ■■■ = 0(5) (2.29) 
0(5) 

and 

(tSBl — i _ 7tx b _ 7t-252 — • • • = 7r(5). (2.30) 
0(5) 

For instance, if 0(5) = 1 — 0i 5 and 0(5) = 1 — 0i5, then 

0(5) = 7—t4 = 1 + (01 - 01)5 + 0!(0! - 0!)52 + 02(0, - 0j)53 + • • •, 

1 - 0i 5 

*(5) = v- t-v = 1 ~ (0i --e^Bl ~-• 

1 — U\ 

From the definition, 0(5)7r(5) = 1. Making use of the fact that Be = c for any 
constant (because the value of a constant is time invariant), we have 

00 _ _0o_ antj 0o _ 0o_ 
W) ~ 1-0!-eq an 00) _ 1 01-0„ ' 

A 5 Representation 
Using the result of long division in Eq. (2.30), the ARMA(/?, q) model can be 
written as 

r‘ = 1+ *1*7-1 + *2^-2 + *3^-3 H-h fl/. (2.31) 

00 

1 C7i • * • Uq 

This representation shows the dependence of the current return rt on the past 
returns where i > 0. The coefficients {n, } are referred to as the n weights of 
an ARMA model. To show that the contribution of the lagged value rf_,- to r, is 
diminishing as i increases, the 7r, coefficient should decay to zero as i increases. An 
ARMA(/?, q) model that has this property is said to be invertible. For a pure AR 
model, 0(5) = 1 so that 7r(5) = 0(5), which is a finite-degree polynomial. Thus, 
m = 0 for i > p, and the model is invertible. For other ARMA models, a sufficient 
condition for invertibility is that all the zeros of the polynomial 0(5) are greater 
than unity in modulus. For example, consider the MA(1) model rt = (1 — 6\ B)at. 
The zero of the first-order polynomial 1 — 9\ B is 5 = l/0i. Therefore, an MA(1) 
model is invertible if |l/0i| > 1. This is equivalent to |0j | < 1. 

From the AR representation in Eq. (2.31), an invertible ARMA(p, q) series rt 
is a linear combination of the current shock at and a weighted average of the past 
values. The weights decay exponentially for more remote past values. 

UNIT-ROOT NONSTATIONARITY 

71 

MA Representation 
Again, using the result of long division in Eq. (2.29), an ARMA{p, q) model can 
also be written as 

rt = ii + at + f\at-\ + f 2^1-2 H-= B)at, (2.32) 

where p = E{rt) = 0o/(l — </>i — • • • — 4>p). This representation shows explicitly 
the impact of the past shock at-i (i > 0) on the current return rt. The coefficients 
{0,} are referred to as the impulse response function of the ARMA model. For 
a weakly stationary series, the coefficients decay exponentially as i increases. 
This is understandable as the effect of shock at-i on the return rt should diminish 
over time. Thus, for a stationary ARMA model, the shock at^i does not have 
a permanent impact on the series. If </>o 7^ 0, then the MA representation has a 
constant term, which is the mean of rt [i.e., 0o/(l — 0i — • • • — 4>p)]- 

The MA representation in Eq. (2.32) is also useful in computing the variance 
of a forecast error. At the forecast origin h, we have the shocks — 
Therefore, the ^-step-ahead point forecast is 

rh(l) = p + fiah + ft+\ah-i-\-, (2.33) 

and the associated forecast error is 

eh(£) = ah+l + flUh+t-l + • - ' + 

Consequently, the variance of ^-step-ahead forecast error is 

Var [eh(l)\ = (1 + + • • • + i)cra2, (2.34) 

which, as expected, is a nondecreasing function of the forecast horizon i. 

Finally, the MA representation in Eq. (2.32) provides a simple proof of mean 
reversion of a stationary time series. The stationarity implies that 0, approaches 
zero as i -a- 00. Therefore, by Eq. (2.33), we have rh(t) -» M as l -a 00. Because 
rh(t) is the conditional expectation of rh+i at the forecast origin h, the result 
says that in the long term the return series is expected to approach its mean, 
that is, the series is mean reverting. Furthermore, using the MA representation in 
Eq. (2.32), we have Var(rf) = (1 + Consequently, by Eq. (2.34), we 
have Var[eh(l)] -> Var(rt) as i -> 00. The speed by which rh{l) approaches p 
determines the speed of mean reverting. 

2.7 UNIT-ROOT NONSTATIONARITY 

So far we have focused on return series that are stationary. In some studies, interest 
rates, foreign exchange rates, or the price series of an asset are of interest. These 
series tend to be nonstationary. For a price series, the nonstationarity is mainly 

72 

LINEAR TIME SERIES ANALYSIS AND ITS APPLICATIONS 

due to the fact that there is no fixed level for the price. In the time series lit¬ 
erature, such a nonstationary series is called unit-root nonstationary time series. 
The best known example of unit-root nonstationary time series is the random-walk 
model. 

2.7.1 Random Walk 

A time series {pt} is a random walk if it satisfies 

Pt =p,-i+a,, (2.35) 

where po is a real number denoting the starting value of the process and {at} is a 
white noise series. If p, is the log price of a particular stock at date t, then po could 
be the log price of the stock at its initial public offering (IPO) (i.e., the logged IPO 
price). If at has a symmetric distribution around zero, then conditional on pt-\, 
pt has a 50-50 chance to go up or down, implying that p, would go up or down 
at random. If we treat the random-walk model as a special AR(1) model, then the 
coefficient of pt_\ is unity, which does not satisfy the weak stationarity condition 
of an AR(1) model. A random-walk series is, therefore, not weakly stationary, and 
we call it a unit-root nonstationary time series. 

The random-walk model has widely been considered as a statistical model for 
the movement of logged stock prices. Under such a model, the stock price is not 
predictable or mean reverting. To see this, the 1-step-ahead forecast of model (2.35) 
at the forecast origin h is 

Phi 1) = E(Ph+l\Ph, Ph-u • • •) = Ph, 

which is the log price of the stock at the forecast origin. Such a forecast has no 
practical value. The 2-step-ahead forecast is 

Phi2) = E(ph+2\ph, Ph-1, • • •) = E{ph+1 +ah+2\ph, Ph-i, • • •) 

= E(ph+i\ph, ph_u ...) = ph{ 1) = Ph, 

which again is the log price at the forecast origin. In fact, for any forecast horizon 
l > 0, we have 

Phil) = Ph- 

Thus, for all forecast horizons, point forecasts of a random-walk model are simply 
the value of the series at the forecast origin. Therefore, the process is not mean 
reverting. 

The MA representation of the random-walk model in Eq. (2.35) is 

Pt — O-t + + at-2 + • ■ •. 

UNIT-ROOT NONSTATION ARITY 

73 

This representation has several important practical implications. First, the f-step- 
ahead forecast error is 

eh(£) — Q-h+e + • • • + &h+U 

so that Var[e^(£)] = which diverges to infinity as i —> oo. The length of an 
interval forecast of ph+i will approach infinity as the forecast horizon-increases. 
This result says that the usefulness of point forecast ph (£) diminishes as i increases, 
which again implies that the model is not predictable. Second, the unconditional 
variance of pt is unbounded because Var[e^(^)] approaches infinity as t increases. 
Theoretically, this means that pt can assume any real value for a sufficiently large t. 
For the log price pt of an individual stock, this is plausible. Yet for market indexes, 
negative log price is very rare if it happens at all. In this sense, the adequacy of a 
random-walk model for market indexes is questionable. Third, from the represen¬ 
tation, i]/i = 1 for all i. Thus, the impact of any past shock at~i on pt does not 
decay over time. Consequently, the series has a strong memory as it remembers all 
of the past shocks. In economics, the shocks are said to have a permanent effect 
on the series. The strong memory of a unit-root time series can be seen from the 
sample ACF of the observed series. The sample ACFs are all approaching 1 as the 
sample size increases. 

2.7.2 Random Walk with Drift 

As shown by empirical examples considered so far, the log return series of a market 
index tends to have a small and positive mean. This implies that the model for the 
log price is 

pt = fi + pt-i+at, (2.36) 

where p = E(pt — pt-\) and {a,} is a zero-mean white noise series. The constant 
term p of model (2.36) is very important in financial study. It represents the time 
trend of the log price pt and is often referred to as the drift of the model. To see 
this, assume that the initial log price is po. Then we have 

Pi=V' + Po + au 

P2 = P + Pi + a2 — 2p + Po + a2 + a\, 

pt — tp + po + at + Of-1 +•••-(- a\. 

The last equation shows that the log price consists of a time trend tp and a 
pure random-walk process ^/=1 ai- Because Var(]T.=1 a,) = towhere o% is the 
variance of at, the conditional standard deviation of pt is y/toa, which grows at a 
slower rate than the conditional expectation of pt. Therefore, if we graph pt against 

74 

LINEAR TIME SERIES ANALYSIS AND ITS APPLICATIONS 

the time index t, we have a time trend with slope pi. A positive slope p implies 
that the log price eventually goes to infinity. In contrast, a negative p implies that 
the log price would converge to —oo as t increases. Based on the above discussion, 
it is then not surprising to see that the log return series of the CRSP value- and 
equal-weighted indexes have a small, but statistically significant, positive mean. 

To illustrate the effect of the drift parameter on the price series, we consider the 
monthly log stock returns of the 3M Company from February 1946 to December 
2008. As shown by the sample EACF in Table 2.5, the series has no significant 
serial correlation. The series thus follows the simple model 

rt = 0.0103 + at, oa= 0.0637, (2.37) 

where 0.0103 is the sample mean of r, and has a standard error 0.0023. The mean 
of the monthly log returns of 3M stock is, therefore, significantly different from 
zero at the 1 % level. As a matter of fact, the one-sample test of zero mean shows 
a t ratio of 4.44 with a p value close to 0. We use the log return series to construct 
two log price series, namely 

t t 

pt = n and p* = ai' 

i=l f=l 

where a,- is the mean-corrected log return in Eq. (2.37) (i.e., at = rt — 0.0103). 
The pt is the log price of 3M stock, assuming that the initial log price is zero 
(i.e., the log price of January 1946 was zero). The p* is the corresponding log 
price if the mean of log returns was zero. Figure 2.10 shows the time plots of 
pt and p* as well as a straight line y, = 0.0103 x t + 1946, where t is the time 
sequence of the returns and 1946 is the starting year of the stock. From the plots, 
the importance of the constant 0.0103 in Eq. (2.37) is evident. In addition, as 
expected, it represents the slope of the upward trend of p,. 

Interpretation of the Constant Term 
From the previous discussions, it is important to understand the meaning of a 
constant term in a time series model. First, for an MA(g) model in Eq. (2.22), the 
constant term is simply the mean of the series. Second, for a stationary AR(p) model 
in Eq. (2.9) or ARMA(p, q) model in Eq. (2.28), the constant term is related to 
the mean via /x = 0o/(l — (f>\ — • • ■ — (pp). Third, for a random walk with drift, the 
constant term becomes the time slope of the series. These different interpretations 
for the constant term in a time series model clearly highlight the difference between 
dynamic and usual linear regression models. 

Another important difference between dynamic and regression models is shown 

by an AR(1) model and a simple linear regression model, 

n = 00 + 0i/7_i + a, and y, = Po + Pixt + at. 

For the AR(1) model to be meaningful, the coefficient </>i must satisfy \<p\ \ < 1. 
However, the coefficient fi\ can assume any fixed real number. 

UNIT-ROOT NONSTATIONARITY 

75 

1950 1960 1970 1980 1990 2000 2010 

Year 

Figure 2.10 Time plots of log prices for 3M stock from February 1946 to December 2008, assuming 

that log price of January 1946 was zero. The “o” line is for log price without time trend. Straight line 
is yt = 0.0103 x t + 1946. 

2.7.3 Trend-Stationary Time Series 

A closely related model that exhibits linear trend is the trend-stationary time series 
model, 

Pt — A) + /fit + ft, 

where rt is a stationary time series, for example, a stationary AR(p) series. Here pt 
grows linearly in time with rate /fi and hence can exhibit behavior similar to that of 
a random-walk model with drift. However, there is a major difference between the 
two models. To see this, suppose that po is fixed. The random-walk model with drift 
assumes the mean E(pt) = po + pt and variance Var(pf) = ta%, both of them are 
time dependent. On the other hand, the trend-stationary model assumes the mean 
E(pt) = /?o + /fit, which depends on time, and variance Var(pt) = Var(rr), which 
is finite and time invariant. The trend-stationary series can be transformed into a 
stationary one by removing the time trend via a simple linear regression analysis. 
For analysis of trend-stationary time series, see the method of Section 2.9. 

2.7.4 General Unit-Root Nonstationary Models 

Consider an ARM A model. If one extends the model by allowing the AR poly¬ 
nomial to have 1 as a characteristic root, then the model becomes the well-known 

76 

LINEAR TIME SERIES ANALYSIS AND ITS APPLICATIONS 

autoregressive integrated moving-average (ARIMA) model. An ARIMA model is 
said to be unit-root nonstationary because its AR polynomial has a unit root. Like 
a random-walk model, an ARIMA model has strong memory because the iJ/j coef¬ 
ficients in its MA representation do not decay over time to zero, implying that the 
past shock a,-i of the model has a permanent effect on the series. A conventional 
approach for handling unit-root nonstationarity is to use differencing. 

Differencing 
A time series y, is said to be an ARIMA(/?, 1, q) process if the change series 
ct = yt — yt~i = (1 — B)yt follows a stationary and invertible ARMA(p, q) model. 
In finance, price series are commonly believed to be nonstationary, but the log 
return series, rt — ln(Pf) - 1 n(_i), is stationary. In this case, the log price series 
is unit-root nonstationary and hence can be treated as an ARIMA process. The 
idea of transforming a nonstationary series into a stationary one by considering 
its change series is called differencing in the time series literature. More formally, 
cf = yt — yt-1 is referred to as the first differenced series of yt. In some scientific 
fields, a time series yt may contain multiple unit roots and needs to be differenced 
multiple times to become stationary. For example, if both yt and its first differenced 
series c, = yt — yt-\ are unit-root nonstationary, but .57 = ct — ct~ 1 = yt — 2yf_i + 
yr_2 is weakly stationary, then yt has double unit roots, and st is the second 
differenced series of yt. In addition, if s, follows an ARMA(/?, q) model, then y, 
is an ARIMA(p, 2, q) process. For such a time series, if s, has a nonzero mean, 
then y, has a quadratic time function and the quadratic time coefficient is related 
to the mean of st. The seasonally adjusted series of U.S. quarterly gross domestic 
product implicit price deflator might have double unit roots. However, the mean 
of the second differenced series is not significantly different from zero; see the 
Exercises of this chapter. Box, Jenkins, and Reinsel (1994) discuss many properties 
of general ARIMA models. 

2.7.5 Unit-Root Test 

To test whether the log price p, of an asset follows a random walk or a random 
walk with drift, we employ the models 

Pt = +e, 

Pt — 0o + 0i Pt-x + et, 

(2.38) 

(2.39) 

where et denotes the error term, and consider the null hypothesis Ho : = 1 
versus the alternative hypothesis Ha : (p\ < 1. This is the well-known unit-root 
testing problem; see Dickey and Fuller (1979). A convenient test statistic is the 
t ratio of the least-squares (LS) estimate of 0i under the null hypothesis. For 
Eq. (2.38), the LS method gives 

Er=i(Pt ~ hPt-i)2 
T - 1 

UNIT-ROOT NONSTATIONARITY 

77 

where po = 0 and T is the sample size. The t ratio is 

DF = t ratio = 

01-1 

std(0i) 

ELi Pt-iet 

which is commonly referred to as the Dickey-Fuller (DF) test. If {et} is a white 
noise series with finite moments of order slightly greater than 2, then the DF statistic 
converges to a function of the standard Brownian motion as T —* oo; see Chan and 
Wei (1988) and Phillips (1987) for more information. If (po is zero but Eq. (2.39) 
is employed anyway, then the resulting t ratio for testing 0i = 1 will converge to 
another nonstandard asymptotic distribution. In either case, simulation is used to 
obtain critical values of the test statistics; see Fuller (1976, Chapter 8) for selected 
critical values. Yet if 0o 7^ 0 and Eq. (2.39) is used, then the t ratio for testing 
0i = 1 is asymptotically normal. However, large sample sizes are needed for the 
asymptotic normal distribution to hold. Standard Brownian motion is introduced in 
Chapter 6. 

For many economic time series, ARIMA(/?, d, q) models might be more appro¬ 
priate than the simple model in Eq. (2.39). In the econometric literature, AR(/?) 
models are often used. Denote the series by xt. To verify the existence of a unit 
root in an AR(p) process, one may perform the test Hq : /3 = 1 vs. Ha : p < 1 
using the regression 

xt = ct + ^ (pi Axt-i + et, (2.40) 

p-1 

/ = 1 

where ct is a deterministic function of the time index t and Axj = xj — xj-\ is the 
differenced series of xt. In practice, ct can be zero or a constant or ct = coq + co\t. 
The t ratio of /3 — 1, 

- 1 

ADF-test =-—, 
stdOS) 

where ft denotes the least-squares estimate of /3, is the well-known augmented 
Dickey-Fuller (ADF) unit-root test. Note that because of the first differencing, 
Eq. (2.40) is equivalent to an AR(p) model with deterministic function ct. Equation 
(2.40) can also be rewritten as 

p- 1 

A xt =ct + Pc*t-1 + XI0' Axt-i + 

Z = 1 

where fic = fi — One can then test the equivalent hypothesis Hq : pc = 0 vs. 

Ha : pc < 0. 

78 

LINEAR TIME SERIES ANALYSIS AND ITS APPLICATIONS 

Year 

(a) 

(c) 

Lag Lag 

(b) (d) 

Figure 2.11 Log series of U.S. quarterly GDP from 1947.1 to 2008.IV: (a) time plot of logged GDP 

series, (b) sample ACF of log GDP data, (c) time plot of first differenced series, and (d) sample PACF 

of differenced series. 

Example 2.2. Consider the log series of U.S. quarterly GDP from 1947.1 to 
2008.IV. The series exhibits an upward trend, showing the growth of the U.S. 
economy, and has high sample serial correlations; see the lower left panel of 
Figure 2.11. The first differenced series, representing the growth rate of U.S. 
GDP and also shown in Figure 2.11, seems to vary around a fixed mean level, 
even though the variability appears to be smaller in recent years. To confirm the 
observed phenomenon, we apply the ADF unit-root test to the log series. Based 
on the sample PACF of the differenced series shown in Figure 2.11, we choose 
= 10. Other values of p are also used, but they do not alter the conclusion of 
the test. With p = 10, the ADF test statistic is —1.701 with a p value 0.4297, indi¬ 
cating that the unit-root hypothesis cannot be rejected. From the attached S-Plus 
output, /3 = 1 + fie = 1 — 0.0008 = 0.9992. 

R Demonstration 

> library(fUnitRoots) 

> da=read.table("q-gdp47 0 8.txt",header=T) 

> gdp=log(da[,4]) 

> ml=ar(diff(gdp),method='mle') 

> ml$order 

[1] 10 
> adfTest(gdp,lags=10,type=c("c")) 

UNIT-ROOT NONSTATIONARITY 

79 

Title: 

Augmented Dickey-Fuller Test 

Test Results: 
PARAMETER: 

Lag Order: 10 

STATISTIC: 

Dickey-Fuller: -1.6109 

P VALUE: 0.4569 

S-Plus Demonstration 
The following output has been edited: 

> adft=unitroot(gdp,trend='c',method='adf',lags=10) 
> summary(adft) 

Test for Unit Root: Augmented DF Test 
Null Hypothesis: there is a unit root 

Type of Test: t-test 
Test Statistic: -1.701 
P-value: 0.4297 

Coefficients: 

lagl - ■0.0008  0.0005 
lag2  0.3799  0.0659 
0.1883  0.0696 
lag3 

Value  Std. Error t value  Pr (>|11) 
-1.7006  0.0904 
5.7637  0.0000 
2.7047  0.0074 

laglO  0.1784  0.0637 
constant  0.0134  0.0045 

2.8023  0.0055 
2.9636  0.0034 

Regression Diagnostics: 

R-Squared 0.2877 
Adjusted R-Squared 0.2564 
Durbin-Watson Stat 1.9940 

Residual standard error: 0.009318 on 234 degrees of freedom 

As another example, consider the log series of the S&P 500 index from Jan¬ 
uary 3, 1950, to April 16, 2008, for 14,462 observations. The series is shown in 
Figure 2.12. Testing for a unit root in the index is relevant if one wishes to verify 
empirically that the Index follows a random walk with drift. To this end, we use 
ct = coo + coit in applying the ADF test. Furthermore, we choose p = 15 based 
on the sample PACF of the first differenced series. The resulting test statistic is 
— 1.998 with a p value 0.602. Thus, the unit-root hypothesis cannot be rejected 
at any reasonable significance level. The constant term is statistically significant, 
whereas the estimate of the time trend is not at the usual 5% level. The latter is 

80 

LINEAR TIME SERIES ANALYSIS AND ITS APPLICATIONS 

significant at the 10% level, however. In summary, for the period from January 
1950 to April 2008, the log series of the S&P 500 index contains a unit root and 
a positive drift, but there is no strong evidence of a time trend. 

R Demonstration 

> library(fUnitRoots) 

> da=read.table("d-sp55008.txt",header=T) 

> sp5=log(da[,7]) 

> m2=ar(diff(sp5),method='mle') 

> m2$order 

[1] 2 
> adfTest(sp5,lags=2,type=("ct")) 

Title: 

Augmented Dickey-Fuller Test 

Test Results: 

PARAMETER: 

Lag Order: 2 

STATISTIC: 

Dickey-Fuller: -2.0179 

P VALUE: 0.5708 

> adfTest(sp5,lags=15,type=("ct")) 

SEASONAL MODELS 

81 

Title: 

Augmented Dickey-Fuller Test 

Test Results: 

PARAMETER: 

Lag Order: 15 

STATISTIC: 

Dickey-Fuller: -1.9946 

P VALUE: 0.5807 

S-Plus Demonstration 
The following output has been edited: 

> adft=unitroot(sp5,method='adf',trend='ct',lags=15) 

> summary(adft) 

Test for Unit Root: Augmented DF Test 

Null Hypothesis: there is a unit root 

Type of Test: t-test 

Test Statistic: -1.998 

P-value: 0.602 

Coefficients: 

Value  Std. Error  t value  Pr (>|11) 

lagl  -0.0005  0.0003 

lag2 

0.0722 

0.0083 

-1.9977  0.0458 
8.7374  0.0000 

lag3  -0.0386  0.0083 

-4.6532 

0.0000 

lag4  -0.0071  0.0083 

-0.8548  0.3927 

lagl5 

0.0133 

0.0083 

1.6122 

0.1069 

constant  0.0019  0.0008 

2.3907 

0.0168 

time  0.0020  0.0011 

1.8507 

0.0642 

Regression Diagnostics: 

R-Squared 0.0081 

Adjusted R-Squared 0.0070 

Durbin-Watson Stat 1.9995 

Residual standard error: 0.008981 on 14643 degrees of freedom 

2.8 SEASONAL MODELS 

Some financial time series such as quarterly earnings per share of a company 
exhibits certain cyclical or periodic behavior. Such a time series is called a sea¬ 
sonal time series. Figure 2.13(a) shows the time plot of quarterly earnings per share 
of Johnson & Johnson from the first quarter of 1960 to the last quarter of 1980. 

82 

LINEAR TIME SERIES ANALYSIS AND ITS APPLICATIONS 

Figure 2.13 Time plots of quarterly earnings per share of Johnson & Johnson from 1960 to 1980: 

(a) observed earnings and (b) log earnings. 

The data obtained from Shumway and Stoffer (2000) possess some special char¬ 
acteristics. In particular, the earnings grew exponentially during the sample period 
and had a strong seasonality. Furthermore, the variability of earnings increased 
over time. The cyclical pattern repeats itself every year so that the periodicity of 
the series is 4. If monthly data are considered (e.g., monthly sales of Wal-Mart 
stores), then the periodicity is 12. Seasonal time series models are also useful in 
pricing weather-related derivatives and energy futures because most environmental 
time series exhibit strong seasonal behavior. 

Analysis of seasonal time series has a long history. In some applications, sea¬ 
sonality is of secondary importance and is removed from the data, resulting in a 
seasonally adjusted time series that is then used to make inference. The procedure 
to remove seasonality from a time series is referred to as seasonal adjustment. Most 
economic data published by the U.S. government are seasonally adjusted (e.g., the 
growth rate of gross domestic product and the unemployment rate). In other appli¬ 
cations such as forecasting, seasonality is as important as other characteristics of 
the data and must be handled accordingly. Because forecasting is a major objective 
of financial time series analysis, we focus on the latter approach and discuss some 
econometric models that are useful in modeling seasonal time series. 

2.8.1 Seasonal Differencing 

Figure 2.13(b) shows the time plot of log earnings per share of Johnson & Johnson. 
We took the log transformation for two reasons. First, it is used to handle the 

SEASONAL MODELS 

83 

exponential growth of the series. Indeed, the plot confirms that the growth is linear 
in the log scale. Second, the transformation is used to stablize the variability of 
the series. Again, the increasing pattern in variability of Figure 2.13(a) disappears 
in the new plot. Log transformation is commonly used in analysis of financial 
and economic time series. In this particular instance, all earnings are positive so 
that no adjustment is needed before taking the transformation. In some cases, 
one may need to add a positive constant to every data point before taking the 
transformation. 

Denote the log earnings by xt. The upper left panel of Figure 2.14 shows the sam¬ 
ple ACF of xt, which indicates that the quarterly log earnings per share has strong 
serial correlations. A conventional method to handle such strong serial correlations 
is to consider the first differenced series of xt [i.e., Axt = xt — xt-\ = (1 — B)xt]. 
The lower left plot of Figure 2.14 gives the sample ACF of Axt. The ACF is strong 
when the lag is a multiple of periodicity 4. This is a well-documented behav¬ 
ior of sample ACF of a seasonal time series. Following the procedure of Box, 
Jenkins, and Reinsel (1994, Chapter 9), we take another difference of the data, 
that is. 

A4(Aa;) = (1 - Ba)Axt = Axt - Axf_4 = xt - xt-\ — xt-4 + xt-5. 

Li- 
o 
< 

o 
I 

in 
o 

O o 
< o 

in 
o 

15 

10 

Lag 

(a) 

0 

5 

15 

10 

Lag 

(b) 

LL 
o 
< 

I I 

CM 
o 
I 

oo 
o 

o 
I 

TT 

O 
< 

0 

5 

TT 

15 

15 

10 

Lag 

(c) 

TT 

10 

Lag 

(d) 

Figure 2.14 Sample ACF of log series of quarterly earnings per share of Johnson & Johnson from 
1960 to 1980. (a) log earnings, (b) first differenced series, (c) seasonally differenced series, and (d) 

series with regular and seasonal differencing. 

84 

LINEAR TIME SERIES ANALYSIS AND ITS APPLICATIONS 

The operation A4 = (1 - 54) is called a seasonal differencing. In general, for a 

seasonal time series yt with periodicity s, seasonal differencing means 

Asyt =yt- yt-s = (1 - Bs)yt. 

The conventional difference Ayt = yt — yt-1 = (1 — B)yt is referred to as the 
regular differencing. The lower right plot of Figure 2.14 shows the sample ACF 
of A4 Axt, which has a significant negative ACF at lag 1 and a marginal negative 
correlation at lag 4. For completeness, Figure 2.14 also gives the sample ACF of 

the seasonally differenced series A4xf. 

2.8.2 Multiplicative Seasonal Models 

The behavior of the sample ACF of (1 — B4)( 1 — B)xt in Figure 2.14 is common 
among seasonal time series. It led to the development of the following special 

seasonal time series model: 

(1 - Bs)( 1 - B)xt = (1 - 0B)( 1 - ®Bs)at, 

(2.41) 

where s is the periodicity of the series, at is a white noise series, |0| < 1, and 
|0| < 1. This model is referred to as the airline model in the literature; see Box, 
Jenkins, and Reinsel (1994, Chapter 9). It has been found to be widely applicable 
in modeling seasonal time series. The AR part of the model simply consists of the 
regular and seasonal differences, whereas the MA part involves two parameters. 

Focusing on the MA part (i.e., on the model), 

wt = (1 — 9B){ 1 — ®Bs)at — a, — 9at-\ — ®at-s + 9®at-s-\, 

where wt = (1 - Bs)(l - B)xt and 5 > 1. It is easy to obtain that E(wt) = 0 and 

Var(u;;) = (1 +6>2)(1 + 02)ct2, 

Cov(wt, wt-1) = -0(1 + ©V2, 

Coy(wt, Wts+i) = 0©^2, 

Cov(wt, wt-s) = -0(1 + 02)<r2, 

Cov(wt, Wt-s-1) = 9®(7q , 

Cov(wt, wt~i) = 0, for t 0, 1, s — 1, s, s + 1. 

Consequently, the ACF of the wt series is given by 

-9 

-0 

Ps — 1 02 ’ Ps-1 — Ps+1 — PlPs — 

(l+02)(l + 02) 

SEASONAL MODELS 

85 

and pi — 0 for l > 0 and £ ^ l, s — 1,5, 5 + 1. For example, if wt is a quarterly 
time series, then 5=4 and for l > 0, the ACF pa is nonzero at lags 1, 3, 4, and 
5 only. 

It is interesting to compare the prior ACF with those of the MA(1) model 
yt = (1 - 0B)at and the MA(i) model zt = (1 - ®Bs)at. The ACF of yt and zt 
series are 

Pi 00 = 

Ps(z) = 

-e 

1 +02 

-0 

1 + 02 

and 

pi(y)  = 0, 

i> l, 

and 

Pi 0)  = o, 

£ > 0, ^ 5 

We see that (i) px = pi(y), (ii) ps = ps(z), and (iii) = ps+l = px(y) x ps(z). 
Therefore, the ACF of wt at lags (s - 1) and (s + 1) can be regarded as the 
interaction between lag-1 and lag-5 serial dependence, and the model of wt is 
called a multiplicative seasonal MA model. In practice, a multiplicative seasonal 
model says that the dynamics of the regular and seasonal components of the series 
are approximately orthogonal. 

The model 

wt = (1-0B -®Bs)at, (2.42) 

where \0\ < 1 and |©| < 1, is a nonmultiplicative seasonal MA model. It is easy 
to see that for the model in Eq. (2.42), psJrX = 0. A multiplicative model is more 
parsimonious than the corresponding nonmultiplicative model because both models 
use the same number of parameters, but the multiplicative model has more nonzero 
ACFs. 

Example 2.3. In this example we apply the airline model to the log series of 
quarterly earnings per share of Johnson & Johnson from 1960 to 1980. Based on 
the exact-likelihood method, the fitted model is 

(1 - 5)(1 - B4)xt = (1 - 0.6785)(1 - 0.314fl4)a,, oa = 0.089, 

where standard errors of the two MA parameters are 0.080 and 0.101, respectively. 
The Ljung-Box statistics of the residuals show <2(12) = 10.0 with a p value of 
0.44. The model appears to be adequate. 

To illustrate the forecasting performance of the prior seasonal model, we rees¬ 
timate the model using the first 76 observations and reserve the last 8 data points 
for forecasting evaluation. We compute l-step- to 8-step-ahead forecasts and then- 
standard errors of the fitted model at the forecast origin h =76. An antilog trans¬ 
formation is taken to obtain forecasts of earnings per share using the relationship 
between normal and lognormal distributions given in Chapter 1. Figure 2.15 shows 
the forecast performance of the model, where the observed data are in solid line, 
point forecasts are shown by dots, and the dashed lines show 95% interval fore¬ 
casts. The forecasts show a strong seasonal pattern and are close to the observed 

86 

LINEAR TIME SERIES ANALYSIS AND ITS APPLICATIONS 

Figure 2.15 Out-of-sample point and interval forecasts for quarterly earnings of Johnson & Johnson. 

Forecast origin is fourth quarter of 1978. In plot, solid line shows actual observations, dots represent 

point forecasts, and dashed lines show 95% interval forecasts. 

data. Finally, for an alternative approach to modeling the quarterly earnings data, 

see Example 11.3. 

When the seasonal pattern of a time series is stable over time (e.g., close to a 
deterministic function), dummy variables may be used to handle the seasonality. 
This approach is taken by some analysts. However, deterministic seasonality is a 
special case of the multiplicative seasonal model discussed before. Specifically, 
if 0 = 1, then model (2.41) contains a deterministic seasonal component. Con¬ 
sequently, the same forecasts are obtained by using either dummy variables or a 
multiplicative seasonal model when the seasonal pattern is deterministic. Yet use of 
dummy variables can lead to inferior forecasts if the seasonal pattern is not deter¬ 
ministic. In practice, we recommend that the exact-likelihood method should be 
used to estimate a multiplicative seasonal model, especially when the sample size is 
small or when there is the possibility of having a deterministic seasonal component. 

Example 2.4. To demonstrate deterministic seasonal behavior, consider the 
monthly simple returns of the CRSP Decile 1 Index from January 1970 to December 
2008 for 468 observations. The series is shown in Figure 2.16(a), and the time plot 
does not show any clear pattern of seasonality. However, the sample ACF of the 
return series shown in Figure 2.16(b) contains significant lags at 12, 24, and 36 as 
well as lag 1. If seasonal ARMA models are entertained, a model in the form 

(1-01 B)( 1 - 012512)R, = (1 - fluBn)at 

SEASONAL MODELS 

87 

Figure 2.16 Monthly simple returns of CRSP Decile 1 index from January 1970 to December 2008: 
(a) time plot of the simple returns, (b) sample ACF of simple returns, (c) time plot of simple returns 
after adjusting for January effect, and (d) sample ACF of adjusted simple returns. 

is identified, where Rt denotes the monthly simple return. Using the conditional- 
likelihood method, the fitted model is 

(1 - 0.18fl)(l - 0.87Bn)R, = (1 - 0.14Bn)at, &a = 0.069. 

See the attached SCA (Scientific Computing Associates) output below. The esti¬ 
mates of the seasonal AR and MA coefficients are of similar magnitude. If the 
exact-likelihood method is used, we have 

(1 - 0.1885)(1 - 0.951 Bn)Rt = (1 - 0.997Bn)at, da = 0.063. 

The cancellation between seasonal AR and MA factors is clearly seen. This high¬ 
lights the usefulness of using the exact-likelihood method and, the estimation result 
suggests that the seasonal behavior might be deterministic. To further confirm this 
assertion, we define the dummy variable for January, that is, 

Jan, 

1 if t is January, 

0 otherwise, 

and employ the simple linear regression 

Rt — fto + fi\ Jan, + et. 

88 

LINEAR TIME SERIES ANALYSIS AND ITS APPLICATIONS 

The fitted model is R, = 0.0029 + 0.1253Janr + et, where the standard errors of 
the estimates are 0.0033 and 0.0115, respectively. The right panel of Figure 2.16 
shows the time plot and sample ACF of the residual series of the prior simple linear 
regression. From the sample ACF, serial correlations at lags 12, 24, and 36 largely 
disappear, suggesting that the seasonal pattern of the Decile 1 returns has been 
successfully removed by the January dummy variable. Consequently, the seasonal 
behavior in the monthly simple return of Decile 1 is mainly due to the January 

effect. 

R Demonstration 
The following output has been edited and % denotes explanation: 

> da=read.table("m-deciles08.txt",header=T) 

> dl=da[,2] 
> jan=rep(c(1,rep(0,11)),39) % Create January dummy. 

> ml=lm(dl jan) 

> summary(ml) 

Call: 

lm(formula = dl ~ jan) 

Coefficients: 

Estimate Std. Error t value Pr(>|t|) 

(Intercept) 0.002864 0.003333 0.859 0.391 
-jan 0.125251 0.011546 10.848 <2e-16  ★ -k -k 

Residual standard error: 0.06904 on 466 degrees of freedom 

Multiple R-squared: 0.2016, Adjusted R-squared: 0.1999 

> m2 =arima(dl,order=c(1,0,0) ,seasonal = list(order=c(1,0,1), 

+ period=12)) 

> m2 

Coefficients: 

arl sari smal intercept 

0.1769 0.9882 -0.9144 0.0118 

s.e. 0.0456 0.0093 0.0335 0.0129 

sigma~2 estimated as 0.004717: log likelihood=584.07, 

aic=-1158.14 

> tsdiag(m2,gof=36) % plot not shown. 

> m2=arima(dl,order=c(1,0,0),seasonal=list(order=c(1,0,1), 

+ period=12),include.mean=F) 

> m2 

Call: 

arima(x=dl,order=c(1,0,0),seasonal=list(order=c(1,0,1), 

period=12),include.mean = F) 

SEASONAL MODELS 

Coefficients : 

arl sari smal 

89 

0.1787 0.9886 -0.9127 % Slightly differ from those of SCA. 

s.e. 0.0456 0.0089 0.0335 

sigma''2 estimated as 0.00472: log likelihood=583.68 , 

aic=-1159.36 

SCA Demonstration 
The following output has been edited: 

input date,decl,d2,d9,dlO. file 'm-deciles08.txt'. 

tsm ml. model (1)(12)decl=(12)noise. 

estim ml. hold resi(rl). % Conditional MLE estimation 

SUMMARY FOR UNIVARIATE TIME SERIES MODEL -- Ml 

VAR TYPE OF ORIGINAL DIFFERENCING 

VARIABLE OR CENTERED 

DEC1  RANDOM 

ORIGINAL 

NONE 

PAR. 

VAR. 

NUM. / 

FACTOR  ORDER  CONS- 

VALUE 

STD 

T 

LABEL  NAME  DENOM 

TRAINT 

ERROR  VALUE 

1 

2 

3 

Dl 

Dl 

Dl 

MA 

AR 

AR 

1 

1 

2 

12 

1 

12 

NONE 

NONE 

NONE 

.7388 

. 0488  15.14 

.1765 

. 0447 

3.95 

.8698 

. 0295  29.49 

EFFECTIVE NUMBER OF OBSERVATIONS 

R-SQUARE . . 

455 

0 .  199 

RESIDUAL STANDARD  ERROR. . 

RESIDUAL STANDARD  ERROR. . 

. . 0.  689906E -01 

. . 0.  705662E -01 

estim ml. method exact, hold resi(rl) % Exact MLE estimation 

SUMMARY FOR UNIVARIATE TIME SERIES MODEL -- Ml 

VAR. 

TYPE OF 

ORIGINAL 

DIFFERENCING 

VAR. 

OR CENTERED 

DECl 

RANDOM 

ORIGINAL 

NONE 

PAR. 

VARI. 

NUM. / 

FACTOR  ORDER  CONS¬  VALUE 

STD 

T 

LABEL  NAME 

DENOM. 

TRAINT 

ERROR  VALUE 

1 

Dl 

MA 

1 

12 

NONE 

.9968 

. 0150 

66.31 

90 

2 

3 

LINEAR TIME SERIES ANALYSIS AND ITS APPLICATIONS 

Dl 

Dl 

AR 

AR 

1 1 
2 12 

NONE .1884 .0448 4.21 

NONE .9505 .0070 135.46 

EFFECTIVE NUMBER OF OBSERVATIONS . . 

R-SQUARE . 

455 

0.328 

RESIDUAL STANDARD ERROR. 

0.631807E-01 

2.9 REGRESSION MODELS WITH TIME SERIES ERRORS 

In many applications, the relationship between two time series is of major interest. 
An obvious example is the market model in finance that relates the excess return 
of an individual stock to that of a market index. The term structure of interest rates 
is another example in which the time evolution of the relationship between interest 
rates with different maturities is investigated. These examples lead naturally to the 

consideration of a linear regression in the form 

yt = a + jixt + et, (2.43) 

where yt and xt are two time series and et denotes the error term. The least- 
squares (LS) method is often used to estimate model (2.43). If {e,} is a white noise 
series, then the LS method produces consistent estimates. In practice, however, it 
is common to see that the error term e, is serially correlated. In this case, we have 
a regression model with time series errors, and the LS estimates of a and /3 may 

not be consistent. 

A regression model with time series errors is widely applicable in economics 
and finance, but it is one of the most commonly misused econometric models 
because the serial dependence in et is often overlooked. It pays to study the model 

carefully. 

We introduce the model by considering the relationship between two U.S. weekly 

interest rate series: 

1. r\t: the 1-year Treasury constant maturity rate 

2. r3r: the 3-year Treasury constant maturity rate 

Both series have 2467 observations from January 5, 1962, to April 10, 2009, 
and are measured in percentages. The series are obtained from the Federal 
Reserve Bank of St Louis. Strictly speaking, we should model the two interest 
series jointly using multivariate time series analysis in Chapter 8. However, for 
simplicity, we focus here on regression-type analysis and ignore the issue of 

simultaneity. 

Figure 2.17 shows the time plots of the two interest rates with a solid line 
denoting the 1-year rate and a dashed line the 3-year rate. Figure 2.18(a) plots ru 
versus r3f, indicating that, as expected, the two interest rates are highly correlated. 
A naive way to describe the relationship between the two interest rates is to use 

REGRESSION MODELS WITH TIME SERIES ERRORS 

91 

Figure 2.17 Time plots of U.S. weekly interest rates (in percentages) from January 5, 1962, to April 
10, 2009. Solid line is Treasury 1-year constant maturity rate and dashed line Treasury 3-year constant 
maturity rate. 

the simple model r^t = a + fir\t + et. This results in a fitted model 

r-it = 0.832 + 0.930rlr + et, oe = 0.523 (2.44) 

with R2 = 96.5% , where the standard errors of the two coefficients are 0.024 and 
0.004, respectively. Model (2.44) confirms the high correlation between the two 
interest rates. However, the model is seriously inadequate, as shown by Figure 2.19, 
which gives the time plot and ACF of its residuals. In particular, the sample ACF 
of the residuals is highly significant and decays slowly, showing the pattern of 
a unit-root nonstationary time series. The behavior of the residuals suggests that 
marked differences exist between the two interest rates. Using the modem econo¬ 
metric terminology, if one assumes that the two interest rate series are unit-root 
nonstationary, then the behavior of the residuals of Eq. (2.44) indicates that the two 
interest rates are not cointegrated', see Chapter 8 for discussion of cointegration. 
In other words, the data fail to support the hypothesis that there exists a long-term 
equilibrium between the two interest rates. In some sense, this is not surprising 
because the pattern of “inverted yield curve” did occur during the data span. By 
inverted yield curve we mean the situation under which interest rates are inversely 
related to their time to maturities. 

The unit-root behavior of both interest rates and the residuals of Eq. (2.44) leads 

to the consideration of the change series of interest rates. Let 

92 

LINEAR TIME SERIES ANALYSIS AND ITS APPLICATIONS 

Figure 2.18 Scatterplots of U.S. weekly interest rates from January 5, 1962, to April 10, 2009: 

(a) 3-year rate vs. 1-year rate and (b) changes in 3-year rate vs. changes in 1-year rate. 

1. C\t = r\t - rj,,_i = (1 - B)r\t for t > 2: changes in the 1-year interest rate 

2. c-n = r-it - r3it_i = (1 - B)rit for t > 2: changes in the 3-year interest rate 

and consider the linear regression c3, = (ic\t + et. Figure 2.20 shows time plots of 
the two change series, whereas Figure 2.18(b) provides a scatterplot between them. 
The change series remain highly correlated with a fitted linear regression model 

given by 

c-it = 0.792cu + et, ae = 0.0690, (2.45) 

with R2 = 82.5%. The standard error of the coefficient is 0.0073. This model 
further confirms the strong linear dependence between interest rates. Figure 2.21 
shows the time plot and sample ACF of the residuals of Eq. (2.45). Once again, 
the ACF shows some significant serial correlations in the residuals, but magnitudes 
of the correlations are much smaller. This weak serial dependence in the residuals 
can be modeled by using the simple time series models discussed in the previous 
sections, and we have a linear regression with time series errors. 

The main objective of this section is to discuss a simple approach for building a 
linear regression model with time series errors. The approach is straightforward. We 
employ a simple time series model discussed in this chapter for the residual series 

REGRESSION MODELS WITH TIME SERIES ERRORS 

93 

Lag 

(b) 

Figure 2.19 Residual series of linear regression (2.44) for two U.S. weekly interest rates: (a) time 
plot and (b) sample ACF. 

and estimate the whole model jointly. For illustration, consider the simple linear 
regression in Eq. (2.45). Because residuals of the model are serially correlated, we 
shall identify a simple ARMA model for the residuals. From the sample ACF of 
the residuals shown in Figure 2.21, we specify an MA(1) model for the residuals 
and modify the linear regression model to 

c3t — ficit+et, et = at — 6\at~ i, (2.46) 

where {at} is assumed to be a white noise series. In other words, we simply use 
an MA(1) model, without the constant term, to capture the serial dependence in 
the error term of Eq. (2.45). The resulting model is a simple example of linear 
regression with time series errors. In practice, more elaborated time series models 
can be added to a linear regression equation to form a general regression model 
with time series errors. 

Estimating a regression model with time series errors was not easy before the 
advent of modern computers. Special methods such as the Cochrane-Orcutt estima¬ 
tor have been proposed to handle the serial dependence in the residuals; see Greene 
(2003, p. 273). By now, the estimation is as easy as that of other time series mod¬ 
els. If the time series model used is stationary and invertible, then one can estimate 
the model jointly via the maximum-likelihood method. This is the approach we 
take by using either the SCA or R package. R and S-Plus demonstrations are given 

94 

LINEAR TIME SERIES ANALYSIS AND ITS APPLICATIONS 

(a) 

(b) 

Figure 2.20 Time plots of change series of U.S. weekly interest rates from January 12, 1962, to April 

10, 2009: (a) changes in Treasury 1-year constant maturity rate and (b) changes in Treasury 3-year 

constant maturity rate. 

later. For the U.S. weekly interest rate data, the fitted version of model (2.46) is 

c3, = 0.794cu + et = at + 0.1823a,_j, aa = 0.0678, (2.47) 

with R2 = 83.1%. The standard errors of the parameters are 0.0075 and 0.0196, 
respectively. The model no longer has a significant lag-1 residual ACF, even though 
some minor residual serial correlations remain at lags 4, 6, and 7. The incremental 
improvement of adding additional MA parameters at lags 4, 6, and 7 to the residual 

equation is small and the result is not reported here. 

Comparing the models in Eqs. (2.44), (2.45), and (2.47), we make the following 
observations. First, the high R2 96.5% and coefficient 0.930 of model (2.44) are 
misleading because the residuals of the model show strong serial correlations. 
Second, for the change series, R2 and the coefficient of Cu of models (2.45) and 
(2.47) are close. In this particular instance, adding the MA(1) model to the change 
series only provides a marginal improvement. This is not surprising because the 
estimated MA coefficient is small numerically, even though it is statistically highly 
significant. Third, the analysis demonstrates that it is important to check residual 

serial dependence in linear regression analysis. 

REGRESSION MODELS WITH TIME SERIES ERRORS 

95 

Figure 2.21 Residual series of linear regression (2.45) for two change series of U.S. weekly interest 
rates: (a) time plot and (b) sample ACF. 

From Eq. (2.47), the model shows that the two weekly interest rate series are 

related as 

r31 — ^*3,r—l + 0.794(r \t — /"it—l) + at + 0.182 at—\. 

The interest rates are concurrently and serially correlated. 

R Demonstration 
The following output has been edited. 

> rl=read.table("w-gslyr.txt",header=T) [, 4] 

> r3=read.table("w-gs3yr.txt",header=T)[,4] 

> ml=lm(r3 rl) 

> summary (ml) 

Call: 

lm(formula = r3 rl) 

Coefficients: 

Estimate Std. Error t value Pr(>|t|) 

(Intercept ) 0.83214 0.02417 34.43 <2e-16 *** 

rl 0.92955 0.00357 260.40 <2e-16 *** 

96 

LINEAR TIME SERIES ANALYSIS AND ITS APPLICATIONS 

Residual standard error: 0.5228 on 2465 degrees of freedom 

Multiple R-squared: 0.9649, Adjusted R-squared: 0.9649 

> plot(ml$residuals,type='1') 

> acf(ml$residuals,lag=36) 

> cl=diff(rl) 

> c3=diff(r3) 

> m2=lm(c3 -1+cl) 

> summary(m2) 

Call: 

lm(formula = c3 ~ -1 + cl) 

Coefficients: 

Estimate Std. Error t value Pr(>|t|) 

cl 0.791935 0.007337 107.9 <2e-16 *** 

Residual standard error: 0.06896 on 2465 degrees of freedom 

Multiple R-squared: 0.8253, Adjusted R-squared: 0.8253 

> acf(m2$residuals,lag=36) 

> m3 =arima(c3,order=c(0,0,1),xreg=cl,include.mean=F) 

> m3 

Call: 

arima(x = c3, 

order = c(0, 0, 

1), xreg = cl. 

include.mean 

F) 

Coefficients: 

mal 

0.1823 

s.e. 0.0196 

cl 

0.7936 

0.0075 

sigma''2 estimated as 0.0046: log likelihood=3136.62 , 

aic=-6267.23 

> 

> rsq= (sum (c3~2 ) - sum (m3 $ residual s''2 ) ) /sum(c3/'2) 

> rsq 

[1] 0.8310077 

Summary 
We outline a general procedure for analyzing linear regression models with time 

series errors: 

1. Fit the linear regression model and check serial correlations of the 

residuals. 

2. If the residual series is unit-root nonstationary, take the first difference of 
both the dependent and explanatory variables. Go to step 1. If the residual 
series appears to be stationary, identify an ARMA model for the residuals 
and modify the linear regression model accordingly. 

CONSISTENT COVARIANCE MATRIX ESTIMATION 

97 

3. Perform a joint estimation via the maximum-likelihood method and check 

the fitted model for further improvement. 

To check the serial correlations of residuals, we recommend that the Ljung-Box 
statistics be used instead of the Durbin—Watson (DW) statistic because the latter 
only considers the lag-1 serial correlation. There are cases in which serial depen¬ 
dence in residuals appears at higher order lags. This is particularly so when the 
time series involved exhibits some seasonal behavior. 

Remark. For a residual series e, with T observations, the Durbin-Watson 

statistic is 

DW = 

Ef=2(gr -gf-i)2 

e2 
2-,t=\ et 

Straightforward calculation shows that DW ^ 2(1 - p{), where pi is the lag-1 ACF 
of {e,}. □ 

In S-Plus, regression models with time series errors can be analyzed by the 
command ols (ordinary least squares) if the residuals assume an AR model. 
Also, to identify a lagged variable, the command is tslag, for example, y = 
tslag (r, 1). For the interest rate series, the relevant commands follow, where % 
denotes explanation of the command: 

> rlt=read.table("w-gslyr.txt",header=T)[,4] %load data 

> r3t=read.table("w-gs3yr.txt",header=T) [, 4] 

> fit=OLS(r3t rlt) % fit the first regression 

> summary(fit) 

> c3t=diff(r3t) % take difference 

> clt=diff(rlt) 

> fitl=OLS(c3t~clt) % fit second regression 

> summary(fitl) 

> fit2=OLS(c3t~clt+tslag(c3t,1)+tslag(clt,1), na.rm=T) 

> summary(fit2) 

See the output in the next section for more information. 

2.10 CONSISTENT COVARIANCE MATRIX ESTIMATION 

Consider again the regression model in Eq. (2.43). There may exist situations in 
which the error term et has serial correlations and/or conditional heteroscedasticity, 
but the main objective of the analysis is to make inference concerning the regression 
coefficients a and fi. See Chapter 3 for discussion of conditional heteroscedasticity. 
In situations under which the OLS estimates of the coefficients remain consistent, 
methods are available to provide consistent estimate of the covariance matrix of 

98 

LINEAR TIME SERIES ANALYSIS AND ITS APPLICATIONS 

the coefficient estimates. Two such methods are widely used. The first method is 
called the heteroscedasticity consistent (HC) estimator; see Eicker (1967) and White 
(1980). The second method is called the heteroscedasticity and autocorrelation 

consistent (HAC) estimator; see Newey and West (1987). 

For ease in discussion, we shall rewrite the regression model as 

yt=x'tfi + et, t = (2.48) 

where yt is the dependent variable, xt = (x\t, ■ ■ ■, xkt)' IS a k-dimensional vector 
of explanatory variables including constant, and /? = {fi\, ..., PkY is the parameter 
vector. Here c' denotes the transpose of the vector c. The LS estimate of fi and 

the associate covariance matrix are 

T 

-i T 

>
5
S
t

I
I

y^Xtyt, Cov(jj) = ae2 

_i=i 

1=1 

- T 

J2X<X> 
_1=1 

where o] is the variance of e, and is estimated by the variance of the residuals of the 
regression. In the presence of serial correlations or conditional heteroscedasticity, 
the prior covariance matrix estimator is inconsistent, often resulting in inflating the 

t ratios of j8. 

The estimator of White (1980) is 

Cov(/?)hc =  Y*XtX't 

_t=i 

. T 
E~2 t 
e,xtxt 

_i=l 

J2XtX‘ ’ 
_i=l 

(2.49) 

where et = yt - x't (i is the residual at time t. The estimator of Newey and West 

(1987) is 

where 

T 

-1 

. T 

Cov(^)hac =  J2XtX‘ 

_f=l 

Chac  J2X‘X‘ 
_t=l 

(2.50) 

T IT 

Chac = y 1 ^‘txtxt 4" y ' wj y ' ixt^tet—jxt-j 4" xt-j£t-j£txt), 

1=1 7=1 1=7 + 1 

where i is a truncation parameter and Wj is a weight function such as the Bartlett 

weight function defined by 

Wj = 1 - 

j 

l + V 

 
 
CONSISTENT COVARIANCE MATRIX ESTIMATION 

99 

Other weight functions can also be used. Newey and West (1987) suggest choosing 
i to be the integer part of 4(7'/100)2/9. This estimator essentially uses a nonpara- 
metric method to estimate the covariance matrix of 

For illustration, we employ the first differenced interest rate series in Eq. (2.45). 
The t ratio of the coefficient of c\t is 107.91 if both serial correlation and het- 
eroscedasticity in the residuals are ignored, it becomes 48.44 when the HC estimator 
is used, and it reduces to 39.92 when the HAC estimator is used. The S-Plus demon¬ 
stration below also uses a regression that includes lagged values and C3jf_i 
as regressors to take care of serial correlations in the residuals. One can also apply 
the HC or HAC estimator to the fitted model to refine the t ratios of the coefficient 
estimates. 

S-Plus Demonstration 
The following output has been edited and % denotes explanation: 

> module(finmetrics) 

> rl=read.table("w-gslyr.txt",header=T) [, 4] % Load data 
> r3=read.table("w-gs3yr.txt",header=T) [, 4] 
> cl=diff(rl) % Take 1st difference 
> c3=diff(r3) 

> reg.fit=OLS(c3 cl) % Fit a simple linear regression. 
> summary(reg.fit) 
Call: 

OLS(formula = c3 cl) 

Residuals: 

Min IQ Median 3Q Max 
-0.4246 -0.0358 -0.0012 0.0347 0.4892 

Coefficients: 

Value Std. Error t value Pr(>|t|) 
(Intercept) -0.0001 0.0014 -0.0757 0.9397 
cl 0.7919 0.0073 107.9063 0.0000 

Regression Diagnostics: 

R-Squared 0.8253 
Adjusted R-Squared 0.8253 
Durbin-Watson Stat 1.6456 

Residual Diagnostics: 

Stat P-Value 
Jarque-Bera 1644.6146 0.0000 
Ljung-Box 230.0477 0.0000 

Residual standard error: 0.06897 on 2464 degrees of freedom 

> summary(reg.fit,correction="white") % Use HC the estimator 

100 

LINEAR TIME SERIES ANALYSIS AND ITS APPLICATIONS 

Coefficients: 

Value Std. Error t value 

(Intercept) -0.0001 0.0014 -0.0757 

cl 0.7919 0.0163 48.4405 

Pr (> 111) 

0.9396 

0.0000 

> summary (reg. fit, correction"nw")  % Use the HAC estimator 

Coefficients: 

Value Std. Error 

t value Pr(>111) 

(Intercept) -0.0001 0.0016 

cl 0.7919 0.0198 

-0.0678 0.9459 

39.9223 0.0000 

% Below, fit a regression model with time series errors. 

> reg.ts=OLS(c3~cl+tslag(c3,l)+tslag(cl,1),na.rm=T) 

> summary(reg.ts) 

Call: 
OLS(formula = c3 cl + tslag(c3, l)+tslag(cl, 1), na.rm = T) 

Residuals: 

Min 

IQ Median 3Q 

Max 

-0.4481 

-0 .  0355 -0.  0008 0.0341  0.4582 

Coefficients: 

(Intercept) 

-0.0001 

0.0014 

-0.0636 

0.9493 

Value  Std. Error 

t value  Pr (>|t|) 

cl 

0.7971 

0.0077 

103.6320 

8.9057 

0.0000 
0.0000 

-9.0583 

0.0000 

tslag(c3,  1) 
tslag(cl,  1) 

0.1766 

-0.1580 

0.0198 

0.0174 

Regression Diagnostics: 

R-Squared 0.8312 

Adjusted R-Squared 0.8310 

Durbin-Watson Stat 1.9865 

Residual Diagnostics: 

Stat P-Value 

Jarque-Bera 1620.5090 0.0000 

Ljung-Box 131.6048 0.0000 

Residual standard error: 0.06785 on 2461 degrees of freedom 

A A A 

Let fij be the jth element of /3. When k> 1, the HC variance of fij in 
Eq. (2.49) can be obtained by using an auxiliary regression. Let X-yr be the 
(,k — 1)-dimensional vector obtained by removing the element xjt from x,. 
Consider the auxiliary regression 

xjt = x'-jjY + vt. 

(2.51) 

LONG-MEMORY MODELS 

101 

Let v, be the least-squares residual of this auxiliary regression. It can be shown 
that 

Var(/§7 )hc = 

where et is the residual of original regression in Eq. (2.48). The auxiliary regression 
is simply a step taken to achieve orthogonality between vt and the rest of the 
regressors so that the formula in Eq. (2.49) can be simplified. 

2.11 LONG-MEMORY MODELS 

We have discussed that for a stationary time series the ACF decays exponentially to 
zero as lag increases. Yet for a unit-root nonstationary time series, it can be shown 
that the sample ACF converges to 1 for all fixed lags as the sample size increases; 
see Chan and Wei (1988) and Tiao and Tsay (1983). There exist some time series 
whose ACF decays slowly to zero at a polynomial rate as the lag increases. These 
processes are referred to as long-memory time series. One such example is the 
fractionally differenced process defined by 

(1 - B)dxt = at, -0.5 < d < 0.5, (2.52) 

where {at} is a white noise series. Properties of model (2.52) have been widely 
studied in the literature (e.g., Hosking, 1981). We now summarize some of these 
properties: 

1. If d < 0.5, then xt is a weakly stationary process and has the infinite MA 

representation 

00 

xt — at + ^ fiat-i with \jrk 

i=1 

d(\ d) • • • (k — IT d) (k T d — 1)! 

~k\ ~ kl(d- 1)! ' 

2. If d > —0.5, then xt is invertible and has the infinite AR representation 

00 

Xt = 22 XiXt-i T at with nk 

i=1 

-d(l-d)---(k-l-d) (k-d-l)l 

k\ 

kl(-d - 1)!' 

102 

LINEAR TIME SERIES ANALYSIS AND ITS APPLICATIONS 

3. For -0.5 < d < 0.5, the ACF of x, is 

d( 1 + d) • • • (k — 1 + d) 

Pk (\-d)(2-d)---(k-d)' 

In particular, Pi = d/( 1 - d) and 

4. For -0.5 < d < 0.5, the PACF of xt is 0*,* = d/(k - d) for k = 1, 2,- 

5. For -0.5 < d < 0.5, the spectral density function f (co) of xt, which is the 

Fourier transform of the ACF of xt, satisfies 

f(co) ~ co 2d, 

as co 

0, 

(2.53) 

where co e [0, 27t] denotes the frequency. 

Of particular interest here is the behavior of ACF of x, when d <0.5. The property 
says that p^ ~ k2d~^, which decays at a polynomial, instead of exponential, rate. 
For this reason, such an x, process is called a long-memory time series. A special 
characteristic of the spectral density function in Eq. (2.53) is that the spectrum 
diverges to infinity as co -> 0. However, the spectral density function of a stationary 

ARMA process is bounded for all co e [0, 2n]. 

Earlier we used the binomial theorem for noninteger powers 

If the fractionally differenced series (1 - B)dxt follows an ARMA(p, q) model, 
then x, is called an ARFIMA(p, d, q) process, which is a generalized ARIMA 

model by allowing for noninteger d. 

In practice, if the sample ACF of a time series is not large in magnitude, 
but decays slowly, then the series may have long memory. As an illustration, 
Figure 2.22 shows the sample ACFs of the absolute series of daily simple returns 
for the CRSP value- and equal-weighted indexes from January 2, 1970, to Decem¬ 
ber 31, 2008. The ACFs are relatively small in magnitude but decay very slowly; 
they appear to be significant at the 5% level even after 300 lags. For more informa¬ 
tion about the behavior of sample ACF of absolute return series, see Ding, Granger, 
and Engle (1993). For the pure fractionally differenced model in Eq. (2.52), one 
can estimate d using either a maximum-likelihood method or a regression method 
with logged periodogram at the lower frequencies. Finally, long-memory models 
have attracted some attention in the finance literature in part because of the work 
on fractional Brownian motion in the continuous-time models. 

appendix: some sc a commands 

103 

o 

0 50 100 150 200 250 300 

Lag 

(b) 

Figure 2.22 Sample autocorrelation function of absolute series of daily simple returns for CRSP 

value- and equal-weighted indexes: (a) value-weighted index return and (b) equal-weighted index return. 
Sample period is from January 2, 1970, to December 31, 2008. 

APPENDIX: SOME SCA COMMANDS 

In this appendix, we give the SCA commands used in Section 2.9. The 1-year 
maturity interest rates are in the file w-gslyr. txt and the 3-year rates are in the 
file w-gs3yr.txt. 

— load the data into SCA, denote the data by ratel and rate3. 
input year,mom,day,ratel. file 'w-gslyr.txt' 

input year,mon,day,rate3. file 'w-gs3yr.txt' 
-- specify a simple linear regression model, 
tsm ml. model rate3=b0+(bl)ratel+noise. 

-- estimate the specified model and store residual in rl. 
estim ml. hold resi(rl). 
-- compute 10 lags of residual acf. 
acf rl. maxi 10. 

-- difference the two series, denote the new series by clt 

and c3t 

diff old ratel,rate3. new clt, c3t. compress. 

-- specify a linear regression model for the differenced data 
tsm m2, model c3t=h0+(hi)clt+noise. 
-- estimation 
estim m2, hold resi(r2). 
-- compute residual acf. 

104 

LINEAR TIME SERIES ANALYSIS AND ITS APPLICATIONS 

acf r2. maxi 10. 
__ specify a regression model with time series errors, 

tsm m3. model c3t=g0+(gl)clt+(1)noise. 

— estimate the model using the exact likelihood method, 

estim m3, method exact, hold resi(r3). 

-- compute residual acf. 

acf r3. maxi 10. 
— refine the model to include more MA lags, 

tsm m4. model c3t=g0+(gl)clt+(1,4,6,7)noise. 

-- estimation 

estim m4. method exact, hold resi(r4). 

-- compute residual acf. 

acf r4. maxi 10. 

-- exit SCA 

stop 

EXERCISES 

If not specifically specified, use 5% significance level to draw conclusions in the 

exercises. 

2.1. Suppose that the simple return of a monthly bond index follows the MA(1) 

model 

Rt — Q-t -f 0.2at-\, <ya — 0.025. 

Assume that aioo = 0.01. Compute the 1-step- and 2-step-ahead forecasts 
of the return at the forecast origin t = 100. What are the standard devia¬ 
tions of the associated forecast errors? Also compute the lag-1 and lag-2 

autocorrelations of the return series. 

2.2. Suppose that the daily log return of a security follows the model 

r, = 0.01 + 0.2rr_2 + a,, 

where {at} is a Gaussian white noise series with mean zero and variance 0.02. 
What are the mean and variance of the return series r,l Compute the lag-1 
and lag-2 autocorrelations of rt. Assume that rioo = —0.01, and 1*99 = 0.02. 
Compute the 1- and 2-step-ahead forecasts of the return series at the forecast 
origin t = 100. What are the associated standard deviations of the forecast 

errors? 

2.3. Consider the monthly U.S. unemployment rate from January 1948 to March 
2009 in the file m-unrate. txt. The data are seasonally adjusted and 
obtained from the Federal Reserve Bank of St Louis. Build a time series 
model for the series and use the model to forecast the unemployment rate 
for the April, May, June, and July of 2009. In addition, does the fitted 
model imply the existence of business cycles? Why? (Note that there are 
more than one model fits the data well. You only need an adequate model.) 

EXERCISES 

105 

2.4. Consider the monthly simple returns of the Decile 1, Decile 2, Decile 9, and 
Decile 10 of NYSE/AMEX/NASDAQ based on market capitalization. The 
data span is from January 1970 to December 2008, and the data are obtained 
from CRSP. 

(a) For the return series of Decile 2 and Decile 10, test the null hypothesis 
that the first 12 lags of autocorrelations are zero at the 5% level. Draw 
your conclusion. 

(b) Build an ARMA model for the return series of Decile 2. Perform model 

checking and write down the fitted model. 

(c) Use the fitted ARMA model to produce 1- to 12-step-ahead forecasts of 

the series and the associated standard errors of forecasts. 

2.5. Consider the daily simple returns of IBM stock from 1970 to 2008 in the 
file d-ibm3dx7 008 . txt. Compute the first 100 lags of ACF of the absolute 
series of daily simple returns of IBM stock. Is there evidence of long-range 
dependence? Why? 

2.6. Consider the demand of electricity of a manufacturing sector in the United 
States. The data are logged, denote the demand of a fixed day of each month, 
and are in power6 . txt. Build a time series model for the series and use the 
fitted model to produce 1- to 24-step-ahead forecasts. 

2.7. Consider the daily simple returns of IBM stock, CRSP value-weighted index, 
CRSP equal-weighted index, and the S&P composite index from January 
1980 to December 2008. The index returns include dividend distributions. 
The data file is d-ibm3dxwkdays8008.txt, which has 12 columns. The 
columns are (year, month, day, IBM, VW, EW, SP, M, T, W, H, F), where M, 
T, W, R, and F denotes indicator variables for Monday to Friday, respectively. 
Use a regression model to study the effects of trading days on the equal- 
weighted index returns. What is the fitted model? Are the weekday effects 
significant in the returns at the 5% level? Use the HAC estimator of the 
covariance matrix to obtain the t ratio of regression estimates. Does the 
HAC estimator change the conclusion of weekday effects? Are there serial 
correlations in the regression residuals? If yes, build a regression model with 
time series error to study weekday effects. 

2.8. Consider the data set of the previous question, but focus on the daily simple 
returns of the S&P composite index. Perform the necessary data analysis 
and statistical tests using the 5% significance level to answer the following 
questions: 

(a) Is there any weekday effect on the daily simple returns of the S&P com¬ 
posite index? You may employ a Unear regression model to answer this 
question. Estimate the model, check its validity, and test the hypothesis 
that there is no Friday effect. Draw your conclusion. 

(b) Check the residual serial correlations using Q(12) statistic. Are there any 
significant serial correlations in the residuals? If yes, build a regression 
model with time series errors for the data. 

106 

LINEAR TIME SERIES ANALYSIS AND ITS APPLICATIONS 

2.9. Now consider similar questions of the previous exercise for the IBM stock 

returns. 
(a) Is there any weekday effect on the daily simple returns of IBM stock? 
Estimate your model and test the hypothesis that there is no Friday effect. 

Draw your conclusion. 

(b) Are there serial correlations in the residuals? Use Q(12) to perform the 

test. Draw your conclusion. 

(c) Refine the above model by using the technique of regression model with 
time series errors. In there a significant weekday effect based on the 

refined model? 

2.10. Consider the weekly yields of Moody’s Aaa and Baa seasoned bonds from 
January 5, 1962, to April 10, 2009. The data are obtained from the Federal 
Reserve Bank of St Louis. Weekly yields are averages of daily yields. Obtain 
the summary statistics (sample mean, standard deviation, skewness, excess 
kurtosis, minimum, and maximum) of the two yield series. Are the bond 
yields skewed? Do they have heavy tails? Answer the questions using 5% 

significance level. 

2.11. Consider the monthly Aaa bond yields of the prior problem. Build a time 

series model for the series. 

2.12. Again, consider the two bond yield series, that is, Aaa and Baa. What is the 
relationship between the two series? To answer this question, build a time 
series model using yields of Aaa bonds as the dependent variable and yields 

of Baa bonds as independent variable. 

2.13. Consider the monthly log returns of CRSP equal-weighted index from Jan¬ 
uary 1962 to December 1999 for 456 observations. You may obtain the data 
from CRSP directly or from the file m-ew62 99. txt on the Web. 

(a) Build an AR model for the series and check the fitted model. 

(b) Build an MA model for the series and check the fitted model. 

(c) Compute 1- and 2-step-ahead forecasts of the AR and MA models built 

in the previous two questions. 

(d) Compare the fitted AR and MA models. 

2.14. This problem is concerned with the dynamic relationship between the spot 
and futures prices of the S&P 500 index. The data file sp5may.dat has 
three columns: log(futures price), log(spot price), and cost-of-carry (xlOO). 
The data were obtained from the Chicago Mercantile Exchange for the S&P 
500 stock index in May 1993 and its June futures contract. The time interval 
is 1 minute (intraday). Several authors used the data to study index futures 
arbitrage. Here we focus on the first two columns. Let ft and st be the 
log prices of futures and spot, respectively. Consider yt — ft — ft-\ and 
x, = st - st-\. Build a regression model with time series errors between {yf} 
and {x,}, with y, being the dependent variable. 

REFERENCES 

107 

2.15. The quarterly gross domestic product implicit price deflator is often used 
as a measure of inflation. The file q-gdpdef. txt contains the data for the 
United States from the first quarter of 1947 to the last quarter of 2008. Data 
format is year, month, day, and deflator. The data are seasonally adjusted 
and equal to 100 for year 2000. Build an ARIMA model for the series and 
check the validity of the fitted model. Use the fitted model to predict the 
inflation for each quarter of 2009. The data are obtained from the Federal 
Reserve Bank of St Louis. 

REFERENCES 

Akaike, H. (1973). Information theory and an extension of the maximum likelihood principle. 
In B. N. Petrov and F. Csaki, (eds.). 2nd International Symposium on Information 
Theory, pp. 267-281. Akademia Kiado, Budapest. 

Box, G. E. P. and Pierce, D. (1970). Distribution of residual autocorrelations in 
autoregressive-integrated moving average time series models. Journal of the American 
Statistical Association 65: 1509-1526. 

Box, G. E. P., Jenkins, G. M., and Reinsel, G. C. (1994). Time Series Analysis: Forecasting 

and Control, 3rd ed. Prentice Hall, Englewood Cliffs, NJ. 

Brockwell, P. J. and Davis, R. A. (1991). Time Series: Theory and Methods, 2nd ed. Springer, 

New York. 

Brockwell, P. J. and Davis, R. A. (1996). Introduction to Time Series and Forecasting. 

Springer, New York. 

Chan, N. H. and Wei, C. Z. (1988). Limiting distributions of least squares estimates of 

unstable autoregressive processes. Annals of Statistics 16: 367-401. 

Dickey, D. A. and Fuller, W. A. (1979). Distribution of the estimates for autoregres¬ 
sive time series with a unit root. Journal of the American Statistical Association 74: 
427-431. 

Ding, Z., Granger, C. W. J., and Engle, R. F. (1993). A long memory property of stock 

returns and a new model. Journal of Empirical Finance 1: 83-106. 

Eicker, F. (1967). Limit theorems for regression with unequal and dependent Errors. In L. 
LeCam and J. Neyman (eds.). Proceedings of the 5th Berkeley Symposium on Mathe¬ 
matical Statistics and Probability. University of California Press, Berkeley. 

Fuller, W. A. (1976). Introduction to Statistical Time Series, Wiley, New York. 

Greene, W. H. (2003). Econometric Analysis, 5th ed. Prentice-Hall, Upper Saddle River, 

NJ. 

Hosking, J. R. M. (1981). Fractional differencing. Biometrika 68: 165-176. 

Ljung, G. and Box, G. E. P. (1978). On a measure of lack of fit in time series models. 

Biometrika 66: 61—12. 

Newey, W. and West, K. (1987). A simple positive semidefinite, heteroscedasticity and 

autocorrelation consistent covariance matrix. Econometrica 55: 863-898. 

Phillips, P. C. B. (1987). Time series regression with a unit root. Econometrica 55: 277-301. 

108 

LINEAR TIME SERIES ANALYSIS AND ITS APPLICATIONS 

Shumway, R. H. and Staffer, D. S. (2000). Time Series Analysis and Its Applications. 

Springer, New York. 

Tiao, G. C. and Tsay, R. S. (1983). Consistency properties of least squares estimates of 
autoregressive parameters in ARMA models. Annals of Statistics 11: 856—871. 

Tsay, R. S. and Tiao, G. C. (1984). Consistent estimates of autoregressive parameters and 
extended sample autocorrelation function for stationary and nonstationary ARMA mod¬ 
els. Journal of the American Statistical Association 79: 84-96. 

White, H. (1980). A heteroscedasticity consistent covariance matrix estimator and a direct 

test for heteroscedasticity. Econometrica 48: 827-838. 

CHAPTER 3 

Conditional Heteroscedastic Models 

The objective of this chapter is to study some methods and econometric models 
available in the literature for modeling the volatility of an asset return. The models 
are referred to as conditional heteroscedastic models. 

Volatility is an important factor in options trading. Here volatility means the 
conditional standard deviation of the underlying asset return. Consider, for example, 
the price of a European call option, which is a contract giving its holder the right, 
but not the obligation, to buy a fixed number of shares of a specified common 
stock at a fixed price on a given date. The fixed price is called the strike price 
and is commonly denoted by K. The given date is called the expiration date. The 
important time duration here is the time to expiration (measured in years), and we 
denote it by i. The well-known Black-Scholes option pricing formula states that 
the price of such a call option is 

ct 

P,(p(x) — Ke rtA>(x — otVl), and 

In(Pt/K) + re , 1 n 

a, ~fl + 

(3.1) 

where Pt is the current price of the underlying stock, r is the continuously com¬ 
pounded risk-free interest rate, ot is the annualized conditional standard deviation 
of the log return of the specified stock, and 4>(jc) is the cumulative distribution 
function of the standard normal random variable evaluated at x. A derivation of 
the formula is given in Chapter 6. The formula has several nice interpretations, 
but it suffices to say here that the conditional standard deviation of the log return 
of the underlying stock plays an important role. This volatility evolves over time. 
If the holder can exercise her right any time on or before the expiration date, then 
the option is called an American call option. 

Volatility has many other financial applications. As discussed in Chapter 7, 
volatility modeling provides a simple approach to calculating value at risk of a 
financial position in risk management. It plays an important role in asset allocation 

Analysis of Financial Time Series, Third Edition, By Ruey S. Tsay 
Copyright © 2010 John Wiley & Sons, Inc. 

109 

110 

CONDITIONAL HETEROSCEDASTIC MODELS 

under the mean-variance framework. Furthermore, modeling the volatility of a 
time series can improve the efficiency in parameter estimation and the accuracy in 
interval forecast. Finally, the volatility index of a market has recently become a 
financial instrument. The VIX volatility index compiled by the Chicago Board of 
Option Exchange (CBOE) started to trade in futures on March 26, 2004. 

The univariate volatility models discussed in this chapter include the autoregres¬ 
sive conditional heteroscedastic (ARCH) model of Engle (1982), the generalized 
ARCH (GARCH) model of Bollerslev (1986), the exponential GARCH (EGARCH) 
model of Nelson (1991), the threshold GARCH (TGARCH) model of Glosten, 
Jagannathan, and Runkle (1993) and Zakoian (1994), the conditional heteroscedas¬ 
tic autoregressive moving-average (CHARMA) model of Tsay (1987), the random 
coefficient autoregressive (RCA) model of Nicholls and Quinn (1982), and the 
stochastic volatility (SV) models of Melino and Turnbull (1990), Taylor (1994), 
Harvey, Ruiz, and Shephard (1994), and Jacquier, Poison, and Rossi (1994). We 
also discuss advantages and weaknesses of each volatility model and show some 
applications of the models. Multivariate volatility models, including those with 
time-varying correlations, are discussed in Chapter 10. The chapter also discusses 
some alternative approaches to volatility modeling in Section 3.15, including use 

of daily high and low prices of an asset. 

3.1 CHARACTERISTICS OF VOLATILITY 

A special feature of stock volatility is that it is not directly observable. For example, 
consider the daily log returns of IBM stock. The daily volatility is not directly 
observable from the return data because there is only one observation in a trading 
day. If intraday data of the stock, such as 10-minute returns, are available, then one 
can estimate the daily volatility. See Section 3.15. The accuracy of such an estimate 
deserves a careful study, however. For example, stock volatility consists of intraday 
volatility and overnight volatility with the latter denoting variation between trading 
days. The high-frequency intraday returns contain only very limited information 
about the overnight volatility. The unobservability of volatility makes it difficult 
to evaluate the forecasting performance of conditional heteroscedastic models. We 

discuss this issue later. 

In options markets, if one accepts the idea that the prices are governed by an 
econometric model such as the Black-Scholes formula, then one can use the price 
to obtain the “implied” volatility. Yet this approach is often criticized for using a 
specific model, which is based on some assumptions that might not hold in practice. 
For instance, from the observed prices of a European call option, one can use the 
Black-Scholes formula in Eq. (3.1) to deduce the conditional standard deviation 
crr. The resulting value of o, is called the implied volatility of the underlying stock. 
However, this implied volatility is derived under the assumption that the price of 
the underlying asset follows a geometric Brownian motion. It might be different 
from the actual volatility. Experience shows that implied volatility of an asset return 
tends to be larger than that obtained by using a GARCH type of volatility model. 

STRUCTURE OF A MODEL 

111 

This might be due to the risk premium for volatility or to the way daily returns are 
calculated. The VIX of CBOE is an implied volatility. 

Although volatility is not directly observable, it has some characteristics that are 
commonly seen in asset returns. First, there exist volatility clusters (i.e., volatility 
may be high for certain time periods and low for other periods). Second, volatil¬ 
ity evolves over time in a continuous manner—that is, volatility jumps are rare. 
Third, volatility does not diverge to infinity—that is, volatility varies within some 
fixed range. Statistically speaking, this means that volatility is often stationary. 
Fourth, volatility seems to react differently to a big price increase or a big price 
drop, referred to as the leverage effect. These properties play an important role 
in the development of volatility models. Some volatility models were proposed 
specifically to correct the weaknesses of the existing ones for their inability to 
capture the characteristics mentioned earlier. For example, the EGARCH model 
was developed to capture the asymmetry in volatility induced by big “positive” 
and “negative” asset returns. 

3.2 STRUCTURE OF A MODEL 

Let rt be the log return of an asset at time index t. The basic idea behind volatility 
study is that the series {r,} is either serially uncorrelated or with minor lower order 
serial correlations, but it is a dependent series. For illustration, consider the monthly 
log stock returns of Intel Corporation from January 1973 to December 2008 shown 
in Figure 3.1. Figure 3.2(a) shows the sample ACF of the log return series, 
which suggests no significant serial correlations except for a minor one at lag 7. 
Figure 3.2(c) shows the sample ACF of the absolute log returns (i.e., \rt\), whereas 
Figure 3.2(b) shows the sample ACF of the squared log returns if. These two plots 
clearly suggest that the monthly log returns are not serially independent. Combin¬ 
ing the three plots, it seems that the log returns are indeed serially uncorrelated but 
dependent. Volatility models attempt to capture such dependence in the return series. 
To put the volatility models in proper perspective, it is informative to consider 

the conditional mean and variance of rt given ; that is, 

Mr = E(rt\F,-i), of = Var(r;|Ff_!) = E[(r, - [it)2\Ft-i], (3.2) 

where Ft-\ denotes the information set available at time t — 1. Typically, Ft-\ 
consists of all linear functions of the past returns. As shown by the empirical 
examples of Chapter 2 and Figure 3.2, serial dependence of a stock return series r, 
is weak if it exists at all. Therefore, the equation for /a, in (3.2) should be simple, 
and we assume that rt follows a simple time series model such as a stationary 
ARMA(p, q) model with some explanatory variables. In other words, we entertain 
the model 

112 

CONDITIONAL HETEROSCEDASTIC MODELS 

c l— 

=3 
a> 

03 
O 

Figure 3.1 Time plot of monthly log returns of Intel stock from January 1973 to December 2008. 

Lag 

(a) 

Lag 

(b) 

Lag 

(c) 

Lag 

(d) 

Figure 3.2 Sample ACF and PACF of various functions of monthly log stock returns of Intel Corpo¬ 
ration from January 1973 to December 2008: (a) ACF of the log returns, (b) ACF of the squared log 

returns, (c) ACF of the absolute log returns, and (d) PACF of the squared log returns. 

MODEL BUILDING 

113 

for rt, where k, p, and q are nonnegative integers, and xit are explanatory variables. 
Here yt is simply a notation representing the adjusted return series after removing 
the effect of explanatory variables. 

Model (3.3) illustrates a possible financial application of the regression model 
with time series errors of Chapter 2. The order (p, q) of an ARMA model may 
depend on the frequency of the return series. For example, daily returns of a market 
index often show some minor serial correlations, but monthly returns of the index 
may not contain any significant serial correlation. The explanatory variables xt in 
Eq. (3.3) are flexible. For example, a dummy variable can be used for the Mondays 
to study the effect of the weekend on daily stock returns. In the capital asset pricing 
model (CAPM), the mean equation of r, can be written as r, = 0O + +at, 
where rmJ denotes the market return. 

Combining Eqs. (3.2) and (3.3), we have 

°?2 = Var(rf|F,_i) = Var(a, IF,-!). (3.4) 

The conditional heteroscedastic models of this chapter are concerned with the 
evolution of of. The manner under which of evolves over time distinguishes one 
volatility model from another. 

Conditional heteroscedastic models can be classified into two general categories. 
Those in the first category use an exact function to govern the evolution of of, 
whereas those in the second category use a stochastic equation to describe of. 
The GARCH model belongs to the first category, whereas the stochastic volatility 
model is in the second category. 

Throughout the book, at is referred to as the shock or innovation of an asset 
return at time t and ot is the positive square root of of. The model for pt in Eq. 
(3.3) is referred to as the mean equation for rt and the model for of is the volatility 
equation for rt. Therefore, modeling conditional heteroscedasticity amounts to aug¬ 
menting a dynamic equation, which governs the time evolution of the conditional 
variance of the asset return, to a time series model. 

3.3 MODEL BUILDING 

Building a volatility model for an asset return series consists of four steps: 

1. Specify a mean equation by testing for serial dependence in the data and, if 
necessary, building an econometric model (e.g., an ARMA model) for the 
return series to remove any linear dependence. 

2. Use the residuals of the mean equation to test for ARCH effects. 

3. Specify a volatility model if ARCH effects are statistically significant, and 

perform a joint estimation of the mean and volatility equations. 

4. Check the fitted model carefully and refine it if necessary. 

114 

CONDITIONAL HETEROSCEDASTIC MODELS 

For most asset return series, the serial correlations are weak, if any. Thus, 
building a mean equation amounts to removing the sample mean from the data if 
the sample mean is significantly different from zero. For some daily return series, a 
simple AR model might be needed. In some cases, the mean equation may employ 
some explanatory variables such as an indicator variable for weekend or January 

effects. 

In what follows, we use R (both with and without OX) and S-Plus in empirical 
illustrations. Other software packages (e.g., Eviews, SCA, and RATS) can also be 

used. 

3.3.1 Testing for ARCH Effect 

For ease in notation, let at — rt — /x, be the residuals of the mean equation. The 
squared series aj is then used to check for conditional heteroscedasticity, which 
is also known as the ARCH effects. Two tests are available. The first test is to 
apply the usual Ljung-Box statistics Q(m) to the {a?} series; see McLeod and Li 
(1983). The null hypothesis is that the first m lags of ACF of the aj series are zero. 
The second test for conditional heteroscedasticity is the Lagrange multiplier test 
of Engle (1982). This test is equivalent to the usual F statistic for testing a, = 0 

(/ = 1, ..., m) in the linear regression 

ctf — cxq T" oiiaj_j + • • • + amaf_m + et, t = m + 1,..., T, 

where et denotes the error term, m is a prespecified positive integer, and T is 
the sample size. Specifically, the null hypothesis is H0 : «i = ■ ■ ■ = am = 0. Let 

SSRo = Er=m+i(af2 - ")2> where (i> = (\/T) Ei 1 «r2 is the sample mean of aj, 
and SSRj = J2j=m+1 where e, is the least-squares residual of the prior linear 
regression. Then we have 

(SSR0-SSRi)/m 

~ SSR!/(T - 2m - 1)’ 

which is asymptotically distributed as a chi-squared distribution with m degrees of 
freedom under the null hypothesis. The decision rule is to reject the null hypothesis 

if F>x^(a), where /«(“) is the upper 100(1 - a)th percentile of x,2, or the 
p value of F is less than a, type-I error. 

To demonstrate, we consider the monthly log stock returns of Intel Corporation 
from 1973 to 2008; see Example 3.1. The series does not have significant serial 
correlations so that it can be directly used to test for the ARCH effect. Indeed, 
the Q(m) statistics of the return series give <2(12) = 18.26 with a p value of 
0.11, confirming no serial correlations in the data. On the other hand, the Lagrange 
multiplier test shows strong ARCH effects with test statistic F ~ 53.62, the p 
value of which is close to zero. The Ljung-Box statistics of the at series also 
shows strong ARCH effects with (9(12) = 89.85, the p value of which is close to 

zero. 

THE ARCH MODEL 

S-Plus Demonstration 

115 

Denote the return series by intc. Note that the command archTest applies 
directly to the at series, not to aj. 

> da=read.table("m-intc7308.txt",header=T) 
> intc=log(da[,2]+1) 
> autocorTest(intc,lag=12) 
Test for Autocorrelation: Ljung-Box 
Null Hypothesis: no autocorrelation 

Test Statistics: 

Test Stat 18.2635 p.value 0.1079 

Dist. under Null: chi-square with 12 degrees of freedom 

Total Observ.: 432 

> archTest(intc,lag=12) 
Test for ARCH Effects: LM Test 
Null Hypothesis: no ARCH effects 

Test Statistics: 
Test Stat 53.6197 p.value 0.0000 

Dist. under Null: chi-square with 12 degrees of freedom 

R Demonstration 

> da=read.table("m-intc7308.txt",header=T) 
> intc=log(da[,2]+1) 
> Box.test(intc,lag=12,type='Ljung') 

Box-Ljung test 

data: intc 

X-squared = 18.2635, df = 12, p-value = 0.1079 

> at=intc-mean(intc) 
> Box.test(atA2,lag=12,type='Ljung') 

Box-Ljung test 

data: at^2 

X-squared = 89.8509, df = 12, p-value = 5.274e-14 

3.4 THE ARCH MODEL 

The first model that provides a systematic framework for volatility modeling 
is the ARCH model of Engle (1982). The basic idea of ARCH models is that 
(a) the shock at of an asset return is serially uncorrelated, but dependent, and 

116 

CONDITIONAL HETEROSCEDASTIC MODELS 

(b) the dependence of at can be described by a simple quadratic function of its 
lagged values. Specifically, an ARCH(m) model assumes that 

at = crt€t, cr2 = ao + u\ctf_-y + • • • + amaf_m, (3.5) 

where {et} is a sequence of independent and identically distributed (iid) random 
variables with mean zero and variance 1, «o > 0, and a, > 0 for i > 0. The coeffi¬ 
cients a, must satisfy some regularity conditions to ensure that the unconditional 
variance of at is finite. In practice, et is often assumed to follow the standard 
normal or a standardized Student-t or a generalized error distribution. 

From the structure of the model, it is seen that large past squared shocks {a2_! }^_1 
imply a large conditional variance cr2 for the innovation a,. Consequently, a, tends 
to assume a large value (in modulus). This means that, under the ARCH framework, 
large shocks tend to be followed by another large shock. Here I use the word 
tend because a large variance does not necessarily produce a large realization. It 
only says that the probability of obtaining a large variate is greater than that of 
a smaller variance. This feature is similar to the volatility clusterings observed in 

asset returns. 

The ARCH effect also occurs in other financial time series. Figure 3.3 shows 
the time plots of (a) the percentage changes in Deutsche mark/U.S. dollar exchange 
rate measured in 10-minute intervals from June 5, 1989, to June 19, 1989, for 2488 
observations, and (b) the squared series of the percentage changes. Big percentage 

Figure 3.3 (a) Time plot of 10-minute returns of exchange rate between Deutsche mark and U.S. 
dollar from June 5, 1989, to June 19, 1989, and (b) the squared returns. 

THE ARCH MODEL 

117 

o 

o 

Figure 3.4 (a) Sample autocorrelation function of return series of mark/dollar exchange rate and (b) 
sample partial autocorrelation function of squared returns. 

changes occurred occasionally, but there were certain stable periods. Figure 3.4(a) 
shows the sample ACF of the percentage change series. Clearly, the series has no 
serial correlation. Figure 3.4(b) shows the sample PACF of the squared series of 
percentage change. It is seen that there are some big spikes in the PACF. Such 
spikes suggest that the percentage changes are not serially independent and have 
some ARCH effects. 

Remark. Some authors use h, to denote the conditional variance in Eq. (3.5). 

In this case, the shock becomes at — *Jh~t€t. □ 

3.4.1 Properties of ARCH Models 

To understand the ARCH models, it pays to carefully study the ARCH(l) model 

at = at€t, a? = aQ + oq a?_v 

where ap > 0 and oq > 0. First, the unconditional mean of at remains zero because 

E(at) = E[E(at\Ft-i)] = E[atE(et)] - 0. 

118 

CONDITIONAL HETEROSCEDASTIC MODELS 

Second, the unconditional variance of at can be obtained as 

Var (at) = E(a2) = E[E{a2\Ft-\)] 

= E(a0 + = a0 + a\E(a2_x). 

Because at is a stationary process with E(at) = 0, Var(a,) = Var(af_i) = E(a2_j). 
Therefore, we have Var(ar) = ao + aqVar(a,) and Var(af) = ao/0 — ai)- Since the 
variance of a, must be positive, we require 0 < a\ < 1. Third, in some applications, 
we need higher order moments of at to exist and, hence, a\ must also satisfy some 
additional constraints. For instance, to study its tail behavior, we require that the 
fourth moment of at is finite. Under the normality assumption of et in Eq. (3.5), 

we have 

Therefore, 

E(aj\Ft-0 = 3[E(a2\Ft^)]2 = 3(ao + 

E(af) = E[E(af\Ft-i)\ = 3E(a0 + cc\a2_x)2 = 3E {a20 + 2aoaxa2_x + a^a^) . 

If a, is fourth-order stationary with = E(af), then we have 

mi = 3[«o + 2o:oaiVar(a,) + 

r\ 'J 

= 3al ^1 + ^ i 011 a ) + 3a2m4. 

Consequently, 

3ag(l + ai) 

m 4 =- 

(1 -ai)(l-3af) 

This result has two important implications: (a) since the fourth moment of at 
is positive, we see that a\ must also satisfy the condition 1 — 3a2 > 0; that is, 
0 < a2 < and (b) the unconditional kurtosis of at is 

E(af) _ gpCl+ofi) x (1 ~ «i)2 _ 3 1 ~ > 3 

[Var(ar)]2 (1 — ai)(l — 3a2) aq 1 — 3a2 

Thus, the excess kurtosis of a, is positive and the tail distribution of at is heavier 
than that of a normal distribution. In other words, the shock a, of a conditional 
Gaussian ARCH(l) model is more likely than a Gaussian white noise series to 
produce “outliers." This is in agreement with the empirical finding that “outliers” 
appear more often in asset returns than that implied by an iid sequence of normal 
random variates. 

These properties continue to hold for general ARCH models, but the formulas 
become more complicated for higher order ARCH models. The condition a,- > 0 in 

THE ARCH MODEL 

119 

Eq. (3.5) can be relaxed. It is a condition to ensure that the conditional variance a} 
is positive for all t. In fact, a natural way to achieve positiveness of the conditional 
variance is to rewrite an ARCH(m) model as 

at = crt€t, of = oiq + A'mt_^Q.Am^t-\, (3.6) 

where ..., at-m)' and £2 is an m x m nonnegative definite matrix. 
The ARCH(m) model in Eq. (3.5) requires £2 to be diagonal. Thus, Engle’s model 
uses a parsimonious approach to approximate a quadratic function. A simple way to 
achieve Eq. (3.6) is to employ a random-coefficient model for at \ see the CHARMA 
and RCA models discussed later. 

3.4.2 Weaknesses of ARCH Models 

The advantages of ARCH models include properties discussed in the previous 
section. The model also has some weaknesses: 

1. The model assumes that positive and negative shocks have the same effects 
on volatility because it depends on the square of the previous shocks. In 
practice, it is well known that the price of a financial asset responds differently 
to positive and negative shocks. 

2. The ARCH model is rather restrictive. For instance, a\ of an ARCH(l) model 
must be in the interval [0, |] if the series has a finite fourth moment. The 
constraint becomes complicated for higher order ARCH models. In practice, 
it limits the ability of ARCH models with Gaussian innovations to capture 
excess kurtosis. 

3. The ARCH model does not provide any new insight for understanding the 
source of variations of a financial time series. It merely provides a mechanical 
way to describe the behavior of the conditional variance. It gives no indication 
about what causes such behavior to occur. 

4. ARCH models are likely to overpredict the volatility because they respond 

slowly to large isolated shocks to the return series. 

3.4.3 Building an ARCH Model 

Among volatility models, specifying an ARCH model is relatively easy. Details 
are given below. 

Order Determination 
If an ARCH effect is found to be significant, one can use the PACF of aj to 
determine the ARCH order. Using PACF of af to select the ARCH order can be 
justified as follows. From the model in Eq. (3.5), we have 

of = a0 + a\af_x H-f oimaf_m. 

120 

CONDITIONAL HETEROSCEDASTIC MODELS 

For a given sample, aj is an unbiased estimate of ay. Therefore, we expect that aj 
is linearly related to aj_x,..., aj_m in a manner similar to that of an autoregressive 
model of order m. Note that a single aj is generally not an efficient estimate of 
of, but it can serve as an approximation that could be informative in specifying 

the order m. 

Alternatively, define ry = aj - a}. It can be shown that {ry} is an uncorrelated 

series with mean 0. The ARCH model then becomes 

aj — ao + a\aj_i + • • • + amaj_m + rjt, 

which is in the form of an AR(m) model for aj, except that [ry] is not an iid series. 
From Chapter 2, PACF of aj is a useful tool to determine the order m. Because 
[ry] are not identically distributed, the least-squares estimates of the prior model 
are consistent but not efficient. The PACF of aj may not be effective when the 

sample size is small. 

Estimation 
Several likelihood functions are commonly used in ARCH estimation, depending on 
the distributional assumption of et. Under the normality assumption, the likelihood 
function of an ARCH(m) model is 

f(a\, ..., aj\a) = /(aj\FT-\)f (Ar-l\Ft-2) • • ■ /(am+i\Fm)f (a\, • • •, am\a) 

T 1 

= n -ih- 

;=m+1 \/2^z 

exp 

2 erf 

x f(ai 

@m l®0> 

where a = (ao, ot\,am)' and f(a\,..., am\a) is the joint probability density 
function of a\,..., am. Since the exact form of f {ai, ..., am\a) is complicated, 
it is commonly dropped from the prior likelihood function, especially when the 
sample size is sufficiently large. This results in using the conditional-likelihood 
function 

f (^m+l ? • • • ? ^1 j • • • > ^/ft) 

t=m +1  y 2n oj 

exp 

2 oj 

where of can be evaluated recursively. We refer to estimates obtained by maximiz¬ 
ing the prior likelihood function as the conditional maximum-likelihood estimates 
(MLEs) under normality. 

Maximizing the conditional-likelihood function is equivalent to maximizing its 

logarithm, which is easier to handle. The conditional log-likelihood function is 

f(Am+i> • • • > aj\oi, a\,..., am) — ^ ^ 

t=m+1 

1 1 9 1 aj 
--ln(2jr) - - In(of) - - — 
2 2 2 of 

THE ARCH MODEL 

121 

Since the first term ln(27r) does not involve any parameters, the log-likelihood 
function becomes 

f , aj\<x, a\, 

•) = - E 

t=m+1 

^ln (*t2) + ^4 

2 2 of J 

where of — ao + + • • • + amaj_m can be evaluated recursively. 

In some applications, it is more appropriate to assume that et follows a 
heavy-tailed distribution such as a standardized Student-/ distribution. Let xv be a 
Student-/ distribution with v degrees of freedom. Then Var(x„) = v/(v -2) for 
v > 2, and we use €t = xv/*Jv/(v — 2). The probability density function of ef is 

r[(n +1)/2] ^ ^ e2 yb+w 

T{v/2)y/{v — 2)7r \ v — 2/ 

v>2, 

(3.7) 

where T(a) is the usual gamma function (i.e., T(jc) = yx~1e~y dy). Using 

= otet, we obtain the conditional-likelihood function of at as 

f (&m+1 > • • • i &T I® 5 ) = 

T 

T[(u + l)/2] 1 

-(i»+l)/2 

f=m+l 

T{v/2)y/{v — 2)n ot 

(V - 2)crj.2 _ 

where v > 2 and Am = (aj, a2,..., am)- We refer to the estimates that maximize the 
prior likelihood function as the conditional MLEs under / distribution. The degrees 
of freedom of the / distribution can be specified a priori or estimated jointly with 
other parameters. A value between 4 and 8 is often used if it is prespecified. 

If the degrees of freedom v of the Student-/ distribution is prespecified, then 

the conditional log-likelihood function is 

f(^m+1) • • • > Am) — 

T 

v + 1 

/=m+1 

In 1 + 

(v 

2 W 

+ 2 ln(°) ) 

(3.8) 

If one wishes to estimate v jointly with other parameters, then the log-likelihood 
function becomes 

£(am+i, ...,aT\ot, v, Am) = (T - m) | In 

u + 1 

In [>■©] 

- 0.51n[(u -2)tt]  + f (flm+i,. • •, flr|or, Am), 

where the second term is given in Eq. (3.8). 

Besides fat tails, empirical distributions of asset returns may also be skewed. 
To handle this additional characteristic of asset returns, the Student-/ distribution 

122 

CONDITIONAL HETEROSCEDASTIC MODELS 

has been modified to become a skew-Student-! distribution. There are multiple ver¬ 
sions of skew-Student-t distribution, but we shall adopt the approach of Fernandez 
and Steel (1998), which can introduce skewness into any continuous unimodal and 
symmetric (with respect to 0) univariate distribution. Specifically, for the innova¬ 
tion €t of an ARCH process, Lambert and Laurent (2001) apply the Fernandez and 
Steel method to the standardized Student-! distribution in Eq. (3.7) to obtain a stan¬ 
dardized skew-Student-! distribution. The resulting probability density function is 

g(etIS. u) = 

e/[£(e*t +<«) I"] 

if €t < —co/q. 

2 
f+T 

Qf[(Q*t + w)/§|v] 

if El > ~(o/q. 

(3.9) 

where /(•) is the probability density function (pdf) of the standardized Student-! 
distribution in Eq. (3.7), £ is the skewness parameter, v > 2 is the degrees of 

freedom, and the parameters q and Ho are given below: 

r[(u-1)/2]nA7^2 1 

v^r(u/2) £ 

e2 = (?2 + TA-l)  nr 

In Eq. (3.9), £2 is equal to the ratio of probability masses above and below the 
mode of the distribution and, hence, it is a measure of the skewness. 

Finally, e, may assume a generalized error distribution (GED) with probability 

density function 

v exp(— j\x/k\v) 

^X) ^ X2ri+i/")r(l/v) ’ 

— OO < X < OO, 0 < V < oo, 

(3.10) 

where T(-) is the gamma function and k = [2(—2/u)T(1/v)/ r(3/u)]l/2. This dis¬ 
tribution reduces to a Gaussian distribution if v = 2, and it has heavy tails when 
v < 2. The conditional log-likelihood function t(am+1, ..., aria, Am) can easily 

be obtained. 

Remark. Skew Student-!, skew normal, and skew GED distributions are avail¬ 
able in the fGarch package of Rmetrics. The commands are sstd, snorm, and 
sged, respectively. See the R demonstration below for an example. □ 

Model Checking 
For a properly specified ARCH model, the standardized residuals 

a, 
at = — 
e, 

form a sequence of iid random variables. Therefore, one can check the adequacy 
of a fitted ARCH model by examining the series {at}. In particular, the Ljung-Box 

THE ARCH MODEL 

123 

statistics of at can be used to check the adequacy of the mean equation and that 
of at can be used to test the validity of the volatility equation. The skewness, 
kurtosis, and quantile-to-quantile plot (i.e., QQ plot) of {at} can be used to check 
the validity of the distribution assumption. Many residual plots are available in 
S-Plus for model checking. 

Forecasting 

Forecasts of the ARCH model in Eq. (3.5) can be obtained recursively as those 
of an AR model. Consider an ARCH(m) model. At the forecast origin h, the 
1-step-ahead forecast of is 

0^(1) =(*() + axa2h +-b ama2h+1_m. 

The 2-step-ahead forecast is 

°/f(2) = «o + <W(1) + a2 a2h + • • • + amal+2_m, 

and the f-step-ahead forecast for o^+l is 

ol{t) = a0 + J2aiah(£ ~ 0, (3.11) 

i=1 

where cr2(l - i) = aif l - i < 0. 

3.4.4 Some Examples 

In this section, we illustrate ARCH modeling by considering two examples. 

Example 3.1. We first apply the modeling procedure to build a simple ARCH 
model for the monthly log returns of Intel stock. The sample ACF and PACF 
of the squared returns in Figure 3.2 clearly show the existence of conditional 
heteroscedasticity. This is confirmed by the ARCH test shown in Section 3.3.1, 
and we proceed to identify the order of an ARCH model. The sample PACF in 
Figure 3.2(d) indicates that an ARCH(3) model might be appropriate. Consequently, 
we specify the model 

rt — T ati at = °t^ti = OtQ + Ol\a2_x + Ot2®^-2 + 

for the monthly log returns of Intel stock. Assuming that e, are iid standard normal, 
we obtain the fitted model 

rt = 0.0122 + at, o2 = 0.0106 + 0.2131a,2., + 0.0770ar2_2 + 0.0599a2_3, 

where the standard errors of the parameters are 0.0057, 0.0010, 0.0757, 0.0480, 
and 0.0688, respectively; see the output below. While the estimates meet the gen¬ 
eral requirement of an ARCH(3) model, the estimates of a2 and a3 appear to be 
statistically nonsignificant at the 5% level. Therefore, the model can be simplified. 

124 

CONDITIONAL HETEROSCEDASTIC MODELS 

S-Plus Demonstration 
The following output has been edited and % marks explanation: 

> module(finmetrics) 
> da=read.table("m-intc7308.txt",header=T) 

> intc=log(da[,2]+1) 
> arch3.fit=garch(intc~l, ~garch(3,0)) 
> summary(arch3.fit) 
garch (formula .mean = into ~ 1, formula.var = ~ garch(3, 0)) 

Mean Equation: structure (.Data = into ~ 1, class - for¬ 

mula" ) 
Conditional Variance Equation: structure (. Data=~garch (3,0),.. ) 

Conditional Distribution: gaussian 

Estimated Coefficients: 

Value Std. Error t value Pr(>|t|) 
C 0.01216 0.0056986 2.1341 0.033402 
A 0.01058 0.0009643 10.9739 0.000000 
ARCH(1) 0.21307 0.0756708 2.8157 0.005093 
ARCH(2) 0.07698 0.0480170 1.6032 0.109638 
ARCH(3) 0.05988 0.0688081 0.8703 0.384628 

> archl=garch(intc~l, ~garch(1,0)) 
> summary(archl) 
garch (formula, mean = intc ~ 1, formula.var = ~ garch(l, 0)) 

Conditional Distribution: gaussian 

Estimated Coefficients: 

Value Std.Error t value Pr(>|t|) 
C 0.01261 0.0052624 2.397 1.695e-02 
A 0.01113 0.0009971 11.164 0.000e+00 
ARCH(1) 0.35602 0.0761267 4.677 3.912e-06 

AIC(3) = -570.0179, BIC(3) = -557.8126 

Ljung-Box test for standardized residuals: 

Statistic P-value ChiA2~d.f. 

14.26 0.2844 12 

Ljung-Box test for squared standardized residuals: 

Statistic P-value ChiA2-d.f. 

32.11 0.001329 12 

> stres=archl$residuals/archl$sigma.t %standardized residuals 

THE ARCH MODEL 

125 

> autocorTest(stres,lag=10) 

Test for Autocorrelation: Ljung-Box 

Null Hypothesis: no autocorrelation 

Test Statistics: 

Test Stat 12.6386, p.value 0.2446 

Dist. under Null: chi-square with 10 degrees of freedom 

> archTest(stres,lag=10) 

Test for ARCH Effects: LM Test 

Null Hypothesis: no ARCH effects 

Test Statistics: 

Test Stat 14.7481, p.value 0.1415 

Dist. under Null: chi-square with 10 degrees of freedom 

> archl$asymp.sd %Obtain unconditional standard error 

[1] 0.1314698 

> plot(archl) % Obtain various plots, including the 

% fitted volatility series. 

Dropping the two nonsignificant parameters, we obtain the model 

rt — 0.0126 + at, a2 = 0.0111 +0.3560at2_1, (3.12) 

where the standard errors of the parameters are 0.0053, 0.0010, and 0.0761, respec¬ 
tively. All the estimates are highly significant. Figure 3.5 shows the standardized 
residuals {at} and the sample ACF of some functions of {at}. The Ljung-Box 
statistics of standardized residuals give (2(10) = 12.64 with a p value of 0.24 
and those of {a2} give Q(10) = 14.75 with a p value of 0.14. See the output. 
Consequently, the ARCH(l) model in Eq. (3.12) is adequate for describing the 
conditional heteroscedasticity of the data at the 5% significance level. 

The ARCH(l) model in Eq. (3.12) has some interesting properties. First, the 
expected monthly log return for Intel stock is about 1.26%, which is remarkable, 
especially since the data span includes the period after the Internet bubble. Sec¬ 
ond, a\ = 0.3562 < | so that the unconditional fourth moment of the monthly 
log return of Intel stock exists. Third, the unconditional standard deviation of rt 
is V0-0111/(1 - 0.356) ^0.1315. Finally, the ARCH(l) model can be used to 
predict the monthly volatility of Intel stock returns. 

t Innovation 

For comparison, we also fit an ARCH(l) model with Student-? innovations to the 
series. The resulting model is 

rt = 0.0169 + at. 

a2 = 0.0120 + 0.2845aj_v 

(3.13) 

126 

CONDITIONAL HETEROSCEDASTIC MODELS 

(a) 

(b) 

(c) 

(d) 

Figure 3.5 Model checking statistics of Gaussian ARCH(l) model in Eq. (3.12) for monthly log 

returns of Intel stock from January 1973 to December 2008: Parts (a), (b), and (c) show sample ACF 
of standardized residuals, their squared series, and absolute series, respectively; part (d) is time plot of 

standardized residuals. 

where the standard errors of the parameters are 0.0053, 0.0017, and 0.1120, respec¬ 
tively. The estimated degrees of freedom is 6.01 with standard error 1.50. All the 
estimates are significant at the 5% level. The unconditional standard deviation 
of at is VO-0120/d - 0.2845) « 0.1295, which is close to that obtained under 
normality. The Ljung-Box statistics of the standardized residuals give <2(12) = 
14.88 with a p value of 0.25, confirming that the mean equation is adequate. 
However, the Ljung-Box statistics for the squared standardized residuals show 
2(12) = 35.42 with a p value of 0.0004. The volatility equation is inadequate 
at the 1% level. Further analysis shows that <2(10) = 15.90 with a p value 
of 0.10 for the squared standardized residuals. The inadequancy of the volatility 
equation is due to a large lag-12 ACF (p12 = 0.188) of the squared standardized 

residuals. 

Comparing models (3.12) and (3.13), we see that (a) using a heavy-tailed distri¬ 
bution for €, reduces the ARCH coefficient, and (b) the difference between the two 
models is small for this particular instance. Finally, a more appropriate conditional 
heteroscedastic model for the monthly log returns of Intel stock is a GARCH(1,1) 

model, which is discussed in the next section. 

THE ARCH MODEL 

127 

S-Plus Demonstration 

Note the following output with t innovations: 

> archlt=garch(intc~l, ~garch(1,0),cond.dist="t") 
> summary(archit) 

Call: 

garch (formula .mean=intc~l, formula. var=~garch (1,0) , 

cond.dist="t") 

Mean Equation: structure(.Data = intc ~ 1, class = "formula") 

Cond. Variance Equation:structure(.Data=~ garch(1,0), ...) 

Cond. Distribution: t 

with estimated parameter 6.012769 and standard error 1.502179 

Estimated Coefficients: 

Value Std.Error t value Pr(>|t|) 

C 0.01688 0.005288 3.193 1.512e-03 

A 0.01195 0.001667 7.169 3.345e-12 

ARCH(1) 0.28445 0.111998 2.540 1.145e-02 

AIC(4) = -597.3379, BIC(4) = -581.0642 

Ljung-Box test for standardized residuals: 

Statistic P-value Chi/'2-d.f. 

14.88 0.2482 12 

Ljung-Box test for squared standardized residuals: 

Statistic P-value Chi/'2-d.f. 

35.42 0.0004014 12 

Remark. In S-Plus, the command garch allows for several conditional distri¬ 
butions. They are specified by cond.dist = ' 't' ' or ' 'ged' '. The default 
is Gaussian. The R output is given below. The estimates are close to those of 
S-Plus. □ 

R Demonstration 

The following output uses the fGarch package with command garchFit and % 
denotes explanation: 

> da=read.table("m-intc730 8.txt",header=T) 

> library(fGarch) % Load the package 

> intc=log(da[,2]+1) 

> ml=garchFit (intc~garch (1,0) , data=intc, trace=F) 

> summary(ml) % Obtain results 

128 

CONDITIONAL HETEROSCEDASTIC MODELS 

Title: 

GARCH Modelling 

Call: 
garchFit (f ormula=intc~garch (1,0) , data=intc, trace=F) 

Mean and Variance Equation: data ~ garch(l, 0) [data = 

Conditional Distribution: norm 

Coefficient(s): 

mu omega alphal 

0.012637 0.011195 0.379492 

Std. Errors: 

based on Hessian 

Error Analysis: 

Estimate  Std. Error 

t value  Pr (> 111) 

mu 

0.012637 

0.005428 

omega 

0.011195 

alphal  0.379492 

0.001239 

0.115534 

2.328  0.01990 * 

9.034 

< 2e-16 *** 

3.285  0.00102 ** 

Log Likelihood: 

288.0589 normalized: 0.6668031 

Standardised Residuals Tests: %Model checking 

Jarque-Bera Test 

Shapiro-Wilk Test 

Ljung-Box Test 

Ljung-Box Test 

Ljung-Box Test 

Ljung-Box Test 

Ljung-Box Test 

Ljung-Box Test 

LM Arch Test 

Statistic  p-Value 

ChiA2  137.919 

0 

W 

Q (10) 

Q (15) 

Q (20) 

Q (10) 

Q (15) 

Q (20) 

TR-^2 

0.9679255  4.025172e-0: 

12.54002  0.2505382 

21.33508  0.1264607 

23.19679  0.2792354 

16.0159 

0.09917815 

36.08022  0.001721296 

37.43683  0.01036728 

26.57744  0.008884587 

R 

R 

R 

R 

R 

<

C
N

RA2 

R^2 

R 

Information Criterion Statistics: 

AIC BIC SIC HQIC 

-1.319717 -1.291464 -1.319813 -1.308563 

> predict(ml,5) % Obtain 

1 to 5-step predictions 

meanForecast meanError 

tandardDeviation 

1 0.01263656 0.1278609 

2 0.01263656 0.1278609 

3 0.01263656 0.1278609 

4 0.01263656 0.1278609 

5 0.01263656 0.1278609 

0.1098306 

0.1255897 

0.1310751 

0.1330976 

0.1338571 

 
 
THE ARCH MODEL 

129 

% The next command fits a GARCH(1,1) model 

> m2=garchFit (intc~garch (1,1) , data=intc, trace=F) 

> summary(m2) % output edited. 

Coefficient(s): 

mu omega alphal betal 

0.01073352 0.00095445 0.08741989 0.85118414 

Error Analysis: 

Estimate  Std. Error 

t value Pr( >|t|) 

mu 0.0107335 

omega 0.0009544 

alphal 0.0874199 

betal 0.8511841 

0.0055289 

0.0003989 

0.0269810 

0.0393702 

1.941 0.  05222 . 

2.392 0.  01674 * 

3.240 0.  00120 ** 

21.620 <  2e-16 *** 

Standardised Residuals Tests: 

Statistic  p-Value 

Jarque-Bera Test 

R ChiA2  165.5740 

0 

Shapiro-Wilk Test  R W 
Ljung-Box Test 

R Q(10) 

0.9712087  1.62 6824e-07 
8.267633 

0.6027128 

Ljung-Box Test 

Ljung-Box Test 

Ljung-Box Test 

Ljung-Box Test 

Ljung-Box Test 

LM Arch Test 

R Q(15) 

14.42612  0.4934871 

R Q (2 0 ) 

15.13331  0.7687297 

RA2 Q(10) 

RA2 Q(15) 

0.9891848  0.9998363 

11.36596  0.7262473 

RA2 Q(2 0) 

12.68143 

0.8906302 

R TRA2 

10.70199  0.5546164 

% The next command fits an ARCH(l) model with Student-t dist. 

> m3=garchFit (intc~garch(l, 0) , data=intc, trace=F, 

cond.dist='std') 

> summary(m3) % Output shortened. 

Call: 

garchFit(formula=intc~garch(l, 0) , data=intc, cond.dist="std", 

trace = F) 

Mean and Variance Equation: data ~ garchfl, 0) [data = intc] 

Conditional Distribution: std % Student-t distribution 

Coefficient(s): 

mu omega alphal shape 

0.016731 0.011939 0.285320 6.015195 

Error Analysis: 

Estimate 

Std. Error 

t value Pr(>|t|) 

mu 0.016731 

omega 0.011939 

alphal 0.285320 

0.005302 

0.001603 

0.110607 

3.155 0.001603 ** 

7.449 9.4e-14 *** 

2.580 0.009892 ** 

shape 6.015195 

1.562620 

3.849 0.000118  * * * 

% Degrees of freedom 

130 

CONDITIONAL HETEROSCEDASTIC MODELS 

% The next command fits an ARCH(l) model with skew 

%Student-t dist. 
> m4=garchFit (intc~garch (1,0) , data=intc, cond. dist- sstd , 

trace=F) 

% Next, fit an ARMA(1,0)+GARCH(1,1) model with 

% Gaussian noises. 
> m5=garchFit (intc~arma (1,0) +garch (1,1) , data=intc, trace=F) 

R Demonstration 
The following output was generated with Ox and G@RCH4.2 package and % 

denotes explanation: 

> source("garchoxfit_R.txt") 

% In G@RCH package, an ARCH(l) model is specified as 

% GARCH(0,1). 

> ml=garchOxFit (formula ,mean=~arma (0,0) , 

formula.var=~garch(0,1) , series=intc) 

% ** SPECIFICATIONS ** 

Dependent variable : X 

Mean Equation : ARMA (0, 0) model. 

No regressor in the mean 

Variance Equation : GARCH (0, 1) model. 

No regressor in the variance 

The distribution is a Gauss distribution. 

Maximum Likelihood Estimation(Std.Errors based on 2nd deriv.) 

Cst(M) 

Cst(V) 

Coefficient Std.Error t-value t-prob 

0.012630 0.0054130 2.333 0.0201 

0.011129 0.0012355 9.007 0.0000 

ARCH(Alpha1) 

0.387223 0.11688 3.313 0.0010 

% ** TESTS ** 

Q-Statistics on Standardized Residuals 

Q(10)=12.4952 [0.2532785], Q(20)=23.1210 [0.2828934] 

HO: No serial correlation ==> Accept HO when prob. is High. 

Q-Statistics on Squared Standardized Residuals 

--> P-values adjusted by 1 degree(s) of freedom 

Q(10)=15.7849 [0.0715122], Q( 20)=37.0238 [0.0078807] 

ARCH 1-10 test: F(10,410)= 1.4423 [0.1592] 

% Apply Student-t distribution 

> m2=garchOxFit (formula ,mean=~arma (0,0) , 

formula.var=~garch (0,1) , 

series=intc,cond.dist="t") 

% ** SPECIFICATIONS 

•k k 

THE GARCH MODEL 

131 

Dependent variable : X 

Mean Equation : ARMA (0, 0) model. 

No regressor in the mean 

Variance Equation : GARCH (0, 1) model. 

No regressor in the variance 

The distribution is a Student distribution, with 6.02272 df. 

Maximum Likelihood Estimation(Std.Errors based on 2nd deriv. 

) 

Cst(M) 

Cst(V) 

ARCH(Alphal) 

Student(DF) 

** TESTS ** 

Coefficient  Std.Error 

t-value 

t-prob 

0.016702 

0.0052934 

0.011870  0.0015969 

0.292318 

6.022723 

0.11223 

1.5663 

3.155  0.0017 

7.433 

0.0000 

2.605  0.0095 

3.845  0.0001 

Q-Statistics on Standardized Residuals 

Q(10)=13.0837 [0.2190281], Q(20)=24.0724 [0.2392436] 

Q-Statistics on Squared Standardized Residuals 

> P-values adjusted by 1 degree(s) of freedom 

Q (10)=18.6982 [0.0278845], Q( 20)=41.7182 [0.0019343] 

Example 3.2. Consider the percentage changes of the exchange rate between 
mark and dollar in 10-minute intervals. The data are shown in Figure 3.3(a). As 
shown in Figure 3.4(a), the series has no serial correlations. However, the sample 
PACF of the squared series af shows some big spikes, especially at lags 1 and 3. 
There are some large PACF at higher lags, but the lower order lags tend to be more 
important. Following the procedure discussed in the previous section, we specify an 
ARCH(3) model for the series. Using the conditional Gaussian likelihood function, 
we obtain the fitted model rt = 0.0018 + atet and 

of = 0.22 x 1(T2 + 0.322a2_1 + 0.074a2_2 + 0.093af_3, 

where all the estimates in the volatility equation are statistically significant at the 5% 
significant level, and the standard errors of the parameters are 0.47 x 10-6, 0.017, 
0.016, and 0.014, respectively. Model checking, using the standardized residual at, 
indicates that the model is adequate. 

3.5 THE GARCH MODEL 

Although the ARCH model is simple, it often requires many parameters to ade¬ 
quately describe the volatility process of an asset return. For instance, consider the 
monthly excess returns of S&P 500 index of Example 3.3. An ARCH(9) model is 
needed for the volatility process. Some alternative model must be sought. Boller- 
slev (1986) proposes a useful extension known as the generalized ARCH (GARCH) 
model. For a log return series rt, let at = rt — fjbt be the innovation at time t. Then 

132 

CONDITIONAL HETEROSCEDASTIC MODELS 

at follows a GARCH(m, s) model if 

a, = at€t, a2 = cxo + ^ ^ Pja?-j> (3.14) 

m s 

1=1 7=1 

where again {cf} is a sequence of iid random variables with mean 0 and variance 

1.0, a0 > 0, on > 0, Pj > 0, and + Pi) < 1. Here it is understood that 
at = 0 for i > m and Pj = 0 for j >s. The latter constraint on at + P, implies 
that the unconditional variance of at is finite, whereas its conditional variance ot 
evolves over time. As before, is often assumed to follow a standard noimal 
or standardized Student-? distribution or generalized error distribution. Equation 
(3.14) reduces to a pure ARCH(m) model if s = 0. The a, and Pj are referred to 

as ARCH and GARCH parameters, respectively. 

To understand properties of GARCH models, it is informative to use the fol¬ 
lowing representation. Let ijt = a2 — cr~ so that o2 = aj — r)t. By plugging at_i = 
aj_. - r]t-i (i = 0, ..., s) into Eq. (3.14), we can rewrite the GARCH model as 

max(m,5) s 

aj =o;o+ ^ (ai + Pi)at-i + 4; ~ ^ Pj*!’-!' 

i=1 7=1 

(3.15) 

It is easy to check that {r\,} is a martingale difference series [i.e., = 0 and 
cov(?7r, rjt-j) = 0 for j > 1]. However, {r]t} in general is not an iid sequence. 
Equation (3.15) is an ARMA form for the squared series a}. Thus, a GARCH 
model can be regarded as an application of the ARMA idea to the squared series 
aj. Using the unconditional mean of an ARMA model, we have 

1 - L;=l W + Pi) 

provided that the denominator of the prior fraction is positive. 

The strengths and weaknesses of GARCH models can easily be seen by focusing 

on the simplest GARCH(1,1) model with 

a'l = «o + U\a^_\ + P\of_\, 0 < ai, P\ < 1, (oti + Pi) < 1. (3.16) 

First, a large ar2_j or af_x gives rise to a large o}. This means that a large a]_x tends 
to be followed by another large a2, generating, again, the well-known behavior 
of volatility clustering in financial time series. Second, it can be shown that if 

1 - 2a2 - («] + Pi)2 > 0, then 

E(af) = 3[l-(«i +Pi)2] 

[E(o2)]2 X-iai+Ptf-la2 

>3. 

THE GARCH MODEL 

133 

Consequently, similar to ARCH models, the tail distribution of a GARCH(1,1) 
process is heavier than that of a normal distribution. Third, the model provides a 
simple parametric function that can be used to describe the volatility evolution. 

Forecasts of a GARCH model can be obtained using methods similar to those 
of an ARM A model. Consider the GARCH(1,1) model in Eq. (3.16) and assume 
that the forecast origin is h. For 1-step-ahead forecast, we have 

ah+1 = “0 + «1 «/) + Pi <7%, 

where ah and al are known at the time index h. Therefore, the 1-step-ahead forecast 
is 

0-^(1) = a0 +«i al + P\o%. 

For multistep-ahead forecasts, we use aj = crfe? and rewrite the volatility equation 
in Eq. (3.16) as 

°V+1 = «0 + («1 + P\)<J? + 0L\o}{€] ~ 1). 

When t = h + 1, the equation becomes 

ah+2 — “o + («i + P\)ol+l + aicr^+1(e^+1 - 1). 

Since £(e|+1 - \ \Fh) = 0, the 2-step-ahead volatility forecast at the forecast origin 
h satisfies the equation 

ah( 2) — «o + (ofi + Pi)a^(l). 

In general, we have 

a^(l) =a0-F(ai + pi)aj;(l- 1), l> 1. (3.17) 

This result is exactly the same as that of an ARMA(1,1) model with AR polynomial 
1 - (oil + P\)B. By repeated substitutions in Eq. (3.17), we obtain that the t-step- 
ahead forecast can be written as 

«o[l ~ Oi + PlY *] 
1 o?t Pi 

+ («i + P\Y ]<y^( 1). 

Therefore, 

ah(0 T--—as l -* oo 

1 - on - Pi 

provided that ai + P\ < 1. Consequently, the multistep-ahead volatility forecasts of 
a GARCH(1,1) model converge to the unconditional variance of at as the forecast 
horizon increases to infinity provided that Var(ar) exists. 

134 

CONDITIONAL HETEROSCEDASTIC MODELS 

The literature on GARCH models is enormous; see Bollerslev, Chou, and Kroner 
(1992), Bollerslev, Engle, and Nelson (1994), and the references therein. The model 
encounters the same weaknesses as the ARCH model. For instance, it responds 
equally to positive and negative shocks. In addition, recent empirical studies of 
high-frequency financial time series indicate that the tail behavior of GARCH 
models remains too short even with standardized Student-! innovations. For further 

information about kurtosis of GARCH models, see Section 3.16. 

3.5.1 An Illustrative Example 

The modeling procedure of ARCH models can also be used to build a GARCH 
model. However, specifying the order of a GARCH model is not easy. Only 
lower order GARCH models are used in most applications, say, GARCH(1,1), 
GARCH(2,1), and GARCH(1,2) models. The conditional maximum-likelihood 
method continues to apply provided that the starting values of the volatility {crr } 
are assumed to be known. Consider, for instance, a GARCH(1,1) model. If ctj is 
treated as fixed, then of can be computed recursively for a GARCH(1,1) model. 
In some applications, the sample variance of at serves as a good starting value 
of of. The fitted model can be checked by using the standardized residual at = 

atlot and its squared process. 

Example 3.3. In this example, we consider the monthly excess returns of 
S&P 500 index starting from 1926 for 792 observations. The series is shown in 
Figure 3.6. Denote the excess return series by rt. Figure 3.7 shows the sample ACF 

o 

C\J 
o 

CM 
o 
l 

-\—-1——— -1-- i ”> 
0 200 400 600 800 

Time index 

Figure 3.6 Time series plot of monthly excess returns of S&P 500 index from 1926 to 1991. 

THE GARCH MODEL 

135 

Figure 3.7 (a) Sample ACF of monthly excess returns of S&P 500 index and (b) sample PACF of 
squared monthly excess returns. Sample period is from 1926 to 1991. 

of rt and the sample PACF of r2. The rt series has some serial correlations at lags 
1 and 3, but the key feature is that the PACF of r2 shows strong linear dependence. 
If an MA(3) model is entertained, we obtain 

rt = 0.0062 + at + 0.0944a,_i - 0.1407a?_3, oa = 0.0576 

for the series, where all of the coefficients are significant at the 5% level. However, 
for simplicity, we use instead an AR(3) model 

rt = 01^7—1 + 4>2r t—2 + 03ri-3 + A) + O-t- 

The fitted AR(3) model, under the normality assumption, is 

rt = 0.088rr_! - 0.023r;_2 - 0.123rr_3 + 0.0066 + at, o2a = 0.00333. (3.18) 

For the GARCH effects, we use the GARCH(1,1) model 

at = <Jtet, a2 = ocq + f}\o2_i + a\af_^. 

A joint estimation of the AR(3)-GARCH(1,1) model gives 

136 

CONDITIONAL HETEROSCEDASTIC MODELS 

rt = 0.0078 + 0.032r,_i - 0.029rf_2 - 0.008rf_3 + at, 

a2 = 0.000084 + 0.1213fl(2_, + 0.8523or2_1. 

From the volatility equation, the implied unconditional variance of a, is 

0.000084 

1 -0.8523 -0.1213 

0.00317, 

which is close to that of Eq. (3.18). However, t ratios of the parameters in the 
mean equation suggest that all three AR coefficients are insignificant at the 5% 
level. Therefore, we refine the model by dropping all AR parameters. The refined 

model is 

rt — 0.0076 + at, a2 = 0.000086 + 0.1216a2_! + 0.85 llcr (3.19) 

The standard error of the constant in the mean equation is 0.0015, whereas those 
of the parameters in the volatility equation are 0.000024, 0.0197, and 0.0190, 
respectively. The unconditional variance of a, is 0.000086/(1 - 0.8511 - 0.1216) 
= 0.00314. This is a simple stationary GARCH(1,1) model. Figure 3.8 shows the 
estimated volatility process, cr,, and the standardized shocks at —atlox for the 

(a) 

1940 I960 1980 

Year 

(b) 

Figure 3.8 (a) Time series plot of estimated volatility (a,) for monthly excess returns of S&P 500 
index and (b) standardized shocks of monthly excess returns of S&P 500 index. Both plots are based 

on GARCH(1,1) model in Eq. (3.19). 

THE GARCH MODEL 

137 

Lag 

(a) 

C\J 
o 

d 

d 

CM 
o ___________ 

5 10 15 20 

Lag 

(b) 

Figure 3.9 Model checking of GARCH(1,1) model in Eq. (3.19) for monthly excess returns of S&P 
500 index: (a) Sample ACF of standardized residuals and (b) sample ACF of the squared standardized 
residuals. 

GARCH(1,1) model in Eq. (3.19). The at series looks like a white noise pro¬ 
cess. Figure 3.9 provides the sample ACF of the standardized residuals at and the 
squared process a?. These ACFs fail to suggest any significant serial correlations 
or conditional heteroscedasticity in the standardized residual series. More specifi¬ 
cally, we have £(12) = 11.99(0.45) and £(24) = 28.52(0.24) for at, and £(12) 
= 13.11(0.36) and £(24) = 26.45(0.33) for a?, where the number in parenthe¬ 
ses is the p value of the test statistic. Thus, the model appears to be adequate in 
describing the linear dependence in the return and volatility series. Note that the 
fitted model shows aq +/?i = 0.9772, which is close to 1. This phenomenon is 
commonly observed in practice and it leads to imposing the constraint aq + = 1 
in a GARCH(1,1) model, resulting in an integrated GARCH (or IGARCH) model; 
see Section 3.6. 

Finally, to forecast the volatility of monthly excess returns of the S&P 500 index, 
we can use the volatility equation in Eq. (3.19). For instance, at the forecast origin 
h, we have a*+1 = 0.000086 + 0.1216aJ + 0.851 lerj;. The 1-step-ahead forecast is 
then 

crj;( 1) = 0.000086 + 0.1216a\ + 0.8511 a£, 

138 

CONDITIONAL HETEROSCEDASTIC MODELS 

TABLE 3.1 Volatility Forecasts for Monthly Excess Returns of S&P 500 Index" 

Horizon 

Return 

Volatility 

1 

0.0076 

0.0536 

2 

0.0076 

0.0537 

3 

0.0076 

0.0537 

4 

0.0076 

0.0538 

5 

0.0076 

0.0538 

oo 

0.0076 

0.0560 

"The forecast origin is h — 792, which corresponds to December 1991. Here volatility denotes condi¬ 

tional standard deviation. 

where ah is the residual of the mean equation at time h and oh is obtained from the 
volatility equation. The starting value <7q is fixed at either zero or the unconditional 
variance of <3;. For multistep-ahead forecasts, we use the recursive formula in Eq. 
(3.17). Table 3.1 shows some mean and volatility forecasts for the monthly excess 
return of the S&P 500 index with forecast origin h = 792 based on the GARCH(1,1) 

model in Eq. (3.19). 

Some S-Plus Commands Used in Example 3.3. 

> f it=garch (sp~ar (3 ) ,~garch(l, 1) ) 

> summary(fit) 

> f it=garch (sp~l, ~garch (1,1) ) 

> summary(fit) 

> names(fit) 

[1] "residuals" "sigma.t" "df.residual" "coef" "model" 

[6] "cond.dist" "likelihood" "opt.index" "cov" 

"prediction" 

[11] "call" "asymp.sd" "series" 

> 
> stdresi=fit$residuals/fit$sigma.t 

> autocorTest(stdresi,lag=24) 

> autocorTest(stdresiA2,lag=24) 

> predict(fit, 5) 

Note that in the prior commands the volatility series a, is stored in f it$sigma. t 
and the residual series of the returns in fit$residuals. 

t Innovation 
Assuming that et follows a standardized Student-! distribution with 5 degrees of 
freedom, we reestimate the GARCH(1,1) model and obtain 

r, = 0.0085 + a,, o2 = 0.00012 + 0.1121a2_! + 0.8432a2_,, (3.20) 

where the standard errors of the parameters are 0.0015, 0.51 xlO-4, 0.0296, and 
0.0371, respectively. This model is essentially an IGARCH(1,1) model as a\ + 

0.95, which is close to 1. The Ljung-Box statistics of the standardized residuals 
give 2(10) = 11.38 with a p value of 0.33 and those of the {a2} series give 2(10) 
= 10.48 with a p value of 0.40. Thus, the fitted GARCH(1,1) model with Student-r 

distribution is adequate. 

THE GARCH MODEL 

S-Plus Commands Used 

139 

> fitl = garch(sp~l, ~garch(1,1),cond.dist='t',cond.par=5, 
+ cond.est=F) 
> summary(fitl) 

> stresi=fitl$residuals/fitl$sigma.t 
> autocorTest(stresi,lag=10) 
> autocorTest(stresi^2,lag=10) 

Estimation of Degrees of Freedom 

If we further extend the GARCH(1,1) model by estimating the degrees of freedom 

of the Student-/ distribution used, we obtain the model 

r, = 0.0085 + at, of = 0.00012 + 0.1 \2\af_x + 0.8432ct,2_1, (3.21) 

where the estimated degrees of freedom is 7.02. Standard errors of the estimates 

in Eq. (3.21) are close to those in Eq. (3.20). The standard error of the estimated 

degrees of freedom is 1.78. Consequently, we cannot reject the hypothesis of using a 

standardized Student-/ distribution with 5 degrees of freedom at the 5% significance 

level. 

S-Plus Commands Used 

> fit2 = garch(sp~l, ~garch(1,1),cond.dist='t') 
> summary(fit2) 

R Commands Used in Example 3.3 

> 1ibrary(fGarch) 
> sp5=scan(file='sp500.txt') % Load data 
> plot(sp5,type='1') 
% Below, fit an AR(3)+GARCH(1,1) model. 
> ml=garchFit (~arma (3,0) +garch (1,1) , data=sp5 , trace=F) 
> summary(ml) 
% Below, fit a GARCH(1,1) model with Student-t distribution. 
> m2=garchFit (~garch (1,1) , data=sp5 , trace=F, cond. dist =" std") 
> summary(m2) 
% Obtain standardized residuals. 
> stresi=residuals (m2 , standardized) 
> plot(stresi,type='1') 
> Box.test(stresi,10,type='Ljung') 
> predict(m2,5) 

3.5.2 Forecasting Evaluation 

Since the volatility of an asset return is not directly observable, comparing the 

forecasting performance of different volatility models is a challenge to data analysts. 

In the literature, some researchers use out-of-sample forecasts and compare the 

140 

CONDITIONAL HETEROSCEDASTIC MODELS 

volatility forecasts cr^(£) with the shock in the forecasting sample to assess 
the forecasting performance of a volatility model. This approach often finds a 
low correlation coefficient between a^+£ and a^(£), that is, low R2. However, 
such a finding is not surprising because a2h+l alone is not an adequate measure 
of the volatility at time index h +1. Consider the 1-step-ahead forecasts. From a 
statistical point of view, E(a%+l\Fh) = <t^+1 so that a|+1 is a consistent estimate 
of ol+x. But it is not an accurate estimate of because a single observation of a 
random variable with a known mean value cannot provide an accurate estimate of 
its variance. Consequently, such an approach to evaluate forecasting performance of 
volatility models is strictly speaking not proper. For more information concerning 
forecasting evaluation of GARCH models, readers are referred to Andersen and 

Bollerslev (1998). 

3.5.3 A Two-Pass Estimation Method 

Based on Eq. (3.15), a two-pass estimation method can be used to estimate GARCH 
models. First, ignoring any ARCH effects, one estimates the mean equation of a 
return series using the methods discussed in Chapter 2 (e.g., maximum-likelihood 
method). Denote the residual series by a,. Second, treating {aj} as an observed 
time series, one applies the maximum-likelihood method to estimate parameters 
of Eq. (3.15). Denote the AR and MA coefficient estimates by </>, and 0,-. The 
GARCH estimates are obtained as /3,- = 0,- and a, = </>,- — 0/. Obviously, such esti¬ 
mates are approximations to the true parameters and their statistical properties have 
not been rigorously investigated. However, limited experience shows that this sim¬ 
ple approach often provides good approximations, especially when the sample size 
is moderate or large. For instance, consider the monthly excess return series of the 
S&P 500 index of Example 3.3. Using the conditional MLE method in SCA, we 

obtain the model 

r, = 0.0061 + at, a2 = 0.00014 + 0.9583a,2_! + m ~ 0.8456rj,-U 

where all estimates are significantly different from zero at the 5% level. From 
the estimates, we have /3i = 0.8456 and a.\ = 0.9583 — 0.8456 = 0.1127. These 
approximate estimates are very close to those in Eq. (3.19) or (3.21). Further¬ 
more, the fitted volatility series of the two-pass method is very close to that of 

Figure 3.8(a). 

3.6 THE INTEGRATED GARCH MODEL 

If the AR polynomial of the GARCH representation in Eq. (3.15) has a unit root, 
then we have an IGARCH model. Thus, IGARCH models are unit-root GARCH 
models. Similar to ARIMA models, a key feature of IGARCH models is that the 
impact of past squared shocks = a2_t — oj_i for i > 0 on a2 is persistent. 

THE INTEGRATED GARCH MODEL 

141 

An IGARCH(1,1) model can be written as 

at = crt€t, of = a0 + facr?_x + (1 - fa 

where {e,} is defined as before and 1 > fa > 0. For the monthly excess returns of 
the S&P 500 index, an estimated IGARCH(1,1) model is 

rt = 0.0067 + at, at = crtet, 

of = 0.000119 + 0X0590^ + 0.1941a,^ 

where the standard errors of the estimates in the volatility equation are 0.0017, 
0.000013, and 0.0144, respectively. The parameter estimates are close to those of 
the GARCH(1,1) model shown before, but there is a major difference between 
the two models. The unconditional variance of at, hence that of rt, is not defined 
under the above IGARCH(1,1) model. This seems hard to justify for an excess 
return series. From a theoretical point of view, the IGARCH phenomenon might 
be caused by occasional level shifts in volatility. The actual cause of persistence 
in volatility deserves a careful investigation. 

When a\+ fa = 1, repeated substitutions in Eq. (3.17) give 

of (£) = of (i) + (£ - l)a0, l> 1, (3.22) 

where h is the forecast origin. Consequently, the effect of of (1) on future volatilities 
is also persistent, and the volatility forecasts form a straight line with slope or0. 
Nelson (1990) studies some probability properties of the volatility process of under 
an IGARCH model. The process a2 is a martingale for which some nice results 
are available in the literature. Under certain conditions, the volatility process is 
strictly stationary but not weakly stationary because it does not have the first two 
moments. 

The case of o?o = 0 is of particular interest in studying the IGARCH(1,1) model. 
In this case, the volatility forecasts are simply of (1) for all forecast horizons; 
see Eq. (3.22). This special IGARCH(l.l) model is the volatility model used in 
RiskMetrics, which is an approach for calculating value at risk; see Chapter 7. 
The model is also an exponential smoothing model for the {a2} series. To see this, 
rewrite the model as 

°f = (1 ~ fa)af-i + A°f_i 

= (1 - fa)a2_j + fa[{\ - faaf_2 + faof_2] 

= (1 - fa)a2_j + (1 - fa)faaf_2 + /32of_2. 

By repeated substitutions, we have 

°f — (1 — fa) (.af—\ + faa?-2 + &la?-3 + •••)> 

142 

CONDITIONAL HETEROSCEDASTIC MODELS 

which is the well-known exponential smoothing formation with being the dis¬ 
counting factor. Exponential smoothing methods can thus be used to estimate such 

an IGARCH(1,1) model. 

3.7 THE GARCH-M MODEL 

In finance, the return of a security may depend on its volatility. To model such 
a phenomenon, one may consider the GARCH-M model, where M stands for 
GARCH in the mean. A simple GARCH(1,1)-M model can be written as 

rt = \i + ca2 + at, at=atet, 

a2 = oiq + Qqa2_i + (3.23) 

where ji and c are constants. The parameter c is called the risk premium parameter. 
A positive c indicates that the return is positively related to its volatility. Other 
specifications of risk premium have also been used in the literature, including 

rt = ji + ccr, + at and rt = /x + c In(cr2) + at. 

The formulation of the GARCH-M model in Eq. (3.23) implies that there are 
serial correlations in the return series rt. These serial correlations are introduced 
by those in the volatility process {a}}. The existence of risk premium is, therefore, 
another reason that some historical stock returns have serial correlations. 

For illustration, we consider a GARCH(1,1)-M model with Gaussian innova¬ 
tions for the monthly excess returns of the S&P 500 index from January 1926 to 

December 1991. The fitted model is 

rt = 0.0055 + 1.09a2 + at, a2 = 8.76 x 10~5 + 0.123a2_! + 0.849a2.!, 

where the standard errors for the two parameters in the mean equation are 0.0023 
and 0.818, respectively, and those for the parameters in the volatility equation are 
2.51 x-5, 0.0205, and 0.0196, respectively. The estimated risk premium for the 
index return is positive but is not statistically significant at the 5% level. Here the 
result is obtained using S-Plus. Other forms of GARCH-M specification in S-Plus 
are given in Table 3.2. The idea of risk premium applies to other GARCH models. 

TABLE 3.2 GARCH-M Models Allowed in S-Plus: 
Mean Equation Is r, = ii + eg (a,) + at 

g(vt) 

a, 
ln(a,2) 

Command 

var.in.mean 
sd.in.mean 
logvar.in.mean 

THE EXPONENTIAL GARCH MODEL 

143 

S-Plus Demonstration 

> sp.fit = garch (sp~l+var. in .mean, ~garch (1,1) ) 

> summary(sp.fit) 

3.8 THE EXPONENTIAL GARCH MODEL 

To overcome some weaknesses of the GARCH model in handling financial time 
series, Nelson (1991) proposes the exponential GARCH (EGARCH) model. In 
particular, to allow for asymmetric effects between positive and negative asset 
returns, he considered the weighted innovation 

g(et) — 0et + ktkrl — £(kf|)L (3-24) 

where 9 and y are real constants. Both et and |ef| - E(\e,\) are zero-mean iid 
sequences with continuous distributions. Therefore, Efgk?)] = 0. The asymmetry 
of g(et) can easily be seen by rewriting it as 

g(e) = \ -y£(M) ifG>0, 
(9 - y)et - yE(\et\) if et < 0. 

Remark. For the standard Gaussian random variable et, E(\et\) = ^/2/n. For 

the standardized Student-t distribution in Eq. (3.7), we have 

E{\e<\) = 

2V^2T[(u + l)/2] 

(v - l)T(u/2)V^ 

□ 

An EGARCH(m, s) model can be written as 

a, = otet. 

ln(a?2) = ao + 

1 + B + • • • + Bs 1 

1 -ai B-amBm 

g(€t-1), 

(3.25) 

where ao is a constant, B is the back-shift (or lag) operator such that Bg(€t) = 

and 1 + /IjB +-b ^S-\BS~{ and 1 — a\B — • • • — amBm are polyno¬ 
mials with zeros outside the unit circle and have no common factors. By outside 
the unit circle we mean that absolute values of the zeros are greater than 1. Again, 
Eq. (3.25) uses the usual ARM A parameterization to describe the evolution of the 
conditional variance of at. Based on this representation, some properties of the 
EGARCH model can be obtained in a similar manner as those of the GARCH 
model. For instance, the unconditional mean of In (a2) is «o- However, the model 
differs from the GARCH model in several ways. First, it uses logged conditional 
variance to relax the positiveness constraint of model coefficients. Second, the use 
of g(et) enables the model to respond asymmetrically to positive and negative 

144 

CONDITIONAL HETEROSCEDASTIC MODELS 

lagged values of at. Some additional properties of the EGARCH model can be 

found in Nelson (1991). 

To better understand the EGARCH model, let us consider the simple model with 

order (1,1): 

at — atet, (1 -aB) In (of) = (1 - a)a0 + g(e,_i), (3.26) 

where the €t are iid standard normal and the subscript of or is omitted. In this 

case, .E(|e,|) = *J2/n and the model for ln(cr?2) becomes 

(1 — aB) ln(cr2) = 

a* + (y T- 9)€t—i if €t—i > 0, 
a* + (y - 0)(-ef-i) if €t-i < 0, 

(3.27) 

where a* — (1 - a)a0 - *j2pKy. This is a nonlinear function similar to that of the 
threshold autoregressive (TAR) model of Tong (1978, 1990). It suffices to say that 
for this simple EGARCH model the conditional variance evolves in a nonlinear 
manner depending on the sign of at-\. Specifically, we have 

of = ct2“, exp (a*) 

exp 

dt— 1 
(y + 0)— 

Ot-\\ 

laf-l 
exp  (y -oy-- 
L <rt-i  _ 

" 

if at-1 > 0, 

if at-1 < 0. 

The coefficients (y + 9) and (y — 9) show the asymmetry in response to posi¬ 
tive and negative at-\. The model is, therefore, nonlinear if 9 ^ 0. Since nega¬ 
tive shocks tend to have larger impacts, we expect 9 to be negative. For higher 
order EGARCH models, the nonlinearity becomes much more complicated. Cao 
and Tsay (1992) use nonlinear models, including EGARCH models, to obtain 
multistep-ahead volatility forecasts. We discuss nonlinearity in financial time series 
in Chapter 4. 

3.8.1 Alternative Model Form 

An alternative form for the EGARCH(m, s) model is 

ii|. m 

hi (of) = <*0 + ^ a,- -Y,a'~l + T> ln(of_j). (3.28) 

• i at-i . , 
*=1 J=1 

Here a positive at-i contributes a,(l + y,)|ef_,'| to the log volatility, whereas a 
negative gives or,-(1 — y()|ef_i|, where et-i = at-i/ot-i. The y(- parameter thus 
signifies the leverage effect of a,~i. Again, we expect y,- to be negative in real 
applications. This is the model form used in S-Plus. 

THE EXPONENTIAL GARCH MODEL 

3.8.2 Illustrative Example 

145 

Nelson (1991) applies an EGARCH model to the daily excess returns of the value- 
weighted market index from the Center for Research in Security Prices from July 
1962 to December 1987. The excess returns are obtained by removing monthly 
Treasury bill returns from the value-weighted index returns, assuming that the 
Treasury bill return was constant for each calendar day within a given month. 
There are 6408 observations. Denote the excess return by rt. The model used is as 
follows: 

r, =0o + 0iA_i +ca? + at, (3.29) 

ln(ar2) = ao + ln(l + wNt) +  -—^-—g(€t-\), 

1 — a\B — 012B- 

where af is the conditional variance of a, given Ft~\, Nt is the number of nontrad¬ 
ing days between trading days t — 1 and t, ao and w are real parameters, g(et) is 
defined in Eq. (3.24), and et follows a generalized error distribution in Eq. (3.10). 
Similar to a GARCH-M model, the parameter c in Eq. (3.29) is the risk premium 
parameter. Table 3.3 gives the parameter estimates and their standard errors of the 
model. The mean equation of model (3.29) has two features that are of interest. 
First, it uses an AR(1) model to take care of possible serial correlation in the excess 
returns. Second, it uses the volatility aJ as a regressor to account for risk premium. 
The estimated risk premium is negative, but statistically insignificant. 

3.8.3 Second Example 

As another illustration, we consider the monthly log returns of IBM stock from 
January 1926 to December 1997 for 864 observations. An AR(1)-EGARCH(1,1) 
model is entertained and the fitted model is 

rt = 0.0105 + 0.092rt_i + at. 

at = crtet, 

(3.30) 

In (of) = -5.496 + 

g(€t-1) 

1 — 0.8565 ’ 

g(et-i) = —0.0795e,_! + 0.2647 (|e,_i | - , 

(3.31) 

TABLE 3.3 Estimated AR(1)-EGARCH(2,2) Model for Daily Excess Returns of 
Value-Weighted CRSP Market Index: July 1962-December 1987 

Parameter 

Estimate 

Error 

Parameter 

Estimate 

Error 

a0 

-10.06 

0.346 

9 

-0.118 

0.009 

w 

0.183 

0.028 

00 

3.5-10-4 

9.9-10-5 

Y 
0.156 

0.013 

01 
0.205 

0.012 

oc\ 

1.929 

0.015 

c 

-3.361 

2.026 

a 2 

-0.929 

0.015 

V 

1.576 

0.032 

P 

-0.978 

0.006 

146 

CONDITIONAL HETEROSCEDASTIC MODELS 

where {et} is a sequence of independent standard Gaussian random variates. All 
parameter estimates are statistically significant at the 5% level. For model check¬ 
ing, the Ljung-Box statistics give <2(10) = 6.31(0.71) and Q(20) = 21.4(0.32) 
for the standardized residual process a, — at/crt and <2(10) = 4.13(0.90) and 
Q(20) = 15.93(0.66) for the squared process a?, where again the number in 
parentheses denotes p value. Therefore, there is no serial correlation or condi¬ 
tional heteroscedasticity in the standardized residuals of the fitted model. The prior 

AR( 1)-EGARCH( 1,1) model is adequate. 

From the estimated volatility equation in (3.31) and using *J2/n ~ 0.7979, we 

obtain the volatility equation as 

In (a,2) = -1.001 + 0.856 ln^) + 

0.1852ef_i ife,_i>0, 
-0.3442^_i if et-i < 0. 

Taking antilog transformation, we have 

2_ 2x0.856 —1.001 v 
at — CT,_! e X 

g0.t8526(_i if €f_{ > 0, 
g—0.3442ef_! ff < 0. 

This equation highlights the asymmetric responses in volatility to the past positive 
and negative shocks under an EGARCF1 model. For example, for a standardized 
shock with magnitude 2 (i.e., two standard deviations), we have 

= -2) _ exp[—0.3442 x (-2)] _ ^0318 = { 

= 2) exp(0.1852 x 2) 

Therefore, the impact of a negative shock of size 2 standard deviations is about 
37.4% higher than that of a positive shock of the same size. This example clearly 
demonstrates the asymmetric feature of EGARCH models. In general, the bigger 
the shock, the larger the difference in volatility impact. 

Finally, we extend the sample period to include the log returns from 1998 to 
2003 so that there are 936 observations and use S-Plus to fit an EGARCF1(1,1) 
model. The results are given below. 

S-Plus Demonstration 
The following output has been edited: 

> ibm.egarch=garch(ibmln~l, ~egarch(1,1) ,leverage=T, 

+ cond.dist='ged') 

> summary(ibm.egarch) 

Call: 

garch (formula .mean = ibmln ~ 1, formula, var = ~ egarch (1, 1) , 

leverage = T,cond.dist = "ged") 

Mean Equation: ibmln ~ 1 

THE EXPONENTIAL GARCH MODEL 

147 

Conditional Variance Equation: ~ egarch(l, 1) 

Conditional Distribution: ged 

with estimated parameter 1.5003 and standard error 0.09912 

Estimated Coefficients: 

Value 

Std.Error 

t value Pr(>|t|) 

C 

A 

0.01181 

0.002012 

5.870 3.033e-09 

-0.55680 

0.171602 

-3.245 6.088e-04 

ARCH(1) 

GARCH(1) 

0.22025 

0.052824 

4.169 1.669e-05 

0.92910 

0.026743 

34.742 0.000e+00 

LEV(1) 

-0.26400 

0.126096 

-2.094 1.828e-02 

Ljung-Box test for standardized residuals: 

Statistic P-value ChiA2-d.f. 

17.87 0.1195 12 

Ljung-Box test for squared standardized residuals: 

Statistic P-value Chi's2-d.f. 

6.723 0.8754 12 

The fitted GARCH(l.l) model is 

rt = 0.0118 + at, at = otet, 

lnO,2) = -0.557 + 0.220|a?~l1 ~°'264^~1 + 0.929 In (3.32) 

Of-1 

where et follows a GED distribution with parameter 1.5. This model is adequate 
and based on the Ljung-Box statistics of the standardized residual series and its 
squared process. As expected, the output shows that the estimated leverage effect 
is negative and is statistically significant at the 5% level with a t ratio of -2.094. 

3.8.4 Forecasting Using an EGARCH Model 

We use the EGARCH(1,1) model to illustrate multistep-ahead forecasts of 
EGARCH models, assuming that the model parameters are known and the 
innovations are standard Gaussian. For such a model, we have 

ln(o-2) = (1 - ai)of0 + a\ In(ct/Lj) + g(er_i), 

g(G-l) = @€t-1 + yQ^t-l I - y/l/lt). 

Taking exponentials, the model becomes 

o? = cr?a{ exp[(l - ai)or0] exp[^(ef_i)], 

g(€t-1) = 0€t-1 + y(\€t-l\ — t/2/tt). 

(3.33) 

148 

CONDITIONAL HETEROSCEDASTIC MODELS 

Let h be the forecast origin. For the 1-step-ahead forecast, we have 

o*+1 = of"1 exp[(l -ai)a0]exp[g(Cfc)], 

where all of the quantities on the right-hand side are known. Thus, the 1-step-ahead 
volatility forecast at the forecast origin h is simply (1) = of+1 given earlier. For 

the 2-step-ahead forecast, Eq. (3.33) gives 

°h+2 ~ ah+1 exPt(l ~ «i)«o] exp[g(e/,+i)]. 

Taking conditional expectation at time h, we have 

<5^(2) = (T^“’(l)exp[(l - ai)a0]Eh{exp[g(€h+i)]}, 

where Eh denotes a conditional expectation taken at the time origin h. The prior 

expectation can be obtained as follows: 

/OO 

exp[6>e + y(\e\ - y/2/jr)\f (e)de 

-OO 

= exP j* 

OO 1 
(Q+V)e 1 

de 

0 1 
(9-v)e 1 

+ / * 
/_ 

-00 

Vln 

-*2'2 de 

exp  (-y/Vxr) 

,(d+y)2/2 O(0 + y) + e(^/)2/2$(y -9) 

where /(e) and 4>(x) are the probability density function and CDF of the stan¬ 
dard normal distribution, respectively. Consequently, the 2-step-ahead volatility 
forecast is 

dl(2) = a?*1 (1) exp [(1 - ai)a0 - 

x {exp[(6> + y)2/2]4>(0 + y) + exp[(0 - y)2/2]<l>(y - 0)). 

Repeating the previous procedure, we obtain a recursive formula for a j-step-ahead 
forecast: 

old) = O'2"1 O' - l)exp(o>) 

x {exp[(0 + y)2/2]<F(6> + y) + exp[(0 - y)2/2]4>(y - 0)}, 

where co = (1 — aq)ao — yThe values of <3?(0 + y) and <t>(y — 9) can be 
obtained from most statistical packages. Alternatively, accurate approximations to 
these values can be obtained by using the method in Appendix B of Chapter 6. 

THE THRESHOLD GARCH MODEL 

149 

For illustration, consider the AR(1)-EGARCH(1,1) model of the previous 
section for the monthly log returns of IBM stock, ending December 1997. Using 
the fitted EGARCH(1,1) model, we can compute the volatility forecasts for the 
series. At the forecast origin t = 864, the forecasts are a264(l) = 6.05 x 10~3, 

<5-8264(2) = 5.82 x 10-3, a264(3) = 5.63 x 10~3, and ct264(10) = 4.94 x 10“3. 
These forecasts converge gradually to the sample variance 4.37 x 10“3 of the 
shock process at of Eq. (3.30). 

3.9 THE THRESHOLD GARCH MODEL 

Another volatility model commonly used to handle leverage effects is the threshold 
GARCH (or TGARCH) model; see Glosten, Jagannathan, and Runkle (1993) and 
Zakoian (1994). A TGARCH(m, s) model assumes the form 

m 

(3.34) 

where Af_;- is an indicator for negative at-i, that is, 

1 if at-i < 0, 
0 if at-i > 0, 

and at, yt, and fij are nonnegative parameters satisfying conditions similar to those 
of GARCH models. From the model, it is seen that a positive ar_,- contributes a,-a2 ( 
to cr2, whereas a negative at-j has a larger impact (a,- + yi)af_t with y, > 0. The 
model uses zero as its threshold to separate the impacts of past shocks. Other 
threshold values can also be used; see Chapter 4 for the general concept of threshold 
models. Model (3.34) is also called the GJR model because Glosten et al. (1993) 
proposed essentially the same model. 

For illustration, consider the monthly log returns of IBM stock from 1926 to 

2003. The fitted TGARCH(1,1) model with conditional GED innovations is 

rt — 0.0121 + at, at—at€t, 

cxf = 3.45 x 10“4 + (0.0658 + 0.0843A,_i)a2_1 + 0.8182a2_!, (3.35) 

where the estimated parameter of the GED is 1.51 with standard error 0.099. The 
standard error of the parameter for the mean equation is 0.002 and the standard 
errors of the parameters in the volatility equation are 1.26x-4, 0.0314, 0.0395, and 
0.049, respectively. To check the fitted model, we have (9(12) = 18.34(0.106) for 
the standardized residual at and (9(12) = 5.36 (0.95) for af. The model is adequate 
in modeling the first two conditional moments of the log return series. Based on 
the fitted model, the leverage effect is significant at the 5% level. 

150 

S-Plus Commands Used 

CONDITIONAL HETEROSCEDASTIC MODELS 

> ibm.tgarch = garch (ibmln~l, ~tgarch (1,1) , leveraged, 

+ cond.dist='ged') 
> summary(ibm.tgarch) 
> plot(ibm.tgarch) 

It is interesting to compare the two models in Eqs. (3.32) and (3.35) for the 
monthly log returns of IBM stock. Assume that at-\ = ±2o>_i so that €t-\ = ±2. 

The EGARCH(1,1) model gives 

°)2(<h-1 = ~2) _ ^0.22x2x0.632 ^ j 264 

offo-1 = 2) 

On the other hand, ignoring the constant term 0.000345, the TGARCH(1,1) model 

gives 

afet-i = -2) _ [(0.0658 + 0.0843)4 + 0.8182]^2_1 

af2(e,_i = 2) (0.0658 x 4 + 0.8182)<rf2_1 

The two models provide similar leverage effects. 

3.10 THE CHARMA MODEL 

Many other econometric models have been proposed in the literature to describe the 
evolution of the conditional variance o2 in Eq. (3.2). We mention the conditional 
heteroscedastic ARMA (CHARMA) model that uses random coefficients to produce 
conditional heteroscedasticity; see Tsay (1987). The CHARMA model is not the 
same as the ARCH model, but the two models have similar second-order conditional 

properties. A CHARMA model is defined as 

rt = fit + at, at = 8\,at-\ + 82tat-2 + • • • + + ry, (3.36) 

where {rjt} is a Gaussian white noise series with mean zero and variance cr2, 
{gf} = {(<$lr, ..., 8mt)'} is a sequence of iid random vectors with mean zero and 
nonnegative definite covariance matrix ft, and {5,} is independent of {ly}. In this 
section, we use some basic properties of vector and matrix operations to simplify 
the presentation. Readers may consult Appendix A of Chapter 8 for a brief review 
of these properties. For m > 0, the model can be written as 

at = a’t_x8, + T}t, 

where at-\ — (af_i,..., af_m)' is a vector of lagged values of at and is available 
at time t — 1. The conditional variance of a, of the CHARMA model in Eq. (3.36) 

THE CHARMA MODEL 

is then 

151 

~ + af-iCov(^/)a?-l 

= ar_m)fl(a,_1,..., a,_m)'. (3.37) 

Denote the (/, y')th element of ft by Because the matrix is symmetric, we have 
cou = coji. If m = 1, then Eq. (3.37) reduces to a} = + conaf_v which is an 
ARCH(l) model. If m = 2, then Eq. (3.37) reduces to 

2 2 2 o 
°t — °n +c°nat-i+ ^nat-\at-2 + a>22 a,_2> 

which differs from an ARCH(2) model by the cross-product term at~\at-2. In 
general, the conditional variance of a CHARMA(m) model is equivalent to that 
of an ARCH(m) model if ft is a diagonal matrix. Because ft is a covariance 
matrix, which is nonnegative definite, and cr^ is a variance, which is positive, we 

have a} > a~ > 0 for all t. In other words, the positiveness of a} is automatically 
satisfied under a CHARMA model. 

An obvious difference between ARCH and CHARMA models is that the latter 
use cross products of the lagged values of at in the volatility equation. The cross- 
product terms might be useful in some applications. For example, in modeling 
an asset return series, cross-product terms denote interactions between previous 
returns. It is conceivable that stock volatility may depend on such interactions. 
However, the number of cross-product terms increases rapidly with the order m, 
and some constraints are needed to keep the model simple. A possible constraint 
is to use a small number of cross-product terms in a CHARMA model. Another 
difference between the two models is that higher order properties of CHARMA 
models are harder to obtain than those of ARCH models because it is in general 
harder to handle multiple random variables. 

For illustration, we employ the CHARMA model 

rt — 00 + ati at — 8ltat—1 + &2tat—7. + T]t 

for the monthly excess returns of the S&P 500 index used before in GARCH 
modeling. The fitted model is 

rt = 0.00635 + a,, of = 0.00179 + (af_i, a,_2)fi(a,_i, ar_2)', 

where 

0.1417(0.0333) -0.0594(0.0365) ' 
-0.0594(0.0365) 0.3081(0.0340) ’ 

where the numbers in parentheses are standard errors. The cross-product term of 
S2 has a t ratio of -1.63, which is marginally significant at the 10% level. If we 
refine the model to 

rt — 0o + 

at — 8itat-i + + 8itat-3 + fif, 

152 

CONDITIONAL HETEROSCEDASTIC MODELS 

but assume that 831 is uncorrelated with (<$ir, 8^), then we obtain the fitted model 

rt = 0.0068 + at, a2 = 0.00136 + af_3)ft(a/-i, at-i, at-3) , 

where the elements of ft and their standard errors, shown in parentheses, are 

ft = 

0.1212(0.0355) -0.0622(0.0283) 0 
-0.0622(0.0283) 0.1913(0.0254) 0 
0 0 0.2988(0.0420) 

All of the estimates are now statistically significant at the 5% level. From the model, 
at — r, — 0.0068 is the deviation of the monthly excess return from its average. 
The fitted CHARMA model shows that there is some interaction effect between 
the first two lagged deviations. Indeed, the volatility equation can be written 

approximately as 

cr2 = 0.00136 + 0. 12<32_j - 0.12ar_iaf_2 + 0.19a2_2 + 0.30a2_3. 

The conditional variance is slightly larger when at-\at-2 is negative. 

3.10.1 Effects of Explanatory Variables 

The CHARMA model can easily be generalized so that the volatility of rt may 
depend on some explanatory variables. Let {xjt}™^ be m explanatory variables 
available at time t. Consider the model 

m 

rt = /Tf -\~o-ti at — 'y ]&itxi,t—1 T Vt i (3.38) 

1=1 

where 8, = (<$if, ..., 8mt)' and iy are random vector and variable defined in Eq. 
(3.36). Then the conditional variance of at is 

<y, — T (xjj—i,..., xmtt—i)ft(-Tl,I—1 > • • • ’ ) • 

In application, the explanatory variables may include some lagged values of a,. 

3.11 RANDOM COEFFICIENT AUTOREGRESSIVE MODELS 

In the literature, the random coefficient autoregressive (RCA) model is introduced 
to account for variability among different subjects under study, similar to the panel 
data analysis in econometrics and the hierarchical model in statistics. We classify 
the RCA model as a conditional heteroscedastic model, but historically it is used 
to obtain a better description of the conditional mean equation of the process by 

STOCHASTIC VOLATILITY MODEL 

153 

allowing for the parameters to evolve over time. A time series r, is said to follow 
an RCA(p) model if it satisfies 

p 

r,=(t)o + ]T(0(- + 8it)rt-i + a„ (3.39) 

i=i 

where p is a positive integer, {5f} = {(<5ir, ..., Spty} is a sequence of independent 
random vectors with mean zero and covariance matrix ft,5, and {6/} is independent 
of {at}; see Nicholls and Quinn (1982) for further discussions of the model. The 
conditional mean and variance of the RCA model in Eq. (3.39) are 

V, = E(n\Ft-i) = 00 + 

p 

1=1 

a/2 = °a + (rr-i, • • •, rt-.p)Sls(rt-1, ..., rt-p)', 

which is in the same form as that of a CHARMA model. However, there is a subtle 
difference between RCA and CHARMA models. For the RCA model, the volatility 
is a quadratic function of the observed lagged values Yet the volatility is a 
quadratic function of the lagged innovations at_i in a CHARMA model. 

3.12 STOCHASTIC VOLATILITY MODEL 

An alternative approach to describe the volatility evolution of a financial time 
series is to introduce an innovation to the conditional variance equation of at; see 
Melino and Turnbull (1990), Taylor (1994), Harvey, Ruiz, and Shephard (1994), and 
Jacquier, Poison, and Rossi (1994). The resulting model is referred to as a stochastic 
volatility (SV) model. Similar to EGARCH models, to ensure positiveness of the 
conditional variance, SV models use ln(cr2) instead of a}. A SV model is defined as 

at = <yt€t, (1 — Qfi B-amBm)ln(cr?) = a0 + vt, (3.40) 

where the et are iid N(0, 1), the vt are iid N{0, ct2), {et} and {u,} are independent, 
a0 is a constant, and all zeros of the polynomial 1 - J2?=i aiB‘ are greater than 
1 in modulus. Adding the innovation vt substantially increases the flexibility of 
the model in describing the evolution of a2, but it also increases the difficulty 
in parameter estimation. To estimate an SV model, we need a quasi-likelihood 
method via Kalman filtering or a Monte Carlo method. Jacquier, Poison, and Rossi 
(1994) provide some comparison of estimation results between quasi-likelihood and 
Markov chain Monte Carlo (MCMC) methods. The difficulty in estimating an SV 
model is understandable because for each shock at the model uses two innovations 
et and vt. We discuss an MCMC method to estimate SV models in Chapter 12. 
For more discussions on stochastic volatility models, see Taylor (1994). 

154 

CONDITIONAL HETEROSCEDASTIC MODELS 

The appendixes of Jacquier, Poison, and Rossi (1994) provide some properties 

of the SV model when m = 1. For instance, with m = 1, we have 

and E(af2) = exp(/-ifc + o£/2), E(af) = 3 exp(2^2 + 2a2), and corr(a2, aj_t) = 

[exp(o-2oij) - l]/[3exp((^2) - 1]. Limited experience shows that SV models often 

provided improvements in model fitting, but their contributions to out-of-sample 

volatility forecasts received mixed results. 

3.13 LONG-MEMORY STOCHASTIC VOLATILITY MODEL 

More recently, the SV model is further extended to allow for long memory in 

volatility, using the idea of fractional difference. As stated in Chapter 2, a time 

series is a long-memory process if its autocorrelation function decays at a hyper¬ 

bolic, instead of an exponential, rate as the lag increases. The extension to long- 

memory models in volatility study is motivated by the fact that the autocorrelation 

function of the squared or absolute-valued series of an asset return often decays 

slowly, even though the return series has no serial correlation; see Ding, Granger, 

and Engle (1993). Figure 3.10 shows the sample ACF of the daily absolute returns 

for IBM stock and the S&P 500 index from July 3, 1962, to December 31, 2003. 

These sample ACFs are positive with moderate magnitude but decay slowly. 

A simple long-memory stochastic volatility (LMSV) model can be written as 

at = otet, ot = a exp(«r/2), (1 - B)dut = ry, (3.41) 

where a > 0, the €t are iid N(0, 1), the ry are iid V(0, a2) and independent of €t, 
and 0 < d < 0.5. The feature of long memory stems from the fractional difference 
(1 - B)d, which implies that the ACF of u, decays slowly at a hyperbolic, instead 

of an exponential, rate as the lag increases. For model (3.41), we have 

ln(a2) = ln(cr2) -I- ut ln(c2) 

= [In (a2) + £(lne2)] + ut + [ln(e2) - £(lne2)] 

= /X + Ut + Cf. 

Thus, the ln(a2) series is a Gaussian long-memory signal plus a non-Gaussian white 

noise; see Breidt, Crato, and de Lima (1998). Estimation of the LMSV model is 
complicated, but the fractional difference parameter d can be estimated by using 
either a quasi-maximum-likelihood method or a regression method. Using the log 

series of squared daily returns for companies in the S&P 500 index, Bollerslev and 
Jubinski (1999) and Ray and Tsay (2000) found that the median estimate of d is 

about 0.38. For applications, Ray and Tsay (2000) studied common long-memory 

APPLICATION 

155 

Him  IBIlllliiliiiiilililMllMiiiiiiiii 

0 

50 

100 150 200 
Lag 

(a) 

linliiaiiii 

o 

0 50 100 150 200 

Lag 

(b) 

Figure 3.10 Sample ACF of daily absolute log returns for (a) S&P 500 index and (b) IBM stock for 
period from July 3, 1962, to December 31, 2003. Two horizontal lines denote asymptotic 5% limits. 

components in daily stock volatilities of groups of companies classified by various 

characteristics. They found that companies in the same industrial or business sector 

tend to have more common long-memory components (e.g., big U.S. national banks 
and financial institutions). 

3.14 APPLICATION 

In this section, we apply the volatility models discussed in this chapter to investigate 

some problems of practical importance. The data used are the monthly log returns 

of IBM stock and the S&P 500 index from January 1926 to December 1999. There 

are 888 observations, and the returns are in percentages and include dividends. 

Figure 3.11 shows the time plots of the two return series. Note that the result of 

this section was obtained by the RATS program. 

Example 3.4. The questions we address here are whether the daily volatility 
of a stock is lower in the summer and, if so, by how much. Affirmative answers 

to these two questions have practical implications in stock option pricing. We use 

the monthly log returns of IBM stock shown in Figure 3.11(a) as an illustrative 

example. 

156 

o 
CO 

CONDITIONAL HETEROSCEDASTIC MODELS 

O ____ 

1 1940 1960 1980 2000 

Year 

(a) 

C 
D 

Q) 
i— 

CD 
O 

Figure 3.11 Time plots of monthly log returns for (a) IBM stock and (b) S&P 500 index. Sample 
period is from January 1926 to December 1999. Returns are in percentages and include dividends. 

Denote the monthly log return series by rt. If Gaussian GARCH models are 

entertained, we obtain the GARCH(1,1) model: 

rt = 1.23 + 0.099rf_i + at, at — crtef, 

ct2 = 3.206 + 0.103a2_( + 0.825ct2_1, (3.42) 

for the series. The standard errors of the two parameters in the mean equation 
are 0.222 and 0.037, respectively, whereas those of the parameters in the volatility 
equation are 0.947, 0.021, and 0.037, respectively. Using the standardized residuals 
at = a,/a„ we obtain £(10) - 7.82(0.553) and £(20) = 21.22(0.325), where the 
p value is in parentheses. Therefore, there are no serial correlations in the residuals 
of the mean equation. The Ljung-Box statistics of the a2 series show £(10) = 
2.89(0.98) and £(20) = 7.26(0.99), indicating that the standardized residuals have 
no conditional heteroscedasticity. The fitted model seems adequate. This model 
serves as a starting point for further study. 

To study the summer effect on stock volatility of an asset, we define an indicator 

variable 

ut = 

1 if t is June, July, or August, 
0 otherwise, 

(3.43) 

APPLICATION 

157 

and modify the volatility equation to 

of = a0 + + fa o1_x + ut(a0 o + aio af_{ + faocr^). 

This equation uses two GARCH(1,1) models to describe the volatility of a stock 
return, one model for the summer months and the other for the remaining months. 
For the monthly log returns of IBM stock, estimation results show that the estimates 
of «io and faQ are statistically nonsignificant at the 10% level. Therefore, we refine 
the equation and obtain the model 

rt = 1.21 + 0.099rf_] + at, at=atet, 

a? = 4.539 + 0.113a]_x + 0M6a^ - 5.154wf. (3.44) 

The standard errors of the parameters in the mean equation are 0.218 and 0.037, 
respectively, and those of the parameters in the volatility equation are 1.071, 0.022, 
0.037, and 1.900, respectively. The Ljung-Box statistics for the standardized resid¬ 
uals at = at/at show 0(10) = 7.66(0.569) and 0(20) = 21.64(0.302). Therefore, 
there are no serial correlations in the standardized residuals. The Ljung-Box 
statistics for aj give 0(10) - 3.38(0.97) and 0(20) = 6.82(0.99), indicating 
no conditional heteroscedasticity in the standardized residuals either. The refined 
model seems adequate. 

Comparing the volatility models in Eqs. (3.42) and (3.44), we obtain the follow¬ 
ing conclusions. First, because the coefficient —5.514 is significantly different from 
zero with a p value of 0.0067, the summer effect on stock volatility is statistically 
significant at the 1% level. Furthermore, the negative sign of the estimate confirms 
that the volatility of IBM monthly log stock returns is indeed lower during the 
summer. Second, rewrite the volatility model in Eq. (3.44) as 

2 _ | -0.615 + 0.113af_1 + 0.816of_j if t is June, July,or August, 

I 4.539 + 0.113af2_j + 0.816of_j otherwise. 

The negative constant term -0.615 = 4.539 - 5.514 is counterintuitive. However, 
since the standard errors of 4.539 and 5.514 are relatively large, the estimated 
difference —0.615 might not be significantly different from zero. To verify the 
assertion, we refit the model by imposing the constraint that the constant term of 
the volatility equation is zero for the summer months. This can easily be done by 
using the equation 

°)2 = 1 + Y( 1 - ut). 

The fitted model is 

rt = 1.21 + 0.099rf_i +at, at = atet, 

a1 = 0.1 +0.81 lat2_1 +4.552(1 - ut). 

(3.45) 

158 

CONDITIONAL HETEROSCEDASTIC MODELS 

The standard errors of the parameters in the mean equation are 0.219 and 0.038, 
respectively, and those of the parameters in the volatility equation are 0.022, 0.034, 
and 1.094, respectively. The Ljung-Box statistics of the standardized residuals 
show 0(10) = 7.68 and 0(20) = 21.67, and those of the a? series give 0(10) = 
3.17 and 0(20) = 6.85. These test statistics are close to what we had before and 

are not significant at the 5% level. 

The volatility Eq. (3.45) can readily be used to assess the summer effect on the 
IBM stock volatility. For illustration, based on the model in Eq. (3.45) the medians 
of aj and of are 29.4 and 75.1, respectively, for the IBM monthly log returns 
in 1999. Using these values, we have of = 0.114 x 29.4 + 0.811 x 75.1 = 64.3 
for the summer months and of = 68.8 for the other months. The ratio of the two 
volatilities is 64.3/68.8 ~ 93%. Thus, there is a 7% reduction in the volatility of 
the monthly log return of IBM stock in the summer months. 

Example 3,5. The S&P 500 index is widely used in the derivative markets. 
As such, modeling its volatility is a subject of intensive study. The question we 
ask in this example is whether the past returns of individual components of the 
index contribute to the modeling of the S&P 500 index volatility in the presence 
of its own returns. A thorough investigation on this topic is beyond the scope of 
this chapter, but we use the past returns of IBM stock as explanatory variables to 

address the question. 

The data used are shown in Figure 3.11. Denote by rt the monthly log return 
series of the S&P 500 index. Using the r, series and Gaussian GARCH models, 
we obtain the following special GARCH(2,1) model: 

rt = 0.609 + at, at = otet, of = 0.717 + 0.147a2_2 + 0.839of_x. (3.46) 

The standard error of the constant term in the mean equation is 0.138, and those of 
the parameters in the volatility equation are 0.214, 0.021, and 0.017, respectively. 
Based on the standardized residuals at = at/ot, we have 0(10) = 11.51(0.32) and 
0(20) = 23.71(0.26), where the number in parentheses denotes the p value. For 
the a2 series, we have 0(10) = 9.42(0.49) and 0(20) = 13.01(0.88). Therefore, 
the model seems adequate at the 5% significance level. 

Next, we evaluate the contributions, if any, of using the past returns of IBM 
stock, which is a component of the S&P 500 index, in modeling the index volatility. 
As a simple illustration, we modify the volatility equation as 

of = a0 + a2af_2 + P\of_x + y(xt-i - 1.24)2, 

where xt is the monthly log return of IBM stock and 1.24 is the sample mean of 
xt. The fitted model for rt becomes 

rt =0.616 + at, at=ot€t, 

of = 1.069 + 0.148fl2_2 + 0.834<t2_1 - 0.0070c,-! - 1.24)2. (3.47) 

ALTERNATIVE APPROACHES 

159 

TABLE 3.4 Fitted Volatilities for Monthly Log Returns of S&P 500 Index from July 
to December 1999 Using Models with and without Past Log Return of IBM Stock 
Month 

7/99 

8/99 

9/99 

10/99 

11/99 

12/99 

Model (3.46) 
Model (3.47) 

26.30 
23.32 

26.01 
23.13 

24.73 
22.46 

21.69 
20.00 

20.71 
19.45 

22.46 
18.27 

The standard error of the parameter in the mean equation is 0.139 and the standard 
errors of the parameters in the volatility equation are 0.271, 0.020, 0.018, and 
0.002, respectively. For model checking, we have 0(10) = 11.39(0.33) and 0(20) 
= 23.63(0.26) for the standardized residuals at = at/at and 0(10) = 9.35(0.50) 
and 0(20) = 13.51(0.85) for the aj series. Therefore, the model is adequate. 

Since the p value for testing y = 0 is 0.0039, the contribution of the lag-1 
IBM stock return to the S&P 500 index volatility is statistically significant at the 
1% level. The negative sign is understandable because it implies that using the 
lag-1 past return of IBM stock reduces the volatility of the S&P 500 index return. 
Table 3.4 gives the fitted volatility of the S&P 500 index from July to December 
of 1999 using models (3.46) and (3.47). From the table, the past value of IBM log 
stock return indeed contributes to the modeling of the S&P 500 index volatility. 

3.15 ALTERNATIVE APPROACHES 

In this section, we discuss two alternative methods to volatility modeling. 

3.15.1 Use of High-Frequency Data 

French, Schwert, and Stambaugh (1987) consider an alternative approach for 
volatility estimation that uses high-frequency data to calculate volatility of 
low-frequency returns. In recent years, this approach has attracted substantial 
interest due to the availability of high-frequency financial data; see Andersen, 
Bollerslev, Diebold, and Labys (2001a, 2001b). 

Suppose that we are interested in the monthly volatility of an asset for which 
daily returns are available. Let r'tn be the monthly log return of the asset at month 
t. Assume that there are n trading days in month t and the daily log returns of the 
asset in the month are {rt,i}ni=v Using properties of log returns, we have 

n 

; = 1 

Assuming that the conditional variance and covariance exist, we have 

n 

Var(/fl^-i) = ^Var(r,,i|F,_i) + 2^Cov[(rM, r?J)|Ef_i], 

(3.48) 

i=l i<j 

160 

CONDITIONAL HETEROSCEDASTIC MODELS 

where Ft~\ denotes the information available at month t - 1 (inclusive). The prior 
equation can be simplified if additional assumptions are made. For example, if we 

assume that {ftj} is a white noise series, then 

Var(r;m|Ft_i) = nVar(rf,i), 

where Var(rM) can be estimated from the daily returns {rt)j}"=1 by 

n 

where rt is the sample mean of the daily log returns in month t [i.e., 

rt = (£!i=i 0,,)/«]. The estimated monthly volatility is then 

= -=LrE^-o2. 
i = 1 

n — 1 

(3.49) 

If {rtJ} follows an MA(1) model, then 

Var(r,m|F,_i) = nVar(r,,i) + 2(n - l)Cov(r/fi, rtt2), 

which can be estimated by 

a2 = —n— y >,, - r,)2 + 2 V'fr,.,- - f,)(rM+1 - r,). (3.50) 

frr 

The previous approach for volatility estimation is simple, but it encounters several 
difficulties in practice. First, the model for daily returns {rtJ} is unknown. This 
complicates the estimation of covariances in Eq. (3.48). Second, there are roughly 
21 trading days in a month, resulting in a small sample size. The accuracy of 
the estimates of variance and covariance in Eq. (3.48) might be questionable. The 
accuracy depends on the dynamic structure of {rui} and their distribution. If the 
daily log returns have high excess kurtosis and serial correlations, then the sample 
estimates ct* in Eqs. (3.49) and (3.50) may not even be consistent; see Bai, Russell, 
and Tiao (2004). Further research is needed to make this approach valuable. 

Example 3.6. Consider the monthly volatility of the log returns of the S&P 
500 index from January 1980 to December 1999. We calculate the volatility by 
three methods. In the first method, we use daily log returns and Eq. (3.49) (i.e., 
assuming that the daily log returns form a white noise series). The second method 
also uses daily returns but assumes an MA(1) model [i.e., using Eq. (3.50)]. The 
third method applies a GARCH(1,1) model to the monthly returns from January 

ALTERNATIVE APPROACHES 

161 

Year 

(a) 

Figure 3.12 Time plots of estimated monthly volatility for log returns of S&P 500 index from January 

1980 to December 1999: (a) assumes that daily log returns form a white noise series, (b) assumes that 
daily log returns follow an MA(1) model, and (c) uses monthly returns from January 1962 to December 
1999 and a GARCH(1,1) model. 

1962 to December 1999. We use a longer data span to obtain a more accurate 
estimate of the monthly volatility. The GARCH(1,1) model used is 

rT = 0-658 + at, a, = atet, ct2 = 3.349 + 0.086a2_j + 0.735<rt2_lf 

where et is a standard Gaussian white noise series. Figure 3.12 shows the time plots 
of the estimated monthly volatility. Clearly the estimated volatilities based on daily 
returns are much higher than those based on monthly returns and a GARCH(1,1) 
model. In particular, the estimated volatility for October 1987 was about 680 when 
daily returns are used. The plots shown were truncated to have the same scale. 

In Eq. (3.49), if we further assume that the sample mean rt is zero, then we have 
&rn ~ Y!i=1 rf,i ■ In this case, the cumulative sum of squares of daily log returns 
in a month is used as an estimate of monthly volatility. This concept has been 
generalized to estimate daily volatility of an asset by using intradaily log returns. 

162 

CONDITIONAL HETEROSCEDASTIC MODELS 

Let rt be the daily log return of an asset. Suppose that there are n equally spaced 

intradaily log returns available such that rt = Xw=i rU- The 9uanthy 

RVt = itrli' 

1=1 

is called the realized volatility of rf; see Andersen et al. (2001 a,b). Mathematically, 
realized volatility is a quadratic variation of rt, and it assumes that forms 
an iid sequence with mean zero and finite variance. Limited experience indicates 
that ln(RVf) often follows approximately a Gaussian ARIMA(0,1,<?) model, which 
can be used to produce forecasts. See demonstration in Section 1.1 for further 

information. , 

Advantages of realized volatility include simplicity and making use of mtradaily 
returns. Intuitively, one would like to use as much information as possible by 
choosing a large n. However, when the time interval between rtj is small, the 
returns are subject to the effects of market microstructure, for example, bid-ask 
bounce, which often results in a biased estimate of the volatility. The problem of 
choosing an optimal time interval for constructing realized volatility has attracted 
much research lately. For heavily traded assets in the United States, a time interval 
of 4-15 minutes is often used. Another problem of using realized volatility for 
stock returns is that the overnight return, which is the return from the closing price 
of day t — 1 to the opening price of t, tends to be substantial. Ignoring overnight 
returns can seriously underestimate the volatility. On the other hand, our limited 
experience shows that overnight returns appear to be small for index returns or 

foreign exchange returns. 

In a series of recent articles, Bamdorff-Nielsen and Shephard (2004) have used 
high-frequency returns to study bi-power variations of an asset return and developed 

some methods to detect jumps in volatility. 

3.15.2 Use of Daily Open, High, Low, and Close Prices 

For many assets, daily opening, high, low, and closing prices are available. Parkin¬ 
son (1980), Garman and Klass (1980), Rogers and Satchell (1991), and Yang and 
Zhang (2000) showed that one can use such information to improve volatility esti¬ 
mation. Figure 3.13 shows a time plot of price versus time for the rth trading day, 
assuming that time is continuous. For an asset, define the following variables: 

• Ct — closing price of the tth trading day. 

• Ot = opening price of the tth trading day. 

• / = fraction of the day (in interval [0,1]) that trading is closed. 

• Ht = highest price of the tth trading period. 

• Lt = lowest price of the tth trading period. 
• Ft-1 = public information available at time t — 1. 

ALTERNATIVE APPROACHES 

163 

The conventional variance (or volatility) is a} — E[(Ct - C,_i)2|F,_i]. Garman 
and Klass (1980) considered several estimates of a2 assuming that the price follows 
a simple diffusion model without drift; see Chapter 6 for more information about 
stochastic diffusion models. The estimators considered include: 

ah = (C, - Q_02. 

■ 2 _ (Ot-C,-i)2 , (Ct-Ot) 
rl ,t 

• = -—-— + -T7T-—, 0 < / < 1. 
2/ ' 2(1 - /) ’ 

2 _(Ht- Lt)2 

• ait = 

41n(2) 

0.3607(77, - Lty. 

a2t = 0.17^ ~C-1)2 +0.83 

/ (l-/)41n(2)’ 

0 < / < 1. 

It ~ 0.5(77, — Lt)2 — [21n(2) - 1](C, - O,)2, which is ~ 0.5(77, - L,)2 - 

0.386(C, - Ot)2. 

a2t = 0.12-—. + 0.88^^, 0 < / < 1. 

/ 

1 -/’ 

A more precise, but complicated, estimator a2t was also considered. However, it 
is close to alt- Defining the efficiency factor of a volatility estimator as 

Eff(a2,) = 

Var(^o,,) 

Var(d-2,) ’ 

164 

CONDITIONAL HETEROSCEDASTIC MODELS 

Garman and Klass (1980) found that Eff(o£) is approximately 2, 5.2, 6.2, 7.4, 
and 8.4 for i = 1, 2, 3, 5, and 6, respectively, for the simple diffusion model 

entertained. Note that a2t was derived by Parkinson (1980) with / = 0. 

Turn to log returns. Define the following: 

. 0t = ln(0,) - ln(CV_i), the normalized open 

. Ut = In{H,) - ln(0,), the normalized high. 

. d, = In(Lf) - ln(<9,), the normalized low. 

• c, = ln(Cf) - ln(0(), the normalized close. 

Suppose that there are n days of data available and the volatility is constant over 

the period. Yang and Zhang (2000) recommend the estimate 

-2 
- o,  + kerf + (1 - k)at 

yz 

as a robust estimator of the volatility, where 

1 J1 1 y x 

d2a =-with o=-J^ot, 

t=l 

, n 1 » 

d2= -r!Z(c'-^)2 With ~C=n^Ct' 
n - 1 n f=l 

1 " 

d2s = — y^[ut{ut — ct) + dt(dt — cf)], 

t=l 

0.34 

1.34+(n + l)/(n - 1) 

k = 

The estimate a2s was proposed by Rogers and Satchell (1991), and the quantity 
k is chosen to minimize the variance of the estimator of d2v which is a linear 

combination of three estimates. 

The quantity Ht — Lt is called the range of the price in the fth day. This 
estimator has led to the use of range-based volatility estimates; see, for instance, 
Alizadeh, Brandt, and Diebold (2002). In practice, stock prices are only observed 
at discrete time points. As such, the observed daily high is likely lower than H, 
and the observed daily low is likely higher than Lt. Consequently, the observed 
daily price range tends to underestimate the actual range and, hence, may lead 
to underestimation of volatility. This bias in volatility estimation depends on the 
trading frequency and tick size of the stocks. For intensively traded stocks, the bias 
should be negligible. For other stocks, further study is needed to better understand 

the performance of range-based volatility estimation. 

KURTOSIS OF GARCH MODELS 

165 

3.16 KURTOSIS OF GARCH MODELS 

Uncertainty in volatility estimation is an important issue, but it is often over¬ 
looked. To assess the variability of an estimated volatility, one must consider the 
kurtosis of a volatility model. In this section, we derive the excess kurtosis of a 
GARCH(1,1) model. The same idea applies to other GARCH models, however. 
The model considered is 

at — crtet, of = ao + ai + fi\af_{, 

where ao > 0, a\ >0,fi\ > 0, aj + /3i < 1, and {et} is an iid sequence satisfying 

E{et) = 0, Var(Ff) = 1, E(t?) = Ke + 3, 

where Kf is the excess kurtosis of the innovation Based on the assumption, we 
have the following: 

• Var(A) = E(o2) = a0/[l - («i + ft)]. 
• E{af) = (Ke + 3)E(o4) provided that E(o4) exists. 

Taking the square of the volatility model, we have 

°t — ao + alat-i + + 2aoaia/2_1 + 2oiqP\o2_{ + 2a\fi\o2_xa2_x. 

Taking expectation of the equation and using the two properties mentioned earlier, 
we obtain 

E(a4) — _ap(l + oq + ft)_ 

~ [1 - (oti + ft)][l - a2(Xf + 2) - (ai + ftft] ’ 

provided that 1 > «i + ft > 0 and 1 - a2(Xf + 2) - (aj + fix)2 > 0. The excess 
kurtosis of at, if it exists, is then 

K a 

E(af) 

[E (a2)]2 

(Ke + 3)[1 — (a\ + ft)2] 

1 - 2a2 - (aj + ft)2 - Kea2 

This excess kurtosis can be written in an informative expression. First, consider 

the case that €t is normally distributed. In this case, Ke = 0, and some algebra 
shows that 

6 a2 

1 ~2a2 - (ai -f- fi\)2' 

166 

CONDITIONAL HETEROSCEDASTIC MODELS 

where the superscript (g) is used to denote Gaussian distribution. This result has 
two important implications: (a) the kurtosis of at exists if 1 — 2a 1 — (ai + fii) > 0, 

and (b) if ot\ =0, then K{g) = 0, meaning that the corresponding GARCH(1,1) 

model does not have heavy tails. 

Second, consider the case that €t is not Gaussian. Using the prior result, we 

have 

Ke - Ke(cti + fii) + 6a:2 + 3Kea\ 

1 - 2a\ - (ai + fi\)2 — Kea2 

KeV - 2a\ - (oti + fii)2] + 6a2 + 5K(a] 

1 - 2a\ - (ofi + fii)2 - K€a2 

Ke + K{g) + lK€K(a8) 

1-i KeK{g) 

This result was obtained originally by George C. Tiao; see Bai, Russell, and Tiao 
(2003). It holds for all GARCH models provided that the kurtosis exists. For 
instance, if /Si = 0, then the model reduces to an ARCH(l) model. In this case, it 
is easy to verify that K^ = 6a^/(l — 3a2) provided that 1 > 3a[ and the excess 

kurtosis of a, is 

(Ke + 3)(1 — a2) _ Ke 4~ 2Kea{ -j- 60^ 

_ 1 - (Ke + 3)a2 ~ 1-3a2- Kea2 

Ke( 1 — 3a2) + 6 a2 + 5 Kea\ 

1 — 3a2 — Kea2 

_ Ke + K(g) + |KeK{ag) 

1 -\KeK(g) 

The prior result shows that for a GARCH(1,1) model the coefficient a j plays 
a critical role in determining the tail behavior of a,. If a\ = 0, then Kc,s) = 0 
and Ka = Ke. In this case, the tail behavior of a, is similar to that of the stan¬ 
dardized noise et. Yet if ai>0, then K(ag)>0 and the at process has heavy 

tails. 

For a (standardized) Student-! distribution with v degrees of freedom, we have 
E(e?) = 6/(v — 4) + 3 if u>4. Therefore, the excess kurtosis of et is Ke = 
6/(u - 4) for v > 4. This is part of the reason that we used t5 in the chapter when 
the degrees of freedom of a t-distribution are prespecified. The excess kurtosis 
of at becomes Ka = [6 + (v + \)Kag)]/[v - 4 - K(ag)] provided that 1 - 2a2(v - 
\)/{v - A) - (ai + /Si)2 > 0. 

appendix: some rats programs for estimating volatility models 

167 

APPENDIX: SOME RATS PROGRAMS FOR ESTIMATING VOLATILITY 
MODELS 

The data file used in the illustration is sp5 0 0.txt, which contains the monthly 

excess returns of the S&P 500 index with 792 observations. Comments in a RATS 

program start with *. 

A Gaussian GARCH(1,1) Model with a Constant Mean Equation 

all 0 792:1 

open data sp500.txt 

data(org=obs) / rt 

*** initialize the conditional variance function 
set h = 0.0 

*** specify the parameters of the model 

nonlin mu aO al bl 

*** specify the mean equation 

frml at = rt(t)-mu 

*** specify the volatility equation 

frml gvar = a0+al*at(t-1)**2+bl*h(t-1) 

*** specify the log likelihood function 

frml garchln = -0.5*log(h(t)=gvar(t))-0.5*at(t)**2/h(t) 

*** sample period used in estimation 

smpl 2 792 

*** initial estimates 

compute aO = 0.01, al = 0.1, bl = 0.5, mu = 0.1 

maximize(method=bhhh,recursive,iterations=150) garchln 

set fv = gvar(t) 

set resid = at(t)/sqrt(fv(t)) 

set residsq = resid(t)*resid(t) 

*** Checking standardized residuals 

cor(qstats,number=20,span=10) resid 

*** Checking squared standardized residuals 

cor(qstats,number=20,span=10) residsq 

A GARCH(1,1) Model with Student-t Innovation 

all 0 792:1 

open data sp500.txt 

data(org=obs) / rt 

set h = 0.0 

nonlin mu aO al bl v 

frml at = rt(t)-mu 

frml gvar = a0+al*at(t-1)**2+bl*h(t-1) 

frml tt = at(t)**2/(h(t)=gvar(t)) 

frml tin = %LNGAMMA((v+1)/2.)-%LNGAMMA(v/2.)-0.5*log(v-2.) 

frml gin = tin-((v+1)/2.)*log(1.0+tt(t)/(v-2.0))-0.5*log(h(t)) 
smpl 2 792 

168 

CONDITIONAL HETEROSCEDASTIC MODELS 

compute aO = 0.01, al = 0.1, bl = 0.5, mu = 0.1, v - 10 
maximize(method=bhhh,recursive,iterations-150) gin 

set fv = gvar(t) 

set resid = at(t)/sqrt(fv(t)) 

set residsq = resid(t)*resid(t) 

cor(qstats,number=2 0,span=10) resid 

cor(qstats,number=20,span=10) residsq 

An AR(1)—EGARCH(1,1) Model for Monthly Log Returns of IBM Stock 

all 0 864:1 

open data m-ibm.txt 

data(org=obs) / rt 

set h = 0.0 

nonlin cO pi th ga aO al 

frml at = rt(t)-c0-pl*rt(t-1) 

frml epsi = at(t)/(sqrt(exp(h(t)))) 

frml g = th*epsi(t)+ga*(abs(epsi(t))-sqrt(2./% PI)) 

frml gvar = al*h(t-1)+(1-al)*a0+g(t-1) 
frml garchln = -0.5*(h(t)=gvar(t))-0.5*epsi(t)**2 

smpl 3 864 
compute cO = 0.01, pi = 0.01, th - 0.1, ga - 0.1 

compute aO = 0.01, al = 0.5 
maximize(method=bhhh,recursive,iterations=150) garchln 

set fv = gvar(t) 

set resid = epsi(t) 
set residsq = resid(t)*resid(t) 

cor(qstats,number=20,span=10) resid 

cor(qstats,number=20,span=10) residsq 

EXERCISES 

3.1. Derive multistep-ahead forecasts for a GARCH(1,2) model at the forecast 

origin h. 

3.2. Derive multistep-ahead forecasts for a GARCH(2,1) model at the forecast 

origin h. 

3.3. Suppose that r\, ... ,rn are observations of a return series that follows the 

AR( 1 )-GARCH( 1,1) model 

rt - ii + (j)\rt-\ +at, a,=crle,, aj - do + a\<%_x + 

where ef is a standard Gaussian white noise series. Derive the conditional 

log-likelihood function of the data. 

3.4. In the equation in Exercise 3.3, assume that follows a standardized Student- 
t distribution with v degrees of freedom. Derive the conditional log-likelihood 

function of the data. 

EXERCISES 

169 

3.5. Consider the monthly simple returns of Intel stock from January 1973 to 
December 2008 in m-intc73 0 8 . txt. Transform the returns into log returns. 
Build a GARCH model for the transformed series and compute 1-step- to 
5-step-ahead volatility forecasts at the forecast origin December 2008. 

3.6. The file m-mrk4608.txt contains monthly simple returns of Merck stock 
from June 1946 to December 2008. The file has two columns denoting date 
and simple return. Transform the simple returns to log returns. 

(a) Is there any evidence of serial correlations in the log returns? Use auto¬ 
correlations and 5% significance level to answer the question. If yes, 
remove the serial correlations. 

(b) Is there any evidence of ARCH effects in the log returns? Use the resid¬ 
ual series if there are serial correlations in part (a). Use Ljung-Box 
statistics for the squared returns (or residuals) with 6 and 12 lags of 
autocorrelations and 5% significance level to answer the question. 

(c) Identify an ARCH model for the data and fit the identified model. Write 

down the fitted model. 

3.7. The file m-3m4608.txt contains two columns. They are date and the 

monthly simple return for 3M stock. Transform the returns to log returns. 

(a) Is there any evidence of ARCH effects in the log returns? Use Ljung-Box 
statistics with 6 and 12 lags of autocorrelations and 5% significance level 
to answer the question. 

(b) Use the PACF of the squared returns to identify an ARCH model. What 

is the fitted model? 

(c) There are 755 data points. Refit the model using the first 750 observations 
and use the fitted model to predict the volatilities for t from 751 to 755 
(the forecast origin is 750). 

(d) Build an ARCH-M model for the log return series of 3M stock. Test the 
hypothesis that the risk premium is zero at the 5% significance level. 
Draw your conclusion. 

(e) Build an EGARCH model for the log return series of 3M stock using 
the first 750 observations. Use the fitted model to compute 1-step- to 
5-step-ahead volatility forecasts at the forecast origin h = 750. 

3.8. The file m-gmsp5 0 08 . txt contains the dates and monthly simple returns of 

General Motors stock and the S&P 500 index from 1950 to 2008. 

(a) Build a GARCH model with Gaussian innovations for the log returns of 

GM stock. Check the model and write down the fitted model. 

(b) Build a GARCH-M model with Gaussian innovations for the log returns 

of GM stock. What is the fitted model? 

(c) Build a GARCH model with Student-r distribution for the log returns 
of GM stock, including estimation of the degrees of freedom. Write 

170 

CONDITIONAL HETEROSCEDASTIC MODELS 

down the fitted model. Let v be the degrees of freedom of the Student-! 
distribution. Test the hypothesis Hq : v = 6 versus Ha : v ^ 6, using the 
5% significance level. 

(d) Build an EGARCH model for the log returns of GM stock. What is the 

fitted model? 

(e) Obtain 1-step- to 6-step-ahead volatility forecasts for all the models 

obtained. Compare the forecasts. 

3.9. Consider the monthly log returns of GM stock in m-gmsp5008.txt. Build 
an adequate TGARCH model for the series. Write down the fitted model and 
test for the significance of the leverage effect. Obtain 1-step- to 6-steps-ahead 
volatility forecasts. 

3.10. Again, consider the returns in m-gmsp5008 . txt. 

(a) Build a Gaussian GARCH model for the monthly log returns of the S&P 

500 index. Check the model carefully. 

(b) Is there a summer effect on the volatility of the index return? Use the 

GARCH model built in part (a) to answer this question. 

(c) Are lagged returns of GM stock useful in modeling the index volatil¬ 
ity? Again, use the GARCH model of part (a) as a baseline model for 
comparison. 

3.11. The file d-gmsp9 9 08 . txt contains the daily simple returns of GM stock and 
the S&P composite index from 1999 to 2008. It has three columns denoting 
date, GM return, and S&P return. 

(a) Compute the daily log returns of GM stock. Is there any evidence of 
ARCH effects in the log returns? You may use 10 lags of the squared 
returns and 5% significance level to perform the test. 

(b) Compute the PACF of the squared log returns (10 lags). 

(c) Specify a GARCH model for the GM log return using a normal distri¬ 
bution for the innovations. Perform model checking and write down the 
fitted model. 

(d) Find an adequate GARCH model for the series but using the generalized 
error distribution for the innovations. Write down the fitted model. 

3.12. Consider the daily simple returns of the S&P composite index in the file 

d-gmsp9908.txt. 

(a) Is there any ARCH effect in the simple return series? Use 10 lags of the 

squared returns and 5% significance level to perform the test. 

(b) Build an adequate GARCH model for the simple return series. 

(c) Compute 1-step- to 4-step-ahead forecasts of the simple return and its 

volatility based on the fitted model. 

3.13. Again, consider the daily simple returns of GM stock in the file 

d-gmsp9908.txt. 

REFERENCES 

171 

(a) Find an adequate GARCH-M model for the series. Write down the fitted 

model. 

(b) Find an adequate EGARCH model for the series. Is the “leverage” effect 

significant at the 5% level? 

3.14. Revisit the file d-gmsp9908.txt. However, we shall investigate the value 
of using market volatility in modeling volatility of individual stocks. Convert 
the two simple return series into percentage log return series. 

(a) Build an AR(5)-GARCH(1,1) model with generalized error distribution 
for the log S&P returns. The AR(5) contains only lags 3 and 5. Denote 
the fitted volatility series by spvol. 

(b) Estimate a GARCH(1,1) model with spvol as an exogenous variable to 
the log GM return series. Check the adequacy of the model, and write 
down the fitted model. In S-Plus, the command is 

fit = garch(gm ~ 1, ~garch(1,1)+spvol, cond.dist='ged') 

(c) Discuss the implication of the fitted model. 

3.15. Again, consider the percentage daily log returns of GM stock and the S&P 
500 index from 1999 to 2008 as before, but we shall investigate whether 
the volatility of GM stock has any contribution in modeling the S&P index 
volatility. Follow the steps below to perform the analysis. 

(a) Fit a GARCH(1,1) model with generalized error distribution to the per¬ 
centage log returns of GM stock. Denote the fitted volatility by gmvol. 
Build an adequate GARCH model plus gmvol as the exogenous variable 
for the log S&P return series. Write down the fitted model. 

(b) Is the volatility of GM stock returns helpful in modeling the volatility 

of the S&P index returns? Why? 

REFERENCES 

Alizadeh, S., Brandt, M., and Diebold, F. X. (2002). Range-based estimation of stochastic 

volatility models. Journal of Finance 57: 1047-1092. 

Andersen, T. G. and Bollerslev, T. (1998). Answering the skeptics: Yes, standard volatility 

models do provide accurate forecasts. International Economic Review 39: 885-905. 

Andersen, T. G., Bollerslev, T., Diebold, F. X., and Labys, P. (2001a). The distribution of 
realized exchange rate volatility. Journal of the American Statistical Association 96: 
42-55. 

Andersen, T. G., Bollerslev, T., Diebold, F. X., and Labys, P. (2001b). The distribution of 

realized stock return volatility. Journal of Financial Economics 61: 43-76. 

Bai, X., Russell, J. R., and Tiao, G. C. (2003). Kurtosis of GARCH and stochastic volatility 

models with non-normal innovations. Journal of Econometrics 114: 349-360. 

172 

CONDITIONAL HETEROSCEDASTIC MODELS 

Bai, X., Russell, J. R., and Tiao, G. C. (2004). Effects of non-normality and dependence 
on the precision of variance estimates using high-frequency financial data. Revised 
working paper, Graduate School of Business, University of Chicago. 

Bamdorff-Nielsen, O. E. and Shephard, N. (2004). Power and bi-power variations with 
stochastic volatility and jumps (with discussion). Journal of Financial Econometrics 2: 
1-48. 

Bollerslev, T. (1986). Generalized autoregressive conditional heteroskedasticity. Journal of 

Econometrics 31: 307-327. 

Bollerslev, T. and Jubinski, D. (1999). Equality trading volume and volatility: Latent infor¬ 
mation arrivals and common long-run dependencies. Journal of Business & Economic 
Statistics 17: 9-21. 

Bollerslev, T., Chou, R. Y., and Kroner, K. F. (1992). ARCH modeling in finance. Journal 

of Econometrics 52: 5-59. 

Bollerslev, T., Engle, R. F., and Nelson, D. B. (1994). ARCH model. In R. F. Engle and D. 
C. McFadden (eds.). Handbook of Econometrics IV, pp. 2959-3038. Elsevier Science, 
Amsterdam. 

Breidt, F. J., Crato, N., and de Lima, P. (1998). On the detection and estimation of long 

memory in stochastic volatility. Journal of Econometrics 83: 325-348. 

Cao, C. and Tsay, R. S. (1992). Nonlinear time series analysis of stock volatilities. Journal 

of Applied Econometrics 7: sl65-sl85. 

Ding, Z., Granger, C. W. J., and Engle, R. F. (1993). A long memory property of stock 

returns and a new model. Journal of Empirical Finance 1: 83-106. 

Engle, R. F. (1982). Autoregressive conditional heteroscedasticity with estimates of the 

variance of United Kingdom inflations. Econometrica 50: 987-1007. 

Fernandez, C. and Steel, M. F. J. (1998). On Bayesian modelling of fat tails and skewness. 

Journal of the American Statistical Association 93: 359-371. 

French, K. R., Schwert, G. W., and Stambaugh, R. F. (1987). Expected stock returns and 

volatility. Journal of Financial Economics 19: 3-29. 

Garman, M. B. and Klass, M. J. (1980). On the estimation of security price volatilities from 

historical data. Journal of Business 53: 67-78. 

Glosten, L. R., Jagannathan, R., and Runkle, D. E. (1993). On the relation between the 
expected value and the volatility of nominal excess return on stocks. Journal of Finance 
48: 1779-1801. 

Harvey, A. C., Ruiz, E., and Shephard, N. (1994). Multivariate stochastic variance models. 

Review of Economic Studies 61: 247-264. 

Jacquier, E., Poison, N. G., and Rossi, P. (1994). Bayesian analysis of stochastic volatility 
models (with discussion). Journal of Business & Economic Statistics 12: 371-417. 

Lambert, P. and Laurent, S. (2001). Modelling financial time series using GARCH-type 

models and a skewed Student density. Working paper, Universite de Liege. 

McLeod, A. I. and Li, W. K. (1983). Diagnostic checking ARMA time series models using 

squared-residual autocorrelations. Journal of Time Series Analysis 4: 269-273. 

Melino, A. and Turnbull, S. M. (1990). Pricing foreign currency options with stochastic 

volatility. Journal of Econometrics 45: 239-265. 

Nelson, D. B. (1990). Stationarity and persistence in the GARCH(1,1) model. Econometric 

Theory 6: 318-334. 

REFERENCES 

173 

Nelson, D. B. (1991). Conditional heteroskedasticity in asset returns: A new approach. 

Econometrica 59: 347-370. 

Nicholls, D. F. and Quinn, B. G. (1982). Random Coefficient Autoregressive Models: An 

Introduction, Lecture Notes in Statistics, 11. Springer, New York. 

Parkinson, M. (1980). The extreme value method for estimating the variance of the rate of 

return. Journal of Business 53: 61-65. 

Ray, B. K. and Tsay, R. S. (2000). Long-range dependence in daily stock volatilities. Journal 

of Business & Economic Statistics 18: 254-262. 

Rogers, L. C. G. and Satchell, S. E. (1991). Estimating variance from high, low and closing 

prices. Annals of Applied Probability 1: 504-512. 

Taylor, S. J. (1994). Modeling stochastic volatility: A review and comparative study. Math¬ 

ematical Finance 4: 183-204. 

Tong, H. (1978). On a threshold model. In C. H. Chen (Ed.). Pattern Recognition and Signal 

Processing. Sijhoff & Noordhoff, Amsterdam. 

Tong, H. (1990). Non-Linear Time Series: A Dynamical System Approach. Oxford University 

Press, Oxford, UK. 

Tsay, R. S. (1987). Conditional heteroscedastic time series models. Journal of the American 

Statistical Association, 82: 590-604. 

Yang, D. and Zhang, Q. (2000). Drift-independent volatility estimation based on high, low, 

open, and close prices. Journal of Business 73: 477-491. 

Zakoian, J. M. (1994). Threshold heteroscedastic models. Journal of Economic Dynamics 

and Control 18: 931-955. 

CHAPTER 4 

Nonlinear Models 
and Their Applications 

This chapter focuses on nonlinearity in financial data and nonlinear econometric 
models useful in analysis of financial time series. Consider a univariate time series 
xt, which, for simplicity, is observed at equally spaced time points. We denote 
the observations by [xt\t = 1, ..., T}, where T is the sample size. As stated in 
Chapter 2, a purely stochastic time series xt is said to be linear if it can be written as 

OO 

(4.1) 

where /x is a constant, i/q are real numbers with t/c0 = 1, and {at} is a sequence of 
independent and identically distributed (iid) random variables with a well-defined 
distribution function. We assume that the distribution of at is continuous and 
E(at) = 0. In many cases, we further assume that Var(at) = or, even stronger, 
that a, is Gaussian. If i/sj < oo, then xt is weakly stationary (i.e., the first 
two moments of xt are time invariant). The ARMA process of Chapter 2 is linear 
because it has an MA representation in Eq. (4.1). Any stochastic process that does 
not satisfy the condition of Eq. (4.1) is said to be nonlinear. The prior definition 
of nonlinearity is for purely stochastic time series. One may extend the definition 
by allowing the mean of xt to be a linear function of some exogenous variables, 
including the time index and some periodic functions. But such a mean function 
can be handled easily by the methods discussed in Chapter 2, and we do not discuss 
it here. Mathematically, a purely stochastic time series model for xt is a function 
of an iid sequence consisting of the current and past shocks—that is. 

Xt — f iflti &t—i» • • •)• 

Analysis of Financial Time Series, Third Edition, By Ruey S. Tsay 
Copyright © 2010 John Wiley & Sons, Inc. 

(4.2) 

175 

176 

NONLINEAR MODELS AND THEIR APPLICATIONS 

The linear model in Eq. (4.1) says that /(•) is a linear function of its arguments. 
Any nonlinearity in /(•) results in a nonlinear model. The general nonlinear model 
in Eq. (4.2) is not directly applicable because it contains too many parameters. 

To put nonlinear models available in the literature in a proper perspective, we 
write the model of xt in terms of its conditional moments. Let Ft-\ be the a 
field generated by available information at time t — 1 (inclusive). Typically, Ft-\ 
denotes the collection of linear combinations of elements in {xt-\, xt-2,...} and 
{at-1, cii—2, ■ ■ •}• The conditional mean and variance of xt given Ft-\ are 

^t = E{xt\Ft-X)=g(Ft-X), a} = Yar(xt\Ft-l) = h(Ft-l), (4.3) 

where g(-) and h(-) are well-defined functions with h(-) > 0. Thus, we restrict the 
model to 

xt — g(Ft-1) + y/h(Ft-\)et. 

where et = at/o, is a standardized shock (or innovation). For the linear series xt 
in Eq. (4.3), g(-) is a linear function of elements of Ft-\ and h(-) = oThe 
development of nonlinear models involves making extensions of the two equations 
in Eq. (4.3). If g(-) is nonlinear, xt is said to be nonlinear in mean. If h(-) is time 
variant, then x, is nonlinear in variance. The conditional heteroscedastic models of 
Chapter 3 are nonlinear in variance because their conditional variances a} evolve 
over time. In fact, except for the GARCH-M models, in which /xt depends on o} 
and hence also evolves over time, all of the volatility models of Chapter 3 focus 
on modifications or extensions of the conditional variance equation in Eq. (4.3). 
Based on the well-known Wold decomposition, a weakly stationary and purely 
stochastic time series can be expressed as a linear function of uncorrelated shocks. 
For stationary volatility series, these shocks are uncorrelated but dependent. The 
models discussed in this chapter represent another extension to nonlinearity derived 
from modifying the conditional mean equation in Eq. (4.3). 

Many nonlinear time series models have been proposed in the statistical liter¬ 
ature, such as the bilinear models of Granger and Andersen (1978), the threshold 
autoregressive (TAR) model of Tong (1978), the state-dependent model of Priest¬ 
ley (1980), and the Markov switching model of Hamilton (1989). The basic idea 
underlying these nonlinear models is to let the conditional mean /j., evolve over 
time according to some simple parametric nonlinear function. Recently, a number 
of nonlinear models have been proposed by making use of advances in comput¬ 
ing facilities and computational methods. Examples of such extensions include the 
nonlinear state-space modeling of Carlin, Poison, and Staffer (1992), the functional 
coefficient autoregressive model of Chen and Tsay (1993a), the nonlinear additive 
autoregressive model of Chen and Tsay (1993b), and the multivariate adaptive 
regression spline of Lewis and Stevens (1991). The basic idea of these extensions 
is either using simulation methods to describe the evolution of the conditional dis¬ 
tribution of xt or using data-driven methods to explore the nonlinear characteristics 
of a series. Finally, nonparametric and semiparametric methods such as kernel 

NONLINEAR MODELS 

177 

regression and artificial neural networks have also been applied to explore the non¬ 
linearity in a time series. We discuss some nonlinear models in Section 4.1 that 
are applicable to financial time series. The discussion includes some nonparametric 
and semiparametric methods. 

Apart from the development of various nonlinear models, there is substantial 
interest in studying test statistics that can discriminate linear series from nonlinear 
ones. Both parametric and nonparametric tests are available. Most parametric tests 
employ either the Lagrange multiplier or likelihood ratio statistics. Nonparametric 
tests depend on either higher order spectra of xt or the concept of dimension 
correlation developed for chaotic time series. We review some nonlinearity tests 
in Section 4.2. Sections 4.3 and 4.4 discuss modeling and forecasting of nonlinear 
models. Finally, an application of nonlinear models is given in Section 4.5. 

4.1 NONLINEAR MODELS 

Most nonlinear models developed in the statistical literature focus on the conditional 
mean equation in Eq. (4.3); see Priestley (1988) and Tong (1990) for summaries 
of nonlinear models. Our goal here is to introduce some nonlinear models that are 
applicable to financial time series. 

4.1.1 Bilinear Model 

The linear model in Eq. (4.1) is simply the first-order Taylor series expansion of 
the /(•) function in Eq. (4.2). As such, a natural extension to nonlinearity is to 
employ the second-order terms in the expansion to improve the approximation. 
This is the basic idea of bilinear models, which can be defined as 

p q ms 

Xt=C + Y, eia'-j + XI £ Pijxt-iat-j + at, (4.4) 

1 = 1 7=1 1=1 7=1 

where p,q,m, and 5 are nonnegative integers. This model was introduced by 
Granger and Andersen (1978) and has been widely investigated. Subba Rao and 
Gabr (1984) discuss some properties and applications of the model, and Liu and 
Brockwell (1988) study general bilinear models. Properties of bilinear models such 
as stationarity conditions are often derived by (a) putting the model in a state- 
space form (see Chapter 11) and (b) using the state transition equation to express 
the state as a product of past innovations and random coefficient vectors. A special 
generalization of the bilinear model in Eq. (4.4) has conditional heteroscedasticity. 
For example, consider the model 

S 

xt = fx + fadt-id, + at, 

(4.5) 

i=l 

178 

NONLINEAR MODELS AND THEIR APPLICATIONS 

where {at} is a white noise series. The first two conditional moments of xt are 

E(xt\F,-i) = \i, Var(*t|F,_i) = ^1 + ^ 

which are similar to that of the RCA or CHARM A model of Chapter 3. 

Example 4.1. Consider the monthly simple returns of the CRSP equal- 
weighted index from January 1926 to December 2008 for 996 observations. 
Denote the series by Rt. The sample PACF of Rt shows significant partial 
autocorrelations at lags 1 and 3 so that an AR(3) model is used. The squared 
series of the AR(3) residuals suggests that the conditional heteroscedasticity might 
depend on lags 1, 3, and 8 of the residuals. Therefore, we employ the special 
bilinear model 

Rt = + 01 Rt-1 + 03-ftf—3 + (1 + P\dt-1 + /63ar_3)af 

for the series, where at = with et being an iid series with mean zero and 
variance 1. Note that lag 8 is omitted for simplicity. Assuming that the conditional 
distribution of at is normal, we use the conditional maximum-likelihood method 
and obtain the fitted model 

Rt = 0.0114 + 0.167R,_j - 0.095R,^ 

+ 0.071(1 + 0.377a?_i - 0.646a,_3)e;, (4.6) 

where the standard errors of the parameters are, in the order of appearance, 0.0023, 
0.032, 0.027, 0.002, 0.147, and 0.136, respectively. All estimates are significantly 
different from zero at the 5% level. Define 

, _ Rt -0.0114 -0.167+ 0.095/?,_3 
~~ 0.071(1 + 0.377a,_! - 0.646fi,_3) ’ 

where e, = 0 for t < 3, as the standardized residual series of the model. The sample 
ACF of €t shows no significant serial correlations, but the series is not independent 
because the squared series ef has significant serial correlations. The validity of 
model (4.6) deserves further investigation. For comparison, we also consider an 
AR(3)-ARCH(3) model for the series and obtain 

Rt = 0.013 + 0.223/?f_i + 0.006Rf_2 — 0.013J?f_3 + at, 

o} = 0.002 + 0.185^, + o.301a;L2 + 0.197«r2_3j 

where all estimates but the coefficients of Rt—2 &nd Rt—3 nre highly significant. 
The standardized residual series of the model shows no serial correlations, but the 
squared residuals show 2(10) = 19.78 with a p value of 0.031. Models (4.6) and 
(4.7) appear to be similar, but the latter seems to fit the data better. Further study 
shows that an AR(1)-GARCH(1,1) model fits the data well. 

NONLINEAR MODELS 

179 

4.1.2 Threshold Autoregressive (TAR) Model 

This model is motivated by several nonlinear characteristics commonly observed 
in practice such as asymmetry in declining and rising patterns of a process. It uses 
piecewise linear models to obtain a better approximation of the conditional mean 
equation. However, in contrast to the traditional piecewise linear model that allows 
for model changes to occur in the “time” space, the TAR model uses threshold 
space to improve linear approximation. Let us start with a simple 2-regime AR(1) 
model: 

xt = 

— 1.5xf_i + at 

if xt-\ < 0, 

0.5x;_i + at 

if xt-\ > 0, 

(4.8) 

where the at are iid N{0, 1). Here the threshold variable is xt-\ so that the delay 
is 1, and the threshold is 0. Figure 4.1 shows the time plot of a simulated series 
of xt with 200 observations. A horizontal line of zero is added to the plot, which 
illustrates several characteristics of TAR models. First, despite the coefficient —1.5 
in the first regime, the process xt is geometrically ergodic and stationary. In fact, 
the necessary and sufficient condition for model (4.8) to be geometrically ergodic 
is < 1, 4>\~] < 1, and 0{1)0p) < 1, where 4>\] is the AR coefficient of regime 
i; see Petruccelli and Woolford (1984) and Chen and Tsay (1991). Ergodicity is 
an important concept in time series analysis. For example, the statistical theory 

Figure 4.1 Time plot of simulated 2-regime TAR(l) series. 

180 

NONLINEAR MODELS AND THEIR APPLICATIONS 

showing that the sample mean x = (J2t=i xt)/T of xt converges to the mean of xt 
is referred to as the ergodic theorem, which can be regarded as the counterpart of 
the central limit theory for the iid case. Second, the series exhibits an asymmetric 
increasing and decreasing pattern. If xt-\ is negative, then xt tends to switch to a 
positive value due to the negative and explosive coefficient —1.5. Yet when x,_i is 
positive, it tends to take multiple time indexes for xt to reduce to a negative value. 
Consequently, the time plot of xt shows that regime 2 has more observations than 
regime 1, and the series contains large upward jumps when it becomes negative. 
The series is therefore not time reversible. Third, the model contains no constant 
terms, but E(xt) is not zero. The sample mean of the particular realization is 
0.61 with a standard deviation of 0.07. In general, E(xt) is a weighted average 
of the conditional means of the two regimes, which are nonzero. The weight for 
each regime is simply the probability that xt is in that regime under its stationary 
distribution. It is also clear from the discussion that, for a TAR model to have 
zero mean, nonzero constant terms in some of the regimes are needed. This is very 
different from a stationary linear model for which a nonzero constant implies that 
the mean of xt is not zero. 

A time series xt is said to follow a ^-regime self-exciting TAR (SETAR) model 

with threshold variable x,-d if it satisfies 

= (f)(0j) + V]-<P{pj)xt-p + a{tj\ if Yj—i < xt^d < yj, (4.9) 

where k and d are positive integers, j = 1,..., k, Yi are real numbers such that 
-oo = yo < Y\ < • • • < Yk-1 < Yk — oo, the superscript (j) is used to signify the 
regime, and {a\j)} are iid sequences with mean 0 and variance oj and are mutually 
independent for different j. The parameter d is referred to as the delay parameter 
and Yj are the thresholds. Here it is understood that the AR models are different 
for different regimes; otherwise, the number of regimes can be reduced. Equation 
(4.9) says that a SETAR model is a piecewise linear AR model in the threshold 
space. It is similar in spirit to the usual piecewise linear models in regression 
analysis, where model changes occur in the order in which observations are taken. 
The SETAR model is nonlinear provided that k > 1. 

Properties of general SETAR models are hard to obtain, but some of them 
can be found in Tong (1990), Chan (1993), Chan and Tsay (1998), and the refer¬ 
ences therein. In recent years, there is increasing interest in TAR models and their 
applications, see, for instance, Hansen (1997), Tsay (1998), and Montgomery et 
al. (1998). Tsay (1989) proposed a testing and modeling procedure for univariate 
SETAR models. The model in Eq. (4.9) can be generalized by using a threshold 
variable zt that is measurable with respect to Ft_x (i.e., a function of elements of 
Ft-\). The main requirements are that zt is stationary with a continuous distribution 
function over a compact subset of the real line and that zt-d is known at time t. 
Such a generalized model is referred to as an open-loop TAR model. 

Example 4.2. To demonstrate the application of TAR models, consider the 
U S. monthly civilian unemployment rate, seasonally adjusted and measured in 

NONLINEAR MODELS 

181 

Figure 4.2 Time plot of monthly U.S. civilian unemployment rate, seasonally adjusted, from January 
1948 to March 2009. 

Year 

percentage, from January 1948 to March 2009 for 735 observations. The data 
are obtained from the Bureau of Labor Statistics, Department of Labor, and are 
shown in Figure 4.2. The plot shows two main characteristics of the data. First, 
there appears to be a slow but upward trend in the overall unemployment rate. 
Second, the unemployment rate tends to increase rapidly and decrease slowly. 
Thus, the series is not time reversible and may not be unit-root stationary, 
either. 

Because the sample autocorrelation function decays slowly, we employ the first 
differenced series yt = (1 — B)ut in the analysis, where ut is the monthly unem¬ 
ployment rate. Using univariate ARIMA models, we obtain the model 

(1 - 1.135 + 0.2752)(1 - 0.51512)yf = (1 - 1.125 + 0.4452)(1 - 0.82Bn)at, 
(4.10) 
where oa =0.187 and all estimates but the AR(2) coefficient are statistically sig¬ 
nificant at the 5% level. The t ratio of the estimate of AR(2) coefficient is —1.66. 
The residuals of model (4.10) give <2(12) = 12.3 and <2(24) = 25.5, respectively. 
The corresponding p values are 0.056 and 0.11, respectively, based on x2 dis¬ 
tributions with 6 and 18 degrees of freedom. Thus, the fitted model adequately 
describes the serial dependence of the data. Note that the seasonal AR and MA 
coefficients are highly significant with standard error 0.049 and 0.035, respec¬ 
tively, even though the data were seasonally adjusted. The adequacy of seasonal 
adjustment deserves further study. Using model (4.10), we obtain the 1-step-ahead 
forecast of 8.8 for the April 2009 unemployment rate, which is close to the actual 

data of 8.9. 

182 

NONLINEAR MODELS AND THEIR APPLICATIONS 

To model nonlinearity in the data, we employ TAR models and obtain the 

model 

y _ |0-083yf_2 + 0.158yf_3 + 0.118yf_4 — 0.180y;_i2 + an if yf-i < 0.1, 

|o.421y?_2 + 0.239yr_3 — 0.127y,_i2 + a2t if yt-\ >0.1, 

(4.11) 
where the standard errors of an are 0.180 and 0.217, respectively, the standard 
errors of the AR parameters in regime 1 are 0.046, 0.043, 0.042, and 0.037, whereas 
those of the AR parameters in regime 2 are 0.054, 0.057, and 0.075, respectively. 
The number of data points in regimes 1 and 2 are 460 and 262, respectively. The 
standardized residuals of model (4.11) only shows some minor serial correlation at 
lag 12. Based on the fitted TAR model, the dynamic dependence in the data appears 
to be stronger when the change in monthly unemployment rate is greater than 
0.1%. This is understandable because a substantial increase in the unemployment 
rate is indicative of weakening in the U.S. economy, and policy makers might be 
more inclined to take action to help the economy, which in turn may affect the 
dynamics of the unemployment rate series. Consequently, model (4.11) is capable 
of describing the time-varying dynamics of the U.S. unemployment rate. 

The MA representation of model (4.10) is 

f(B) ^ 1 + 0.015+ 0.18R2 + 0.20R3 + 0.18R4 + 0.15£5 + -... 

It is then not surprising to see that no y,-\ term appears in model (4.11). 

As mentioned in Chapter 3, threshold models can be used in finance to handle 
the asymmetric responses in volatility between positive and negative returns. The 
models can also be used to study arbitrage tradings in index futures and cash prices; 
see Chapter 8 on multivariate time series analysis. Here we focus on volatility 
modeling and introduce an alternative approach to parameterization of TGARCH 
models. In some applications, this new general TGARCH model fares better than 
the GJR model of Chapter 3. 

Example 4.3. Consider the daily log returns, in percentage and including 
dividends, of IBM stock from July 3, 1962, to December 31, 2003, for 10,446 
observations. Figure 4.3 shows the time plot of the series, which is one of the 
longer return series analyzed in the book. The volatility seems to be larger in the 
latter years of the data. Because general TGARCH models are used in the analysis, 
we use the SCA package to perform estimation in this example. 

If GARCH models of Chapter 3 are entertained, we obtain the following 

AR(2)-GARCH(1,1) model for the series: 

rt = 0.062 — 0.024r;_2 + at, at = crt€t, 

a? — 0.037 + 0.077a2_, + 0.913ct2_j, (4.12) 

where rt is the log return, {€,} is a Gaussian white noise sequence with mean 
zero and variance 1.0, the standard errors of the parameters in the mean equation 

NONLINEAR MODELS 

183 

c 
i— 
13 
"53 
v_ 

03 
O 

Figure 4.3 Time plot of daily log returns for IBM stock from July 3, 1962, to December 31, 2003. 

Year 

are 0.015 and 0.010, and those of the volatility equation are 0.004, 0.003, and 
0.003, respectively. All estimates are statistically significant at the 5% level. 
The Ljung-Box statistics of the standardized residuals give <2(10) = 5.19(0.82) 
and (9(20) = 24.38(0.18), where the number in parentheses denotes the p value 
obtained using X^-i distribution because of the estimated AR(2) coefficient. 
For the squared standardized residuals, we obtain (9(10) = 11.67(0.31) and 
<2(20) = 18.25(0.57). The model is adequate in modeling the serial dependence 
and conditional heteroscedasticity of the data. But the unconditional mean for 
rt of model (4.12) is 0.060, which is substantially larger than the sample mean 
0.039, indicating that the model might be misspecified. 

Next, we employ the TGARCH model of Chapter 3 and obtain 

rt = 0.014 — 0.028r,_2 + at, at = atet, 

a? = 0.075 + 0.081 Pt-\c$_x + 0.157Ar_iaf_1 + 0 M3cr?_lt (4.13) 

where Pt-\ = 1 — Nt-1, A^_i is the indicator for negative at-\ such that Nt-\ — 1 
if at-1 < 0 and = 0 otherwise, the standard errors of the parameters in the mean 
equation are 0.013 and 0.009, and those of the volatility equation are 0.007, 
0.008, 0.010, and 0.010, respectively. All estimates except the constant term of 
the mean equation are significant. Let at be the standardized residuals of model 
(4.13). We have (9(10) = 2.47(0.98) and £9(20) = 25.90(0.13) for the {at} series 

184 

NONLINEAR MODELS AND THEIR APPLICATIONS 

and 0(10) = 97.07(0.00) and 0(20) = 170.3(0.00) for {fi2}. The model fails to 
describe the conditional heteroscedasticity of the data. 

The idea of TAR models can be used to refine the prior TGARCH model by 
allowing for increased flexibility in modeling the asymmetric response in volatility. 
More specifically, we consider an AR(2)-TAR-GARCH(1,1) model for the series 
and obtain 

rt — 0.033 — 0.023rf_2 + at, at = o>e,, 

o-2 = 0.075 + 0.04 la2_j + 0.903a2_, 

+ (0.030a2_j + 0.062a2_|)A?_i, (4.14) 

where Nt-\ is defined in Eq. (4.13). All estimates in model (4.14) are significantly 
different from zero at the usual 1% level. Let at be the standardized residuals of 
model (4.14). We obtain 0(10) = 6.09(0.73) and 0(20) = 25.29(0.15) for {at} 
and 0(10) = 13.54(0.20) and 0(20) = 19.56(0.49) for {a2}. Thus, model (4.14) 
is adequate in modeling the serial correlation and conditional heteroscedasticity 
of the daily log returns of IBM stock considered. The unconditional mean return 
of model (4.14) is 0.033, which is much closer to the sample mean 0.039 than 
those implied by models (4.12) and (4.13). Comparing the two fitted TGARCH 
models, we see that the asymmetric behavior in daily IBM stock volatility is much 
stronger than what is allowed in a GJR model. Specifically, the coefficient of a2_ j 
also depends on the sign of at-\. Note that model (4.14) can be further refined by 
imposing the constraint that the sum of the coefficients of af_x and is one 
when < 0. 

Remark. A RATS program to estimate the AR(2)—TAR—GARCH( 1,1) model 
used is given in Appendix A. The results might be slightly different from those of 
SCA given in the text. □ 

4.1.3 Smooth Transition AR (STAR) Model 

A criticism of the SETAR model is that its conditional mean equation is not con¬ 
tinuous. The thresholds {yj} are the discontinuity points of the conditional mean 
function In response to this criticism, smooth TAR models have been proposed; 
see Chan and Tong (1986) and Terasvirta (1994) and the references therein. A time 
series xt follows a 2-regime STAR(/?) model if it satisfies 

p / 
= Co + ^ 0o,iXt-i + F I 

Xt-d - A 

1=1 ' 

+ 

i=l 

iXt- 

+ at, 

(4.15) 

where d is the delay parameter, A and s are parameters representing the location and 
scale of model transition, and F(-) is a smooth transition function. In practice, F(-) 
often assumes one of three forms—namely, logistic, exponential, or a cumulative 

NONLINEAR MODELS 

185 

distribution function. From Eq. (4.15) and with 0 < F(-) < 1, the conditional mean 
of a STAR model is a weighted linear combination between the following two 
equations: 

p 

Mil — Q) T ^ '00 ,ixt—i, 

1=1 

p 

M21 = (c0 + Cl) + £(00,1 + 4>l,i)xt-i- 
i=1 

The weights are determined in a continuous manner by F[(xt-d — A)/^]. The 
prior two equations also determine properties of a STAR model. For instance, a 
prerequisite for the stationarity of a STAR model is that all zeros of both AR 
polynomials are outside the unit circle. An advantage of the STAR model over 
the TAR model is that the conditional mean function is differentiable. However, 
experience shows that the transition parameters A and 5 of a STAR model are hard 
to estimate. In particular, most empirical studies show that standard errors of the 
estimates of A and s are often quite large, resulting in t ratios of about 1.0; see 
Terasvirta (1994). This uncertainty leads to various complications in interpreting 
an estimated STAR model. 

Example 4.4. To illustrate the application of STAR models in financial time 
series analysis, we consider the monthly simple stock returns for Minnesota Mining 
and Manufacturing (3M) Company from February 1946 to December 2008. If 
ARCH models are entertained, we obtain the following ARCH(2) model: 

Rt = 0.013 + at, at = atet, o2 = 0.003 + O.OSSa2^ + 0.109a2_2, 

(4.16) 

where standard errors of the estimates are 0.002, 0.0003, 0.047, and 0.050, respec¬ 
tively. As discussed before, such an ARCH model fails to show the asymmetric 
responses of stock volatility to positive and negative prior shocks. The STAR model 
provides a simple alternative that may overcome this difficulty. Applying STAR 
models to the monthly returns of 3M stock, we obtain the model 

R, = 0.015 + at, a, = otet, 

_ , , 0.001 - 0.239a2 , 

= (0-003 + a205“- + 0M2a'->) + l+exp(-1000;,-,)- (4-l7) 

where the standard error of the constant term in the mean equation is 0.002 and the 
standard errors of the estimates in the volatility equation are 0.0002, 0.074, 0.043, 
0.0004, and 0.080, respectively. The scale parameter 1000 of the logistic transition 
function is fixed a priori to simplify the estimation. This STAR model provides 
some support for asymmetric responses to positive and negative prior shocks. For 

186 

NONLINEAR MODELS AND THEIR APPLICATIONS 

a large negative at_\, the volatility model approaches the ARCH(2) model 

of = 0.003 + 0.205af_1 + 0.092af_2. 

Yet for a large positive a,_ i, the volatility process behaves like the ARCH(2) model 

of = 0.004 - 0.034a;2_j + 0.092a2_2. 

The negative coefficient of a2_, in the prior model is counterintuitive, but the 
magnitude is small. As a matter of fact, for a large positive shock at_u the ARCH 
effects appear to be weak even though the parameter estimates remain statistically 
significant. The results shown are obtained using the command optim in R. A 
RATS program for estimating the STAR model is given in Appendix A. 

R Program for Estimating the STAR Model Used 

> da=read.table("m-3m4608.txt",header=T) 
> rtn=da[,2] 
> source("star.R") 
> par=c(.001, .002, .256, .141, .002,-.314) 
> m2=optim(par,star,methodic("BFGS"),hessian=T) 

# function to calculate the likelihood of a STAR model, 
star <- function(par){ 
f = 0 
Tl=length(rtn) 
h=c(1,1) 
at=c(0,0) 
for (t in 3:T1){ 
resi = rtn[t]-par[1] 
at=c(at,resi) 
sig=par[2]+par[3]*at[t-1]A2+par[4]*at[t-2]~2 
sigl=par[5]+par[6]*at[t-1]^2 
tt=sqrt(sig+sigl/(1+exp(-1000*at[t-1]))) 
h=c(h,tt) 
x=resi/tt 
f=f+log(tt)+0.5*x*x 
} 
f 
} 

4.1.4 Markov Switching Model 

The idea of using probability switching in nonlinear time series analysis is discussed 
in Tong (1983). Using a similar idea, but emphasizing aperiodic transition between 
various states of an economy, Hamilton (1989) considers the Markov switching 

NONLINEAR MODELS 

187 

autoregressive (MSA) model. Here the transition is driven by a hidden two-state 
Markov chain. A time series x, follows an MSA model if it satisfies 

jci+n=i^*'-'+a>> if* = >’ (4.i8) 

°2 + XX1 02,iXt-i + a2t lf st = 2, 

where st assumes values in {1,2} and is a first-order Markov chain with transition 
probabilities 

P(st = 2\st-i = 1) = w i, P(st = l|5f_i = 2) = u>2. 

The innovational series {a\t} and {a2t} are sequences of iid random variables with 
mean zero and finite variance and are independent of each other. A small wq 
means that the model tends to stay longer in state i. In fact, 1 / Wj is the expected 
duration of the process to stay in state i. From the definition, an MSA model uses a 
hidden Markov chain to govern the transition from one conditional mean function 
to another. This is different from that of a SETAR model for which the transition is 
determined by a particular lagged variable. Consequently, a SETAR model uses a 
deterministic scheme to govern the model transition, whereas an MSA model uses a 
stochastic scheme. In practice, the stochastic nature of the states implies that one is 
never certain about which state xt belongs to in an MSA model. When the sample 
size is large, one can use some filtering techniques to draw inference on the state of 
xt. Yet as long as xt-d is observed, the regime of xt is known in a SETAR model. 
This difference has important practical implications in forecasting. For instance, 
forecasts of an MSA model are always a linear combination of forecasts produced 
by submodels of individual states. But those of a SETAR model only come from 
a single regime provided that xt-d is observed. Forecasts of a SETAR model also 
become a linear combination of those produced by models of individual regimes 
when the forecast horizon exceeds the delay d. It is much harder to estimate 
an MSA model than other models because the states are not directly observable. 
Hamilton (1990) uses the EM algorithm, which is a statistical method iterating 
between taking expectation and maximization. McCulloch and Tsay (1994) consider 
a Markov chain Monte Carlo (MCMC) method to estimate a general MSA model. 
We discuss MCMC methods in Chapter 12. 

McCulloch and Tsay (1993) generalize the MSA model in Eq. (4.18) by let¬ 
ting the transition probabilities uq and w2 be logistic, or probit, functions of some 
explanatory variables available at time t — 1. Chen, McCulloch, and Tsay (1997) 
use the idea of Markov switching as a tool to perform model comparison and selec¬ 
tion between nonnested nonlinear time series models (e.g., comparing bilinear and 
SETAR models). Each competing model is represented by a state. This approach 
to select a model is a generalization of the odds ratio commonly used in Bayesian 
analysis. Finally, the MSA model can easily be generalized to the case of more 
than two states. The computational intensity involved increases rapidly, however. 
For more discussions of Markov switching models in econometrics, see Hamilton 

(1994, Chapter 22). 

188 

NONLINEAR MODELS AND THEIR APPLICATIONS 

Example 4.5. Consider the growth rate, in percentages, of the U.S. quarterly 
real gross national product (GNP) from the second quarter of 1947 to the first 
quarter of 1991. The data are seasonally adjusted and shown in Figure 4.4, where 
a horizontal line of zero growth is also given. It is reassuring to see that a majority 
of the growth rates are positive. This series has been widely used in nonlinear 
analysis of economic time series. Tiao and Tsay (1994) and Potter (1995) use TAR 
models, whereas Hamilton (1989) and McCulloch and Tsay (1994) employ Markov 
switching models. 

Employing the MSA model in Eq. (4.18) with p = 4 and using a Markov chain 
Monte Carlo method, which is discussed in Chapter 12, McCulloch and Tsay (1994) 
obtain the estimates shown in Table 4.1. The results have several interesting find¬ 
ings. First, the mean growth rate of the marginal model for state 1 is 0.909/(1 - 
0.265 — 0.029 + 0.126 + 0.11) = 0.965 and that of state 2 is —0.42/(1 — 0.216 — 
0.628 + 0.073 + 0.097) = —1.288. Thus, state 1 corresponds to quarters with posi¬ 
tive growth, or expansion periods, whereas state 2 consists of quarters with negative 
growth, or a contraction period. Second, the relatively large posterior standard devi¬ 
ations of the parameters in state 2 reflect that there are few observations in that state. 
This is expected as Figure 4.4 shows few quarters with negative growth. Third, 
the transition probabilities appear to be different for different states. The estimates 
indicate that it is more likely for the U.S. GNP to get out of a contraction period 
than to jump into one -0.286 versus 0.118. Fourth, treating l/iuf as the expected 
duration for the process to stay in state i, we see that the expected durations for 

Figure 4.4 Time plot of growth rate of U.S. quarterly real GNP from 1947.11 to 1991.1. Data are 
seasonally adjusted and in percentages. 

NONLINEAR MODELS 

189 

03 
State 1 
-0.126 
0.103 

TABLE 4.1 Estimation Results of Markov Switching Model with p = 4 for Growth 
Rate of U.S. Quarterly Real GNP, Seasonally Adjusted0 
Parameter 

Oi 

Wi 

01 

0 2 

Ci 

04 

Estimate 
Standard Error 

0.909 
0.202 

0.265 
0.113 

0.029 
0.126 

Estimate 
Standard Error 

-0.420 
0.324 

0.216 
0.347 

0.628 
0.377 

State 2 

-0.073 
0.364 

-0.110 
0.109 

0.816 
0.125 

0.118 
0.053 

-0.097 
0.404 

1.017 
0.293 

0.286 
0.064 

“The estimates and their standard errors are posterior means and standard errors of a Gibbs sampling 
with 5000 iterations. 

a contraction period and an expansion period are approximately 3.69 and 11.31 
quarters. Thus, on average, a contraction in the U.S. economy lasts about a year, 
whereas an expansion can last for 3 years. Finally, the estimated AR coefficients 
of xt-2 differ substantially between the two states, indicating that the dynamics of 
the U.S. economy are different between expansion and contraction periods. 

4.1.5 Nonparametric Methods 

In some financial applications, we may not have sufficient knowledge to prespecify 
the nonlinear structure between two variables Y and X. In other applications, we 
may wish to take advantage of the advances in computing facilities and compu¬ 
tational methods to explore the functional relationship between Y and X. These 
considerations lead to the use of nonparametric methods and techniques. Nonpara¬ 
metric methods, however, are not without cost. They are highly data dependent 
and can easily result in overfitting. Our goal here is to introduce some nonparamet¬ 
ric methods for financial applications and some nonlinear models that make use 
of nonparametric methods and techniques. The nonparametric methods discussed 
include kernel regression, local least-squares estimation, and neural network. 

The essence of nonparametric methods is smoothing. Consider two financial 

variables Y and X, which are related by 

Yt = m(Xt) +at. 

(4.19) 

where m(-) is an arbitrary, smooth, but unknown function and {at} is a white 
noise sequence. We wish to estimate the nonlinear function m(-) from the data. For 
simplicity, consider the problem of estimating m(-) at a particular date for which 
X = x. That is, we are interested in estimating m(x). Suppose that at X = x we 
have repeated independent observations yi,... ,yr- Then the data become 

yt = m(x) + at, t = 1,..., T. 

Taking the average of the data, we have 

T 

— m(x) + 

T 

190 

NONLINEAR MODELS AND THEIR APPLICATIONS 

By the law of large numbers, the average of the shocks converges to zero as T 
increases. Therefore, the average y — (JTj=i yt)/T is a consistent estimate of m{x). 
That the average y provides a consistent estimate of m(x) or, alternatively, that the 
average of shocks converges to zero shows the power of smoothing. 

In financial time series, we do not have repeated observations available at X = x. 
What we observed are {(y,,x,)} for / = 1, ..., T. But if the function m(-) is 
sufficiently smooth, then the value of Yt for which X, x continues to provide 
accurate approximation of m(x). The value of Y, for which Xt is far away from 
x provides less accurate approximation for m(x). As a compromise, one can use a 
weighted average of y, instead of the simple average to estimate m(x). The weight 
should be larger for those Yt with Xt close to x and smaller for those Yt with 
X, far away from x. Mathematically, the estimate of m(x) for a given x can be 
written as 

m (*) = j: 

t=l 

(4.20) 

where the weights wt(x) are larger for those y, with xt close to x and smaller for 
those yt with xt far away from x. In Eq. (4.20), we assume that the weights sum 
to T. One can treat \/T as part of the weights and make the weights sum to one. 
From Eq. (4.20), the estimate m(x) is simply a local weighted average with 
weights determined by two factors. The first factor is the distance measure (i.e., 
the distance between x, and x). The second factor is the assignment of weight for 
a given distance. Different ways to determine the distance between xt and x and to 
assign the weight using the distance give rise to different nonparametric methods. 
In what follows, we discuss the commonly used kernel regression and local linear 
regression methods. 

Kernel Regression 

Kernel regression is perhaps the most commonly used nonparametric method in 
smoothing. The weights here are determined by a kernel, which is typically a 
probability density function, is denoted by K(x), and satisfies 

K{x) > 0, J K(z)dz = 1. 

However, to increase the flexibility in distance measure, one often rescales the 
kernel using a variable h > 0, which is referred to as the bandwidth. The rescaled 
kernel becomes 

Kh(x) = ^K(x/h), J Kh(z)dz = 1. (4.21) 

The weight function can now be defined as 

wt(x) = 

Kh(x - xt) 
eLi Kh(x-xty 

(4.22) 

NONLINEAR MODELS 

191 

Figure 4.5 Standard normal kernel (solid line) and Epanechnikov kernel (dashed line) with bandwidth 
h = 1. 

X 

where the denominator is a normalization constant that makes the smoother adap¬ 
tive to the local intensity of the X variable and ensures the weights sum to one. 
Plugging Eq. (4.22) into the smoothing formula (4.20), we have the well-known 
Nadaraya-Watson kernel estimator 

T 

m(x) = t(x)yt = 

t=t 

E/=i Kh(x - xt)yt _ 

EliKh(x-xt) 

(4.23) 

see Nadaraya (1964) and Watson (1964). In practice, many choices are available 
for the kernel K(x). However, theoretical and practical considerations lead to a 
few choices, including the Gaussian kernel 

Kh(x) = 

and the Epanechnikov kernel (Epanechnikov, 1969) 

Kh(x) = 

where 1(A) is an indicator such that 1(A) — 1 if A holds and 1(A) — 0 otherwise. 
Figure 4.5 shows the Gaussian and Epanechnikov kernels for h = 1. 

192 

NONLINEAR MODELS AND THEIR APPLICATIONS 

To understand the role played by the bandwidth h, we evaluate the 
Nadaraya-Watson estimator with the Epanechnikov kernel at the observed values 
{x,} and consider two extremes. First, if h -» 0, then 

m(xt) 

Kh{0)yt 

Kh( 0) 

indicating that small bandwidths reproduce the data. Second, if h —► oo, then 

m{xt) 

E/=i Kh(0)yt 
Eli Kh{0) 

i=i 

suggesting that large bandwidths lead to an oversmoothed curve—the sample mean. 
In general, the bandwidth function h acts as follows. If h is very small, then the 
weights focus on a few observations that are in the neighborhood around each xt. 
If h is very large, then the weights will spread over a larger neighborhood of xt. 
Consequently, the choice of h plays an important role in kernel regression. This is 
the well-known problem of bandwidth selection in kernel regression. 

Bandwidth Selection 

There are several approaches for bandwidth selection; see Hardle (1990) and Fan 
and Yao (2003). The first approach is the plug-in method, which is based on 
the asymptotic expansion of the mean integrated squared error (MISE) for kernel 
smoothers 

MISE = E / [m(x) - m(x)]2dx, 

J — OO 

where m(-) is the true function. The quantity E[m(x) - m(x)]2 of the MISE is a 
pointwise measure of the mean squared error (MSE) of m(x) evaluated at *. Under 
some legularity conditions, one can derive the optimal bandwidth that minimizes 
the MISE. The optimal bandwidth typically depends on several unknown quantities 
that must be estimated from the data with some preliminary smoothing. Several 
iterations are often needed to obtain a reasonable estimate of the optimal bandwidth. 
In practice, the choice of preliminary smoothing can become a problem. Fan and 
Yao (2003) give a normal reference bandwidth selector as 

r 1.06sT 1/15 for the Gaussian kernel, 
n opf; — ' 

2.34sT C5 for the Epanechnikov kernel. 

where 5 is the sample standard error of the independent variable, which is assumed 
to be stationary. 

NONLINEAR MODELS 

193 

The second approach to bandwidth selection is the leave-one-out cross valida¬ 
tion. First, one observation (xj, yj) is left out. The remaining T - 1 data points 
are used to obtain the following smoother at xj: 

mhj(Xj) = 

which is an estimate of yj, where the weights wt(xj) sum to T - 1. Second, 
perform step 1 for j = 1,..., T and define the function 

1 r 

CV(/i) = — ~ ™h,j(xj)]2W(xj), 

j=i 

where VE(-) is a nonnegative weight function satisfying J2'j=l W(xj) = T, that 
can be used to down-weight the boundary points if necessary. Decreasing the 
weights assigned to data points close to the boundary is needed because those 
points often have fewer neighboring observations. The function CV(/z) is called 
the cross-validation function because it validates the ability of the smoother to 
predict {yt}J=i - One chooses the bandwidth h that minimizes the CV(-) function. 

Local Linear Regression Method 
Assume that the second derivative of m(-) in model (4.19) exists and is continuous 
at x, where x is a given point in the support of m(-). Denote the data available by 
{(jr> xt)}J=l. The local linear regression method to nonparametric regression is to 
find a and b that minimize 

T 

L(a, b) = ^[y, - a - b{x - xt)]2Kh(x - xt), (4.24) 

i=l 

where Kh(-) is a kernel function defined in Eq. (4.21) and h is a bandwidth. Denote 
the resulting value of a by a. The estimate of m{x) is then defined as a. In practice, 
x assumes an observed value of the independent variable. The estimate b can be 
used as an estimate of the first derivative of m(-) evaluated at x. 

Under the least-squares theory, Eq. (4.24) is a weighted least-squares problem 
and one can derive a closed-form solution for a. Specifically, taking the partial 
derivatives of L(a, b) with respect to both a and b and equating the derivatives to 
zero, we have a system of two equations with two unknowns: 

T T T 

^Kh(x - xt)yt =a^Kh(x - x,) + b^(x - xt)Kh(x - xt), 

t=\ t=l t=l 

T T T 

]Pyf(x - xt)Kh(x -xt) = aY^(x -xt)Kh(x - xt) + b^(x - x,)2Kh(x - xt). 

/=! r= 1 t=1 

194 

Define 

NONLINEAR MODELS AND THEIR APPLICATIONS 

T 

ST,e = ^2 Kh(x - xt)(x -xt)e, £ — 0, 1,2. 

t=l 

The prior system of equations becomes 

•S'7’,0 St, 1 
st, i st,2 

a 

b 

E/=i Kh(x - xt)yt 
Er=i (x - xt)Kh(x - xt)yt 

Consequently, we have 

£ _ ST,2 E/=1 Kh(x ~ xt)yt - Jr,i E/=i(* - xt)Kh(x - x,)yt 

ST,0$T,2 ~ X 

The numerator and denominator of the prior fraction can be further simplified as 

T T 

st.2 J2Kh(x- x,)yt - sTA ^(x - xt)Kh(x - xt)y, 

i=l r=l 

T 

= /.{Kh(x - xt)\sT,i — (x — x?)s7\i]}y,. 

r=l 

T T 

st,oSt,2 ~ Sj i = Kh(x — xt)sr,2 — — x,)Kh(x — xt)sTj 

1=1 f=1 

T 

= ^2Kh(x- Xt)[sT,2 - (x - A,)^]. 

f=l 

In summary, we have 

where wt is defined as 

(4.25) 

wt — Kh(x — Xt)[sj,2 — (x — Xt)Sj- i]. 

In practice, to avoid possible zero in the denominator, we use the following m(x) 
to estimate m(x): 

m(x) - - 

E/=i v>tyt 

ELi wt + l/T2 

(4.26) 

NONLINEAR MODELS 

195 

Notice that a nice feature of Eq. (4.26) is that the weight wt satisfies 

T 

Also, if one assumes that m(-) of Eq. (4.19) has the first derivative and finds the 
minimizer of 

r 

y^J.yt — a)2Kh{x - xt), 

t=t 

then the resulting estimator is the Nadaraya-Watson estimator mentioned earlier. 
In general, if one assumes that m(x) has a bounded kth derivative, then one can 
replace the linear polynomial in Eq. (4.24) by a (k - l)-order polynomial. We refer 
to the estimator in Eq. (4.26) as the local linear regression smoother. Fan (1993) 
shows that, under some regularity conditions, the local linear regression estimator 
has some important sampling properties. The selection of bandwidth can be carried 
out via the same methods as before. 

Time Series Application 
In time series analysis, the explanatory variables are often the lagged values of 
the series. Consider the simple case of a single explanatory variable. Here model 
(4.19) becomes 

xt = m(xt-1) +at, 

and the kernel regression and local linear regression method discussed before are 
directly applicable. When multiple explanatory variables exist, some modifications 
are needed to implement the nonparametric methods. For the kernel regression, one 
can use a multivariate kernel such as a multivariate normal density function with 
a prespecified covariance matrix: 

Kh(x) =--exp 

(hV2jt)P\^\l/2 

where p is the number of explanatory variables and £ is a prespecified positive- 
definite matrix. Alternatively, one can use the product of univariate kernel functions 
as a multivariate kernel—for example, 

This latter approach is simple, but it overlooks the relationship between the explana¬ 
tory variables. 

196 

NONLINEAR MODELS AND THEIR APPLICATIONS 

Example 4.6. To illustrate the application of nonparametric methods in finance, 
consider the weekly 3-month Treasury bill secondary market rate from 1970 to 1997 
for 1461 observations. The data are obtained from the Federal Reserve Bank of St. 
Louis and are shown in Figure 4.6. This series has been used in the literature as 
an example of estimating stochastic diffusion equations using discretely observed 
data. See references in Chapter 6. Here we consider a simple model 

yt — n(xt-i)dt + o(xt-\)dwt, 

where xt is the 3-month Treasury bill rate, yt — xt — xt-\, wt is a standard Brow¬ 
nian motion, and /x(-) and cr(-) are smooth functions of xt-\, and apply the local 
smoothing function lowess of R or S-Plus to obtain nonparametric estimates of 
/r(-) and cr(-); see Cleveland (1979). For simplicity, we use \yt\ as a proxy of the 
volatility of xt. 

For the simple model considered, /r(x,_i) is the conditional mean of yt given 
■*7-1’ that is, /x(xt_i) = E{yt\xt-\). Figure 4.7(a) shows the scatterplot of y(t) 
versus xt-\. The plot also contains the local smooth estimate of /x(jcr_i) obtained 
by lowess of R or S-Plus. The estimate is essentially zero. However, to better 
understand the estimate, Figure 4.7(b) shows the estimate £(*,_i) on a finer scale. 
It is interesting to see that jX{xt-\) is positive when xt-\ is small but becomes 
negative when xt-\ is large. This is in agreement with the common sense that 

Figure 4.6 Time plot of U.S. weekly 3-month Treasury bill rate in secondary market from 1970 to 

NONLINEAR MODELS 

197 

(b) (d) 

Figure 4.7 Estimation of conditional mean and volatility of weekly 3-month Treasury bill rate via a 

local smoothing method: (a) yt vs. xr_i, where yt = x, — xf_i and x, is interest rate; (b) estimate of 

(c) |y,| vs. xt-\\ and (d) estimate of <r(x,_i). 

when the interest rate is high, it is expected to come down, and when the rate is 
low, it is expected to increase. Figure 4.7(c) shows the scatterplot of |y(t)| versus 
xt-\ and the estimate of a(xt-\) via lowess. The plot confirms that the higher the 
interest rate, the larger the volatility. Figure 4.7(d) shows the estimate o(xt-\) on 
a finer scale. Clearly, the volatility is an increasing function of xt-\ and the slope 
seems to accelerate when x,_i is approaching 10%. This example demonstrates 
that simple nonparametric methods can be helpful in understanding the dynamic 
structure of a financial time series. 

R and S-Plus Commands Used in Example 4.6 

> zl=read.table('w-3mtbs7097.txt',header=T) 
> x= z1[4,1:1460]/100 
> y=(zl[4,2:1461]-z1[4,1:1460] )/10 0 
> par(mfcol=c(2,2)) 
> plot(x,y,pch=' *' ,xlab='x(t-1)',ylab='y(t)') 
> lines(lowess(x,y)) 
> title(main='(a) y(t) vs x(t-l)') 
> fit=lowess(x,y) 

198 

NONLINEAR MODELS AND THEIR APPLICATIONS 

> plot(fit$x,fit$y,xlab='x(t-1)',ylab='mu',type='1', 

+ ylim=c(-.002, .002) ) 

> title(main='(b) Estimate of mu(.)') 

> plot(x,abs(y),pch='*',xlab='x(t-1)',ylab='abs(y)') 

> lines(lowess(x,abs(y))) 

> title(main='(c) abs(y) vs x(t-l)') 

> fit2=lowess(x,abs(y)) 

> plot(fit2$x,fit2$y,type='1',xlab='x(t-1)',ylab='sigma', 

+ ylim=c(0,.01)) 

> title(main='(d) Estimate of sigma(.)') 

The following nonlinear models are derived with the help of nonparametric 

methods. 

4.1.6 Functional Coefficient AR Model 

Recent advances in nonparametric techniques enable researchers to relax parametric 
constraints in proposing nonlinear models. In some cases, nonparametric methods 
are used in a preliminary study to help select a parametric nonlinear model. This is 
the approach taken by Chen and Tsay (1993a) in proposing the functional coefficient 
autoregressive (FAR) model that can be written as 

xt = f\(Xt-\)xt-i + • • • + fp(Xt-i)xt-p + a„ (4.27) 

where Xt-\ = ..., xt-k)' is a vector of lagged values of xt. If necessary, 
Xf_i may also include other explanatory variables available at time t - 1. The 
functions /,(•) of Eq. (4.27) are assumed to be continuous, even twice differen¬ 
tiable, almost surely with respect to their arguments. Most of the nonlinear models 
discussed before are special cases of the FAR model. In application, one can use 
nonparametric methods such as kernel regression or local linear regression to esti¬ 
mate the functional coefficients /)(•), especially when the dimension of Xt-\ is low 
(e.g., Ar_i is a scalar). Recently, Cai, Fan, and Yao (2000) applied the local linear 
regression method to estimate /, (•) and showed that substantial improvements in 
1-step-ahead forecasts can be achieved by using FAR models. 

4.1.7 Nonlinear Additive AR Model 

A major difficulty in applying nonparametric methods to nonlinear time series anal¬ 
ysis is the “curse of dimensionality.” Consider a general nonlinear AR(p) process 
xt = f(xt-1, ..., xt-p) + a,. A direct application of nonparametric methods to esti¬ 
mate f (•) would require p-dimensional smoothing, which is hard to do when p is 
large, especially if the number of data points is not large. A simple, yet effective 
way to overcome this difficulty is to entertain an additive model that only requires 
lower dimensional smoothing. A time series xt follows a nonlinear additive AR 

NONLINEAR MODELS 

(NAAR) model if 

199 

p 

*t = Mt) + J2 fi(xt-i) + at, (4.28) 

i=i 

where the /,■(•) are continuous functions almost surely. Because each function /)(•) 
has a single argument, it can be estimated nonparametrically using one-dimensional 
smoothing techniques and hence avoids the curse of dimensionality. In application, 
an iterative estimation method that estimates /)(•) nonparametrically conditioned 
on estimates of /)(•) for all j ^ i is used to estimate a NAAR model; see Chen 
and Tsay (1993b) for further details and examples of NAAR models. 

The additivity assumption is rather restrictive and needs to be examined carefully 
in application. Chen, Liu, and Tsay (1995) consider test statistics for checking the 
additivity assumption. 

4.1.8 Nonlinear State-Space Model 

Making using of recent advances in MCMC methods (Gelfand and Smith, 1990), 
Carlin, Poison, and Stoffer (1992) propose a Monte Carlo approach for nonlinear 
state-space modeling. The model considered is 

St = ft(St-i) + ut, xt = gt(St) + vt, (4.29) 

where St is the state vector, /,(•) and g,(-) are known functions depending on some 
unknown parameters, {ut} is a sequence of iid multivariate random vectors with 
zero mean and nonnegative definite covariance matrix XM, {ut} is a sequence of 
iid random variables with mean zero and variance and {ut} is independent of 
{iy}. Monte Carlo techniques are employed to handle the nonlinear evolution of the 
state transition equation because the whole conditional distribution function of S, 
given St-i is needed for a nonlinear system. Other numerical smoothing methods 
for nonlinear time series analysis have been considered by Kitagawa (1998) and the 
references therein. MCMC methods (or computing-intensive numerical methods) 
are powerful tools for nonlinear time series analysis. Their potential has not been 
fully explored. However, the assumption of knowing /,(•) and gt(-) in model (4.29) 
may hinder practical use of the proposed method. A possible solution to overcome 
this limitation is to use nonparametric methods such as the analyses considered in 
FAR and NAAR models to specify /,(•) and g, (•) before using nonlinear state-space 
models. 

4.1.9 Neural Networks 

A popular topic in modem data analysis is neural networks, which can be classified 
as a semiparametric method. The literature on neural networks is enormous, and 
its application spreads over many scientific areas with varying degrees of success; 

200 

NONLINEAR MODELS AND THEIR APPLICATIONS 

O 
U 
T 
P 
U 
T 

Figure 4.8 Feed-forward neural network with one hidden layer for univariate time series analysis. 

see Section 2 of Ripley (1993) for a list of applications and Section 10 for remarks 
concerning its application in finance. Cheng and Titterington (1994) provide infor¬ 
mation on neural networks from a statistical viewpoint. In this subsection, we focus 
solely on the feed-forward neural networks in which inputs are connected to one 
or more neurons, or nodes, in the input layer, and these nodes are connected 
forward to further layers until they reach the output layer. Figure 4.8 shows an 
example of a simple feed-forward network for univariate time series analysis with 
one hidden layer. The input layer has two nodes, and the hidden layer has three. 
The input nodes are connected forward to each and every node in the hidden layer, 
and these hidden nodes are connected to the single node in the output layer. We call 
the network a 2-3-1 feed-forward network. More complicated neural networks, 
including those with feedback connections, have been proposed in the literature, 
but the feed-forward networks are most relevant to our study. 

Feed-Forward Neural Networks 

A neural network processes information from one layer to the next by an “activation 
function.” Consider a feed-forward network with one hidden layer. The yth node 
in the hidden layer is defined as 

hJ = fj «0 j + WUxi 

(4.30) 

where x, is the value of the ith input node, fj (•) is an activation function typically 
taken to be the logistic function 

fj (z) 

exp (z) 

1 + exp(z) ’ 

uoj is called the bias, the summation i —> j means summing over all input nodes 
feeding to j, and wu are the weights. For illustration, the jth node of the hidden 
layer of the 2-3-1 feed-forward network in Figure 4.8 is 

exp(o'o7- + w\jX\ + w2jx2) 

1 + exp(Q!oj + U>i jXi + W2jX2) ' 

j = 1,2,3. 

(4.31) 

NONLINEAR MODELS 

201 

For the output layer, the node is defined as 

o = fo gpo 4- F,  u)j0hj 

(4.32) 

where the activation function /0(-) is either linear or a Heaviside function. If f0(-) 
is linear, then 

k 

o = «0o + E wj°hj> 

;=i 

where A: is the number of nodes in the hidden layer. By a Heaviside function, 
we mean f0{z) = 1 if z > 0 and /0(z) = 0 otherwise. A neuron with a Heaviside 
function is called a threshold neuron, with 1 denoting that the neuron fires its 
message. For example, the output of the 2-3-1 network in Figure 4.8 is 

o = a Oo + w\0hi + w2oh2 + w2oh2, 

if the activation function is linear; it is 

o 

I 

if a0o + w\0h\ + w2oh2 + W20h2 > 0, 

0 

if a0o + w\0h\ + w2oh2 + w2oh2 < 0, 

if fo(•) is a Heaviside function. 

Combining the layers, the output of a feed-forward neural network can be writ¬ 

ten as 

fo 

o  + J2 wJ°fj a°j + 

WijXi 

(4.33) 

If one also allows for direct connections from the input layer to the output layer, 
then the network becomes 

0 = fo 

OtQo  + J20110X1 + J2 wjofj «oj + J2 WijXi 

J~*o 

(4.34) 

where the first summation is summing over the input nodes. When the activation 
function of the output layer is linear, the direct connections from the input nodes 
to the output node represent a linear function between the inputs and output. Con¬ 
sequently, in this particular case model (4.34) is a generalization of linear models. 

202 

NONLINEAR MODELS AND THEIR APPLICATIONS 

For the 2-3-1 network in Figure 4.8, if the output activation function is linear, 
then Eq. (4.33) becomes 

3 

o cxq0 -f- w jQh j, 

7=1 

where hj is given in Eq. (4.31). The network thus has 13 parameters. If Eq. (4.34) 
is used, then the network becomes 

2 3 

O = «0o + E OlioXi + ^ Wj0hj , 

1=1 7=1 

where again hj is given in Eq. (4.31). The number of parameters of the network 
increases to 15. 

We refer to the function in Eq. (4.33) or (4.34) as a semiparametric function 
because its functional form is known, but the number of nodes and their biases and 
weights are unknown. The direct connections from the input layer to the output 
layer in Eq. (4.34) mean that the network can skip the hidden layer. We refer to 
such a network as a skip-layer feed-forward network. 

Feed-forward networks are known as multilayer percetrons in the neural network 
literature. They can approximate any continuous function uniformly on compact sets 
by increasing the number of nodes in the hidden layer; see Homik, Stinchcombe, 
and White (1989), Hornik (1993), and Chen and Chen (1995). This property of neu¬ 
ral networks is the universal approximation property of the multilayer percetrons. 
In short, feed-forward neural networks with a hidden layer can be seen as a way 
to parameterize a general continuous nonlinear function. 

Training and Forecasting 

Application of neural networks involves two steps. The first step is to train the 
network (i.e., to build a network, including determining the number of nodes and 
estimating their biases and weights). The second step is inference, especially fore¬ 
casting. The data are often divided into two nonoverlapping subsamples in the 
training stage. The first subsample is used to estimate the parameters of a given 
feed-forward neural network. The network so built is then used in the second^sub¬ 
sample to perform forecasting and compute its forecasting accuracy. By comparing 
the forecasting performance, one selects the network that outperforms the others 
as the “best’' network for making inference. This is the idea of cross validation 
widely used in statistical model selection. Other model selection methods are also 
available. 

In a time series application, let {{r„ xt)\t = 1,..., T] be the available data for 
network training, where xt denotes the vector of inputs and rt is the series of 
interest (e.g., log returns of an asset). For a given network, let ot be the output of 

NONLINEAR MODELS 

203 

the network with input xt; see Eq. (4.34). Training a neural network amounts to 
choosing its biases and weights to minimize some fitting criterion—for example, 
the least squares 

s2 = 5z^t - °^2- 

/=l 

This is a nonlinear estimation problem that can be solved by several iterative meth¬ 
ods. To ensure the smoothness of the fitted function, some additional constraints 
can be added to the prior minimization problem. In the neural network literature, 
the back propagation (BP) learning algorithm is a popular method for network 
training. The BP method, introduced by Bryson and Ho (1969), works backward 
starting with the output layer and uses a gradient rule to modify the biases and 
weights iteratively. Appendix 2A of Ripley (1993) provides a derivation of back 
propagation. Once a feed-forward neural network is built, it can be used to compute 
forecasts in the forecasting subsample. 

Example 4.7. To illustrate applications of the neural network in finance, we 
consider the monthly log returns, in percentages and including dividends, for IBM 
stock from January 1926 to December 1999. We divide the data into two subsam¬ 
ples. The first subsample consisting of returns from January 1926 to December 
1997 for 864 observations is used for modeling. Using model (4.34) with three 
inputs and two nodes in the hidden layer, we obtain a 3-2-1 network for the 
series. The three inputs are rf_i,/y_2, and /y_3 and the biases and weights are 
given next: 

r, = 3.22 - 1.81/,(/•,_!) - 2.28/2(r,_i) - 0.09rt_i - 0.05r,_2 

0.12/7-3, 

(4.35) 

where r,_ 1 = {rt-\, rf_2, 17 _3) and the two logistic functions are 

exp(—8.34- 18.97r,_i + 2.17r,_2 - 19.17r,_3) 

1 + exp(-8.34- 18.97/7_i + 2.17r,_2 - 19.17rf_3)’ 

exp(39.25 - 22.17r;_i - 17.34r,_2 - 5.98rr_3) 

1 +exp(39.25 - 22.17r,_i - 17.34rf_2 - 5.98rf_3)' 

The standard error of the residuals for the prior model is 6.56. For comparison, we 
also built an AR model for the data and obtained 

rt = 1.101 + 0.077t7_i + at, oa = 6.61. (4.36) 

The residual standard error is slightly greater than that of the feed-forward model 
in Eq. (4.35). 

204 

NONLINEAR MODELS AND THEIR APPLICATIONS 

Forecast Comparison 

The monthly returns of IBM stock in 1998 and 1999 form the second subsample and 
are used to evaluate the out-of-sample forecasting performance of neural networks. 
As a benchmark for comparison, we use the sample mean of rt in the first subsample 
as the 1-step-ahead forecast for all the monthly returns in the second subsample. 
This corresponds to assuming that the log monthly price of IBM stock follows a 
random walk with drift. The mean squared forecast error (MSFE) of this benchmark 
model is 91.85. For the AR(1) model in Eq. (4.36), the MSFE of 1-step-ahead 
forecasts is 91.70. Thus, the AR(1) model slightly outperforms the benchmark. 
For the 3-2-1 feed-forward network in Eq. (4.35), the MSFE is 91.74, which is 
essentially the same as that of the AR(1) model. 

Remark. The estimation of feed-forward networks is done by using the nnet 
package of S-Plus with default starting weights; see Venables and Ripley (1999) 
for more information. Our limited experience shows that the estimation results 
vary. For the IBM stock returns used in Example 4.7, the out-of-sample MSE for 
a 3-2-1 network can be as low as 89.46 and as high as 93.65. If we change the 
number of nodes in the hidden layer, the range for the MSE becomes even wider. 
The S-Plus commands used in Example 4.7 are given in Appendix B. □ 

Example 4.8. Nice features of the feed-forward network include its flexibility 
and wide applicability. For illustration, we use the network with a Heaviside acti¬ 
vation function for the output layer to forecast the direction of price movement for 
IBM stock considered in Example 4.7. Define a direction variable as 

We use eight input nodes consisting of the first four lagged values of both rt and 
dt and four nodes in the hidden layer to build an 8-4-1 feed-forward network 
for dt in the first subsample. The resulting network is then used to compute the 
1-step-ahead probability of an “upward movement” (i.e., a positive return) for the 
following month in the second subsample. Figure 4.9 shows a typical output of 
probability forecasts and the actual directions in the second subsample with the 
latter denoted by circles. A horizontal line of 0.5 is added to the plot. If we take a 
rigid approach by letting d, = 1 if the probability forecast is greater than or equal to 
0.5 and dt = 0 otherwise, then the neural network has a successful rate of 0.58. The 
success rate of the network varies substantially from one estimation to another, and 
the network uses 49 parameters. To gain more insight, we did a simulation study of 
running the 8-4-1 feed-forward network 500 times and computed the number of 
errors in predicting the upward and downward movement using the same method 
as before. The mean and median of errors over the 500 runs are 11.28 and 11, 
respectively, whereas the maximum and minimum number of errors are 18 and 4. 

NONLINEARITY TESTS 

205 

Figure 4.9 One-step-ahead probability forecasts for positive monthly return for IBM stock using an 

8-4-1 feed-forward neural network. Forecasting period is from January 1998 to December 1999. 

For comparison, we also did a simulation with 500 runs using a random walk with 
drift—that is, 

1 if rt = 1.19+ ef > 0, 

0 otherwise, 

where 1.19 is the average monthly log return for IBM stock from January 1926 to 
December 1997 and {et} is a sequence of iid 77(0, 1) random variables. The mean 
and median of the number of forecast errors become 10.53 and 11, whereas the 
maximum and minimum number of errors are 17 and 5, respectively. Figure 4.10 
shows the histograms of the number of forecast errors for the two simulations. The 
results show that the 8-4-1 feed-forward neural network does not outperform the 
simple model that assumes a random walk with drift for the monthly log price of 
IBM stock. 

4.2 NONLINEARITY TESTS 

In this section, we discuss some nonlinearity tests available in the literature that 
have decent power against the nonlinear models considered in Section 4.1. The tests 
discussed include both parametric and nonparametric statistics. The Ljung-Box 

206 

NONLINEAR MODELS AND THEIR APPLICATIONS 

4 6 8 10 12 14 16 18 

Neural network 

6 8 10 12 14 16 

Random walk with a drift 

Figure 4.10 Histograms of number of forecasting errors for directional movements of monthly log 
returns of IBM stock. Forecasting period is from January 1998 to December 1999. 

statistics of squared residuals, the bispectral test, and the Brock, Dechert, and 
Scheinkman (BDS) test are nonparametric methods. The RESET test (Ramsey, 
1969), the F tests of Tsay (1986, 1989), and other Lagrange multiplier and like¬ 
lihood ratio tests depend on specific parametric functions. Because nonlinearity 
may occur in many ways, there exists no single test that dominates the others in 
detecting nonlinearity. 

4.2.1 Nonparametric Tests 

Under the null hypothesis of linearity, residuals of a properly specified linear model 
should be independent. Any violation of independence in the residuals indicates 
inadequacy of the entertained model, including the linearity assumption. This is 
the basic idea behind various nonlinearity tests. In particular, some of the nonlin¬ 
earity tests are designed to check for possible violation in quadratic forms of the 
underlying time series. 

Q-Statistic of Squared Residuals 

McLeod and Li (1983) apply the Ljung-Box statistics to the squared residuals of 
an ARMA(p, q) model to check for model inadequacy. The test statistic is 

NONLINEARITY TESTS 

207 

where T is the sample size, m is a properly chosen number of autocorrelations 
used in the test, a, denotes the residual series, and Pi(af) is the lag-i ACF of a2. 
If the entertained linear model is adequate, Q(m) is asymptotically a chi-squared 
random variable with m — p — q degrees of freedom. As mentioned in Chapter 
3, the prior ^-statistic is useful in detecting conditional heteroscedasticity of at 
and is asymptotically equivalent to the Lagrange multiplier test statistic of Engle 
(1982) for ARCH models; see Section 3.4.3. The null hypothesis of the test is 
Hq : Pi =■■• = Pm = 0, where Pi is the coefficient of a2_t in the linear regression 

a't = Po + P\a^-\ + • • ■ + Pma?-m + et 

for t = m + 1,..., T. Because the statistic is computed from residuals (not directly 
from the observed returns), the number of degrees of freedom is m — p — q. 

Bispectral Test 
This test can be used to test for linearity and Gaussianity. It depends on the result 
that a properly normalized bispectrum of a linear time series is constant over all 
frequencies and that the constant is zero under normality. The bispectrum of a time 
series is the Fourier transform of its third-order moments. For a stationary time 
series xt in Eq. (4.1), the third-order moment is defined as 

OO 

c(u,v) = g fkfk+ufk+v, (4.37) 

k=—oo 

where u and v are integers, g = E(a2), xpo — 1, and i/r* = 0 for k < 0. Taking 
Fourier transforms of Eq. (4.37), we have 

&3(uh, ^2) = -~T[-{wx + w2)]r(wi)r(w2), (4.38) 

where r(u;) — exp(—iwu) with i = T, and Wi are frequencies. Yet 
the spectral density function of xt is given by 

where w denotes the frequency. Consequently, the function 

P(w) = ^-\r(w)\2, 
2n 

b(w 1, w2) = ————— -- = constant for all (101, w2). (4.39) 

\b2(uh, w2)\2 

p(wi)p(w2)p(wi + w2) 

The bispectrum test makes use of the property in Eq. (4.39). Basically, it estimates 
the function b(w\,w2) in Eq. (4.39) over a suitably chosen grid of points and 
applies a test statistic similar to Hotelling’s T2 statistic to check the constancy of 
^(101, w2). For a linear Gaussian series, E(aj) — g — 0 so that the bispectrum is 
zero for all frequencies (uq; w2). For further details of the bispectral test, see Priest¬ 
ley (1988), Subba Rao and Gabr (1984), and Hinich (1982). Limited experience 
shows that the test has decent power when the sample size is large. 

208 

NONLINEAR MODELS AND THEIR APPLICATIONS 

BDS Statistic 
Brock, Dechert, and Scheinkman (1987) propose a test statistic, commonly referred 
to as the BDS test, to detect the iid assumption of a time series. The statistic is, 
therefore, different from other test statistics discussed because the latter mainly 
focus on either the second- or third-order properties of xt. The basic idea of the 
BDS test is to make use of a “correlation integral” popular in chaotic time series 
analysis. Given a /c-dimensional time series Xt and observations {Xt}J^j, define 
the correlation integral as 

Ck(S) = 

2 
lim - 
Tt—oo ^(7* - 0 

i<j 

(4.40) 

where I$(u, u) is an indicator variable that equals one if ||« — u|| <8, and zero 
otherwise, where || • || is the supnorm. The correlation integral measures the fraction 
of data pairs of {X,} that are within a distance of <5 from each other. Consider 
next a time series xt. Construct /c-dimensional vectors Xk = (xt, xt+i, ..., xt+k-\)r, 
which are called k histories. The idea of the BDS test is as follows. Treat a k 
history as a point in the ^-dimensional space. If {xt}J=l are indeed iid random 

variables, then the ^-histories should show no pattern in the ^-dimensional 
space. Consequently, the correlation integrals should satisfy the relation Ck(8) = 
[Ci((5)]*. Any departure from the prior relation suggests that xt are not iid. As 
a simple, but informative example, consider a sequence of iid random variables 
from the uniform distribution over [0, 1], Let [a, b] be a subinterval of [0, 1] and 
consider the “2-history” (xt, xt+\), which represents a point in the two-dimensional 
space. Under the iid assumption, the expected number of 2-histories in the subspace 
[a, b] x [a, b] should equal the square of the expected number of x, in [a, b]. This 
idea can be formally examined by using sample counterparts of correlation integrals. 
Define 

Q(8, T) = 

Tk(Tk - 1) 

J>(X*,X*), 

t = l,k. 

*</ 

where 7> = T — i + 1 and X* = x{ if i = 1 and X* = Xkt if t = k. Under the 
null hypothesis that {x,} are iid with a nondegenerated distribution function F(-), 
Brock, Dechert, and Scheinkman (1987) show that 

Ck(8, T) -> [Ci(<$)]A with probability 1, as T ->• oo 

for any fixed k and 8. Furthermore, the statistic Vf{Ck(8, T) - [Ci(<5, T)]*} is 
asymptotically distributed as normal with mean zero and variance: 

al(8) = 4 

+ 2 Nk~jClj + (k- 1 )2C2k - k2NC2k-2 j 

NONLINEARITY TESTS 

209 

where C = f[F(z + 8) — F(z — 8)]dF(z) and N = f[F(z + 8) — F(z — 8)]2 
dF(z). Note that Ci(<5, T) is a consistent estimate of C, and N can be consistently 
estimated by 

Ar(S, T) = 

6 

Tk(Tk - 1)(7* -2) 

t<s<u 

h(xt,xs)Is(xs,xu). 

The BDS test statistic is then defined as 

„ s Vf{Ck(8,T)-[Ci(8,T)]k} 
Dk(8, T) =- ' ^ U (4.41) 

crk(8, T) 

where crk(8, T) is obtained from ak{8) when C and N are replaced by C\{8, T) and 
N(8, T), respectively. This test statistic has a standard normal limiting distribution. 
For further discussion and examples of applying the BDS test, see Hsieh (1989) 
and Brock, Hsieh, and LeBaron (1991). In application, one should remove linear 
dependence, if any, from the data before applying the BDS test. The test may be 
sensitive to the choices of 8 and k, especially when k is large. 

4.2.2 Parametric Tests 

Turning to parametric tests, we consider the RESET test of Ramsey (1969) and its 
generalizations. We also discuss some test statistics for detecting threshold non¬ 
linearity. To simplify the notation, we use vectors and matrices in the discussion. 
If necessary, readers may consult Appendix A of Chapter 8 for a brief review on 
vectors and matrices. 

The RESET Test 
Ramsey (1969) proposes a specification test for linear least-squares regression anal¬ 
ysis. The test is referred to as a RESET test and is readily applicable to linear AR 
models. Consider the linear AR(p) model 

Xt = + at, (4.42) 

where Xt-\ — (1, jcf_i, ..., xt-p)' and 0 = (0o, </>i, • • •, 4>P)'- The first step of the 
RESET test is to obtain the least-squares estimate 0 of Eq. (4.42) and compute 
the fit xt — X'_j0, the residual at = xt — xt, and the sum of squared residuals 

SSRo = Ef.p+l where T is the sample size. In the second step, consider the 
linear regression 

at = X't_{a\ + M't_{a2 + vt, (4.43) 

where Mt~i = (x2,..., xf+1)' for some 5 > 1, and compute the least-squares resid¬ 
uals 

vt=dt- X't_{ai - 2 

210 

NONLINEAR MODELS AND THEIR APPLICATIONS 

and the sum of squared residuals SSRj = YfJ=p+1 °f ^e regressi°n- The basic 
idea of the RESET test is that if the linear AR(/?) model in Eq. (4.42) is adequate, 
then a i and 0C2 of Eq. (4.43) should be zero. This can be tested by the usual F 
statistic of Eq. (4.43) given by 

(SSRp-SSR Q/g 

SSR x/(T-p-g) 

with g = s + p + 1, 

(4.44) 

which, under the linearity and normality assumption, has an F distribution with 
degrees of freedom g and T — p — g. 

Remark. Because xf for k = 2,..., s + 1 tend to be highly correlated 
with Xt-i and among themselves, principal components of Mt-i that are not 
co-linear with Xt-\ are often used in fitting Eq. (4.43). Principal component 
analysis is a statistical tool for dimension reduction; see Chapter 8 for more 
information. □ 

Keenan (1985) proposes a nonlinearity test for time series that uses xf only and 
modifies the second step of the RESET test to avoid multicollinearity between xf 
and Xi—j. Specifically, the linear regression (4.43) is divided into two steps. In 
step 2(a), one removes linear dependence of xf on Xt_\ by fitting the regression 

xf = X't_}p + ut 

and obtaining the residual ut = xf — Xt-\ji. In step 2(b), consider the linear regres¬ 
sion 

a, = u,a + vt, 

and obtain the sum of squared residuals SSRi = Jfj=p+] ~ uta)2 = J2t=p+i vf 
to test the null hypothesis a = 0. 

The F Test 

To improve the power of Keenan’s test and the RESET test, Tsay (1986) uses a 
different choice of the regressor Specifically, he suggests using Mt-\ = 
vech(Xr_iX't_\), where vech(A) denotes the half-stacking vector of the matrix 
A using elements on and below the diagonal only; see Appendix B of Chapter 8 
for more information about the operator. For example, if p — 2, then Mt-\ = 
(xf_vxt-\xt-2, xf_2y. The dimension of M,_i is p(p + l)/2 for an AR(/?) model. 
In practice, the test is simply the usual partial F statistic for testing a = 0 in the 
linear least-squares regression 

xt — X't_\0 + M't_ ]Q! + et, 

where et denotes the error term. Under the assumption that xt is a linear AR(p) 
process, the partial F statistic follows an F distribution with degrees of freedom 

NONLINEARITY TESTS 

211 

g and T — p — g — 1, where g = p(p + l)/2. We refer to this F test as the Ori- 
F test. Luukkonen, Saikkonen, and Terasvirta (1988) further extend the test by 
augmenting M(_i with cubic terms xf_t for i = 1 

Threshold Test 

When the alternative model under study is a SETAR model, one can derive specific 
test statistics to increase the power of the test. One of the specific tests is the 
likelihood ratio statistic. This test, however, encounters the difficulty of undefined 
parameters under the null hypothesis of linearity because the threshold is undefined 
for a linear AR process. Another specific test seeks to transform testing threshold 
nonlinearity into detecting model changes. It is then interesting to discuss the 
differences between these two specific tests for threshold nonlinearity. 

To simplify the discussion, let us consider the simple case that the alterna¬ 
tive model is a 2-regime SETAR model with threshold variable xt-d■ The null 
hypothesis Hq: xt follows the linear AR(p) model 

p 

xt =(p0 + ^2 (piXt-i + at, 
i=1 

whereas the alternative hypothesis Ha\ xt follows the SETAR model 

x _ 10o1} + IXi <t>\l)xt-i + ait if xt-d < ru 
\ 0q2) + IXl <t>i2)xt-i + a2t if xt-d > r\, 

(4.45) 

(4.46) 

where r\ is the threshold. For a given realization {xt}J_, and assuming normality, 

let /o(0, of) be the log-likelihood function evaluated at the maximum-likelihood 
estimates of 0 = (0o, • • •, (ppY and o%. This is easy to compute. The likelihood 
function under the alternative is also easy to compute if the threshold ri is given. 
Let /i (rj; , (Tj2; 02, ct2) be the log-likelihood function evaluated at the maximum- 

likelihood estimates of 0; = (0q \ ..., (p^)' and of conditioned on knowing the 
threshold ri. The log-likelihood ratio l{r\) defined as 

Hji) = /i (ri; 0j, d,2; 02, d22) - /o(0, of) 

is then a function of the threshold ri, which is unknown. Yet under the null hypoth¬ 
esis, there is no threshold and r\ is not defined. The parameter r\ is referred to 
as a nuisance parameter under the null hypothesis. Consequently, the asymptotic 
distribution of the likelihood ratio is very different from that of the conventional 
likelihood ratio statistics. See Chan (1991) for further details and critical values of 

the test. A common approach is to use /max = suPu<n<M ^(ri) as tf*e test statistic, 
where v and u are prespecified lower and upper bounds of the threshold. Davis 
(1987) and Andrews and Ploberger (1994) provide further discussion on hypothesis 
testing involving nuisance parameters under the null hypothesis. Simulation is often 
used to obtain empirical critical values of the test statistic /max. which depends on 

212 

NONLINEAR MODELS AND THEIR APPLICATIONS 

the choices of v and u. The average of l{r\) over r\ e [v, u] is also considered by 
Andrews and Ploberger as a test statistic. 

Tsay (1989) makes use of arranged autoregression and recursive estimation to 
derive an alternative test for threshold nonlinearity. The arranged autoregression 
seeks to transfer the SETAR model under the alternative hypothesis Ha into a model 
change problem with the threshold r\ serving as the change point. To see this, the 
SETAR model in Eq. (4.46) says that xt follows essentially two linear models 
depending on whether xt-d < r\ or xt-d > r\. For a realization [xt}J=l, xt-d can 
assume values {xi,..., xj-d)- Let X(i) < x@) < • • • < X{j-d) be the ordered statis¬ 
tics of {xt}J~i (i.e., arranging the observations in increasing order). The SETAR 
model can then be written as 

p 

X(j)+d = fio T 'y fijX(j)+d—i T" a{j)+di j = !>•••! T d, (4.47) 

i=l 

where = <f>^ if X(p < r\ and = 0;U) if X(j) > r\. Consequently, the threshold 
r\ is a change point for the linear regression in Eq. (4.47), and we refer to Eq. (4.47) 
as an arranged autoregression (in increasing order of the threshold xt-d). Note that 
the arranged autoregression in (4.47) does not alter the dynamic dependence of xt 
on xt^i for / = 1,..., p because x^)+d still depends on x(j)+d-i for i = 1,..., p. 
What is done is simply to present the SETAR model in the threshold space instead 
of in the time space. That is, the equation with a smaller xt-d appears before that 
with a larger xt-d■ The threshold test of Tsay (1989) is obtained as follows. 

• Step 1. Fit Eq. (4.47) using j = l,... ,m, where m is a prespecified positive 
integer (e.g., 30). Denote the least-squares estimates of fy by where m 
denotes the number of data points used in estimation. 

• Step 2. Compute the predictive residual 

/V ^ V A 

a(m+\)+d = *(m+l)+d ~ Po,m ~ / y Pi,mX(m + l)+d-i 

p 

1 = 1 

and its standard error. Let e^m+\)+d be the standardized predictive residual. 

• Step 3. Use the recursive least-squares method to update the least-squares 

estimates to /3;>m+1 by incorporating the new data point jc(m+1)+rf. 

• Step 4. Repeat steps 2 and 3 until all data points are processed. 

• Step 5. Consider the linear regression of the standardized predictive residual 

p 

e{m+j)+d = «0 + ^ OCiX(m+j)+d-i + vt, j = l,..., T — d — m (4.48) 

1=1 

and compute the usual F statistic for testing at = 0 in Eq. (4.48) for i = 
0,..., p. Under the null hypothesis that xt follows a linear AR(p) model, 

NONLINEARITY TESTS 

213 

the F ratio has a limiting F distribution with degrees of freedom p + 1 and 
T — d — m — p. 

We refer to the earlier F test as a TAR-F test. The idea behind the test is that 
under the null hypothesis there is no model change in the arranged autoregression 
in Eq. (4.47) so that the standardized predictive residuals should be close to iid 
with mean zero and variance 1. In this case, they should have no correlations with 
the regressors X(m+j)+d-i. For further details including formulas for a recursive 
least-squares method and some simulation study on performance of the TAR-F 
test, see Tsay (1989). The TAR-F test avoids the problem of nuisance parameters 
encountered by the likelihood ratio test. It does not require knowing the threshold 
r\. It simply tests that the predictive residuals have no correlations with regressors 
if the null hypothesis holds. Therefore, the test does not depend on knowing the 
number of regimes in the alternative model. Yet the TAR-F test is not as powerful 
as the likelihood ratio test if the true model is indeed a 2-regime SETAR model 
with a known innovational distribution. 

4.2.3 Applications 

In this subsection, we apply some of the nonlinearity tests discussed previously to 
five time series. For a real financial time series, an AR model is used to remove 
any serial correlation in the data, and the tests apply to the residual series of the 
model. The five series employed are as follows: 

1. r\t: A simulated series of iid iV(0, 1) with 500 observations. 

2. rjt'- A simulated series of iid .Student-/ distribution with 6 degrees of freedom. 

The sample size is 500. 

3. ait: The residual series of monthly log returns of CRSP equal-weighted index 
from 1926 to 1997 with 864 observations. The linear AR model used is 

(1 - 0.1805 + 0.09951 2 3 4-0.10559)r3f = 0.0086 + a3t. 

4. The residual series of monthly log returns of CRSP value-weighted index 
from 1926 to 1997 with 864 observations. The linear AR model used is 

(1 - 0.0985 + O.lllfl3 - 0.088fl5 * * *)r4r = 0.0078 + a4t. 

5. a^t'- The residual series of monthly log returns of IBM stock from 1926 to 

1997 with 864 observations. The linear AR model used is 

(1 -0.077 B)r5t =0.011 +a5t. 

Table 4.2 shows the results of the nonlinearity test. For the simulated series and 
IBM returns, the F tests are based on an AR(6) model. For the index returns, the 

214 

NONLINEAR MODELS AND THEIR APPLICATIONS 

TABLE 4.2 Nonlinearity Tests for Simulated Series and Some Log Stock Returns0 

Data 

A (0,1) 

ln(ew) 

ln(vw) 

ln(ibm) 

Q 

(5) 

3.2 
0.9 
2.9 
1.0 
0.6 

Q 

(10) 

6.5 
1.7 
4.9 
9.8 
7.1 

d = 1 

2 

-0.32 
-0.87 
9.94 
8.61 
4.96 

Data 

Ori-F 

TAR-F 

2 

A (0,1) 

t6 
ln(ew) 

ln(vw) 

ln(ibm) 

1.13 
0.69 
5.05 
4.95 
1.32 

0.87 
0.81 
6.77 
6.85 
1.51 

-0.77 
-0.35 
10.01 
7.01 
3.82 

BDS(<$ = = 1-5 &a) 

3 

4 

-0.14 
-1.18 
11.72 
9.88 
6.09 

-0.15 
-1.56 
12.83 
10.70 
6.68 

BDS(<$  = °a) 

3 

4 

-0.71 
-0.76 
11.85 
7.83 
4.70 

-1.04 
-1.25 
13.14 
8.64 
5.45 

5 

-0.33 
-1.71 
13.65 
11.29 
6.82 

5 

-1.27 
-1.49 
14.45 
9.53 
5.72 

“The sample  size of simulated series  is 500 and that of stock returns  is 864. The  BDS test uses 
k = 2, 

AR order is the same as the model given earlier. For the BDS test, we chose S — 
oa and S = 1.5cra with k = 2,... ,5. Also given in the table are the Ljung-Box 
statistics that confirm no serial correlation in the residual series before applying 
nonlinearity tests. Compared with their asymptotic critical values, the BDS test and 
F tests are insignificant at the 5% level for the simulated series. However, the BDS 
tests are highly significant for the real financial time series. The F tests also show 
significant results for the index returns, but they fail to suggest nonlinearity in the 
IBM log returns. In summary, the tests confirm that the simulated series are linear 
and suggest that the stock returns are nonlinear. 

4.3 MODELING 

Nonlinear time series modeling necessarily involves subjective judgment. However, 
there are some general guidelines to follow. It starts with building an adequate lin¬ 
ear model on which nonlinearity tests are based. For financial time series, the 
Ljung-Box statistics and Engle’s test are commonly used to detect conditional 
heteroscedasticity. For general series, other tests of Section 4.2 apply. If nonlin¬ 
earity is statistically significant, then one chooses a class of nonlinear models to 
entertain. The selection here may depend on the experience of the analyst and the 
substantive matter of the problem under study. For volatility models, the order 
of an ARCH process can often be determined by checking the partial autocorre¬ 
lation function of the squared series. For GARCH and EGARCH models, only 
lower orders such as (1,1), (1,2), and (2,1) are considered in most applications. 

FORECASTING 

215 

Higher order models are hard to estimate and understand. For TAR models, one 
may use the procedures given in Tong (1990) and Tsay (1989, 1998) to build an 
adequate model. When the sample size is sufficiently large, one may apply non- 
parametric techniques to explore the nonlinear feature of the data and choose a 
proper nonlinear model accordingly; see Chen and Tsay (1993a) and Cai, Fan, and 
Yao (2000). The MARS procedure of Lewis and Stevens (1991) can also be used 
to explore the dynamic structure of the data. Finally, information criteria such as 
the Akaike information criterion (Akaike, 1974) and the generalized odd ratios in 
Chen, McCulloch, and Tsay (1997) can be used to discriminate between competing 
nonlinear models. The chosen model should be carefully checked before it is used 
for prediction. 

4.4 FORECASTING 

Unlike the linear model, there exist no closed-form formulas to compute forecasts 
of most nonlinear models when the forecast horizon is greater than 1. We use 
parametric bootstraps to compute nonlinear forecasts. It is understood that the model 
used in forecasting has been rigorously checked and is judged to be adequate for the 
series under study. By a model, we mean the dynamic structure and innovational 
distributions. In some cases, we may treat the estimated parameters as given. 

4.4.1 Parametric Bootstrap 

Let T be the forecast origin and l be the forecast horizon (£ > 0). That is, we 
are at time index T and interested in forecasting xj+i. The parametric bootstrap 
considered computes realizations xt+\,..., Xj+e sequentially by (a) drawing a 
new innovation from the specified innovational distribution of the model, and (b) 
computing xj+i using the model, data, and previous forecasts xj+\, ■ ■ ■,xj+i-i- 
This results in a realization for xj+t- The procedure is repeated M times to obtain 
M realizations of xt+i denoted by The point forecast of xr+e is then 

the sample average of x^+l. Let the forecast be Xj(£). We used M = 3000 in 

some applications and the results seem fine. The realizations {Xj+(]^=l can also 
be used to obtain an empirical distribution of Xj+i- We make use of this empirical 
distribution later to evaluate forecasting performance. 

4.4.2 Forecasting Evaluation 

There are many ways to evaluate the forecasting performance of a model, ranging 
from directional measures to magnitude measures to distributional measures. A 
directional measure considers the future direction (up or down) implied by the 
model. Predicting that tomorrow’s S&P 500 index will go up or down is an example 
of directional forecasts that are of practical interest. Predicting the year-end value 
of the daily S&P 500 index belongs to the case of magnitude measure. Finally, 

216 

NONLINEAR MODELS AND THEIR APPLICATIONS 

assessing the likelihood that the daily S&P 500 index will go up 10% or more 
between now and the year end requires knowing the future conditional probability 
distribution of the index. Evaluating the accuracy of such an assessment needs a 
distributional measure. 

In practice, the available data set is divided into two subsamples. The first sub¬ 
sample of the data is used to build a nonlinear model, and the second subsample 
is used to evaluate the forecasting performance of the model. We refer to the 
two subsamples of data as estimation and forecasting subsamples. In some stud¬ 
ies, a rolling forecasting procedure is used in which a new data point is moved 
from the forecasting subsample into the estimation subsample as the forecast origin 
advances. In what follows, we briefly discuss some measures of forecasting perfor¬ 
mance that are commonly used in the literature. Keep in mind, however, that there 
exists no widely accepted single measure to compare models. A utility function 
based on the objective of the forecast might be needed to better understand the 
comparison. 

Directional Measure 

A typical measure here is to use a 2 x 2 contingency table that summarizes the 
number of “hits” and “misses” of the model in predicting ups and downs of xt+i 
in the forecasting subsample. Specifically, the contingency table is given as 

Actual 

Predicted 

Up 

Down 

Up 

mu  mi2 
Down  m21  m2 2 

m0i  m02 

mjo 

m2 o 

m 

where m is the total number of f-step-ahead forecasts in the forecasting subsample, 
mn is the number of “hits” in predicting upward movements, m2\ is the number 
of “misses” in predicting downward movements of the market, and so on. Larger 
values in mu and m22 indicate better forecasts. The test statistic 

2 2 

= ££ 

(=1 j=i 

(mjj - mi0moj/mY 

miomoj/m 

can then be used to evaluate the performance of the model. A large x2 signifies that 
the model outperforms the chance of random choice. Under some mild conditions, 
X 2 has an asymptotic chi-squared distribution with 1 degree of freedom. For further 
discussion of this measure, see Dahl and Hylleberg (1999). 

For illustration of the directional measure, consider the 1-step-ahead probability 
forecasts of the 8-4-1 feed-forward neural network shown in Figure 4.9. The 
2x2 table of “hits” and “misses” of the network is 

FORECASTING 

217 

Actual 

Predicted 

Up 

Down 

Up  Down 

12 

8 

20 

2 

2 

4 

14 

10 

24 

The table shows that the network predicts the upward movement well, but fares 
poorly in forecasting the downward movement of the stock. The chi-squared statis¬ 
tic of the table is 0.137 with a p value of 0.71. Consequently, the network does 
not significantly outperform a random-walk model with equal probabilities for 
“upward” and “downward” movements. 

Magnitude Measure 

Three statistics are commonly used to measure performance of point forecasts. They 
are the mean squared error (MSE), mean absolute deviation (MAD), and mean 
absolute percentage error (MAPE). For ^-step-ahead forecasts, these measures are 
defined as 

^ m—1 

MSE(£) = - YUt+i+j - xT+j(l)]2, (4.49) 

m z—' 
7=0 

1 m — 1 

MAD(£) = - Y |XT+i+j - xT+j (£) |, (4.50) 

m 

7=0 

MAPE(-f) = - V | +J - 1 |, (4.51) 

1 ^ xT+i(l) 

m % XT+jU 

where m is the number of f-step-ahead forecasts available in the forecasting 
subsample. In application, one often chooses one of the above three measures, and 
the model with the smallest magnitude on that measure is regarded as the best i- 
step-ahead forecasting model. It is possible that different l may result in selecting 
different models. The measures also have other limitations in model comparison; 
see, for instance, Clements and Hendry (1993). 

Distributional Measure 
Practitioners recently began to assess forecasting performance of a model using 
its predictive distributions. Strictly speaking, a predictive distribution incorporates 
parameter uncertainty in forecasts. We call it conditional predictive distribution if 
the parameters are treated as fixed. The empirical distribution of xj+t obtained 
by the parametric bootstrap is a conditional predictive distribution. This empirical 
distribution is often used to compute a distributional measure. Let ut(£) be the 
percentile of the observed xj+i in the prior empirical distribution. We then have 

218 

NONLINEAR MODELS AND THEIR APPLICATIONS 

a set of m percentiles {ut+j(0}JZq , where again m is the number of l-step- 
ahead forecasts in the forecasting subsample. If the model entertained is adequate, 
{uT+j(t)} should be a random sample from the uniform distribution on [0, 1]. 
For a sufficiently large m, one can compute the Kolmogorov-Smimov statistic of 
{uT+j(t)} with respect to uniform [0, 1]. The statistic can be used for both model 
checking and forecasting comparison. 

4.5 APPLICATION 

In this section, we illustrate nonlinear time series models by analyzing the quarterly 
U.S. civilian unemployment rate, seasonally adjusted, from 1948 to 1993. This 
series was analyzed in detail by Montgomery et al. (1998). We repeat some of 
the analyses here using nonlinear models. Figure 4.11 shows the time plot of 
the data. Well-known characteristics of the series include that (a) it tends to move 
countercyclically with U.S. business cycles, and (b) the rate rises quickly but decays 
slowly. The latter characteristic suggests that the dynamic structure of the series is 
nonlinear. 

Denote the series by x, and let Ax, = x, — x,-\ be the change in unemployment 

rate. The linear model 

(1 -0.31fl4)(l -0.655)Ajq = (1 -0.7854M, a2 = 0.090 (4.52) 

Figure 4.11 Time plot of U.S. quarterly unemployment rate, seasonally adjusted, from 1948 to 1993. 

APPLICATION 

219 

was built by Montgomery et al. (1998), where the standard errors of the three 
coefficients are 0.11, 0.06, and 0.07, respectively. This is a seasonal model even 
though the data were seasonally adjusted. It indicates that the seasonal adjustment 
procedure used did not successfully remove the seasonality. This model is used as 
a benchmark model for forecasting comparison. 

To test for nonlinearity, we apply some of the nonlinearity tests of Section 4.2 
with an AR(5) model for the differenced series Ax,. The results are given 
in Table 4.3. All of the tests reject the linearity assumption. In fact, the 
linearity assumption is rejected for all AR(p) models we applied, where p = 2, 
..., 10. 

Using a modeling procedure similar to that of Tsay (1989), Montgomery et al. 

(1998) build the following TAR model for the Ax, series: 

f0.01 + 0.73Ax,_i + 0.10Ax,_2 + a\t if Ax,_2 < 0.1, 

10.18 + 0.80Ax,_i — 0.56 Ax, _2 + a2t otherwise. 

The sample variances of a\, and a2t are 0.76 and 0.165, respectively, the standard 
errors of the three coefficients of regime 1 are 0.03, 0.10, and 0.12, respectively, 
and those of regime 2 are 0.09, 0.1, and 0.16. This model says that the change in the 
U.S. quarterly unemployment rate, Ax,, behaves like a piecewise linear model in 
the reference space of x,_2 — x,_3 with threshold 0.1. Intuitively, the model implies 
that the dynamics of unemployment act differently depending on the recent change 
in the unemployment rate. In the first regime, the unemployment rate has had either 
a decrease or a minor increase. Here the economy should be stable, and essentially 
the change in the rate follows a simple AR(1) model because the lag-2 coefficient is 
insignificant. In the second regime, there is a substantial jump in the unemployment 
rate (0.1 or larger). This typically corresponds to the contraction phase in the 
business cycle. It is also the period during which government interventions and 
industrial restructuring are likely to occur. Here Ax, follows an AR(2) model with a 
positive constant, indicating an upward trend in x,. The AR(2) polynomial contains 
two complex characteristic roots, which indicate possible cyclical behavior in Ax,. 
Consequently, the chance of having a turning point in x, increases, suggesting 
that the period of large increases in x, should be short. This implies that the 
contraction phases in the U.S. economy tend to be shorter than the expansion 
phases. 

TABLE 4.3 Nonlinearity Test for Changes in the U.S. Quarterly Unemployment 
Rate: 1948.11-1993.IVfl 

Type 

Test 
p Value 

Ori-F 

2.80 
.0007 

LST 

2.83 
.0002 

TAR(l) 

TAR(2) 

TAR(3) 

TAR(4) 

2.41 
.0298 

2.16 
.0500 

2.84 
.0121 

2.98 
.0088 

“An AR(5) model was used in the tests, where LST denotes the test of Luukkonen et al. (1988) and 

TAR(cf) means threshold test with delay d. 

220 

NONLINEAR MODELS AND THEIR APPLICATIONS 

Applying a Markov chain Monte Carlo method, Montgomery et al. (1998) obtain 

the following Markov switching model for Ax,: 

Ax, — 

-0.07 + 0.38Ax,_i - 0.05Ax,_2 + 

0.16 4- 0.86Ax,_i — 0.38Ax/_2 -I- 62, 

if st = 1, 

if st = 2. 

(4.54) 

The conditional means of Ax, are —0.10 for st = 1 and 0.31 for st = 2. Thus, the 
first state represents the expansionary periods in the economy, and the second state 
represents the contractions. The sample variances of €\t and C2t are 0.031 and 0.192, 
respectively. The standard errors of the three parameters in state st = 1 are 0.03, 
0.14, and 0.11, and those of state st = 2 are 0.04, 0.13, and 0.14, respectively. The 
state transition probabilities are P(st = 2|s,_i = 1) = 0.084(0.060) and P(st = 
l|s,_i = 2) = 0.126(0.053), where the number in parentheses is the corresponding 
standard error. This model implies that in the second state the unemployment rate x, 
has an upward trend with an AR(2) polynomial possessing complex characteristic 
roots. This feature of the model is similar to the second regime of the TAR model 
in Eq. (4.53). In the first state, the unemployment rate x, has a slightly decreasing 
trend with a much weaker autoregressive structure. 

Forecasting Performance 
A rolling procedure was used by Montgomery et al. (1998) to forecast the unem¬ 
ployment rate x,. The procedure works as follows: 

1. Begin with forecast origin T — 83, corresponding to 1968.11, which was used 
in the literature to monitor the performance of various econometric models in 
forecasting unemployment rate. Estimate the linear, TAR, and MSA models 
using the data from 1948.1 to the forecast origin (inclusive). 

2. Perform 1-quarter to 5-quarter ahead forecasts and compute the forecast errors 
of each model. Forecasts of nonlinear models used are computed by using 
the parametric bootstrap method of Section 4.4. 

3. Advance the forecast origin by 1 and repeat the estimation and forecasting 

processes until all data are employed. 

4. Use MSE and mean forecast error to compare performance of the models. 

Table 4.4 shows the relative MSE of forecasts and mean forecast errors for the 
linear model in Eq. (4.52), the TAR model in Eq. (4.53), and the MSA model in 
Eq. (4.54), using the linear model as a benchmark. The comparisons are based on 
overall performance as well as the status of the U.S. economy at the forecast origin. 
From the table, we make the following observations: 

1. For the overall comparison, the TAR model and the linear model are very 
close in MSE, but the TAR model has smaller biases. Yet the MSA model 
has the highest MSE and smallest biases. 

APPLICATION 

221 

TABLE 4.4 Out-of-Sample Forecast Comparison among Linear, TAR, and MSA 
Models for the U.S. Quarterly Unemployment Rate" 

Model 

1-step 

2-step 

3-step 

4-step 

5-step 

Relative MSE of Forecast 

Linear 
TAR 
MSA 
MSE 

Linear 
TAR 
MSA 
MSE 

Linear 
TAR 
MSA 
MSE 

1.00 
1.00 
1.19 
0.08 

1.00 
0.85 
0.97 
0.22 

1.00 
1.06 
1.31 
0.06 

Overall Comparison 

1.00 
1.04 
1.39 
0.31 

1.00 
0.99 
1.40 
0.67 

1.00 
0.98 
1.45 
1.13 

Forecast Origins in Economic Contractions 

1.00 
0.91 
1.03 
0.97 

1.00 
0.83 
0.96 
2.14 

1.00 
0.72 
0.86 
3.38 

Forecast Origins i in Economic Expansions 

1.00 
1.13 
1.64 
0.21 

1.00 
1.10 
1.73 
0.45 

1.00 
1.15 
1.84 
0.78 

Mean of Forecast Errors 

1.00 
1.03 
1.61 
1.54 

1.00 
0.72 
1.02 
3.46 

1.00 
1.17 
1.87 
1.24 

Model 

1-step 

2-step 

3-step 

4-step 

5-step 

Linear 
TAR 
MSA 

Linear 
TAR 
MSA 

Linear 
TAR 
MSA 

Overall Comparison 

0.09 
-0.02 
-0.02 

0.17 
-0.03 
-0.04 

0.25 
-0.03 
-0.07 

Forecast Origins in Economic Contractions 

0.68 
0.56 
0.41 

1.08 
0.87 
0.57 

1.41 
1.01 
0.52 

Forecast Origins in Economic Expansions 

0.00 
-0.11 
-0.08 

0.03 
-0.17 
-0.13 

0.08 
-0.19 
-0.17 

0.03 
-0.10 
0.00 

0.31 
0.24 
0.20 

-0.01 
-0.05 
-0.03 

0.33 
-0.01 
-0.12 

1.38 
0.86 
0.14 

0.17 
-0.14 
-0.16 

°The starting forecast origin is 1968.11, where the row marked by MSE shows the MSE of the benchmark 

linear model. 

222 

NONLINEAR MODELS AND THEIR APPLICATIONS 

2. For forecast origins in economic contractions, the TAR model shows 
improvements over the linear model both in MSE and bias. The MSA model 
also shows some improvement over the linear model, but the improvement 

is not as large as that of the TAR model. 

3. For forecast origins in economic expansions, the linear model outperforms 

both nonlinear models. 

The results suggest that the contributions of nonlinear models over linear ones in 
forecasting the U.S. quarterly unemployment rate are mainly in the periods when 
the U.S. economy is in contraction. This is not surprising because, as mentioned 
before, it is during the economic contractions that government interventions and 
industrial restructuring are most likely to occur. These external events could intro¬ 
duce nonlinearity in the U.S. unemployment rate. Intuitively, such improvements 
are important because it is during the contractions that people pay more attention 
to economic forecasts. 

APPENDIX A: SOME RATS PROGRAMS FOR NONLINEAR 
VOLATILITY MODELS 

Program Used to Estimate an AR(2)-TAR-GARCH(1,1) Model for Daily Log 
Returns of IBM Stock 
Assume that the data file is d-ibmln03 . txt. 

all 0 10446:1 

open data d-ibmln03.txt 

data(org=obs) / rt 

set h = 0.0 

nonlin mu p2 aO al bl a2 b2 

frml at = rt(t)-mu-p2*rt(t-2) 

frml gvar = aO + al*at(t—1)**2+bl*h(t-l) $ 

+ % if(at(t — 1) < 0,a2*at(t-1)**2+b2*h(t-1),0) 

frml garchln = -0.5*log(h(t)=gvar(t))-0.5*at(t)**2/h(t) 

smpl 4 10446 

compute mu = 0.03, p2 = -0.03 

compute aO = 0.07, al = 0.05, a2 = 0.05, bl = 0.85, b2 = 0.05 

maximize(method=simplex,iterations=10) garchln 

smpl 4 10446 

maximize(method=bhhh,recursive,iterations=150) garchln 

set fv = gvar(t) 

set resid = at(t)/sqrt(fv(t)) 

set residsq = resid(t)*resid(t) 

cor(qstats,number=20,span=10) resid 

cor(qstats,number=20,span=10) residsq 

APPENDIX b: r AND S-PLUS commands for neural network 

223 

Program Used to Estimate a Smooth TAR Model for the Monthly Simple 
Returns of 3M Stock 

The data file is m-3m4608 . txt. 

all 0 755:1 

open data m-3m4608.txt 

data (org=obs) / date inmm 

set h = 0.0 

nonlin aO al a2 aOO all mu 

frml at = mmm(t) - mu 

frml varl = aO+al*at(t—1)**2+a2*at(t—2)**2 

frml var2 = a00+all*at(t—1)**2 

frml gvar = varl(t)+var2(t)/(1.0 + exp(-at(t-1)*1000.0) ) 

frml garchlog = -0.5*log(h(t)=gvar(t))-0.5*at(t)**2/h(t) 

smpl 3 623 

compute aO = .01, al = 0.2, a2 = 0.1 

compute aOO = .01, all = -.2, mu = 0.02 

maximize(method=bhhh,recursive,iterations=150) garchlog 

set fv = gvar(t) 

set resid = at(t)/sqrt(fv(t)) 

set residsq = resid(t)*resid(t) 

cor(qstats,number=20,span=10) resid 

cor(qstats,number=20,span=10) residsq 

APPENDIX B: R AND S-PLUS COMMANDS FOR NEURAL NETWORK 

The following commands are used in R or S-Plus to build the 3-2-1 skip-layer 
feed-forward network of Example 4.7. A line starting with # denotes a comment. 
The data file is m-ibmln. txt. The library used is nnet. 

# load the data into R or S-Plus workspace. 

x_scan(file='m-ibmln.txt') 

# select the output: r(t) 

y_x[4:864] 

# obtain the input variables: r(t-l), r(t-2), and r(t-3) 

ibm.x_cbind(x[3:863]_,x[2:862],x[1:861]) 

# build a 3-2-1 network with skip layer connections 

# and linear output. 

ibm.nn_nnet(ibm.x,y,size=2,linout=T,skip=T,maxit=10000, 

decay=le-2,reltol=le-7,abstol=le-7,range=l.0) 

# print the summary results of the network 

summary(ibm.nn) 

# compute \& print the residual sum of squares. 
sse_sum((y-predict(ibm.nn,ibm.x))*2) 

print(sse) 

224 

NONLINEAR MODELS AND THEIR APPLICATIONS 

#eigen(nnet.Hess(ibm.nn,ibm.x,y),T)$values 
# setup the input variables in the forecasting subsample 
ibm.p_cbind(x[8 6 4:8 8 7],x[863:886],x[862:885]) 
# compute the forecasts 
yh_predict(ibm.nn,ibm.p) 
# The observed returns in the forecasting subsample 
yo_x[865:888] 
# compute \& print the sum of squares of forecast errors 
ssfe_sum((yo-yh)A2) 
print(ssfe) 
# quit S-Plus or R 
q() 

EXERCISES 

4.1. Consider the daily simple returns of Johnson & Johnson stock from January 
1998 to December 2008. The data are in the file d-jnj9808.txt or can be 
obtained from CRSP. Convert the returns into log returns in percentage, (a) 
Build a GJR model for the log return series. Write down the fitted model. Is 
the leverage effect significant at the 1% level? (b) Build a general threshold 
volatility model for the log return series, (c) Compare the two TGARCH 
models. 

4.2. Consider the monthly simple returns of General Electric (GE) stock from 
January 1926 to December 2008 with 996 observations. You may download 
the data from CRSP or use the file m-ge2 6 0 8.txt on the Web. Convert 
the returns into log returns in percentages. Build a TGARCH model with 
GED innovations for the series using at-\ as the threshold variable with zero 
threshold, where at-\ is the shock at time t — 1. Write down the fitted model. 
Is the leverage effect significant at the 5% level? 

4.3. Suppose that the monthly log returns of GE stock, measured in percentages, 
follow a smooth threshold IGARCH(1,1) model. For the sampling period from 
January 1926 to December 2008, the fitted model is 

r, = 1.14 + at, at = at€t 

at = 0.119af2_j + 0.881or2_ + _-J_--(4.276 - 0.084a2_1), 

1 + exp(-10af_i) 

where all of the estimates are highly significant, the coefficient 10 in the 
exponent is fixed a priori to simplify the estimation, and {e,} are iid N(0, 1). 
Assume that a996 = —5.06 and crc2% = 50.5. What is the 1-step-ahead volatility 
forecast ct996(1)? Suppose instead that a996 = 5.06. What is the 1-step-ahead 
volatility forecast cf996(l)? 

EXERCISES 

225 

4.4. Suppose that the monthly log returns, in percentages, of a stock follow the 

following Markov switching model: 

r, = 1.25 + at, at=at€t, 

0.10af2_j + 0.93o-f2_j if st = 1, 

4.24 + 0.10a2_j + if st = 2, 

where the transition probabilities are 

P(st = 2|sr_! = 1) = 0.15, P(st = l|5f_i = 2) = 0.05. 

Suppose that <2100 = 6.0, crj^Q = 50.0, and S100 = 2 with probability 1.0. What 
is the 1-step-ahead volatility forecast at the forecast origin t = 100? Also, if 
the probability of s'ioo = 2 is reduced to 0.8, what is the 1-step-ahead volatility 
forecast at the forecast origin t = 100? 

4.5. Consider the monthly simple returns of GE stock from January 1926 to Decem¬ 

ber 2008. Use the last three years of data for forecasting evaluation. 

(a) Using lagged returns rf_ 1, rt-2, rt_3 as input, build a 3-2-1 feed-forward 
network to forecast 1-step-ahead returns. Calculate the mean squared error 
of forecasts. 

(b) Again, use lagged returns rt-\, rt-2, rf_3 and their signs (directions) to 
build a 6-5-1 feed-forward network to forecast the 1-step ahead direction 
of GE stock price movement with 1 denoting upward movement. Calculate 
the mean squared error of forecasts. 
Note: Let rtn denote a time series in R or S-Plus. To create a direction 
variable for rtn, use the command 

drtn = ifelse(rtn > 0, 1, 0) 

4.6. Because of the existence of inverted yield curves in the term structure of 
interest rates, the spread of interest rates should be nonlinear. To verify this, 
consider the weekly U.S. interest rates of (a) Treasury 1-year constant maturity 
rate and (b) Treasury 3-year constant maturity rate. As in Chapter 2, denote 
the two interest rates by r\t and r3(, respectively, and the data span is from 
January 5, 1962, to April 10, 2009. The data are in files w-gs3yr.txt and 
w-gslyr.txt on the Web and can be obtained from the Federal Reserve 
Bank of St. Louis. 

(a) Let st — r-it — r\t be the spread in log interest rates. Is {^} linear? Perform 
some nonlinearity tests and draw the conclusion using the 5% significance 

level. 

226 

NONLINEAR MODELS AND THEIR APPLICATIONS 

(b) Let s* = (>3? — r-i^t-x) — (r\, — r— st — st-\ be the change in inter¬ 
est rate spread. Is {s*} linear? Perform some nonlinearity tests and draw 
the conclusion using the 5% significance level. 

(c) Build a threshold model for the s, series and check the fitted model. 

(d) Build a threshold model for the s* series and check the fitted model. 

REFERENCES 

Akaike, H. (1974). A new look at the statistical model identification. IEEE Transactions on 

Automatic Control AC-19: 716-723. 

Andrews, D. W. K. and Ploberger, W. (1994). Optimal tests when a nuisance parameter is 

present only under the alternative. Econometrica 62: 1383-1414. 

Brock, W., Dechert, W. D., and Scheinkman, J. (1987). A test for independence based on 
the correlation dimension. Working paper, Department of Economics, University of 
Wisconsin, Madison. 

Brock, W., Hsieh, D. A., and LeBaron, B. (1991). Nonlinear Dynamics, Chaos and Insta¬ 

bility: Statistical Theory and Economic Evidence. MIT Press, Cambridge, MA. 

Bryson, A. E. and Ho, Y. C. (1969). Applied Optimal Control. Blaisdell, New York 

Cai, Z., Fan, J., and Yao, Q. (2000). Functional-coefficient regression models for nonlinear 

time series. Journal of the American Statistical Association 95: 941-956. 

Carlin, B. P., Poison, N. G., and Staffer, D. S. (1992). A Monte Carlo approach to nonnormal 
and nonlinear state space modeling. Journal of the American Statistical Association 87: 
493-500. 

Chan, K. S. (1991). Percentage points of likelihood ratio tests for threshold autoregression. 

Journal of the Royal Statistical Society Series B 53: 691-696. 

Chan, K. S. (1993). Consistency and limiting distribution of the least squares estimator of a 

threshold autoregressive model. Annals of Statistics 21: 520-533. 

Chan, K. S. and Tong, H. (1986). On estimating thresholds in autoregressive models. Journal 

of Time Series Analysis 7: 179-190. 

Chan, K. S. and Tsay, R. S. (1998). Limiting properties of the conditional least squares 

estimator of a continuous TAR model. Biometrika 85: 413-426. 

Chen, C., McCulloch, R. E., and Tsay, R. S. (1997). A unified approach to estimating and 
modeling univariate linear and nonlinear time series. Statistica Sinica 7: 451-472. 

Chen, R. and Tsay, R. S. (1991). On the ergodicity of TAR(l) processes. Annals of Applied 

Probability 1: 613-634. 

Chen, R. and Tsay, R. S. (1993a). Functional-coefficient autoregressive models. Journal of 

the American Statistical Association 88: 298-308. 

Chen, R. and Tsay, R. S. (1993b). Nonlinear additive ARX models. Journal of the American 

Statistical Association 88: 955-967. 

Chen, R., Liu, J., and Tsay, R. S. (1995). Additivity tests for nonlinear autoregressive models. 

Biometrika (1995) 82: 369-383. 

Chen, T. and Chen, H. (1995). Universal approximation to nonlinear operators by neural 
networks with arbitrary activation functions and its application to dynamical systems. 
IEEE Transactions on Neural Networks 6: 911-917. 

REFERENCES 

227 

Cheng, B. and Titterington, D. M. (1994). Neural networks: A review from a statistical 

perspective. Statistical Science 9: 2-54. 

Clements, M. P. and Hendry, D. F. (1993). On the limitations of comparing mean square 

forecast errors. Journal of Forecasting 12: 617-637. 

Cleveland, W. S. (1979). Robust locally weighted regression and smoothing scatterplots. 

Journal of the American Statistical Association 74: 829-836. 

Dahl, C. M. and Hylleberg, S. (1999). Specifying nonlinear econometric models by flexible 
regression models and relative forecast performance. Working paper. Department of 
Economics, University of Aarhus, Denmark. 

Davis, R. B. (1987). Hypothesis testing when a nuisance parameter is present only under 

the alternative. Biometrika 74: 33-43. 

Engle, R. F. (1982). Autoregressive conditional heteroscedasticity with estimates of the 

variance of United Kingdom inflations. Econometrica 50: 987-1007. 

Epanechnikov, V. (1969). Nonparametric estimates of a multivariate probability density. 

Theory of Probability and Its Applications 14: 153-158. 

Fan, J. (1993). Local linear regression smoother and their minimax efficiencies. Annals of 

Statistics 21: 196-216. 

Fan, J. and Yao, Q. (2003). Nonlinear Time Series: Nonparametric and Parametric Methods. 

Springer, New York. 

Gelfand, A. E. and Smith, A. F. M. (1990). Sampling-based approaches to calculating 
marginal densities. Journal of the American Statistical Association 85: 398-409. 

Granger, C. W. J. and Andersen, A. P. (1978). An Introduction to Bilinear Time Series 

Models. Vandenhoek and Ruprecht, Gottingen. 

Hamilton, J. D. (1989). A new approach to the economic analysis of nonstationary time 

series and the business cycle. Econometrica 57: 357-384. 

Hamilton, J. D. (1990). Analysis of time series subject to changes in regime. Journal of 

Econometrics 45: 39-70. 

Hamilton, J. D. (1994). Time Series Analysis. Princeton University Press, Princeton, NJ. 

Hansen, B. E. (1997). Inference in TAR models. Studies in Nonlinear Dynamics and Econo¬ 

metrics 1: 119-131. 

Hardle, W. (1990). Applied Nonparametric Regression. Cambridge University Press, New 

York. 

Hinich, M. (1982). Testing for Gaussianity and linearity of a stationary time series. Journal 

of Time Series Analysis 3: 169-176. 

Homik, K. (1993). Some new results on neural network approximation. Neural Networks 6: 

1069-1072. 

Homik, K., Stinchcombe, M., and White, H. (1989). Multilayer feedforward networks are 

universal approximators. Neural Networks 2: 359-366. 

Hsieh, D. A. (1989). Testing for nonlinear dependence in daily foreign exchange rates. 

Journal of Business 62: 339-368. 

Keenan, D. M. (1985). A Tukey non-additivity-type test for time series nonlinearity. 

Biometrika 72: 39-44. 

Kitagawa, G. (1998). A self-organizing state space model. Journal of the American Statistical 

Association 93: 1203-1215. 

228 

NONLINEAR MODELS AND THEIR APPLICATIONS 

Lewis, P. A. W. and Stevens, J. G. (1991). Nonlinear modeling of time series using multivari¬ 
ate adaptive regression spline (MARS). Journal of the American Statistical Association 
86: 864-877. 

Liu, J. and Brockwell, P. J. (1988). On the general bilinear time-series model. Journal of 

Applied Probability 25: 553-564. 

Luukkonen, R., Saikkonen, P., and Terasvirta (1988). Testing linearity against smooth tran¬ 

sition autoregressive models. Biometrika 75: 491-499. 

McCulloch, R. E. and Tsay, R. S. (1993). Bayesian inference and prediction for mean 
and variance shifts in autoregressive time series. Journal of the American Statistical 
Association 88: 968-978. 

McCulloch, R. E. and Tsay, R. S. (1994). Statistical inference of macroeconomic time series 

via Markov switching models. Journal of Time Series Analysis 15: 523-539. 

McLeod, A. I. and Li, W. K. (1983). Diagnostic checking ARMA time series models using 

squared-residual autocorrelations. Journal of Time Series Analysis 4: 269-273. 

Montgomery, A. L., Zarnowitz, V., Tsay, R. S., and Tiao, G. C. (1998). Forecasting the U.S. 
unemployment rate, Journal of the American Statistical Association 93: 478-493. 

Nadaraya, E. A. (1964). On estimating regression. Theory and Probability Application 10: 

186-190. 

Petruccelli, J. and Woolford, S. W. (1984). A threshold AR(1) model. Journal of Applied 

Probability 21: 270-286. 

Potter, S. M. (1995). A nonlinear approach to U.S. GNP. Journal of Applied Econometrics 

10: 109-125. 

Priestley, M. B. (1980). State-dependent models: A general approach to nonlinear time series 

analysis. Journal of Time Series Analysis 1: 47-71. 

Priestley, M. B. (1988). Non-linear and Non-stationary Time Series Analysis, Academic 

Press, London, UK. 

Ramsey, J. B. (1969). Tests for specification errors in classical linear least squares regression 

analysis. Journal of the Royal Statistical Society Series B 31: 350-371. 

Ripley, B. D. (1993). Statistical aspects of neural networks. In O. E. Bamdorff-Nielsen, J. 
L. Jensen, and W. S. Kendall (eds.). Networks and Chaos—Statistical and Probabilistic 
Aspects, pp. 40-123. Chapman and Hall, London, UK. 

Subba Rao, T. and Gabr, M. M. (1984). An Introduction to Bispectral Analysis and Bilinear 

Time Series Models, Lecture Notes in Statistics, vol. 24. Springer, New York. 

Terasvirta, T. (1994). Specification, estimation, and evaluation of smooth transition autore¬ 

gressive models. Journal of the American Statistical Association 89: 208-218. 

Tiao, G. C. and Tsay, R. S. (1994). Some advances in nonlinear and adaptive modeling in 

time series. Journal of Forecasting 13: 109-131. 

Tong, H. (1978). On a threshold model. In C. H. Chen (ed.). Pattern Recognition and Signal 

Processing. Sijhoff & Noordhoff, Amsterdam. 

Tong, H. (1983). Threshold Models in Nonlinear Time Series Analysis, Lecture Notes in 

Statistics, Springer, New York. 

Tong, H. (1990). Non-Linear Time Series: A Dynamical System Approach, Oxford University 

Press, Oxford, UK. 

Tsay, R. S. (1986). Nonlinearity tests for time series. Biometrika 73: 461-466. 

REFERENCES 

229 

Tsay, R. S. (1989). Testing and modeling threshold autoregressive processes. Journal of the 

American Statistical Association 84: 231-240. 

Tsay, R. S. (1998). Testing and modeling multivariate threshold models. Journal of the 

American Statistical Association 93: 1188-1202. 

Venables, W. N. and Ripley, B. D. (1999). Modern Applied Statistics with S-Plus, 3rd edn. 

Springer, New York. 

Watson, G. S. (1964). Smooth regression analysis. Sankhya Series A 26: 359-372. 

CHAPTER 5 

High-Frequency Data Analysis 
and Market Microstructure 

High-frequency data are observations taken at fine time intervals. In finance, they 
often mean observations taken daily or at a finer time scale. These data have 
become available primarily due to advances in data acquisition and processing 
techniques, and they have attracted much attention because they are important in 
empirical study of market microstructure and realized volatility. The ultimate high- 
frequency data in finance are the transaction-by-transaction or trade-by-trade data in 
security markets. Here time is often measured in seconds. The Trades and Quotes 
(TAQ) database of the New York Stock Exchange (NYSE) contains all equity 
transactions reported on the Consolidated Tape from 1992 to the present, which 
includes transactions on the NYSE, AMEX, NASDAQ, and the regional exchanges. 
The Berkeley Options Data Base provides similar data for options transactions 
from August 1976 to December 1996. More high-frequency options data are also 
available; see the website of Chicago Board Options Exchange. Transactions data 
for many other securities and markets, both domestic and foreign, are continuously 
collected and processed. Wood (2000) provides some historical perspective of high- 
frequency financial study. 

High-frequency financial data are important in studying a variety of issues related 
to the trading process and market microstructure. They can be used to compare the 
efficiency of different trading systems in price discovery (e.g., the open out-cry 
system of the NYSE and the computer trading system of NASDAQ). They can 
also be used to study the dynamics of bid-and-ask quotes of a particular stock (e.g., 
Hasbrouck, 1999; Zhang, Russell, and Tsay, 2008). In an order-driven stock market 
(e.g., the Taiwan Stock Exchange), high-frequency data can be used to study the 
order dynamics and, more interesting, to investigate the question of “who provides 
the market liquidity.” Cho, Russell, Tiao, and Tsay (2003) use intraday 5-minute 
returns of more than 340 stocks traded on the Taiwan Stock Exchange to study the 

Analysis of Financial Time Series, Third Edition, By Ruey S. Tsay 
Copyright © 2010 John Wiley & Sons, Inc. 

231 

232 

HIGH-FREQUENCY DATA ANALYSIS AND MARKET MICROSTRUCTURE 

impact of daily stock price limits and find significant evidence of magnet effects 
toward the price ceiling. 

However, high-frequency data have some unique characteristics that do not 
appear in lower frequencies. Analysis of these data thus introduces new challenges 
to financial economists and statisticians. In this chapter, we study these special 
characteristics, consider methods for analyzing high-frequency data, and discuss 
implications of the results obtained. In particular, we discuss nonsynchronous 
trading, bid-ask spread, duration models, price movements, and bivariate mod¬ 
els for price changes and time durations between transactions associated with price 
changes. The models discussed are also applicable to other scientific areas such as 
telecommunications and environmental studies. 

5.1 NONSYNCHRONOUS TRADING 

We begin with nonsynchronous trading. Stock tradings such as those on the NYSE 
do not occur in a synchronous manner; different stocks have different trading 
frequencies, and even for a single stock the trading intensity varies from hour to 
hour and from day to day. Yet we often analyze a return series in a fixed time 
interval such as daily, weekly, or monthly. For daily series, the price of a stock is 
its closing price, which is the last transaction price of the stock in a trading day. The 
actual time of the last transaction of the stock varies from day to day. As such we 
incorrectly assume daily returns as an equally spaced time series with a 24-hour 
interval. It turns out that such an assumption can lead to erroneous conclusions 
about the predictability of stock returns even if the true return series are serially 
independent. 

For daily stock returns, nonsynchronous trading can introduce (a) lag-1 cross 
correlation between stock returns, (b) lag-1 serial correlation in a portfolio return, 
and (c) in some situations negative serial correlations of the return series of a single 
stock. Consider stocks A and B. Assume that the two stocks are independent, and 
stock A is traded more frequently than stock B. For special news affecting the 
market that arrives near the closing hour on one day, stock A is more likely than 
B to show the effect of the news on the same day simply because A is traded 
more frequently. The effect of the news on B will eventually appear, but it may be 
delayed until the following trading day. If this situation indeed happens, return of 
stock A appears to lead that of stock B. Consequently, the return series may show 
a significant lag-1 cross correlation from A to B even though the two stocks are 
independent. For a portfolio that holds stocks A and B, the prior cross correlation 
would become a significant lag-1 serial correlation. 

In a more complicated manner, nonsynchronous trading can also induce erro¬ 
neous negative serial correlations for a single stock. There are several models 
available in the literature to study this phenomenon; see Campbell, Fo, and MacKin- 
lay (1997) and the references therein. Here we adopt a simplified version of the 
model proposed in Fo and MacKinlay (1990). Fet rt be the continuously com¬ 
pounded return of a security at the time index t. For simplicity, assume that {r?} 

NONSYNCHRONOUS TRADING 

233 

is a sequence of independent and identically distributed random variables with 
mean E(rt) = /x and variance Var(r,) — a2. For each time period, the probabil¬ 
ity that the security is not traded is jt, which is time invariant and independent 
of rt. Let r° be the observed return. When there is no trade at time index t, we 
have r° = 0 because there is no information available. Yet when there is a trade at 
time index t, we define r° as the cumulative return from the previous trade (i.e., 
rf = rt + rt-\ + • • • + rt-k,, where kt is the largest nonnegative integer such that 
no trade occurred in the periods t - kt, t - kt + 1, ..., t - 1). Mathematically, the 
relationship between rt and r° is 

0 

rt 

with probability jt 

with probability (1 — jt)2 

rt + rt-1 

with probability (1 — jt)2jt 

rt +/-,_!+ rf_ 2  with probability (1 — tt)2tt2 

E,-=o rt-i 

with probability (1 — n)27Tk 

These probabilities are easy to understand. For example, r° = rt if and only if there 
are trades at both t and t — 1, r° = rt + rt-\ if and only if there are trades at t 
and t — 2, but no trade at t — 1, and r° = rt + rt-\ + 2 if and only if there are 
trades at t and t — 3, but no trades at r — 1 and t — 2, and so on. As expected, the 
total probability is 1 given by 

7T + (1 — 7r)2(l + Jt + n2 + ■ • •) = 7T + (1 — 7r)2- = 7T + 1— 7T = 1. 

1 — 71 

We are ready to consider the moment equations of the observed return series 

{r°}. First, the expectation of r° is 

E{r°) = (1 - ic)2E(rt) + (1 - 7x)2ttE(rt + r,_0 + • • • 

= (1 — tt)2 fi + (1 — 7r)27r2/x + (1 — 7r)27r23/x + • • • 

= (1 — 7r)2/U,(l + 27r + 3n2 + 47r3 + • • •) 

= (1 ~7r)Vn 1 \2 ~ (5-2) 

(1 -ny 

In the prior derivation, we use the result 1 + 27r + 3;r2 + An2 H-= 1/(1 — jt)2. 
Next, for the variance of rf, we use Var(r°) = E[(r°)2] — [E{r°)]2 and 

E(r°f = (1 - 7t)2E[(rt)2\ + (1 - 7ifnE[{rt + r^)2} + • • • 

= (1 - 7T)2[(cr2 + fx2) + Tt(2o2 + 4/x2) + ji2(3a2 + 9/x2) 4-] (5.3) 

234 

HIGH-FREQUENCY DATA ANALYSIS AND MARKET MICROSTRUCTURE 

— (1 — n)~[cr^ (1 + In + 3 n" + •••) + /z (1 + 477 + 9n + •••)] (5-4) 

a2 + ^2 

2 

1 — n 

1 

In Eq. (5.3), we use 

(5.5) 

=Var(Xr'-') + 

U=0 

E (Er- 

1=0 

w= 0 

-.2 

(k+l)a2 + [(k+\)n]2 

under the serial independence assumption of rt. Using techniques similar to that 
of Eq. (5.2), we can show that the first term of Eq. (5.4) reduces to a2. For the 
second term of Eq. (5.4), we use the identity 

1 + 477 + 9n~ + 167T3 H-= —--3 - —--j, 
(1 — ny (1 — n)z 

which can be obtained as follows. Let 

H = 1 + 477 -f 9n2 + I6773 H- and G = 1 + 377 + 5n2 + In3 + • • •. 

Then (1 — n)H = G and 

(1 — n)G =1+2 77 + 2n2 + 2n3 + ••• 

= 2(1+71 + n~ + • ■ •) — 1 =-1. 

1 — n 

Consequently, from Eqs. (5.2) and (5.5), we have 

Var (rf) = cr2 + y2 f —?■-l) - y2 = a2 + ■ (5.6) 

\ 1 — n J 1 — n 

Consider next the lag-1 autocovariance of {r°}. Here we use Cov(r°,r°_x) = 
E(r°r°_ 1) — E{r®)E(r°_x) = E{r°r°_t) — y2. The question then reduces to finding 
E(r°r°_x). Notice that r°r°_x is zero if there is no trade at 7, no trade at 7 — 1, or 
no trade at both 7 and 7 — 1. Therefore, we have 

0 

7(6-1 

with probability 2n — n2 

with probability (1 — 77)3 

rt(rt-1 + r,_2) 

with probability (1 — 77)3 77 

rt(rt-\ + r,_2 + r,_3) 

with probability (1 — 77)3 77 2 

r°r° 
't' t- 

(5.7) 

7((Ef=lG-l) 

with probability (1 — 7r)37rfc_1 

BID-ASK SPREAD 

235 

Again the total probability is unity. To understand the prior result, notice that 
rtrt-i = rtrt-1 if and only if there are three consecutive trades at t — 2, t — 1, and 
t. Using Eq. (5.7) and the fact that E(rtrt-j) = E(r,)E(r,-j) = /i2 for j > 0, we 
have 

E(rfrf_ i) 

= (1 - tt)3  E{rtrt_ i) + tv E[rt{rt^\ + r?_2)] + rt2E  n  E 

n-i 

+ 

w'=l 

— (1 — 7T)3/jL2(\ T 2tv + 3jt~ H-) — (1 — tc)ii2. 

The lag-1 autocovariance of {r°} is then 

Cov(rf, r°_x) = -tt/i2. (5.8) 

Provided that fi is not zero, the nonsynchronous trading induces a negative lag-1 
autocorrelation in r° given by 

, -(l-7r)7r^2 

) = -r~i T o—2 • 
(1 — 7t)ol + 2 Tl IJL1 

In general, we can extend the prior result and show that 

Co\(r°, r°_j) = -/x2Tt], j > 1. 

The magnitude of the lag-1 ACF depends on the choices of /x, tt, and a and can 
be substantial. Thus, when /x^O, the nonsynchronous trading induces negative 
autocorrelations in an observed security return series. 

The previous discussion can be generalized to the return series of a portfolio 
that consists of N securities; see Campbell et al. (1997, Chapter 3). In the time 
series literature, effects of nonsynchronous trading on the return of a single security 
are equivalent to that of random temporal aggregation on a time series, with the 
trading probability re governing the mechanism of aggregation. 

5.2 BID-ASK SPREAD 

In some stock exchanges (e.g., NYSE), market makers play an important role in 
facilitating trades. They provide market liquidity by standing ready to buy or sell 
whenever the public wishes to buy or sell. By market liquidity, we mean the ability 
to buy or sell significant quantities of a security quickly, anonymously, and with 
little price impact. In return for providing liquidity, market makers are granted 
monopoly rights by the exchange to post different prices for purchases and sales of 
a security. They buy at the bid price Pb and sell at a higher ask price Pa. (For the 

236 

HIGH-FREQUENCY DATA ANALYSIS AND MARKET MICROSTRUCTURE 

public, Pb is the sale price and Pa is the purchase price.) The difference Pa — Pb is 
call the bid-ask spread, which is the primary source of compensation for market 
makers. Typically, the bid-ask spread is small—namely, one or two cents. 

The existence of a bid-ask spread, although small in magnitude, has several 
important consequences in time series properties of asset returns. We briefly discuss 
the bid-ask bounce—namely, the bid-ask spread introduces negative lag-1 serial 
correlation in an asset return. Consider the simple model of Roll (1984). The 
observed market price P, of an asset is assumed to satisfy 

Pt = P* + ltS~, (5.9) 

where S = Pa — Pb is the bid-ask spread, P* is the time-r fundamental value of 
the asset in a frictionless market, and {/,} is a sequence of independent binary 
random variables with equal probabilities (i.e., It = 1 with probability 0.5 and 
= —1 with probability 0.5). The I, can be interpreted as an order-type indicator, 
with 1 signifying buyer-initiated transaction and —1 seller-initiated transaction. 
Alternatively, the model can be written as 

p — p* j+5/2 with probability 0.5, 

’ [—5/2 with probability 0.5. 

If there is no change in P*, then the observed process of price changes is 

APt = (I, -7,_i)|. (5.10) 

Under the assumption of I, in Eq. (5.9), E(It) = 0 and Var(/f) = 1, and we have 
E(APt) = 0 and 

Var(AP,) = 52/2, (5.11) 

Cov(AP(, APf_i) = —S2/4, (5.12) 

Cov(APt, AP,-j) = 0, j > 1. (5.13) 

Therefore, the autocorrelation function of A P, is 

MAPf) = {o°’5 V=\' (5J4) 

[0 if y > i. 

The bid-ask spread thus introduces a negative lag-1 serial correlation in the series 
of observed price changes. This is referred to as the bid-ask bounce in the finance 
literature. Intuitively, the bounce can be seen as follows. Assume that the funda¬ 
mental price P* is equal to (Pa + Pb)/2. Then P, assumes the value Pa or Pb. If 
the previously observed price is Pa (the higher value), then the current observed 
price is either unchanged or lower at Pb. Thus, APt is either 0 or -S. However, if 

EMPIRICAL CHARACTERISTICS OF TRANSACTIONS DATA 

237 

the previous observed price is Pb (the lower value), then A P, is either 0 or S. The 
negative lag-1 correlation in APt becomes apparent. The bid-ask spread does not 
introduce any serial correlation beyond lag 1, however. 

A more realistic formulation is to assume that P* follows a random walk so that 
AP* = P* — P*_l = et, which forms a sequence of independent and identically 
distributed random variables with mean zero and variance a1 2. In addition, {et} is 
independent of {/,}. In this case, Var(APr) = a2 + S2/2, but Cov(APt, APf_y) 
remains unchanged. Therefore, 

—S2/4 

MAF') = ¥j2h-°- 

The magnitude of the lag-1 autocorrelation of A Pt is reduced, but the negative 
effect remains when S = Pa — Pb > 0. In finance, it might be of interest to study 
the components of the bid-ask spread. Interested readers are referred to Campbell 
et al. (1997) and the references therein. 

The effect of bid-ask spread continues to exist in portfolio returns and in mul¬ 
tivariate financial time series. Consider the bivariate case. Denote the bivariate 
order-type indicator by It = (I\t, I2/)', where /j, is for the first security and I21 
for the second security. If I\t and Ur are contemporaneously positively correlated, 
then the bid-ask spreads can introduce negative lag-1 cross correlations. 

5.3 EMPIRICAL CHARACTERISTICS OF TRANSACTIONS DATA 

Let ti be the calendar time, measured in seconds from midnight, at which the z'th 
transaction of an asset takes place. Associated with the transaction are several vari¬ 
ables such as the transaction price, the transaction volume, the prevailing bid and 
ask quotes, and so on. The collection of ti and the associated measurements are 
referred to as the transactions data. These data have several important character¬ 
istics that do not exist when the observations are aggregated over time. Some of 
the characteristics are given next. 

1. Unequally Spaced Time Intervals. Transactions such as stock tradings on an 
exchange do not occur at equally spaced time intervals. As such, the observed 
transaction prices of an asset do not form an equally spaced time series. The 
time duration between trades becomes important and might contain useful 
information about market microstructure (e.g., trading intensity). 

2. Discrete-Valued Prices. The price change of an asset from one transaction 
to the next only occurred in multiples of tick size before January 29, 2001. 
On the NYSE, the tick size was one-eighth of a dollar before June 24, 1997 
and was one-sixteenth of a dollar before January 29, 2001. Therefore, the 
price was a discrete-valued variable in transactions data. Although all equity 
markets in the United States now use the decimal system, the price change in 
consecutive trades tends to occur in multiples of one cent and can be treated 

238 HIGH-FREQUENCY DATA ANALYSIS AND MARKET MICROSTRUCTURE 

approximately as a discrete-valued variable. In some markets, price change 
may also be subject to limit constraints set by regulators. 

3. Existence of a Daily Periodic or Diurnal Pattern. Under the normal trading 
conditions, transaction activity can exhibit a periodic pattern. For instance, 
on the NYSE, transactions are “heavier” at the beginning and closing of 
the trading hours and “thinner” during lunch hour, resulting in a U-shaped 
transaction intensity. Consequently, time durations between transactions also 
exhibit a daily cyclical pattern. 

4. Multiple Transactions within a Single Second. It is possible that multiple 
transactions, even with different prices, occur at the same time. This is partly 
due to the fact that time is measured in seconds, which may be too long a 
time scale in periods of heavy trading. 

To demonstrate these characteristics, we consider first the IBM transactions data 
from November 1, 1990, to January 31, 1991. These data are from the Trades, 
Orders Reports, and Quotes (TORQ) data set; see Hasbrouck (1992). There are 
63 trading days and 60,328 transactions. To simplify the discussion, we ignore the 
price changes between trading days and focus on the transactions that occurred in 
the normal trading hours from 9:30 am to 4:00 pm Eastern time. It is well known 
that overnight stock returns differ substantially from intraday returns; see Stoll 
and Whaley (1990) and the references therein. Table 5.1 gives the frequencies in 
percentages of price change measured in the tick size of $| = $0,125. From the 
table, we make the following observations: 

1. About two-thirds of the intraday transactions were without price change. 

2. The price changed in one tick approximately 29% of the intraday transactions. 

3. Only 2.6% of the transactions were associated with two-tick price changes. 

4. Only about 1.3% of the transactions resulted in price changes of three ticks 

or more. 

5. The distribution of positive and negative price changes was approximately 

symmetric. 

Consider next the number of transactions in a 5-minute time interval. Denote 
the series by xt. That is, x\ is the number of IBM transactions from 9:30 am to 
9:35 am on November 1, 1990, Eastern time; X2 is the number of transactions from 
9:35 am to 9:40 am; and so on. The time gaps between trading days are ignored. 
Figure 5.1(a) shows the time plot of xt, and Figure 5.1(b) shows the sample ACF 

TABLE 5.1 Frequencies of Price Change in Multiples of Tick Size for IBM Stock 
from November 1, 1990, to January 31, 1991 

Number (tick) < —3 —2 —1 0 1 2 >3 

Percentage 0.66 1.33 14.53 67.06 14.53 1.27 0.63 

EMPIRICAL CHARACTERISTICS OF TRANSACTIONS DATA 

239 

(/) 
0 
"O 
0 

o 
C\J 

O 
00 
o 
CD 
O 

o 
C\J 
o 

5-minute time intervals 

(a) 

Figure 5.1 IBM intraday transactions data from 11/01/90 to 1/31/91: (a) number of transactions in 
5-minute time intervals and (b) sample ACF of series in part (a). 

(b) 

of xt for lags 1-260. Of particular interest is the cyclical pattern of the ACF with 
a periodicity of 78, which is the number of 5-minute intervals in a trading day. 
The number of transactions thus exhibits a daily pattern. To further illustrate the 
daily trading pattern, Figure 5.2 shows the average number of transactions within 
5-minute time intervals over the 63 days. There are 78 such averages. The plot 
exhibits a “smiling” or u shape, indicating heavier trading at the opening and 
closing of the market and thinner trading during the lunch hours. 

Since we focus on transactions that occurred during normal trading hours of 
a trading day, there are 59,838 time intervals in the data. These intervals are 
called the intraday durations between trades. For IBM stock, there were 6531 
zero time intervals. That is, during the normal trading hours of the 63 trading 
days from November 1, 1990, to January 31, 1991, multiple transactions in a 
second occurred 6531 times, which is about 10.91%. Among these multiple trans¬ 
actions, 1002 of them had different prices, which is about 1.67% of the total 
number of intraday transactions. Therefore, multiple transactions (i.e., zero dura¬ 
tions) may become an issue in statistical modeling of the time durations between 
trades. 

Table 5.2 provides a two-way classification of price movements. Here price 
movements are classified into “up,” “unchanged,” and “down.” We denote them 
by +, 0, and —, respectively. The table shows the price movements between two 

240 

HIGH-FREQUENCY DATA ANALYSIS AND MARKET MICROSTRUCTURE 

Figure 5.2 Time plot of average number of transactions in 5-minute time intervals. There are 78 
observations, averaging over 63 trading days from 11/01/90 to 1/31/91 for IBM stock. 

TABLE 5.2 Two-Way Classification of Price Movements in Consecutive Intraday 
Trades for IBM Stock" 

(i — l)th trade 

+ 
0 
— 
Margin 

+ 

441 
4867 
4580 
9888 

f th Trade 

0 

5498 
29779 
4841 
40118 

3948 
5473 
410 
9831 

Margin 

9887 
40119 
9831 
59837 

“The price movements are classified into 
November 1, 1990, to January 31, 1991. 

“up,” “unchanged,” and  “down.” The  data span is from 

consecutive trades [i.e., from the (i - l)th to the ith transaction] in the sample. 
From the table, trade-by-trade data show that: 

1. Consecutive price increases or decreases are relatively rare, which are about 

441/59837 — 0.74% and 410/59837 = 0.69%, respectively. 

2. There is a slight edge to move from up to unchanged rather than to down; 

see row 1 of the table. 

3. There is a high tendency for the price to remain unchanged. 

EMPIRICAL CHARACTERISTICS OF TRANSACTIONS DATA 

241 

4. The probabilities of moving from down to up or unchanged are about the 

same; see row 3. 

The first observation mentioned before is a clear demonstration of bid-ask 
bounce, showing price reversals in intraday transactions data. To confirm this phe¬ 
nomenon, we consider a directional series £), for price movements, where Z); 
assumes the value +1, 0, and —1 for up, unchanged, and down price movement, 
respectively, for the ith transaction. The ACF of {D,} has a single spike at lag 1 
with value -0.389, which is highly significant for a sample size of 59,837 and 
confirms the price reversal in consecutive trades. 

As a second illustration, we consider the transactions data of IBM stock in 
December 1999 obtained from the TAQ database. The normal trading hours are 
from 9:30 am to 4:00 pm Eastern time, except for December 31 when the market 
closed at 1:00 pm. Comparing with the 1990-1991 data, two important changes 
have occurred. First, the number of intraday tradings has increased sixfold. There 
were 134,120 intraday tradings in December 1999 alone. The increased trading 
intensity also increased the chance of multiple transactions within a second. The 
percentage of trades with zero time duration doubled to 22.98%. At the extreme, 
there were 42 transactions within a given second that happened twice on December 
3, 1999. Second, the tick size of price movement was = $0.0625 instead of 

$i. The change in tick size should reduce the bid-ask spread. Figure 5.3 shows the 
daily number of transactions in the new sample. Figure 5.4(a) shows the time plot 
of time durations between trades, measured in seconds, and Figure 5.4(b) is the 

Figure 5.3 IBM transactions data for December 1999. Box plot shows the number of transactions in 

each trading day with after-hours portion denoting number of trades with time stamp after 4:00 PM. 

242 

HIGH-FREQUENCY DATA ANALYSIS AND MARKET MICROSTRUCTURE 

Q) 
CD 
C 
03 
_C 
O 

|lr VV 

hi 
p"! 

20000 40000 

60000 80000 

100000 120000 

Sequence 

(b) 

Figure 5.4 IBM transactions data for December 1999. (a) Time plot of time durations between trades 
and (b) time plot of price changes in consecutive trades measured in multiples of tick size of $1/16. 

Only data during normal trading hours are included. 

time plot of price changes in consecutive intraday trades, measured in multiples 
of the tick size of $^. As expected, Figures 5.3 and 5.4(a) show clearly the 
inverse relationship between the daily number of transactions and the time interval 
between trades. Figure 5.4(b) shows two unusual price movements for IBM stock 
on December 3, 1999. They were a drop of 63 ticks followed by an immediate 
jump of 64 ticks and a drop of 68 ticks followed immediately by a jump of 68 ticks. 
Unusual price movements like these occurred infrequently in intraday transactions. 
Focusing on trades recorded within regular trading hours, we have 61,149 trades 
out of 133,475 with no price change. This is about 45.8% and substantially lower 
than that between November 1990 and January 1991. It seems that reducing the 
tick size increased the chance of a price change. Table 5.3 gives the percentages of 
trades associated with a price change. The price movements remain approximately 

TABLE 5.3 Percentages of Intraday Transactions Associated with a Price Change 
for IBM Stock Traded in December 1999" 

Size 

1 

2 

3 4 5 

6 

7 

>7 

Percentage 

18.03 

5.80 

1.79 0.66 0.25 

0.15 

0.09 

0.32 

Downward Movements 

Upward Movements 

Percentage 

18.24 

5.57 

1.79 0.71 0.24 

0.17 

0.10 

0.31 

"The percentage of transactions without price change is 45.8% and the total number of transactions 

recorded within regular trading hours is 133,475. The size is measured in multiples of tick size $ 1/16. 

EMPIRICAL CHARACTERISTICS OF TRANSACTIONS DATA 

243 

o 

CO 

c 
03 

o o 
CD O 
O 

0. (T) 

Seconds 

(a) 

0 10000 20000 30000 40000 

Index 

(b) 

Figure 5.5 Transactions data of Boeing stock on December 1, 2008. (a) Price series over calendar time 
measured in seconds from midnight and (b) time plot of price changes in consecutive trades measured 

in cents. Only data during normal trading hours are included. 

symmetric with respect to zero. Large price movements in intraday tradings are 
still relatively rare. 

Finally, we consider the transactions data of Boeing stock on December 1, 
2008. There are 43,894 transactions within the regular trading hours. Figure 5.5(a) 
shows the transaction prices versus the calendar time measured in seconds from 
the midnight, and Figure 5.5(b) shows the time plot of price changes. In this 
particular instance, the price shows a downward trend within the day, but the 
price changes continue to exhibit patterns similar to those before using the decimal 
system. Figure 5.6 shows the histogram of the price changes for the Boeing stock. 
The histogram shows some distinct characteristics. First, the price changes appear to 
be symmetric with respective to zero. Second, the price changes indeed concentrate 
on multiples of one cent. Out of the 43,894 transactions, 58.5% have no price 
change; see the big spike of the histogram. Details of the summary of price changes 
for the Boeing stock are given in Table 5.4. The remaining 4.59% of the price 
changes not shown in Table 5.4 are not in multiples of one cent. 

Remark. The recordkeeping of high-frequency data is often not as good as 
that of observations taken at lower frequencies. Data cleaning becomes a necessity 
in high-frequency data analysis. For transactions data, missing observations may 
happen in many ways, and the accuracy of the exact transaction time might be 
questionable for some trades. For example, recorded trading times may be beyond 

244 

HIGH-FREQUENCY DATA ANALYSIS AND MARKET MICROSTRUCTURE 

0.3 0.2 0.1 0.0 0.1 0.2 0.3 0.4 

Price change 

Figure 5.6 Histogram of price changes for Boeing stock on December 1, 2008. 

TABLE 5.4 Frequencies of Price Change for Boeing Stock on December 1, 2008 

Cents <-3-3-2-10 1 2 3 >3 
Percentage 1.63 1.05 3.51 12.6 58.5 12.2 3.45 0.94 1.53 

4:00 pm Eastern time even before the opening of after-hours tradings. How to handle 
these observations deserves a careful study. A proper method of data cleaning 
requires a deep understanding of the way in which the market operates. As such, 
it is important to specify clearly and precisely the methods used in data cleaning. 
These methods must be taken into consideration in making inference. □ 

Again, let r, be the calendar time, measured in seconds from midnight, when the 
z'th transaction took place. Let Ptj be the transaction price. The price change from 
the (z — l)th to the z'th trade is y,- = APti = P,t — Pti ] and the time duration is 
At, = tj — 6„i. Here it is understood that the subscript i in At,- and y, denotes the 
time sequence of transactions, not the calendar time. In what follows, we consider 
models for y,- and At, both individually and jointly. 

5.4 MODELS FOR PRICE CHANGES 

The discreteness and concentration on “no change” make it difficult to model 
the intraday price changes. Campbell et al. (1997) discuss several econometric 

MODELS FOR PRICE CHANGES 

245 

models that have been proposed in the literature. Here we mention two models 
that have the advantage of employing explanatory variables to study the intraday 
price movements. The first model is the ordered probit model used by Hauseman, 
Lo, and MacKinlay (1992) to study the price movements in transactions data. The 
second model has been considered recently by McCulloch and Tsay (2000) and is 
a simplified version of the model proposed by Rydberg and Shephard (2003); see 
also Ghysels (2000). 

5.4.1 Ordered Probit Model 

Let y* be the unobservable price change of the asset under study (i.e., y* = P* — 
where P* is the virtual price of the asset at time t. The ordered probit model 

assumes that y* is a continuous random variable and follows the model 

y* ~XiP + €i, (5.15) 

where x, is a p-dimensional row vector of explanatory variables available at 
time ti-i, P is a p x 1 parameter vector, £(e;|x,) = 0, Var(e;|jCj) = of, and 
Cov(e;, €j) = 0 for i 7^ j. The conditional variance of is assumed to be a positive 
function of the explanatory variable u>;, that is, 

of = g(u>i), (5.16) 

where g(-) is a positive function. For financial transactions data, tr, may contain 
the time interval — f,-_i and some conditional heteroscedastic variables. Typically, 
one also assumes that the conditional distribution of 6; given X; and Wi is Gaussian. 
Suppose that the observed price change y,- may assume k possible values. In 
theory, k can be infinity, but countable. In practice, k is finite and may involve 
combining several categories into a single value. For example, we have k = 7 in 
Table 5.1, where the first value “—3 ticks” means that the price change is —3 ticks 
or lower. We denote the k possible values as {si,..., s*}. The ordered probit model 
postulates the relationship between y,- and y* as 

yi=sj if aj-i < y* <ctj, j = l,...,k, (5.17) 

where aj are real numbers satisfying — oo = ao < “l <•••< of*_i < = oo. 
Under the assumption of conditional Gaussian distribution, we have 

P(yi = sj\xi, Wi) = P{oij-1 < Xifi + €i < ctj\xi, 

P(XiP + €t < ai|Xf, to,-) if j = L 
P{otj-\ < Xip + €t < <Xj\Xi, Wi) if j = 2,..., k - 1, 

Piptk-1 < XiP + €i\Xi, Wi) if j = k, 

246 

HIGH-FREQUENCY DATA ANALYSIS AND MARKET MICROSTRUCTURE 

if j - 1, 

(Xj-l - XiP 

- 4) 

Oi{Wi) 

if j — 2,..., k - 1, 

1 - <D 

1 -*// 

(Ti(Wi) 

if j - k, 

(5.18) 

where 4>(x) is the cumulative distribution function of the standard normal random 
variable evaluated at x, and we write 07(10,) to denote that af is a positive function 
of u>(. From the definition, an ordered probit model is driven by an unobservable 
continuous random variable. The observed values, which have a natural ordering, 
can be regarded as categories representing the underlying process. 

The ordered probit model contains parameters >3, o'/ (i = 1, — 1), and 
those in the conditional variance function 07(10,) in Eq. (5.16). These parame¬ 
ters can be estimated by the maximum-likelihood or Markov chain Monte Carlo 

methods. 

Example 5.1. Hauseman et al. (1992) apply the ordered probit model to 
the 1988 transactions data of more than 100 stocks. Here we only report then- 
result for IBM. There are 206,794 trades. The sample mean (standard deviation) 
of price change y,-, time duration At/, and bid-ask spread are —0.0010(0.753), 
27.21(34.13), and 1.9470(1.4625), respectively. The bid-ask spread is measured 
in ticks. The model used has nine categories for price movement, and the functional 
specifications are 

3 3 3 

Xi fi = P\ At* + ^ /Wiyi-\> + ^ /W4SP5j-u + ^ /W7IBSi-v 

l)=l U=1 U=1 

3 

+ J2Pv+io[TM-v) X IBS,_„], (5.19) 

U=1 

of{Wi) - 1.0 + Y\ At* + y22AB/_i, (5.20) 

where T\{V) = (Fx — 1)/A is the Box-Cox (1964) transformation of V with A e 
[0, 1] and the explanatory variables are defined by the following: 

• At* — (t{ — t;_i)/100 is a rescaled time duration between the (i — l)th and 

/th trades with time measured in seconds. 

• AB,_i is the bid-ask spread prevailing at time r,_ 1 in ticks. 

MODELS FOR PRICE CHANGES 

247 

• yi-v (v — 1, 2, 3) is the lagged value of price change at in ticks. With k 
= 9, the possible values of price changes are {-4, —3, -2, —1,0, 1, 2, 3, 4} 
in ticks. 

• Vi-v (v — 1, 2, 3) is the lagged value of dollar volume at the (i — u)th trans¬ 
action, defined as the price of the (i — i>)th transaction in dollars times the 
number of shares traded (denominated in hundreds of shares). That is, the 
dollar volume is in hundreds of dollars. 

• Sf^5i—y (v = 1, 2, 3) is the 5-minute continuously compounded returns of the 
Standard and Poor’s 500 index futures price for the contract maturing in the 
closest month beyond the month in which transaction (i — v) occurred, where 
the return is computed with the futures price recorded 1 minute before the 
nearest round minute prior to _v and the price recorded 5 minutes before this. 

• IBS,_„ (v = 1, 2, 3) is an indicator variable defined by 

fi if p,-„></£„ + j£„)/2, 

IBS,-, = 0 if Pi-, = (/>*_„ + />'" ,)/2, 

l-i if />,_„< (/>»_„ + Pl,yi, 

where Pj and Pb- are the ask and bid price at time tj. 

The parameter estimates and their t ratios are given in Table 5.5. All the t ratios 
are large except one, indicating that the estimates are highly significant. Such high 
t ratios are not surprising as the sample size is large. For the heavily traded IBM 
stock, the estimation results suggest the following conclusions: 

1. The boundary partitions are not equally spaced but are almost symmetric 

with respect to zero. 

TABLE 5.5 Parameter Estimates of Ordered Probit Model in Eqs. (5.19) and (5.20) 
for the 1988 Transaction Data of IBM, Where t Denotes the t Ratio 

Boundary Partitions of the Probit Model 

Parameter 

a\ 

a2 

“3 

Ct4 

a5 

a 6 

ai 

«8 

Estimate 

-4.67 

-4.16 

-3.11 

-1.34 

1.33 

3.13 

4.21 

4.73 

t 

-145.7 

-157.8 

-171.6 

-155.5 

154.9 

167.8 

152.2 

138.9 

Parameter 

Estimate 

t 

n 
0.40 

15.6 

Parameter 

fi 

71.1 

/6s 

Equation Parameters of the Probit Model 

n 

/fi : At* 

0.52 

-0.12 

ft : y-i 
-1.01 

ft 
-0.53 

ft 
-0.21 

@5 

1.12 

ft 
-0.26 

-11.4 

-135.6 

-85.0 

-47.2 

54.2 

-12.1 

Estimate 

0.01 

-1.14 

-0.37 

-0.17 

t 

0.26 

-63.6 

-21.6 

-10.3 

ft: 

fto 

ft] 

0.12 

47.4 

fl2 

ft3 

0.05 

18.6 

0.02 

7.7 

Source: Reprinted with permission of Elsevier from Journal of Financial Economics (1992, Vol. 31, 

p. 345) 

248 

HIGH-FREQUENCY DATA ANALYSIS AND MARKET MICROSTRUCTURE 

2. The transaction duration Att affects both the conditional mean and condi¬ 

tional variance of y,- in Eqs. (5.19) and (5.20). 

3. The coefficients of lagged price changes are negative and highly significant, 

indicating price reversals. 

4. As expected, the bid-ask spread at time ?,•_] significantly affects the condi¬ 

tional variance. 

5.4.2 Decomposition Model 

An alternative approach to modeling price change is to decompose it into three 
components and use conditional specifications for the components; see Rydberg 
and Shephard (2003). The three components are an indicator for price change, the 
direction of price movement if there is a change, and the size of price change if a 
change occurs. Specifically, the price change at the ith transaction can be written as 

yi = Pu Pti-1 — AiDiSi, 

(5.21) 

where A, is a binary variable defined as 

Aj = 

1 

0 

if there is a price change at the ith trade, 

if price remains the same at the ith trade, 

(5.22) 

Dj is also a discrete variable signifying the direction of the price change if a change 
occurs, that is, 

DMA, =1) = 

1 if price increases at the ith trade, 

— 1 if price drops at the ith trade, 

(5.23) 

where D,-|(A, = 1) means that D,• is defined under the condition of A,- = 1, and S,- is 
the size of the price change in ticks if there is a change at the ith trade and S, = 0 
if there is no price change at the ith trade. When there is a price change, S, is a 
positive integer-valued random variable. 

Note that D, is not needed when A/ = 0, and there is a natural ordering in the 
decomposition. D, is well defined only when A, = 1 and 5, is meaningful when 
A/ = 1 and D, is given. Model specification under the decomposition makes use 
of the ordering. 

Let Fj be the information set available at the ith transaction. Examples of 
elements in F, are Ati-j, Ai-j, Di-j, and S,_7- for j > 0. The evolution of price 
change under model (5.21) can then be partitioned as 

P(yi\Fi-\) — P(AiDiSi\Fi-i) 

= P(Si\Di, A,-, Fi-i)P(Dt\Ait Fi-i)P(Ai\Fi-i). (5.24) 

MODELS FOR PRICE CHANGES 

249 

Since A; is a binary variable, it suffices to consider the evolution of the probability 
Pi = P(Aj = 1) over time. We assume that 

/ D: \ ex'^ 

ln ( 1- ) = Xi& °r Pi = T'l XB' (5-25) 

\1 — Pi) 1 +ex‘P 

where Xj is a finite-dimensional vector consisting of elements of F,-_i and /? is a 
parameter vector. Conditioned on A,- = 1, D, is also a binary variable, and we use 
the following model for 5,- = P(D, = 1|A,- = 1): 

/ 8t \ ez‘y 

in(j^)=ziy or s, = T+i*r' (5-26) 

where Zi is a finite-dimensional vector consisting of elements of F,-_ i and y is 
a parameter vector. To allow for asymmetry between positive and negative price 
changes, we assume that 

Si\(Di, A,- = 1) ~ 1 + 

g(Kj) 

g&d,t) 

if Z); = 1, A, = 1, 

if Di = — 1, A, = 1, 

(5.27) 

where g(k) is a geometric distribution with parameter A. and the parameters kjj 
evolve over time as 

ln 

h j,i 

^ ^j,i 

= WjO j or kjti 

,WiOj 

1 + ew‘*J 

j = u, d, 

(5.28) 

where ip,- is again a finite-dimensional explanatory variable in F,_i and 6j is a 
parameter vector. 

In Eq. (5.27), the probability mass function of a random variable x, which 

follows the geometric distribution g(k), is 

p{x = m) = A(1 — k)m, m = 0, 1, 2,_ 

We added 1 to the geometric distribution so that the price change, if it occurs, 
is at least 1 tick. In Eq. (5.28), we take the logistic transformation to ensure that 

7-jj e [0, 1]. 

The previous specification classifies the /th trade, or transaction, into one of 

three categories: 

1. No price change: A, = 0 and the associated probability is (1 — pi). 

2. A price increase: A,- = 1, £>, = 1, and the associated probability is piSj. The 

size of the price increase is governed by 1 + g(kuj). 

3. A price drop: A, = 1, D, = —1, and the associated probability is /?, (1 — <5,). 

The size of the price drop is governed by 1 -f g(kdj). 

250 

HIGH-FREQUENCY DATA ANALYSIS AND MARKET MICROSTRUCTURE 

Let Ij(j) for j — 1, 2, 3 be the indicator variables of the prior three categories. 
That is, Ii{j) — 1 if the /th category occurs and I, (j) = 0 otherwise. The log- 

likelihood function of Eq. (5.24) becomes 

MPiyilFi-i)] = 7/(l)ln[(l - pt)] + /,-(2)[ln(Pi) + ln(5f) 

+ In(ku,i) + (Si - 1) ln(l — Xuj)] 

+ /,(3)[ln(pf) +ln(l - Si) + ln(Xd,i) + ($ — 1) ln(l — XdJ)], 

and the overall log-likelihood function is 

ln[P(yi, ...,y„|F0)] = J^ln[P(y/|F,_i)], 

(5.29) 

n 

i=1 

which is a function of parameters (3, y, 0U, and 0d. 

Example 5.2. We illustrate the decomposition model by analyzing the intraday 
transactions of IBM stock from November 1, 1990, to January 31, 1991. There were 
63 trading days and 59,838 intraday transactions in the normal trading hours. The 
explanatory variables used are: 

1. A,_i: the action indicator of the previous trade [i.e., the (i — l)th trade within 

a trading day] 

2. Z), _ i: the direction indicator of the previous trade 

3. 5(_i: the size of the previous trade 

4. Vi-1: the volume of the previous trade, divided by 1000 

5. Ati-\: time duration from the (i — 2)th to (i — l)th trade 

6. BAj: the bid-ask spread prevailing at the time of transaction 

Because we use lag-1 explanatory variables, the actual sample size is 59,775. It 
turns out that V,_i, and BAi are not statistically significant for the model 
entertained. Thus, only the first three explanatory variables are used. The model 
employed is 

In 

Pi 

1 - Pi 

(5.30) 

MODELS FOR PRICE CHANGES 

251 

TABLE 5.6 Parameter Estimates of ADS Model in Eq. (5.30) for IBM Intraday 
Transactions from December 1, 1990, to January 31, 1991 

Parameter 

Estimate 

Standard Error 

Parameter 

Estimate 

Standard Error 

Po 
-1.057 

0.104 

&u,0 

2.235 

0.029 

Pi 
0.962 

0.044 

0«,l 

-0.670 

0.050 

Vo 

-0.067 

0.023 

Qd, 0 
2.085 

0.187 

n 
-2.307 

0.056 

Qd,\ 
-0.509 

0.139 

The parameter estimates, using the log-likelihood function in Eq. (5.29), are given 
in Table 5.6. The estimated simple model shows some dynamic dependence in the 
price change. In particular, the trade-by-trade price changes of IBM stock exhibit 
some appealing features: 

1. The probability of a price change depends on the previous price change. 

Specifically, we have 

P(Ai = l|A/_i = 0) = 0.258, P(At = 1| Aj_i = 1) = 0.476. 

The result indicates that a price change may occur in clusters and, as expected, 
most transactions are without price change. When no price change occurred 
at the (i — l)th trade, then only about one out of four trades in the subsequent 
transaction has a price change. When there is a price change at the (i — l)th 
transaction, the probability of a price change in the /th trade increases to 
about 0.5. 

2. The direction of price change is governed by 

0.483 

if Di-X = 0 (i.e., Aj_! = 0), 

P(Di = 1| Fi-uAi) 

0.085 

if Dt-i = 1 ,At = 1, 

0.904 

if Dt-i = -l,Ai = 1. 

This result says that (a) if no price change occurred at the (i — l)th trade, 
then the chances for a price increase or decrease at the /th trade are about 
even; and (b) the probabilities of consecutive price increases or decreases are 
very low. The probability of a price increase at the /th trade given that a price 
change occurs at the /th trade and there was a price increase at the (/ — l)th 
trade is only 8.6%. However, the probability of a price increase is about 
90% given that a price change occurs at the /th trade and there was a price 
decrease at the (/ — l)th trade. Consequently, this result shows the effect of 
bid-ask bounce and supports price reversals in high-frequency trading. 

3. There is weak evidence suggesting that big price changes have a higher 
probability to be followed by another big price change. Consider the size of 

252 

HIGH-FREQUENCY DATA ANALYSIS AND MARKET MICROSTRUCTURE 

a price increase. We have 

Si | (A = 1) ~ 1 + Ki = 2.235 - 0.6705,-1. 

Using the probability mass function of a geometric distribution, we obtain 
that the probability of a price increase by one tick is 0.827 at the /th trade 
if the transaction results in a price increase and 5,_i = 1. The probability 
reduces to 0.709 if S,_i = 2 and to 0.556 if 5,_i = 3. Consequently, the 
probability of a large 5, is proportional to 5,_i given that there is a price 
increase at the /th trade. 

A difference between the ADS of Eq. (5.21) and ordered probit models is that 
the former does not require any truncation or grouping in the size of a price change. 

R Demonstration for Logistic Linear Regression 
The following output has been edited: 

> da=read.table("ibm91-ads.txt",header=T) 

> dal=read.table("ibm91-adsx.txt",header=T) 

> Ai=da[,l] % Select the variables 

> Di=da[,2] 

> Aiml=dal[,4] 

> Diml=dal[,5] 

> 

> ml=glm (Ai~Aiml, f amily=binomial) %Fit a linear 

logistic model 

> summary(ml) 

Call: 

glm(formula = Ai ~ Aiml, family = binomial) 

Deviance Residuals: 

Min IQ Median 3Q Max 

-1.1373 -0.7724 -0.7724 1.2180 1.6462 

Coefficients: 

Estimate Std. Error z value Pr(>|z|) 

(Intercept) -1.05667 0.01142 -92.55 <2e-16 *** 

Aiml 0.96164 0.01827 52.62 <2e-16 *** 

> 

> di=Di[Ai==l] % Select the cases in which Ai = 1. 

> diml=Diml[Ai==l] 

> di=(di+abs(di))/2 % Logistic regression works for 1 or 0, 

% but di is coded 1 or -1 so that change is needed. 

> m2=glm (di~diml, f amily=binomial) 

> summary(m2) 

Call: 

glm(formula = di ~ diml, family = binomial) 

DURATION MODELS 

253 

Deviance Residuals: 

Min 
-2.1640 

IQ Median 
-1.1493 0.4497 

3Q Max 
1.2058 2.2193 

Coefficients: 

Estimate Std. Error z value Pr(>|z|) 

(Intercept) -0.06663 0.01728 -3.855 0.000116 *** 
diml -2.30693 0.03595 -64.171 < 2e-16 *** 

5.5 DURATION MODELS 

Duration models are concerned with time intervals between trades. Longer dura¬ 
tions indicate lack of trading activities, which in turn signify a period of no new 
information. The dynamic behavior of durations thus contains useful information 
about intraday market activities. Using concepts similar to the ARCH models for 
volatility, Engle and Russell (1998) propose an autoregressive conditional dura¬ 
tion (ACD) model to describe the evolution of time durations for (heavily traded) 
stocks. Zhang et al. (2001) extend the ACD model to account for nonlinearity and 
structural breaks in the data. In this section, we introduce some simple duration 
models. As mentioned before, intraday transactions exhibit some diurnal pattern. 
Therefore, we focus on the adjusted time duration 

At* = A ti/fiti), 

(5.31) 

where /(*,-) is a deterministic function consisting of the cyclical component of 
Ati. Obviously, /(f,-) depends on the underlying asset and the systematic behavior 
of the market. In practice, there are many ways to estimate /(?,), but no single 
method dominates the others in terms of statistical properties. A common approach 
is to use smoothing spline. Here we use simple quadratic functions and indicator 
variables to take care of the deterministic component of daily trading activities. 
For the IBM data employed in the illustration of ADS models, we assume 

7 

(5.32) 

where 

0 

0 

otherwise, 

otherwise, 

254 

HIGH-FREQUENCY DATA ANALYSIS AND MARKET MICROSTRUCTURE 

Figure 5.7 Quadratic functions used to remove deterministic component of IBM intraday trading 
durations: (a)-(d) are functions /i(-) to /4O) of Eq. (5.32), respectively. 

and fs (ti) and fe(tt) are indicator variables for the first and second 5 minutes of 
market opening [i.e., /5O) = 1 if and only if 7,- is between 9:30 am and 9:35 am 
Eastern time], and fj(ti) is the indicator for the last 30 minutes of daily trading 
[i.e., fj(tj) = 1 if and only if the trade occurred between 3:30 pm and 4:00 pm 
Eastern time]. Figure 5.7 shows the plot of /■(•) for / = 1,..., 4, where the time 
scale on the a axis is in minutes. Note that /3(43200) = /4(43200), where 43,200 
corresponds to 12:00 noon. 

The coefficients //■ of Eq. (5.32) are obtained by the least-squares method of 

the linear regression 

7 

In(Aff) = p0 + PjfjOi) + e/. 

7=1 

The fitted model is 

ln(Afj-) = 2.555 + 0.159/i (7,) + 0.270/2(7,) + 0.384/3(7,) 

+ 0.061/4(7, ) - 0.611/5(7,) - 0.157/6(7, ) + 0.073/7(7,). 

Figure 5.8 shows the time plot of average durations in 5-minute time intervals over 
the 63 trading days before and after adjusting for the deterministic component. 
Figure 5.8(a) shows the average durations of A 7; and, as expected, exhibits a 

DURATION MODELS 

255 

5-minute intervals 5-minute intervals 
(a) (b) 

Figure 5.8 IBM transactions data from 11/01/90 to 1/31/91: (a) average durations in 5-minute time 

intervals and (b) average durations in 5-minute time intervals after adjusting for deterministic component. 

diurnal pattern. Figure 5.8(b) shows the average durations of At* (i.e., after the 
adjustment), and the diurnal pattern is largely removed. 

5.5.1 The ACD Model 

The autoregressive conditional duration (ACD) model uses the idea of GARCH 
models to study the dynamic structure of the adjusted duration At* of Eq. (5.31). 
For ease in notation, we define x, = At*. 

Let i/o = E(xi\Fi-{) be the conditional expectation of the adjusted duration 
between the (i — l)th and zth trades, where Fj-_i is the information set available 
at the (z — l)th trade. In other words, Vo is the expected adjusted duration given 
Fi-1. The basic ACD model is defined as 

Xi = Voq, (5.33) 

where {e, } is a sequence of independent and identically distributed nonnegative 
random variables such that £(q) — 1. In Engle and Russell (1998), e,- follows a 
standard exponential or a standardized Weibull distribution, and i/o assumes the 
form 

^i-co + J^ YjXi-j + cojfi-j. 

(5.34) 

j=1 7=1 

256 

HIGH-FREQUENCY DATA ANALYSIS AND MARKET MICROSTRUCTURE 

Such a model is referred to as an ACD(r, s) model. When the distribution of c, 
is exponential, the resulting model is called an EACD(r, 5) model. Similarly, if 
e, follows a Weibull distribution, the model is a WACD(r, 5) model. If necessary, 
readers are referred to Appendix A for a quick review of exponential and Weibull 
distributions. 

Similar to GARCH models, the process 77/ = x— 1/r,- is a martingale difference 

sequence [i.e., E(rji\Fi-i) = 0], and the ACD(r, s) model can be written as 

max(r,s) s 

Xi =co+ Y (Yj + (Oj)xi-j - Y VjVi-j + r>j’ (5-35) 

7=1 7=1 

which is in the form of an ARMA process with non-Gaussian innovations. It is 
understood here that yj = 0 for j > r and o)j = 0 for j > s. Such a representation 
can be used to obtain the basic conditions for weak stationarity of the ACD model. 
For instance, taking expectation on both sides of Eq. (5.35) and assuming weak 
stationarity, we have 

' (rj +a>A 

Therefore, we assume co > 0 and 1 > (Yj + Mj) because the expected duration is 
positive. As another application of Eq. (5.35), we study properties of the EACD(1,1) 
model. 

EACD(1,1) Model 
An EACD(1,1) model can be written as 

Xi = fi€i, fi = co + y\Xi-i + (5.36) 

where e, follows the standard exponential distribution. Using the moments of a 
standard exponential distribution in Appendix A, we have £(£,•) = 1, Var(e;) = 1, 
and E(ef) = Var(jq) + [^(a,)]2 = 2. Assuming that x,- is weakly stationary (i.e., 
the first two moments of x, are time invariant), we derive the variance of x;. First, 
taking the expectation of Eq. (5.36), we have 

£(x,) = E[E{fiei\Fi_x)\ = E(^), 

E(fi) = co + yiE(xt-i) +a>iE(j/fi-i). (5.37) 

Under weak stationarity, £’(^() = £(^1-1) so that Eq. (5.37) gives 

[Xx = E{xi) - E(fi) - --. 

l-yi-coi 

(5.38) 

Next, because E(e?) = 2, we have E(xf) = E[E(x^fef\Fi-i)] = 2£(V'f). 

DURATION MODELS 

257 

Taking the square of iJ/j in Eq. (5.36) and the expectation and using weak 

stationarity of t/^ and we have, after some algebra, that 

= /x* x 

1 - (yi +mQ2 

1 — 2/j2 — co2 — 2/i o>i 

(5.39) 

Finally, using Var(jc,-) = E(xf) — [E(xi)]2 and E(xf) = 2E(\Js2), we have 

Var(Xj) = 2£(Vt) - /x2 = /x2 x 

1 — o»2 — 2yiO)i 

1 — o)2 — 2/i o>i — 2/j2 ’ 

where yu* is defined in Eq. (5.38). This result shows that, to have time-invariant 
unconditional variance, the EACD(1,1) model in Eq. (5.36) must satisfy 1 > 2/2 + 
a)2 + 2/i o>i. The variance of a WACD(1,1) model can be obtained by using the 
same techniques and the first two moments of a standardized Weibull distribution. 

ACD Models with a Generalized Gamma Distribution 
In the statistical literature, intensity function is often expressed in terms of hazard 
function. As shown in Appendix B, the hazard function of an EACD model is 
constant over time and that of a WACD model is a monotonous function. These 
hazard functions are rather restrictive in application as the intensity function of 
stock transactions might not be constant or monotone over time. To increase the 
flexibility of the associated hazard function, Zhang et al. (2001) employ a (stan¬ 
dardized) generalized gamma distribution for e,-. See Appendix A for some basic 
properties of a generalized gamma distribution. The resulting hazard function may 
assume various patterns, including U shape or inverted U shape. We refer to an 
ACD model with innovations that follow a generalized gamma distribution as a 
GACD(r, s) model. 

5.5.2 Simulation 

To illustrate ACD processes, we generated 500 observations from the ACD(1,1) 

model: 

Xi = tfoei, Vfi = 0.3 + 0.2xf_i + 0.7^_i (5.40) 

using two different innovational distributions for e,-. In case 1, e* is assumed to 
follow a standardized Weibull distribution with parameter a = 1.5. In case 2, 
follows a (standardized) generalized gamma distribution with parameters k = 1.5 
and a = 0.5. 

Figure 5.9(a) shows the time plot of the WACD(1,1) series, whereas 
Figure 5.10(a) is the GACD(1,1) series. Figure 5.11 plots the histograms of both 
simulated series. The difference between the two models is evident. Finally, 
the sample ACFs of the two simulated series are shown in Figures 5.12(a) and 
5.13(b), respectively. The serial dependence of the data is clearly seen. 

258 

HIGH-FREQUENCY DATA ANALYSIS AND MARKET MICROSTRUCTURE 

(a) 

(b) 

Figure 5.9 Simulated WACD(1,1) series in Eq. (5.40): (a) original series and (b) standardized series 
after estimation. There are 500 observations. 

(a) 

(b) 

Figure 5.10 Simulated GACD(1,1) series in Eq. (5.40): (a) original series and (b) standardized series 
after estimation. There are 500 observations. 

DURATION MODELS 

259 

0 2 4 6 8 10 0 20 40 60 80 

Z X 

(a) (b) 

Figure 5.11 Histograms of simulated duration processes with 500 observations: (a) WACD(1,1) model 

and (b) GACD(l.l) model. 

(a) 

(b) 

Figure 5.12 Sample autocorrelation function of simulated WACD(1,1) series with 500 observations: 

(a) original series and (b) standardized residual series. 

260 

HIGH-FREQUENCY DATA ANALYSIS AND MARKET MICROSTRUCTURE 

LL 
o 
< 

in 
o 

co 
o 

o 

o 

Lag 

(a) 

Lag 

(b) 

Figure 5.13 Sample autocorrelation function of simulated GACD(1,1) series with 500 observations: 
(a) original series and (b) standardized residual series. 

5.5.3 Estimation 

For an ACD(r, s) model, let i0 = max(r, 5) and xt = (jci, ..., x,)'. The likelihood 
function of the durations x\, ..., xj is 

f{xT\6) 

T 

n f(*i\Fi-uO) 
i=i0-\-1 

X f(xio\0), 

where 0 denotes the vector of model parameters, and T is the sample size. The 
marginal probability density function f(xio\6) of the previous equation is rather 
complicated for a general ACD model. Because its impact on the likelihood function 
is diminishing as the sample size T increases, this marginal density is often ignored, 
resulting in use of the conditional-likelihood method. For a WACD model, we use 
the probability density function (pdf) of Eq. (5.56) and obtain the conditional 
log-likelihood function 

T 

t{x\e,xio)= a ln 

r 

1=10+1 

+ In 

+ a ln 

T(1 + 1 /a)xj  a 

(5.41) 

DURATION MODELS 

261 

TABLE 5.7 Estimation Results for Simulated ACD(1,1) Series with 500 
Observations: For WACD(1,1) Series and GACD(1,1) Series 

WACD(1,1) Model 

Parameter 

True 

Estimate 

co y\ a)[ a 

0.3 0.2 0.7 1.5 

0.364 0.100 0.767 1.477 

Standard Error 

(0.139) (0.025) (0.060) (0.052) 

GACD(1,1) Model 

Parameter 

True 

Estimate 

co y\ co{ a k 

0.3 0.2 0.7 0.5 1.5 

0.401 0.343 0.561 0.436 2.077 

Standard Error 

(0.117) (0.074) (0.065) (0.078) (0.653) 

where ft =co + Yjxi-j + l 0 = (.<*>, yi,, yr, a>u ..., cos,a)', 
and x = (xi0+\, ..., xt)'. When a — 1, the (conditional) log-likelihood function 
reduces to that of an EACD(r, s) model. 

For a GACD(r, s) model, the conditional log-likelihood function is 

t(x\9,xio) 

T 

-2> 

i—i0-\-\ 

a 

foo  + (kol — 1) In (a, ) — Ka ln(A^) — 

(5.42) 

where X = T{k)/T(k + \/a) and the parameter vector 0 now also includes k. As 
expected, when k = l, A = l/r(l + l/a) and the log-likelihood function in Eq. 
(5.42) reduces to that of a WACD(r, v) model in Eq. (5.41). This log-likelihood 
function can be rewritten in many ways to simplify the estimation. 

Under some regularity conditions, the conditional maximum-likelihood estimates 
are asymptotically normal; see Engle and Russell (1998) and the references therein. 
In practice, simulation can be used to obtain finite-sample reference distributions 
for the problem of interest once a duration model is specified. 

x _ A a 

Example 5.3. (Simulated ACD(1,1) series, continued). Consider the simu¬ 
lated WACD(1,1) and GACD(1,1) series of Eq. (5.40). We apply the conditional- 
likelihood method and obtain the results in Table 5.7. The estimates appear to be 
reasonable. Let i/9 be the 1-step-ahead prediction of i/'V and q = xi/^i be the stan¬ 
dardized series, which can be regarded as standardized residuals of the series. If 
the model is adequately specified, {£,-} should behave as a sequence of independent 
and identically distributed random variables. Figures 5.9(b) and 5.10(b) show the 
time plot of e,- for both models. The sample ACF of c,- for both fitted models are 
shown in Figures 5.12(b) and 5.13(b), respectively. It is evident that no significant 
serial correlations are found in the c,- series. 

Example 5.4. As an illustration of duration models, we consider the transac¬ 
tion durations of IBM stock on five consecutive trading days from November 1 

262 

HIGH-FREQUENCY DATA ANALYSIS AND MARKET MICROSTRUCTURE 

c 
o 

05 
i_ 

3 T> 
-a 
<n 
-4—1 
(n 

■O 

< 

o 
■'i' 

o 
co 

o 
CM 

O 

0 

1000 2000 3000 

Sequence 

(a) 

Figure 5.14 Time plots of durations for IBM stock traded in first five trading days of November 1990: 

(a) adjusted series and (b) normalized innovations of an WACD(1,1) model. There are 3534 nonzero 
durations. 

to November 7, 1990. Focusing on positive transaction durations, we have 3534 
observations. In addition, the data have been adjusted by removing the determinis¬ 
tic component in Eq. (5.32). That is, we employ 3534 positive adjusted durations 
as defined in Eq. (5.31). 

Figure 5.14(a) shows the time plot of the adjusted (positive) durations for the 
first five trading days of November 1990, and Figure 5.15(a) gives the sample ACF 
of the series. There exist some serial correlations in the adjusted durations. We fit 
a WACD(1,1) model to the data and obtain the model 

Xi = fi€i, = 0.169 + 0.064jc/_i + 0.885^-1, (5.43) 

where {<?,•} is a sequence of independent and identically distributed random variates 
that follow the standardized Weibull distribution with parameter a = 0.879(0.012), 
where 0.012 is the estimated standard error. Standard errors of the estimates in 
Eq. (5.43) are 0.039, 0.010, and 0.018, respectively. All t ratios of the estimates 
are greater than 4.2, indicating that the estimates are significant at the 1% level. 
Figure 5.14(b) shows the time plot of I,- = jq/i/q, and Figure 5.15(b) provides the 
sample ACF of c,-. The Ljung-Box statistics show 0(10) = 4.96 and 0(20) = 
10.75 for the e,- series. Clearly, the standardized innovations have no significant 
serial correlations. In fact, the sample autocorrelations of the squared series {c;2} 

DURATION MODELS 

o 

263 

Lag 

(b) 

Figure 5.15 Sample autocorrelation function of adjusted durations for IBM stock traded in first five 

trading days of November 1990: (a) adjusted series and (b) normalized innovations for WACD(1,1) 

model. 

are also small with 2(10) = 6.20 and <2(20) = 11.16, further confirming lack of 
serial dependence in the normalized innovations. In addition, the mean and standard 
deviation of a standardized Weibull distribution with a = 0.879 are 1.00 and 1.14, 
respectively. These numbers are close to the sample mean and standard deviation 
of {I,}, which are 1.01 and 1.22, respectively. The fitted model seems adequate. 

In model (5.43), the estimated coefficients show y\ + cb\ & 0.949, indicating 
certain persistence in the adjusted durations. The expected adjusted duration is 
0.169/(1 — 0.064 — 0.885) = 3.31 seconds, which is close to the sample mean 3.29 
of the adjusted durations. The estimated a of the standardized Weibull distribution 
is 0.879, which is less than but close to 1. Thus, the conditional hazard function is 
monotonously decreasing at a slow rate. 

If a generalized gamma distribution function is used for the innovations, then 

the fitted GACD(1,1) model is 

x i = tpi€i, fi — 0.141 + 0.063jcj_i + 0.897i/q_i, (5.44) 

where {<?,} follows a standardized, generalized gamma distribution in Eq. (5.57) 
with parameters k = 4.248(1.046) and a = 0.395(0.053), where the number in 
parentheses denotes estimated standard error. Standard errors of the three parame¬ 
ters in Eq. (5.44) are 0.041, 0.010, and 0.019, respectively. All of the estimates are 

264 

HIGH-FREQUENCY DATA ANALYSIS AND MARKET MICROSTRUCTURE 

statistically significant at the 1% level. Again, the normalized innovational process 
{£,■} and its squared series have no significant serial correlation, where it = /\j/i 
based on model (5.44). Specifically, for the process, we have <2(10) = 4.95 and 
0(20) = 10.28. For the if series, we have 0(10) = 6.36 and 0(20) = 10.89. 

The expected duration of model (5.44) is 3.52, which is slightly greater than 
that of the WACD(1,1) model in Eq. (5.43). Similarly, the persistence parameter 
y\ + cb\ of model (5.44) is also slightly higher at 0.96. 

Remark. Estimation of EACD models can be carried out by using programs 
for ARCH models with some minor modification; see Engle and Russell (1998). In 
this book, we use either the RATS program or some Fortran programs developed 
by the author to estimate the duration models. Limited experience indicates that it 
is harder to estimate a GACD model than an EACD or a WACD model. RATS 
programs used to estimate WACD and GACD models are given in Appendix C. 

□ 

5.6 NONLINEAR DURATION MODELS 

Nonlinear features are also commonly found in high-frequency data. As an illus¬ 
tration, we apply some nonlinearity tests discussed in Chapter 4 to the normalized 
innovations it of the WACD(1,1) model for the IBM transaction durations in 
Example 5.4; see Eq. (5.43). Based on an AR(4) model, the test results are given 
in part (a) of Table 5.8. As expected from the model diagnostics of Example 5.4, 
the Ori-F test indicates no quadratic nonlinearity in the normalized innovations. 
However, the TAR-F test statistics suggest strong nonlinearity. 

Based on the test results in Table 5.8, we entertain a threshold duration model 
with two regimes for the IBM intraday durations. The threshold variable is Jtr_i 
(i.e., lag-1 adjusted duration). The estimated threshold value is 3.79. The fitted 
threshold WACD(1,1) model is jq = i/qe,-, where 

fi = 

0.020 T 0.257x,_i T 0.847t//'(_i, 

€i ~ io(0.901) if jc/_i < 3.79, 

1.808 T 0.027x,_i -(- 0.501 ^9—i> 

£,• ~ u>(0.845) if Xi-1 > 3.79, 

(5.45) 

where w(a) denotes a standardized Weibull distribution with parameter a. The 
number of observations in the two regimes are 2503 and 1030, respectively. In Eq. 
(5.45), the standard errors of the parameters for the first regime are 0.043, 0.041, 
0.024, and 0.014, whereas those for the second regime are 0.526, 0.020, 0.147, and 
0.020, respectively. 

Consider the normalized innovations it —Xi/fi of the threshold WACD(1,1) 
model in Eq. (5.45). We obtain 0(12) = 9.8 and 0(24) = 23.9 for it and 0(12) = 
8.0 and 0(24) = 16.7 for if. Thus, there are no significant serial correlations in 
the €,■ and if series. Furthermore, applying the same nonlinearity tests as before 
to this newly normalized innovational series it, we detect no nonlinearity; see part 

BIVARIATE MODELS FOR PRICE CHANGE AND DURATION 

265 

TABLE 5.8 Nonlinearity Tests for IBM Transaction Durations from November 1 to 
November 7, 1990" 

(a) Normalized Innovations of a WACD(1,1) Model 

Type 

Test 

p Value 

Ori-F 

0.343 

0.969 

TAR-F(l) 

TAR-F(2) 

TAR-F(3) 

TAR-F(4) 

3.288 

0.006 

3.142 

0.008 

3.128 

0.008 

0.297 

0.915 

(b) Normalized Innovations of a Threshold WACD(fl) Model 

Test 

p Value 

0.163 

0.998 

0.746 

0.589 

1.899 

0.091 

1.752 

0.119 

0.270 

0.929 

“Only intraday durations are used. The number in parentheses of TAR-/7 tests denotes time delay. 

(b) of Table 5.8. Consequently, the two-regime threshold WACD(1,1) model in Eq. 
(5.45) is adequate. 

If we classify the two regimes as heavy and thin trading periods, then the thresh¬ 
old model suggests that the trading dynamics measured by intraday transaction 
durations are different between heavy and thin trading periods for IBM stock even 
after the adjustment of diurnal pattern. This is not surprising as market activities 
are often driven by the arrival of news and other information. 

The estimated threshold WACD(1,1) model in Eq. (5.45) contains some insignif¬ 

icant parameters. We refine the model and obtain the result: 

_ |o.225v,_i +0.867^-1, Q ~ tn(0.902) if xt-\ < 3.79, 

11.618 + 0.614^-1, a ~ w(0.846) if JCj_i > 3.79. 

All of the estimates of the refined model are highly significant. The Ljung-Box 
statistics of the standardized innovations e,- —Xi/fa show <2(10) = 5.91(0.82) 
and <2(20) = 16.04(0.71) and those of ef give <2(10) = 5.35(0.87) and <2(20) = 
15.20(0.76), where the number in parentheses is the p value. Therefore, the refined 
model is adequate. The RATS program used to estimate the prior model is given 
in Appendix C. 

5.7 BIVARIATE MODELS FOR PRICE CHANGE AND DURATION 

In this section, we introduce a model that considers jointly the process of price 
change and the associated duration. As mentioned before, many intraday transac¬ 
tions of a stock result in no price change. Those transactions are highly relevant 
to trading intensity, but they do not contain direct information on price movement. 
Therefore, to simplify the complexity involved in modeling price change, we focus 
on transactions that result in a price change and consider a price change and dura¬ 
tion (PCD) model to describe the multivariate dynamics of price change and the 
associated time duration. 

266 

HIGH-FREQUENCY DATA ANALYSIS AND MARKET MICROSTRUCTURE 

We continue to use the same notation as before, but the definition is changed to 
transactions with a price change. Let t,- be the calendar time of the ith price change 
of an asset. As before, tj is measured in seconds from midnight of a trading day. Let 
Pt. be the transaction price when the ith price change occurred and At,- = — fi-i 
be the time duration between price changes. In addition, let A,- be the number of 
trades in the time interval fi) that result in no price change. This new variable 
is used to represent trading intensity during a period of no price change. Finally, 
let Di be the direction of the zth price change with D, = 1 when price goes up 
and Di — — 1 when the price comes down, and let Sj be the size of the ith price 
change measured in ticks. Under the new definitions, the price of a stock evolves 
over time by 

(5.46) 

and the transactions data consist of {At,-, A,-, Dj, 5,-} for the ith price change. The 
PCD model is concerned with the joint analysis of (At,-, Nj, D,-, 5,-). 

Remark. Focusing on transactions associated with a price change can reduce 
the sample size dramatically. For example, consider the intraday data of IBM stock 
from November 1, 1990 to January 31, 1991. There were 60,265 intraday trades, 
but only 19,022 of them resulted in a price change. In addition, there is no diurnal 
pattern in time durations between price changes. □ 

To illustrate the relationship among the price movements of all transactions 
and those of transactions associated with a price change, we consider the intraday 
tradings of IBM stock on November 21, 1990. There were 726 transactions on that 
day during normal trading hours, but only 195 trades resulted in a price change. 
Figure 5.16 shows the time plot of the price series for both cases. As expected, the 
price series are the same. 

The PCD model decomposes the joint distribution of (Att, A,, D,-, Sj-) given 

Ft-i as 

(5.47) 

This partition enables us to specify suitable econometric models for the condi¬ 
tional distributions and, hence, to simplify the modeling task. There are many 
ways to specify models for the conditional distributions. A proper specification 
might depend on the asset under study. Here we employ the specifications used by 
McCulloch and Tsay (2000), who use generalized linear models for the discrete¬ 
valued variables and a time series model for the continuous variable ln(Af,). 

For the time duration between price changes, we use the model 

ln(Af,-) = /So + )8i ln(At,-_i) + foSt-1 + aeit 

(5.48) 

BIVARIATE MODELS FOR PRICE CHANGE AND DURATION 

267 

Figure 5.16 Time plots of intraday transaction prices of IBM stock on November 21, 1990: (a) all 

transactions and (b) transactions that resulted in price change. 

where a is a positive number and {e,} is a sequence of iid N(0, 1) random variables. 
This is a multiple linear regression model with lagged variables. Other explanatory 
variables can be added if necessary. The log transformation is used to ensure the 
positiveness of time duration. 

The conditional model for Nj is further partitioned into two parts because empir¬ 
ical data suggest a concentration of Nj at 0. The first part of the model for Nj is 
the logit model 

p(Nj = 0| Atj, F;_i) = logit[a0 + a\ ln(At,)], (5.49) 

where logit(x) = exp(x)/[l + exp(x)], whereas the second part of the model is 

Nj\{Ni > 0, Atj, F,-_i) 

. , , . exp[yo + yi ln(Aff)] 
1 + g(^i')> — -7—-} .. /A.xA 
1 + exp [ yo + Y1 ln(A /,■)] 

(5.50) 

where ~ means “is distributed as,” and g{X) denotes a geometric distribution with 
parameter k, which is in the interval (0, 1). 

The model for direction D, is 

Dj\(Nj, Atj, Fj-\) = sign(jit + a^). 

(5.51) 

268 

HIGH-FREQUENCY DATA ANALYSIS AND MARKET MICROSTRUCTURE 

where e is a A(0, 1) random variable, and 

jXi = coq + A_l + (t>2 ln(At,), 

In (or,-) = 0  E a-j 

7 = 1 

= j8|A-l + A-2 + A—3 + A-4|. 

In other words, A is governed by the sign of a normal random variable with mean 
/U and variance of. A special characteristic of the prior model is the function 
for Info-,). For intraday transactions, a key feature is the price reversal between 
consecutive price changes. This feature is modeled by the dependence of A on 
A_ i in the mean equation with a negative a>\ parameter. However, there exists 
an occasional local trend in the price movement. The previous variance equation 
allows for such a local trend by increasing the uncertainty in the direction of price 
movement when the past data showed evidence of a local trend. For a normal 
distribution with a fixed mean, increasing its variance makes a random draw have 
the same chance to be positive and negative. This in turn increases the chance for 
a sequence of all positive or all negative draws. Such a sequence produces a local 
trend in price movement. 

To allow for different dynamics between positive and negative price movements, 

we use different models for the size of a price change. Specifically, we have 

A|(A = -1, Nit Ati, F,_i) ~ p(kd,i) + 1- with (5-52) 

In(kd,i) = Vd.o + r]d,\Ni + rjd,2 ln(At,) + t]d.sSi-1 

Al(A = 1, Ni, Ati, F,_i) ~ p{K,i) + 1, with (5.53) 

ln(AM>I) = rju<o + rjU'iNt + Vu,2 ln(Afi) + ^A-t, 

where p(X) denotes a Poisson distribution with parameter A, and 1 is added to the 
size because the minimum size is 1 tick when there is a price change. 

The specified models in Eqs. (5.48)—(5.53) can be estimated jointly by either 
the maximum-likelihood method or the Markov chain Monte Carlo methods. Based 
on Eq. (5.47), the models consist of six conditional models that can be estimated 
separately. 

Example 5.5. Consider the intraday transactions of IBM stock on November 
21, 1990. There are 194 price changes within normal trading hours. Figure 5.17 
shows the histograms of ln(Ar,), Ni, 7),, and S). The data for A are about equally 
distributed between “upward” and “downward” movements. Only a few transac¬ 
tions resulted in a price change of more than 1 tick; as a matter of fact, there were 
7 changes with 2 ticks and 1 change with 3 ticks. Using Markov chain Monte Carlo 
(MCMC) methods (see Chapter 12), we obtained the following models for the data. 
The reported estimates and their standard deviations are the posterior means and 

BIVARIATE MODELS FOR PRICE CHANGE AND DURATION 

269 

1.0 1.5 2.0 2.5 3.0 

Size, in ticks 

(c) 

0 5 10 15 20 

Number of trades 

(d) 

Figure 5.17 Histograms of intraday transactions data for IBM stock on November 21, 1990: (a) log 

durations between price changes, (b) direction of price movement, (c) size of price change measured in 
ticks, and (d) number of trades without price change. 

standard deviations of MCMC draws with 9500 iterations. The model for the time 
duration between price changes is 

ln(Aff) = 4.023 + 0.032ln(Af,-_i) - 0.025Sf_i + 1.403ef, 

where standard deviations of the coefficients are 0.415, 0.073, 0.384, and 0.073, 
respectively. The fitted model indicates that there was no dynamic dependence in 
the time duration. For the Nj variable, we have 

Pr(7V; > 0|At;, F(_0 = logit[—0.637 + 1.740ln(Af,)}, 

where standard deviations of the estimates are 0.238 and 0.248, respectively. Thus, 
as expected, the number of trades with no price change in the time interval r, ) 
depends positively on the length of the interval. The magnitude of Ni when it is 
positive is 

Ni\(Ni >0, ArifF,_1)~l+g(Aj), 

exp[0.178 — 0.910 ln(At;)] 

1 +exp[0.178 -0.9101n(Af/)]’ 

where standard deviations of the estimates are 0.246 and 0.138, respectively. The 
negative and significant coefficient of ln(At/) means that iV(- is positively related 
to the length of the duration A because a large In (A I,) implies a small X;, which 

270 

HIGH-FREQUENCY DATA ANALYSIS AND MARKET MICROSTRUCTURE 

in turn implies higher probabilities for larger A; see the geometric distribution in 

Eq. (5.27). 

The fitted model for A is 

!M = 0.049 - 0.840A-1 - 0.004 ln(Afi), 

ln(cr;) = 0.244| A_l + A_ 2 + A-3 + A-4I1 

where standard deviations of the parameters in the mean equation are 0.129, 0.132, 
and 0.082, respectively, whereas the standard error for the parameter in the variance 
equation is 0.182. The price reversal is clearly shown by the highly significant 
negative coefficient of A_u The marginally significant parameter in the variance 
equation is exactly as expected. Finally, the fitted models for the size of a price 

change are 

ln(A,t) = 1.024 - 0.327A + 0.412 ln(Afi) - 4.474S/_i, 

In (At) = -3.683 - 1.542A +0.419 ln(Af/) + 0.921$-!, 

where standard deviations of the parameters for the “down size” are 3.350, 0.319, 
0.599, and 3.188, respectively, whereas those for the “up size” are 1.734, 0.976, 
0.453, and 1.459. The interesting estimates of the prior two equations are the 
negative estimates of the coefficient of A,. A large A means there were more 
transactions in the time interval (t,_i, fi) with no price change. This can be taken 
as evidence of no new information available in the time interval (At, U). Conse¬ 
quently, the size for the price change at tt should be small. A small Xuj or A,i for 
a Poisson distribution gives precisely that. 

In summary, granted that a sample of 194 observations in a given day may not 
contain sufficient information about the trading dynamics of IBM stock, but the 
fitted models appear to provide some sensible results. McCulloch and Tsay (2000) 
extend the PCD model to a hierarchical framework to handle all the data of the 
63 trading days between November 1, 1990, and January 31, 1991. Many of the 
parameter estimates become significant in this extended sample, which has more 
than 19,000 observations. For example, the overall estimate of the coefficient of 
ln(At;_i) in the model for time duration ranges from 0.04 to 0.1, which is small, 
but significant. 

Finally, using transactions data to test microstructure theory often requires a 
careful specification of the variables used. It also requires a deep understanding of 
the way by which the market operates and the data are collected. However, ideas of 
the econometric models discussed in this chapter are useful and widely applicable 
in analysis of high-frequency data. 

5.8 APPLICATION 

In this section we apply the ACD model to stock volatility modeling. Consider the 
daily range of the log price of Apple stock from January 4, 1999, to November 20, 

APPLICATION 

271 

2007. The data are obtained from Yahoo Finance and consist of 2235 observations. 
This series was analyzed in Tsay (2009). The range of daily log prices has been 
used in the literature as a robust alternative to volatility modeling; see Chapter 3 
and Chou (2005) and the references therein. Apple stock had two-for-one splits on 
June 21, 2000, and February 28, 2005, during the sample period, but no adjustments 
are needed for the splits because we use daily range of log price. As mentioned 
before, stock prices in the U.S. markets switched from the tick size ^ of a dollar 
to the decimal system on January 29, 2001. Such a change affected the bid-ask 
spread of stock prices. We shall employ intervention analysis to study the impact 
of such a policy change on the stock volatility. 

The sample mean, standard deviation, minimum, and maximum of the range 
of log prices are 0.0407, 0.0218, 0.0068, and 0.1468, respectively. The sample 
skewness and excess kurtosis are 1.3 and 2.13, respectively. Figure 5.18(a) shows 
the time plot of the range series. The volatility seems to be increasing from 2000 
to 2001, then decreasing to a stable level after 2002. It seems to increase somewhat 
at the end of the series. Figure 5.19(a) shows the sample ACF of the daily range 
series. The sample ACFs are highly significant and decay slowly. 

We fit EACD(1,1), WACD(1,1), and GACD(1,1) models to the daily range 
series. The estimation results, along with the Ljung-Box statistics for the standard¬ 
ized residual series and its squared process, are given in Table 5.9. The parameter 
estimates for the duration equation are stable for all three models, except for the 

0) 
CO 
c 
CO 
CL 

CD O 

C\J 
O 

CO 

CM 

CO 

2000 

2002 

2004 

2006 

2008 

Time 

(a) 

2000 2002 2004 2006 2008 

Time 

(b) 

Figure 5.18 Time plots of daily range of log price of Apple stock from January 4, 1999, to November 
20, 2007: (a) observed daily range and (b) standardized residuals of a GACD(1,1) model. 

272 

HIGH-FREQUENCY DATA ANALYSIS AND MARKET MICROSTRUCTURE 

Lag 

(a) 

Figure 5.19 Sample autocorrelation function of daily range of log prices of Apple stock from January 
4, 1999, to November 20, 2007: (a) ACF of daily range and (b) ACF of standardized residual series of 

GACD(1,1) model. 

TABLE 5.9 Estimation Results of EACD(1,1), WACD(1,1), and GACD(1,1) Models 
for Daily Range of Log Prices of Apple Stock from January 4, 1999 to November 20, 
2007“ 

Parameters 

Checking 

Model 

a0 

a l 

fii 

a 

K 

2(10) 

2*(10) 

EACD 

WACD 

GACD 

0.0007 
(0.0005) 
0.0013 
(0.0003) 
0.0010 
(0.0002) 

0.133 
(0.036) 
0.131 
(0.015) 
0.133 
(0.015) 

0.849 
(0.044) 
0.835 
(0.021) 
0.843 
(0.019) 

2.377 
(0.031) 
1.622 
(0.029) 

2.104 
(0.040) 

16.65 
(0.082) 
13.66 
(0.189) 
14.62 
(0.147) 

12.12 
(0.277) 
9.74 
(0.464) 
11.21 
(0.341) 

"The standard errors of the estimates and the p values of the Ljung-Box statistics are in parentheses, 

where Q(10) and <2*(10) are for standardized residual series and its squared process, respectively. 

constant term of the EACD model, which appears to be statistically insignificant 
at the usual 5% level. Indeed, in this particular instance, the EACD(1,1) model 
fares slightly worse than the other two ACD models. Between the WACD(1,1) and 
GACD(1,1) models, we slightly prefer the GACD(1,1) model because it fits the 
data better and is more flexible. 

APPLICATION 

273 

Figure 5.19(b) shows the sample ACFs of the standardized residuals of the 
fitted GACD(1,1) model. From the plot, the standardized residuals do not have 
significant serial correlations, even though the lag-1 sample ACF is slightly above 
its two standard error limit. The lag-1 serial correlation is removed when we use 
nonlinear ACD models later. Figure 5.18(b) shows the time plot of the standardized 
residuals of the GACD(1,1) model. The residuals do not show any pattern of 
model inadequacy. The mean, standard deviation, minimum, and maximum of the 
standardized residuals are 0.203, 4.497, 0.999, and 0.436, respectively. 

It is interesting to see that the estimates of the shape parameter a are greater 
than 1 for both WACD(1,1) and GACD(1,1) models, indicating that the hazard 
function of the daily range is monotonously increasing. This is consistent with the 
idea of volatility clustering, for large volatility tends to be followed by another 
large volatility. 

Threshold ACD model 
To refine the GACD(1,1) model for the daily range of log prices of Apple stock, 
we employ a two-regime threshold WACD(1,1) model. Some preliminary analysis 
of the threshold WACD models indicates that the major difference in the parameter 
estimates between the two regimes is the shape parameter of the Weibull distribu¬ 
tion. Thus, we focus on a TWACD(2;1,1) model with different shape parameters 
for the two regimes. 

Table 5.10 gives the maximized log-likelihood value of a TWACD(2;1,1) model 
with delay d = 1 and threshold r e {*{q)\q = 60, 65, ..., 95}, where X(q) denotes 
the sample qih percentile. From the table, the threshold 0.04753 is selected, which 
is the 70th percentile of the data. The fitted model is 

xt = ifi = 0.0013 + 0.1539x,_i + 0.8131i/r,-_i, 

where the standard errors of the coefficients are 0.0003, 0.0164, and 0.0215, respec¬ 
tively, and e, follows the standardized Weibull distribution as 

W (2.2156) if Xi-i < 0.04753, 

€i ~ (W(2.7119) otherwise, 

where the standard errors of the two shape parameters are 0.0394 and 0.0717, 
respectively. 

TABLE 5.10 Selection of Threshold of TWACD(2;1,1) Model for Daily Range of 
Log Prices of Apple Stock from January 4, 1999, to November 20, 2007° 

Quantile 

r x 100 

l(r) x 103 

60 

4.03 

6.073 

65 

4.37 

70 

4.75 

6.076 

6.079 

75 

5.15 

6.076 

80 

5.58 

85 

6.16 

90 

7.07 

95 

8.47 

6.078 

6.074 

6.072 

6.066 

The threshold variable is x,_]. 

274 

HIGH-FREQUENCY DATA ANALYSIS AND MARKET MICROSTRUCTURE 

(a) 

(b) 

Figure 5.20 Model fitting for daily range of log price of Apple stock from January 4, 1999, to 

November 20, 2007: (a) conditional expected durations of fitted TWACD(2;1,1) model and (b) sample 

ACF of standardized residuals. 

Figure 5.20(a) shows the time plot of the conditional expected duration for 
the fitted TWACD(2;1,1) model, that is, i/q, whereas Figure 5.20(b) gives the 
residual ACFs for the fitted model. All residual ACFs are within the two stan¬ 
dard error limits. Indeed, we have Q( 1) = 4.01(0.05) and <2(10) = 9.84(0.45) for 
the standardized residuals and Q*( 1) = 0.83(0.36) and <2*(10) =9.35(0.50) for 
the squared series of the standardized residuals, where the number in parentheses 
denotes p value. Note that the threshold variable jc,_i is also selected based on the 
value of the log-likelihood function. For instance, the log-likelihood function of 
the TWACD(2;1,1) model assumes the value 6.069xl03 and 6.070 x 103, respec¬ 
tively, for d = 2 and 3 when the threshold is 0.04753. These values are lower than 
that when <7=1. 

Intervention Analysis 
High-frequency financial data are often influenced by external events, for example, 
an increase or drop in interest rates by the U.S. Federal Open Market Committee 
or a jump in the oil price. Applications of ACD models in finance are often faced 
with the problem of outside interventions. To handle the effects of external events, 
the intervention analysis of Box and Tiao (1975) can be used. Here we apply the 
analysis to the daily range series of Apple stock to study the impact of change in 
tick size on the stock volatility. 

APPLICATION 

275 

Let t0 be the time of intervention. For the Apple stock, ta = 522, which cor¬ 
responds to January 26, 2001, the last trading day before the change in tick size. 
Since more observations in the sample are after the intervention, we define the 
indicator variable 

0 otherwise, 

to signify the absence of intervention. Since a larger tick size tends to increase the 
observed daily price range, it is reasonable to assume that the conditional expected 
range would be higher before the intervention. A simple intervention model for the 
daily range of Apple stock is then given by 

Xi = fi 

€l i 

^2 i 

if jc/_i < 0.04753, 

otherwise, 

where Vfi follows the model 

ti = «o + yIito) + aixi-i + Mi-i, 

(5.54) 

where y denotes the decrease in expected duration due to the decimalization of 
stock prices. In other words, the expected durations before and after the intervention 
are 

«o + Y , a o 

- and -, 
1 — ofi - ft 1 -ai -ft 

respectively. We expect y > 0. 

The fitted duration equation for the intervention model is 

ft = 0.0021 + 0.0011//522) + 0.1595x,_i + 0.7828^,-1, 

where the standard errors of the estimates are 0.0004, 0.0003, 0.0177, and 0.0264, 
respectively. The estimate y is significant at the 1% level. For the innovations, we 
have 

W(2.2835) if xt-i < 0.04753, 

W(2.7322) otherwise. 

The standard errors of the two estimates of the shape parameter are 0.0413 and 
0.0780, respectively. Figure 5.21(a) shows the expected durations of the inter¬ 
vention model, and Figure 5.21(b) shows the ACF of the standardized residuals. 
All residual ACFs are within the two standard error limits. Indeed, for the stan¬ 
dardized residuals, we have Q{\) = 2.37(0.12) and £7(10) = 6.24(0.79). For 
the squared series of the standardized residuals, we have Q*(l) = 0.34(0.56) and 
<2*(10) = 6.79(0.75). As expected, y > 0 so that the decimalization indeed reduces 
the expected value of the daily range. This simple analysis shows that, as expected, 
adopting the decimal system reduces the volatility of Apple stock. 

276 

HIGH-FREQUENCY DATA ANALYSIS AND MARKET MICROSTRUCTURE 

(a) 

(b) 

Figure 5.21 Model fitting for daily range of log price of Apple stock from January 4, 1999, to 

November 20, 2007: (a) conditional expected durations of fitted TWACD(2;1,1) model with intervention 
and (b) sample ACF of corresponding standardized residuals. 

APPENDIX A: REVIEW OF SOME PROBABILITY DISTRIBUTIONS 

Exponential Distribution 
A random variable X has an exponential distribution with parameter P > 0 if its 
probability density function (pdf) is given by 

me) =  p 
o 

if x > 0, 

otherwise. 

Denoting such a distribution by X ~ exp(£), we have E(X) = p and Var(X) = 
P2. The cumulative distribution function (CDF) of X is 

F(x\P) = 

0 

if x < 0, 

1 - e~xW 

if jc > 0. 

When P = 1, X is said to have a standard exponential distribution. 

APPENDIX A: REVIEW OF SOME PROBABILITY DISTRIBUTIONS 

277 

Gamma Function 
For k > 0, the gamma function T(/c) is defined by 

OO 

r (k)= / xK-le~xdx. 

o 

The most important properties of the gamma function are: 

1. For any k> 1, TO) = (k - \)T(k - 1). 

2. For any positive integer m, T{m) = (m — 1)!. 

3. r(i) = V*. 

The integration 

is an incomplete gamma function. Its values have been tabulated in the literature. 
Computer programs are now available to evaluate the incomplete gamma function. 

Gamma Distribution 
A random variable X has a gamma distribution with parameter k and /I (k > 0, 
P > 0) if its pdf is given by 

f(x\K, P) = J PkT(k) 

e x^ if x > 0, 

0 

otherwise. 

By changing variable y = x//3, one can easily obtain the moments of X: 

In particular, the mean and variance of X are E(X) = left and Var(X) = /r^2. When 
ft = 1, the distribution is called a standard gamma distribution with parameter k. 
We use the notation G ~ gamma(v) to denote that G follows a standard gamma 
distribution with parameter k. The moments of G are 

m > 0. 

(5.55) 

278 

HIGH-FREQUENCY DATA ANALYSIS AND MARKET MICROSTRUCTURE 

Weibull Distribution 
A random variable X has a Weibull distribution with parameters a and P (a > 0, 
P > 0) if its pdf is given by 

f(x\a, P) 

a 

pa 
0 

a — 1 

-(*/«“ 

if x > 0, 

if a < 0, 

where P and a are the scale and shape parameters of the distribution. The mean 
and variance of X are 

E(X)=p T 1 + 

1 

a 

Var(X) = pz 

and the CDF of X is 

f / 2 \ 

i
"

1

+
V a / 

1

r / im 2' 
+
V a /. 

1
-

i

F{x\a, P) 

0 if x < 0, 

1 — if x > 0. 

When a = 1, the Weibull distribution reduces to an exponential distribution. 
Define Y = X/[PY{\ + 1/a)]. We have E{Y) = 1 and the pdf of Y is 

1

a 

1
-

f(y l«) 

1

 a 
  _
+
\ a / 

ya~x exp  _  r(i + -)y 
L V a J J 

a ■ 

0 otherwise, 

(5.56) 

where the scale parameter P disappears due to standardization. The CDF of the 
standardized Weibull distribution is 

0 

F(y\a) = 

1 — exp 

r 

if y < 0, 

if y > 0, 

and we have E(Y) = 1 and Var(7) = T(1 -t-2/a)/[T(l + 1/a)]2 - 1. For a dura¬ 
tion model with Weibull innovations, the pdf in Eq. (5.56) is used in the maximum- 
likelihood estimation. 

Generalized Gamma Distribution 
A random variable X has a generalized gamma distribution with parameter a, p, k 
(a > 0, P > 0, and k > 0) if its pdf is given by 

f(x\a, p, k) = 

axK0C * 
PKaV(K) 

0 

r 
. V/sJ . 

if a > 0, 

otherwise, 

 
 
 
 
 
 
 
 
 
APPENDIX B: HAZARD FUNCTION 

279 

where f is a scale parameter, and a and k are shape parameters. This distribution 
can be written as 

where G is a standard gamma random variable with parameter k. The pdf of X 
can be obtained from that of G by the technique of changing variables. Similarly, 
the moments of X can be obtained from that of G in Eq. (5.55) by 

E(X") = EHfia'/’T) = Pm e(g"'“)=rr^+-m/-} = 

H H H r(/c) t(k) 

When k = 1, the generalized gamma distribution reduces to that of a Weibull 
distribution. Thus, the exponential and Weibull distributions are special cases of 
the generalized gamma distribution. 

The expectation of a generalized gamma distribution is E{X) — /3F(/c + 1 /or)/ 
T(x). In duration models, we need a distribution with unit expectation. Therefore, 
defining a random variable Y — XX/ft, where X = r(/c)/r(/c + 1/a), we have 
E(Y) = 1 and the pdf of Y is 

f(y\oc,K) 

ay k a—1 

xKar (k) 
o 

exp 

if y > 0, 

otherwise, 

(5.57) 

where again the scale parameter /3 disappears and X = Y(ic)/ V(k + 1/a). 

APPENDIX B: HAZARD FUNCTION 

A useful concept in modeling duration is the hazard function implied by a dis¬ 
tribution function. For a random variable X, the survival function is defined as 

SOc) = P(X > jc) = 1 - P(X < x) = 1 - CDF(x), x > 0, 

which gives the probability that a subject, which follows the distribution of X, 
survives at the time x. The hazard function (or intensity function) of X is then 
defined by 

h(x) = 

fix) 

S(x)’ 

(5.58) 

where /(•) and S(-) are the pdf and survival function of X, respectively. 

280 

HIGH-FREQUENCY DATA ANALYSIS AND MARKET MICROSTRUCTURE 

Example 5.6. For the Weibull distribution with parameters a and (5, the sur¬ 

vival function and hazard function are 

In particular, when a = 1, we have h(x\/3) = 1//3. Therefore, for an exponential 
distribution, the hazard function is constant. For a Weibull distribution, the haz¬ 
ard is a monotone function. If a > 1, then the hazard function is monotonously 
increasing. If a < 1, the hazard function is monotonously decreasing. For the gen¬ 
eralized gamma distribution, the survival function and, hence, the hazard function 
involve the incomplete gamma function. Yet the hazard function may exhibit vari¬ 
ous patterns, including U shape or inverted U shape. Thus, the generalized gamma 
distribution provides a flexible approach to modeling the duration of stock trans¬ 

actions. 

For the standardized Weibull distribution, the survival and hazard functions are 

APPENDIX C: SOME RATS PROGRAMS FOR DURATION MODELS 

The data used are adjusted time durations of intraday transactions of IBM stock 
from November 1 to November 9, 1990. The file name is ibmlto5 . txt and it has 
3534 observations. 

Program for Estimating a WACD(1,1) Model 

all 0 3534:1 

open data ibmlto5.txt 

data(org=obs) / x rl 

set psi = 1.0 

nonlin aO al bl al 

frml gvar = a0+al*x(t-l)+bl*psi(t—1) 

frml gma = %LNGAMMA(1.0+1.0/al) 

frml gin =al*gma(t)+log(al)-log(x(t)) $ 

+al*log(x(t)/(psi(t)=gvar(t)))-(exp(gma(t))*x(t)/psi(t))**al 

smpl 2 3534 

compute aO = 0.2, al = 0.1, bl = 0.1, al = 0.8 

maximize(method=bhhh,recursive,iterations=150) gin 

set fv = gvar(t) 

set resid = x(t)/fv(t) 

set residsq = resid(t)*resid(t) 

APPENDIX C: SOME RATS PROGRAMS FOR DURATION MODELS 

281 

cor(qstats,number=20,span=10) resid 

cor(qstats,number=20,span=10) residsq 

Program for Estimating a GACD(1,1) Model 

all 0 3534:1 

open data ibmlto5.txt 

data(org=obs) / x rl 

set psi = 1.0 

nonlin aO al bl al ka 

frml cv = a0+al*x(t-l)+bl*psi(t-1) 

frml gma = %LNGAMMA(ka) 

frml lam = exp(gma(t))/exp(%LNGAMMA(ka+(1.0/al))) 

frml xlam = x(t)/(lam(t)*(psi(t)=cv(t))) 

frml gin =-gma(t)+log(al/x(t))+ka*al*log(xlam(t)) 

-(xlam(t))**al 

smpl 2 3534 

compute aO = 0.238, al = 0.075, bl = 0.857, al = 0.5, ka = 4.0 

nlpar(criterion=value,cvcrit=0.00001) 

maximize(method=bhhh,recursive,iterations=150) gin 

set fv = cv(t) 

set resid = x(t)/fv(t) 

set residsq = resid(t)*resid(t) 

cor(qstats,number=20,span=10) resid 

cor(qstats,number=20,span=10) residsq 

Program for Estimating a TAR-WACD(1,1) Model 
The threshold 3.79 is prespecified. 

all 0 3534:1 

open data ibmlto5.txt 

data(org=obs) / x rt 

set psi = 1.0 

nonlin al a2 al bO b2 bl 

frml u = ((x(t-l)-3.79)/abs(x(t-l)-3.79)+1.0)/2.0 

frml cpl = al*x(t-1)+a2*psi(t-1) 

frml gmal = %LNGAMMA(1.0+1.0/al) 

frml cp2 = b0+b2*psi(t-1) 

frml gma2 = %LNGAMMA(1.0+1.0/bl) 

frml cp = cpl(t)*(l-u(t))+cp2(t)*u(t) 

frml glnl =al*gmal(t)+log(al)-log(x(t)) $ 

+al*log (x (t) / (psi (t) =cp (t) ) ) - (exp (gmal (t) ) *x (t) /psi (t) ) **al 

frml gln2 =bl*gma2(t)+log(bl)-log(x(t)) $ 

+bl*log (x (t) / (psi (t) =cp (t) ) ) - (exp (gma2 (t) ) *x (t) /psi (t) ) **bl 

frml gin = glnl(t)*(1-u(t))+gln2(t)*u(t) 

smpl 2 3534 

compute al = 0.2, a2 = 0.85, al -  0.9 

282 

HIGH-FREQUENCY DATA ANALYSIS AND MARKET MICROSTRUCTURE 

compute bO = 1.8, b2 = 0.5, bl = 0.8 

maximize(method=bhhh,recursive,iterations=150) gin 

set fv = cp(t) 

set resid = x(t)/fv(t) 

set residsq = resid(t)*resid(t) 

cor(qstats,number=20,span=10) resid 

cor(qstats,number=20,span=10) residsq 

EXERCISES 

5.1. Let rt be the log return of an asset at time t. Assume that {rt} is a Gaussian 
white noise series with mean 0.05 and variance 1.5. Suppose that the proba¬ 
bility of a trade at each time point is 40% and is independent of rt. Denote 
the observed return by r°. Is r° serially correlated? If yes, calculate the first 
three lags of autocorrelations of r°. 

5.2. Let P, be the observed market price of an asset, which is related to the fun¬ 
damental value of the asset P* via Eq. (5.9). Assume that AP* = P* — P*_{ 
forms a Gaussian white noise series with mean zero and variance 1.0. Sup¬ 
pose that the bid-ask spread is two ticks. What is the lag-1 autocorrela¬ 
tion of the price change series AP, = P, — Pr_j when the tick size is $|? 
What is the lag-1 autocorrelation of the price change when the tick size 
is $jg? 

5.3. The file ibm-d2-dur. txt contains the adjusted durations between trades of 
IBM stock on November 2, 1990. The file has three columns consisting of 
day, time of trade measured in seconds from midnight, and adjusted durations. 

(a) Build an EACD model for the adjusted duration and check the fitted 

model. 

(b) Build a WACD model for the adjusted duration and check the fitted 

model. 

(c) Build a GACD model for the adjusted duration and check the fitted 

model. 

(d) Compare the prior three duration models. 

5.4. The file mmm9 912-dtp. txt contains the transactions data of the stock of 3M 
Company in December 1999. There are three columns: day of the month, time 
of transaction in seconds from midnight, and transaction price. Transactions 
that occurred after 4:00 pm Eastern time are excluded. 

(a) Is there a diurnal pattern in 3M stock trading? You may construct a time 
series nt, which denotes the number of trades in a 5-minute time interval 
to answer this question. 

(b) Use the price series to confirm the existence of a bid-ask bounce in 

intraday trading of 3M stock. 

EXERCISES 

283 

(c) Tabulate the frequencies of price change in multiples of tick size $-^. 
You may combine changes with 5 ticks or more into a category and those 
with —5 ticks or beyond into another category. 

5.5. Consider again the transactions data of 3M stock in December 1999. 

(a) Use the data to construct an intraday 5-minute log return series. Use the 
simple average of all transaction prices within a 5-minute interval as the 
stock price for the interval. Is the series serially correlated? You may use 
Ljung-Box statistics to test the hypothesis with the first 10 lags of the 
sample autocorrelation function. 

(b) There are seventy-seven 5-minute returns in a normal trading day. Some 
researchers suggest that the sum of squares of the intraday 5-minute 
returns can be used as a measure of daily volatility. Apply this approach 
and calculate the daily volatility of the log return of 3M stock in Decem¬ 
ber 1999. Discuss the validity of such a procedure to estimate daily 
volatility. 

5.6. The file mmm9912-adur.txt contains an adjusted intraday trading duration 
of 3M stock in December 1999. There are thirty-nine 10-minute time intervals 
in a trading day. Let d, be the average of all log durations for the zth 10- 
minute interval across all trading days in December 1999. Define an adjusted 
duration as tj/exp(di), where j is in the z'th 10-minute interval. Note that 
more sophisticated methods can be used to adjust the diurnal pattern of 
trading duration. Here we simply use a local average. 

(a) Is there a diurnal pattern in the adjusted duration series? Why? 

(b) Build a duration model for the adjusted series using exponential innova¬ 

tions. Check the fitted model. 

(c) Build a duration model for the adjusted series using Weibull innovations. 

Check the fitted model. 

(d) Build a duration model for the adjusted series using generalized gamma 

innovations. Check the fitted model. 

(e) Compare and comment on the three duration models built before. 

5.7. To gain experience in analyzing high-frequency financial data, consider the 
trade data of Boeing stock from December 1 to December 5, 2008. The data 
are in five files: taq-td-bal2012008 . txt to taq-td-bal2052008 . txt. 
Each file has five columns, namely hour, minute, second, price, and vol¬ 
ume. Only transactions within the normal trading hours (9:30 am to 4:00 pm 
Eastern time) are kept. Construct a time series of the number of trades in an 
intraday 5-minute time interval. Is there any diurnal pattern in the constructed 
series? You can simply compute the sample ACF of the series to answer this 

question. 

5.8. Again, consider the high-frequency data of Boeing stock from December 1 
to December 5, 2008. Construct an intraday 5-minute return series. Note that 

284 

HIGH-FREQUENCY DATA ANALYSIS AND MARKET MICROSTRUCTURE 

the price of the stock in a 5-minute interval (e.g., 9:30 to 9:35 am) is the last 
transaction price within the time interval. For simplicity, ignore overnight 
returns. Are there serial correlations in the 5-minute return series? Use 10 
lags of the ACF and 5% significance level to perform of test. 

5.9. Consider the same problem as in Exercise 5.8, but use 10-minute time inter¬ 

vals. 

5.10. Again, consider the high-frequency data of Boeing stock. Compute the per¬ 
centage of consecutive transactions without price change in the sample. 

REFERENCES 

Box, G. E. P. and Cox, D. R. (1964). An analysis of transformations (with discusions). 

Journal of the Royal Statistical Society, Series B 26: 211-246. 

Box, G. E. P. and Tiao, G. C. (1975). Intervention analysis with applications to economic and 

environmental problems. Journal of the American Statistical Association 70: 70-79. 

Campbell, J. Y., Lo, A. W„ and MacKinlay, A. C. (1997). The Econometrics of Financial 

Markets. Princeton University Press, Princeton, NJ. 

Cho, D., Russell, J. R., Tiao, G. C., and Tsay, R. S. (2003). The magnet effect of price limits: 
Evidence from high frequency data on Taiwan stock exchange. Journal of Empirical 
Finance 10: 133-168. 

Chou, R. Y. (2005). Forecasting financial volatilities with extreme values: The condi¬ 
tional autoregressive range (CARR) model. Journal of Money, Credit and Banking 37: 
561-582. 

Engle, R. F. and Russell, J. R. (1998). Autoregressive conditional duration: A new model 

for irregularly spaced transaction data. Econometrica 66: 1127-1162. 

Ghysels, E. (2000). Some econometric recipes for high-frequency data cooking. Journal of 

Business and Economic Statistics 18: 154-163. 

Hasbrouck, J. (1992). Using the TORQ Database. Stem School of Business, New York 

University, New York. 

Hasbrouck, J. (1999). The dynamics of discrete bid and ask quotes. Journal of Finance 54: 

2109-2142. 

Hauseman, J., Lo, A., and MacKinlay, C. (1992). An ordered probit analysis of transaction 

stock prices. Journal of Financial Economics 31: 319-379. 

Lo, A. and MacKinlay, A. C. (1990). An econometric analysis of nonsynchronous trading. 

Journal of Econometrics 45: 181-212. 

McCulloch, R. E. and Tsay, R. S. (2000). Nonlinearity in high frequency data and hierarchical 

models. Studies in Nonlinear Dynamics and Econometrics 5: 1-17. 

Roll, R. (1984). A simple implicit measure of the effective bid-ask spread in an efficient 

market. Journal of Finance 39: 1127-1140. 

Rydberg, T. H. and Shephard, N. (2003). Dynamics of trade-by-trade price movements: 

Decomposition and models. Journal of Financial Econometrics 1: 2-25. 

Stoll, H. and Whaley, R. (1990). Stock market structure and volatility. Review of Financial 

Studies 3: 37-71. 

REFERENCES 

285 

Tsay, R. S. (2009). Autoregressive conditional duration models. In Applied Econometrics, 
Palgrave Handbook of Econometrics, Vol. 2, T. C. Mills and K. Patterson (eds.), Bas¬ 
ingstoke, Hampshire, UK. 

Wood, R. A. (2000). Market microstructure research databases: History and projections. 

Journal of Business & Economic Statistics 18: 140-145. 

Zhang, M. Y., Russell, J. R., and Tsay, R. S. (2001). A nonlinear autoregressive conditional 
duration model with applications to financial transaction data. Journal of Econometrics 
104: 179-207. 

Zhang, M. Y., Russell, J. R., and Tsay, R. S. (2008). Determinants of bid and ask quotes 

and implications for the cost of trading. Journal of Empirical Finance 15: 656-678. 

■ 

CHAPTER 6 

Continuous-Time Models 
and Their Applications 

The price of a financial asset evolves over time and forms a stochastic process, 
which is a statistical term used to describe the evolution of a random variable over 
time. The observed prices are a realization of the underlying stochastic process. The 
theory of stochastic process is the basis on which the observed prices are analyzed 
and statistical inference is made. 

There are two types of stochastic process for modeling the price of an asset. The 
first type is called the discrete-time stochastic process, in which the price changes at 
discrete time points. All the processes discussed in the previous chapters belong to 
this category. For example, the daily closing price of IBM stock on the New York 
Stock Exchange forms a discrete-time stochastic process. Here the price changes 
only at the closing of a trading day. Price movements within a trading day are 
not necessarily relevant to the observed daily price. The second type of stochastic 
process is the continuous-time process, in which the price changes continuously, 
even though the price is only observed at discrete time points. One can think of 
the price as the “true value” of the stock that always exists and is time varying. 

For both types of process, the price can be continuous or discrete. A continuous 
price can assume any positive real number, whereas a discrete price can only 
assume a countable number of possible values. Assume that the price of an asset is 
a continuous-time stochastic process. If the price is a continuous random variable, 
then we have a continuous-time continuous process. If the price itself is discrete, 
then we have a continuous-time discrete process. Similar classifications apply to 
discrete-time processes. The series of price change in Chapter 5 is an example of 
a discrete-time discrete process. 

In this chapter, we treat the price of an asset as a continuous-time continuous 
stochastic process. Our goal is to introduce the statistical theory and tools needed 
to model financial assets and to price options. We begin the chapter with some 
terminologies of stock options used in the chapter. In Section 6.2, we provide a brief 

Analysis of Financial Time Series, Third Edition, By Ruey S. Tsay 
Copyright © 2010 John Wiley & Sons, Inc. 

287 

288 

CONTINUOUS-TIME MODELS AND THEIR APPLICATIONS 

introduction of Brownian motion, which is also known as a Wiener process. We 
then discuss some diffusion equations and stochastic calculus, including the well- 
known Ito lemma. Most option pricing formulas are derived under the assumption 
that the price of an asset follows a diffusion equation. We use the Black-Scholes 
formula to demonstrate the derivation. Finally, to handle the price variations caused 
by rare events (e.g., a profit warning), we also study some simple diffusion models 

with jumps. 

If the price of an asset follows a diffusion equation, then the price of an option 
contingent to the asset can be derived by using hedging methods. However, with 
jumps the market becomes incomplete and there is no perfect hedging of options. 
The price of an option is then valued either by using diversifiability of jump risk 
or defining a notion of risk and choosing a price and a hedge that minimize this 
risk. For basic applications of stochastic processes in derivative pricing, see Cox 
and Rubinstein (1985) and Hull (2007). 

6.1 OPTIONS 

A stock option is a financial contract that gives the holder the right to trade a certain 
number of shares of a specified common stock by a certain date for a specified 
price. There are two types of options. A call option gives the holder the right to 
buy the underlying stock; see Chapter 3 for a formal definition. A put option gives 
the holder the right to sell the underlying stock. The specified price in the contract 
is called the strike price or exercise price. The date in the contract is known as the 
expiration date or maturity. American options can be exercised at any time up to 
the expiration date. European options can be exercised only on the expiration date. 
The value of a stock option depends on the value of the underlying stock. Let 
K be the strike price and P be the stock price. A call option is in-the-money when 
P > K, at-the-money when P = K, and out-of-the-money when P < K. A put 
option is in-the-money when P < K, at-the-money when P = K, and out-of-the- 
money when P > K. In general, an option is in-the-money when it would lead to 
a positive cash flow to the holder if it were exercised immediately. An option is 
out-of-the-money when it would lead to a negative cash flow to the holder if it 
were exercised immediately. Finally, an option is at-the-money when it would lead 
to zero cash flow if it were exercised immediately. Obviously, only in-the-money 
options are exercised in practice. For more information on options, see Hull (2007). 

6.2 SOME CONTINUOUS-TIME STOCHASTIC PROCESSES 

In mathematical statistics, a continuous-time continuous stochastic process is 
defined on a probability space (£2, F, P), where £2 is a nonempty space, F is a a 
field consisting of subsets of £2, and P is a probability measure; see Chapter 1 of 
Billingsley (1986). The process can be written as {x(17, r)}, where t denotes time 
and is continuous in [0, 00). For a given t, x(rj, t) is a real-valued continuous 

SOME CONTINUOUS-TIME STOCHASTIC PROCESSES 

289 

random variable (i.e., a mapping from £2 to the real line), and r] is an element of 
£2. For the price of an asset at time t, the range of x(rj, t) is the set of nonnegative 
real numbers. For a given rj, {x(r],t)} is a time series with values depending on 
the time t. For simplicity, we write a continuous-time stochastic process as {xt} 
with the understanding that, for a given t, x, is a random variable. In the literature, 
some authors use x(t) instead of xt to emphasize that t is continuous. However, 
we use the same notation xt, but call it a continuous-time stochastic process. 

6.2.1 Wiener Process 

In a discrete-time econometric model, we assume that the shocks form a white 
noise process, which is not predictable. What is the counterpart of shocks in a 
continuous-time model? The answer is the increments of a Wiener process, which 
is also known as a standard Brownian motion. There are many ways to define a 
Wiener process {wt}. We use a simple approach that focuses on the small change 
Aw, = u;f+Af — w, associated with a small increment At in time. A continuous¬ 
time stochastic process {wt} is a Wiener process if it satisfies 

1. Aw, = Ca/a7, where € is a standard normal random variable; and 

2. Aw, is independent of Wj for all j < t. 

The second condition is a Markov property saying that conditional on the present 
value w,, any past information of the process, Wj with j < t, is irrelevant to the 
future w,+i with £ >0. From this property, it is easily seen that for any two nonover¬ 
lapping time intervals Ai and A2, the increments w,1+&x — wh and w,2+a2 ~ Wt2 
are independent. In finance, this Markov property is related to a weak form of 
efficient market. 

From the first condition, Aw, is normally distributed with mean zero and vari¬ 
ance At. That is, Aw, ~ N(0, At), where ~ denotes probability distribution. 
Consider next the process w,. We assume that the process starts at t = 0 with 
initial value wq, which is fixed and often set to zero. Then w, — wq can be treated 
as a sum of many small increments. More specifically, define T = t/At, where At 
is a small positive increment. Then 

T T 

W, - Wo = WTAt - w0 = Aw‘ = X} e‘V^’ 

1 = 1 1=1 

where Awt = wiAt - W(i-i)At- Because the e, are independent, we have 

E(w, - w0) — 0, Var(u;i - w0) = At = T At = t. 

i=i 

T 

Thus, the increment in w, from time 0 to time t is normally distributed with 
mean zero and variance t. To put it formally, for a Wiener process wt, we have 

290 

CONTINUOUS-TIME MODELS AND THEIR APPLICATIONS 

Figure 6.1 Four simulated Wiener processes. 

that w, — wo ~ ./V(0, t). This says that the variance of a Wiener process increases 
linearly with the length of time interval. 

Figure 6.1 shows four simulated Wiener processes on the unit time interval [0, 1], 
They are obtained by using a simple version of Donsker’s theorem in the statistical 
literature with n — 3000; see Donsker (1951) or Billingsley (1968). The four plots 
start with wq = 0 but drift apart as time increases, illustrating that the variance of 
a Wiener process increases with time. A simple time transformation from [0, 1) to 
[0, oo) can be used to obtain simulated Wiener processes for t e [0, oo). 

Donsker’s Theorem 
Assume that {z,}"=1 is a sequence of independent standard normal random variates. 

For any t e [0, 1], let [nt] be the integer part of nt. Define wnJ = (1 / Jn) Z/. 
Then wn<t converges in distribution to a Wiener process wt on [0, 1] as n goes to 
infinity. 

R or S-Plus Commands for Generating a Wiener Process 

n = 3000 

epsi = rnormfn,0,1) 

w=cumsum(epsi)/sqrt(n) 

plot(w,type='1') 

Remark. A formal definition of a Brownian motion wt on a probability space 
(Q, F, P) is that it is a real-valued, continuous stochastic process for t > 0 with 
independent and stationary increments. In other words, w, satisfies the following: 

SOME CONTINUOUS-TIME STOCHASTIC PROCESSES 

291 

1. Continuity: The map from t to wt is continuous almost surely with respect 

to the probability measure P. 

2. Independent increments: If s < t, wt — ws is independent of wv for all v < s. 

3. Stationary increments: If s < t, wt — ws and wt-s — wq have the same prob¬ 

ability distribution. 

It can be shown that the probability distribution of the increment wt — ws is nor¬ 
mal with mean pit — s) and variance a2(t — s). Furthermore, for any given time 
indexes 0 < t\ < ^ < • • • < 4, the random vector (u>tl, wt2,..., wtk) follows a 
multivariate normal distribution. Finally, a Brownian motion is standard if u>o = 0 
almost surely, /x = 0, and a2 = 1. □ 

Remark. An important property of Brownian motions is that their paths are 
not differentiable almost surely. In other words, for a standard Brownian motion 
wt, it can be shown that dwt/dt does not exist for all elements of except for 
elements in a subset c £2 such that P(£2 j) =0. As a result, we cannot use 
the usual integration in calculus to handle integrals involving a standard Brownian 
motion when we consider the value of an asset over time. Another approach must be 
sought. This is the purpose of discussing Ito’s calculus in the next section. □ 

6.2.2 Generalized Wiener Process 

The Wiener process is a special stochastic process with zero drift and variance 
proportional to the length of the time interval. This means that the rate of change 
in expectation is zero and the rate of change in variance is 1. In practice, the mean 
and variance of a stochastic process can evolve over time in a more complicated 
manner. Hence, further generalization of a stochastic process is needed. To this 
end, we consider the generalized Wiener process in which the expectation has a 
drift rate /z and the rate of variance change is a2. Denote such a process by xt and 
use the notation dy for a small change in the variable y. Then the model for xt is 

dxt = ttdt + a dwt, (6.1) 

where wt is a Wiener process. If we consider a discretized version of Eq. (6.1), 

then 

xt — xq = p,t + ae Vt 

for increment from 0 to I. Consequently, 

E(xt - x0) = /xt, Var(jq - *0) = o2t. 

The results say that the increment in xt has a growth rate of p for the expectation 
and a growth rate of o2 for the variance. In the literature, /z and o of Eq. (6.1) 
are referred to as the drift and volatility parameters of the generalized Wiener 

process xt. 

292 

CONTINUOUS-TIME MODELS AND THEIR APPLICATIONS 

6.2.3 Ito Process 

The drift and volatility parameters of a generalized Wiener process are time invari¬ 
ant. If one further extends the model by allowing fi and o to be functions of the 
stochastic process xt, then we have an Ito process. Specifically, a process xt is an 

Ito process if it satisfies 

dx, = /x(xt, t) dt + o(xt, t) dwt, 

(6.2) 

where wt is a Wiener process. This process plays an important role in mathematical 

finance and can be written as 

xt 

— Xo + /.i(xs,s)ds+ / a(xs,s)dw. 

L 

f Jo 

where xo denotes the starting value of the process at time 0 and the last term on the 
right-hand side is a stochastic integral. Equation (6.2) is referred to as a stochastic 
diffusion equation with [x(xt, t) and cr(jcf, t) being the drift and diffusion functions, 
respectively. 

The Wiener process is a special Ito process because it satisfies Eq. (6.2) with 

ji{xt, t) = 0 and a(xt, t) = 1. 

6.3 ITO’S LEMMA 

In finance, when using continuous-time models, it is common to assume that the 
price of an asset is an Ito process. Therefore, to derive the price of a financial 
derivative, one needs to use Ito’s calculus. In this section, we briefly review Ito’s 
lemma by treating it as a natural extension of the differentiation in calculus. Ito’s 
lemma is the basis of stochastic calculus. 

6.3.1 Review of Differentiation 

Let G(x) be a differentiable function of x. Using the Taylor expansion, we have 

AG = G(x + Ax) - G(x) = —Ax + --^-(Ax)2 -f --^-y(Ax)3 H-. 

9G 1 92G , 1 93G , 

Taking the limit as Ax —*■ 0 and ignoring the higher order terms of Ax, we have 

dG 

9G 

9x 

-dx. 

When G is a function of x and y, we have 

9G 

9G 

1 92G 

AG = -2—Ax + 221 Ay + ll-^-(Ax)2 + 
dx dy 2 9xz 

92G 1 92G , 

3^*XAy+2W(Ay) + 

ITO’S LEMMA 

293 

Taking the limit as Ax —> 0 and Ay -> 0, we have 

3 G 3 G 

dG = —dx -dy. 

dx 3 y 

6.3.2 Stochastic Differentiation 

Turn next to the case in which G is a differentiable function of xt and t, and xt is 
an Ito process. The Taylor expansion becomes 

3 G 

3 G 

1 3 ZG 

AG-^-Ax + ^At + -^(AJ:) + 

3 2G 

dx 31 

Ax At + 

1 3 2G 

2 312 

(At) + 

A discretized version of the Ito process is 

Ax = fi At + creV~At, 

(6.3) 

(6.4) 

where, for simplicity, we omit the arguments of n and a, and Ax = xt+At — xt. 
From Eq. (6.4), we have 

(Ax)2 = /z2(A02 + o2£2 At + 2/xae(At)3/2 = cr2*?2 At + H(At), (6.5) 

where H(At) denotes higher order terms of At. This result shows that (Ax)2 
contains a term of order At, which cannot be ignored when we take the limit as 
At —> 0. However, the first term on the right-hand side of Eq. (6.5) has some nice 
properties: 

E{o2e2 At) = a2 At, 

Var(ctV At) = £[aV(At)2] - [E(a2e2 At)]2 = 2a\At)2, 

where we use E(e4) = 3 for a standard normal random variable. These two prop¬ 
erties show that a2e2 At converges to a nonstochastic quantity a2 At as At -> 0. 
Consequently, from Eq. (6.5), we have 

(Ax)2 —> o2 dt as At —> 0. 

Plugging the prior result into Eq. (6.3) and using Ito’s equation of x, in Eq. (6.2), 
we obtain 

3G dG 132G 2 , 
dG = yiix + + dl 

(dG 
= ( + 

3 G 1 3 2G 
+ - 
dt 2 dx2 

J dG , 
dt -|-cr dwt, 
dx 

which is the well-known Ito lemma in stochastic calculus. 

Recall that we suppressed the argument (xt, t) from the drift and volatility terms 
ix and cr in the derivation of Ito’s lemma. To avoid any possible confusion in the 

future, we restate the lemma as follows. 

294 

CONTINUOUS-TIME MODELS AND THEIR APPLICATIONS 

I to’s Lemma 
Assume that x, is a continuous-time stochastic process satisfying 

dxt = ix(xt, t) dt + a(xt, t) dwt, 

where wt is a Wiener process. Furthermore, G(xt, t) is a differentiable function of 

xt and t. Then, 

3G , 8G 1 92G 2/ ' 

dG = 

dt + —cr(jc,, t)dwt. 

dG 

dx 

(6.6) 

Example 6.1. As a simple illustration, consider the square function G(wt, t) = 

w2 of the Wiener process. Here we have /x(wt, t) = 0, cr(wt, t) = 1, and 

9G 

= 2wt, 

d2G 

dw2 

Therefore, 

dw2 = (2n>, x0 + 0+ ^ x2x \)dt + 2wt dwt = dt + 2u;r JiP;. (6.7) 

6.3.3 An Application 

Let Pj be the price of a stock at time t, which is continuous in [0, oo). In the 
literature, it is common to assume that P, follows the special Ito process 

dPt = n-Pt dt + ctP, dwt, (6.8) 

where /x and a are constant. Using the notation of the general Ito process in 
Eq. (6.2), we have /x(xt, t) = /xxt and a(xt, t) — oxt, where xt = P,. Such a spe¬ 
cial process is referred to as a geometric Brownian motion. We now apply Ito’s 
lemma to obtain a continuous-time model for the logarithm of the stock price Pt. 
Let G{Pt,t) = LK-P,) be the log price of the underlying stock. Then we have 

dG _ 1 dG _ o 1 92G _ 1 -1 

97) ~ ~Pt’ If- ’ 2 9^_2P^' 

Consequently, via Ito’s lemma, we obtain 

d In(P,) = + \~pi°2p^ dt + -yoPtdwt 

( cr2 \ 

= I/t-— \ dt + o dwt. 

This result shows that the logarithm of a price follows a generalized Wiener process 
with drift rate /z — a2/2 and variance rate o2 if the price is a geometric Brownian 

ITO’S LEMMA 

295 

motion. Consequently, the change in logarithm of price (i.e., log return) between 
current time t and some future time T is normally distributed with mean (/x — 
o2/2)(T — t) and variance o2(J — t). If the time interval T — t — A is fixed and 
we are interested in equally spaced increments in log price, then the increment 
series is a Gaussian process with mean (/x — cr2/2) A and variance a2 A. 

6.3.4 Estimation of /x and a 

The two unknown parameters /x and o of the geometric Brownian motion in 
Eq. (6.8) can be estimated empirically. Assume that we have n + 1 observations of 
stock price P, at equally spaced time interval A (e.g., daily, weekly, or monthly). 
We measure A in years. Denote the observed prices as {Po, Pi,..., Pn) and let 
rt = ln(P,) — ln(Pr_i) for t — 1,..., n. 

Since P, = P,_i exp(ry), r, is the continuously compounded return in the rth time 
interval. Using the result of the previous section and assuming that the stock price 
Pt follows a geometric Brownian motion, we obtain that rt is normally distributed 
with mean (/x — cr2/2) A and variance a2 A. In addition, the rt are not serially 
correlated. 

For simplicity, define /xr = E(rt) = (/x — cr2/2) A and a2 = var(rt) — a2 A. 

Let r and sr be the sample mean and standard deviation of the data—that is, 

r =  E"=t A 

Sr 

l 

- r)2. 

t=1 

As mentioned in Chapter 1, r and sr are consistent estimates of the mean and 
standard deviation of r,-, respectively. That is, f -> /xr and sr —>■ ar as n -> oo. 
Therefore, we may estimate a by 

sr 

a = —=. 

\/A 

Furthermore, it can be shown that the standard error of this estimate is approxi¬ 
mately &/V2n. From jlr = r, we can estimate /x by 

When the series r, is serially correlated or when the price of the asset does not 
follow the geometric Brownian motion in Eq. (6.8), then other estimation methods 
must be used to estimate the drift and volatility parameters of the diffusion equation. 

We return to this issue later. 

Example 6.2. Consider the daily log returns of IBM stock in 1998. 
Figure 6.2(a) shows the time plot of the data, which have 252 observations. 
Figure 6.2(b) shows the sample autocorrelations of the series. It is seen that 

296 

CONTINUOUS-TIME MODELS AND THEIR APPLICATIONS 

0 5 10 15 20 

Lag 

Figure 6.2 Daily returns of IBM stock in 1998: (a) log returns and (b) sample autocorrelations. 

the log returns are indeed serially uncorrelated. The Ljung-Box statistic 
gives <2(10) =4.9, which is highly insignificant compared with a chi-squared 
distribution with 10 degrees of freedom. 

If we assume that the price of IBM stock in 1998 follows the geometric Brownian 
motion in Eq. (6.8), then we can use the daily log returns to estimate the parameters 
!± and a. From the data, we have f = 0.002276 and sr = 0.01915. Since 1 trading 
day is equivalent to A = 1/252 year, we obtain that 

<t = —]= = 0.3040, /x = — + — = 0.6198. 

\ A A 2 

• 2 

Thus, the estimated expected return was 61.98% and the standard deviation was 
30.4% per annum for IBM stock in 1998. 

The normality assumption of the daily log returns may not hold, however. In this 
particular instance, the skewness —0.464(0.153) and excess kurtosis 2.396(0.306) 
raise some concern, where the number in parentheses denotes asymptotic standard 
error. 

Example 6.3. Consider the daily log return of the stock of Cisco Systems, Inc. 
in 2007. There are 251 observations, and the sample mean and standard deviation 
are —3.81 x 10-5 and 0.0174, respectively. The log return series also shows no 
serial correlation with <2(12) = 12.30 with a p value of 0.42. Therefore, we have 

a - 

sr 
7a 

0.0174 

71-0/251.0 

0.275, 

-0.0094. 

DISTRIBUTIONS OF STOCK PRICES AND LOG RETURNS 

297 

Consequently, the estimated expected log return for Cisco Systems’ stock was 
—0.94% per annum, and the estimated standard deviation was 27.5% per annum 
in 2007. 

6.4 DISTRIBUTIONS OF STOCK PRICES AND LOG RETURNS 

The result of the previous section shows that if one assumes that price of a stock 
follows the geometric Brownian motion 

dPt = i±Pt dt + a Pt dwt, 

then the logarithm of the price follows a generalized Wiener process 

where P, is the price of the stock at time t and w, is a Wiener process. Therefore, 
the change in log price from time t to T is normally distributed as 

Consequently, conditional on the price Pt at time t, the log price at time T > t is 
normally distributed as 

Using the result of lognormal distribution discussed in Chapter 1, we obtain the 
(conditional) mean and variance of Pt as 

E(Pt) = Pt exp[fi(T - f)], 

Var(/Y) = P? exp[2fi(T - t)]{exp[cr2(T - I)] - !}• 

Note that the expectation confirms that fx is the expected rate of return of the stock. 
The prior distribution of stock price can be used to make inferences. For 
example, suppose that the current price of stock A is $50, the expected return of the 
stock is 15% per annum, and the volatility is 40% per annum. Then the expected 
price of stock A in 6 months (0.5 year) and the associated variance are given by 

E(Pt) = 50 exp(0.15 x 0.5) = 53.89, 

Var(Pt) = 2500 exp(0.3 x 0.5)[exp(0.16 x 0.5) — 1) = 241.92. 

The standard deviation of the price 6 months from now is a/241.92 = 15.55. 

298 

CONTINUOUS-TIME MODELS AND THEIR APPLICATIONS 

Next, let r be the continuously compounded rate of return per annum from time 

t to T. Then we have 

PT = Pt exp[r(T - t)], 

where T and t are measured in years. Therefore, 

1 inf* 
Pt 

T-t 

By Eq. (6.9), we have 

In 

Pt 

Pt 

N 

(T -t),a2{T -t) 

Consequently, the distribution of the continuously compounded rate of return per 
annum is 

r 

The continuously compounded rate of return is, therefore, normally distributed with 
mean /x — cr2/2 and standard deviation cr/y/T — t. 

Consider a stock with an expected rate of return of 15% per annum and a 
volatility of 10% per annum. The distribution of the continuously compounded 
rate of return of the stock over 2 years is normal with mean 0.15 — 0.01/2 = 0.145 
or 14.5% per annum and standard deviation 0.1 /y/l = 0.071 or 7.1% per annum. 
These results allow us to construct confidence intervals (Cl) for r. For instance, 
a 95% Cl for r is 0.145±1.96 x 0.071 per annum (i.e., 0.6%, 28.4%). 

6.5 DERIVATION OF BLACK-SCHOLES DIFFERENTIAL EQUATION 

In this section, we use Ito’s lemma and assume no arbitrage to derive the 
Black-Scholes differential equation for the price of a derivative contingent to 
a stock valued at P,. Assume that the price P, follows the geometric Brownian 
motion in Eq. (6.8) and Gt = G(Pt,t) is the price of a derivative (e.g., a call 
option) contingent on P,. By Ito’s lemma, 

dGt 

dGt dGt 1 3 2Gt , , 
!iPt + -^1 + --la2P? 

dP, 

dt 

2 dP,1 

dt 4-a Pt du>t. 

dGt 

dPt 

The discretized versions of the process and previous result are 

APt — iiPt At + crP, Awt, 

A Gt = 

dG, dGt 1 3lGt , , 
fJiPt + -^L + --la2P} 

dt 

2 dPf 

3 Pt 

At H-a P, A wt, 

dG, 

dPt 

(6.11) 

(6.12) 

DERIVATION OF BLACK-SCHOLES DIFFERENTIAL EQUATION 

299 

where APt and AG, are changes in Pt and G, in a small time interval At. Because 
Aui, = €\/~Kt for both Eqs. (6.11) and (6.12), one can construct a portfolio of the 
stock and the derivative that does not involve the Wiener process. The appropriate 
portfolio is short on derivative and long dGr/dPt shares of the stock. Denote the 
value of the portfolio by Vt. By construction, 

V, =  -G,+ 

The change in V, is then 

AV, = -AG, + —-A Pt. 

3G, 

dP, 

Substituting Eqs. (6.11) and (6.12) into Eq. (6.14), we have 

AV, 

3 G, 1 3 2G 

31 

2 dPt‘ 

2-a2P2 ) At. 

(6.13) 

(6.14) 

(6.15) 

This equation does not involve the stochastic component Awt. Therefore, under 
the no arbitrage assumption, the portfolio V, must be riskless during the small 
time interval At. In other words, the assumptions used imply that the portfolio 
must instantaneously earn the same rate of return as other short-term, risk-free 
securities. Otherwise there exists an arbitrage opportunity between the portfolio 
and the short-term, risk-free securities. Consequently, we have 

AV, = rVt At = (rAt)Vt, 

(6.16) 

where r is the risk-free interest rate. By Eqs. (6.13)—(6.16), we have 

3 G, 

~dF 

+ 

Therefore, 

1 32G,_2 

a P 
2 t 

2 ~dP‘ 

At = r G, 

3 G, 

3Tt Pr At. 

3G, 

31 

+ rPt 

3 G, 

3 P, 

1 o ? 32G, 
+ -a2P2-± 
2 1 dP? 

rGt. 

(6.17) 

This is the Black-Scholes differential equation for derivative pricing. It can be 
solved to obtain the price of a derivative with Pt as the underlying variable. The 
solution so obtained depends on the boundary conditions of the derivative. For a 
European call option, the boundary condition is 

Gt = max(Pt — K, 0), 

300 

CONTINUOUS-TIME MODELS AND THEIR APPLICATIONS 

where T is the expiration time and K is the strike price. For a European put option, 
the boundary condition becomes 

Gj = max(/T — Pj, 0). 

Example 6.4. As a simple example, consider a forward contract on a stock 

that pays no dividend. In this case, the value of the contract is given by 

G, = P, - K exp[-r(T - t)], 

where K is the delivery price, r is the risk-free interest rate, and T is the expiration 
time. For such a function, we have 

dGt 

3 Pt 

Substituting these quantities into the left-hand side of Eq. (6.17) yields 

—rK exp[—r(T — t)] + fPt — r{Pt — K exp[—r(T — r)]}, 

which equals the right-hand side of Eq. (6.17). Thus, the Black-Scholes differential 
equation is indeed satisfied. 

6.6 BLACK-SCHOLES PRICING FORMULAS 

Black and Scholes (1973) successfully solved their differential equation in Eq. 
(6.17) to obtain exact formulas for the price of European call-and-put options. In 
what follows, we derive these formulas using what is called risk-neutral valuation 
in finance. 

6.6.1 Risk-Neutral World 

The drift parameter /r drops out from the Black-Scholes differential equation. In 
finance, this means the equation is independent of risk preferences. In other words, 
risk preferences cannot affect the solution of the equation. A nice consequence of 
this property is that one can assume that investors are risk neutral. In a risk-neutral 
world, we have the following results: 

• The expected return on all securities is the risk-free interest rate r. 

• The present value of any cash flow can be obtained by discounting its expected 

value at the risk-free rate. 

6.6.2 Formulas 

The expected value of a European call option at maturity in a risk-neutral world is 

£*[max(Pr -£,0)], 

BLACK-SCHOLES PRICING FORMULAS 

301 

where E* denotes expected value in a risk-neutral world. The price of the call 
option at time t is 

ct = exp[—r(T — t)]E*[max(PT — K, 0)]. (6.18) 

Yet in a risk-neutral world, we have /a = r, and by Eq. (6.10), ln(/Y) is normally 
distributed as 

ln(Pr)  N 

In (/>,) + 

(T -t),a2(T -t) 

Let g(Pr) be the probability density function of Pj. Then the price of the call 
option in Eq. (6.18) is 

POO 
ct = exp[ r(T t)] / (PT - K)g{PT) dPT. 
Jk 

By changing the variable in the integration and some algebraic calculations (details 
are given in Appendix A), we have 

ct = P,<t>(h+) - K exp[~r(T - t)]<t>(h_), (6.19) 

where 0(a) is the cumulative distribution function (CDF) of the standard normal 
random variable evaluated at a, 

h+ 

/i_ 

\n(Pt/K) + (r + o2/2){T — t) 

O y/T ~ t 

\n(Pt/K) + (r-cr2/2)(T -t) 

Oy/T — t 

— h+ — Oy/r 

In practice, O(a) can easily be obtained from most statistical packages. Alterna¬ 
tively, one can use an approximation given in Appendix B. 

The Black-Scholes call formula in Eq. (6.19) has some nice interpretations. 
First, if we exercise the call option on the expiration date, we receive the stock, 
but we have to pay the strike price. This exchange will take place only when the call 
finishes in-the-money (i.e., PT > K). The first term P,<t>(h+) is the present value 
of receiving the stock if and only if PT > K and the second term -K exp[-r(T - 
0]<h(/i-) is the present value of paying the strike price if and only if PT > K. 
A second interpretation is particularly useful. As shown in the derivation of the 
Black-Scholes differential equation in Section 6.5, <t>(h+) = dGt/8Pt is the num¬ 
ber of shares in the portfolio that does not involve uncertainty, the Wiener process. 
This quantity is known as the delta in hedging. We know that ct = Pt<$>{h+) + Bt, 
where Bt is the dollar amount invested in risk-free bonds in the portfolio (or 
short on the derivative). We can then see that Bt = -K exp[-r(T - r)]<£(/?_) 

302 

CONTINUOUS-TIME MODELS AND THEIR APPLICATIONS 

directly from inspection of the Black-Scholes formula. The first term of the for¬ 
mula, P,0(fi+), is the amount invested in the stock, whereas the second term, 
K exp[—r(T — t)]<£(/i_), is the amount borrowed. 

Similarly, we can obtain the price of a European put option as 

pt = K exp[—r(T — t)]4>(—fi_) — Pr<£>(—h+). (6.20) 

Since the standard normal distribution is symmetric with respect to its mean 0.0, 
we have $(t) = 1 - 0(—x) for all x. Using this property, we have = 
1 — <t>(hj). Thus, the information needed to compute the price of a put option is 
the same as that of a call option. Alternatively, using the symmetry of normal 
distribution, it is easy to verify that 

Pt-ct = K exp[—r(T - f)] - Pt, 

which is referred to as the put-call parity and can be used to obtain pt from ct. The 
put-call parity can also be obtained by considering the following two portfolios: 

1. Portfolio A. One European call option plus an amount of cash equal to 

K exp[—r{T — f)]. 

2. Portfolio B. One European put option plus one share of the underlying stock. 

The payoff of these two portfolios is 

max(Pr, K) 

at the expiration of the options. Since the options can only be exercised at the 
expiration date, the portfolios must have identical value today. This means 

ct + K exp[—r(T - t)] = pt + Pt, 

which is the put-call parity given earlier. 

Example 6.5. Suppose that the current price of Intel stock is $80 per share 
with volatility o = 20% per annum. Suppose further that the risk-free interest rate 
is 8% per annum. What is the price of a European call option on Intel with a strike 
price of $90 that will expire in 3 months? 

From the assumptions, we have P, — 80, K — 90, T — t — 0.25, ct = 0.2, and 

r = 0.08. Therefore, 

ln(80/90) + (0.08 + 0.04/2) x 0.25 

h+ = —1--- ' -= -0.9278, 

0.2VO25 

h_=h+- 0.2VO25 = -1.0278. 

BLACK-SCHOLES PRICING FORMULAS 

303 

Using any statistical software (e.g., R or S-Plus) or the approximation in Appendix 
B, we have 

$ (-0.9278) = 0.1767, $ (-1.0278) = 0.1520. 

Consequently, the price of a European call option is 

ct = $80$(-0.9278) - $90$(—1.0278) exp(—0.02) = $0.73. 

The stock price has to rise by $10.73 for the purchaser of the call option to break 
even. 

Under the same assumptions, the price of a European put option is 

pt = $90 exp(—0.08 x 0.25)$(1.0278) - $80$(0.9278) = $8.95. 

Thus, the stock price can rise an additional $1.05 for the purchaser of the put option 
to break even. 

Example 6.6. The strike price of the previous example is well beyond the 
current stock price. A more realistic strike price is $81. Assume that the other 
conditions of the previous example continue to hold. We now have Pt = 80, K = 
81, r = 0.08, and T — t = 0.25, and the hi become 

ln(80/81) + (0.08 + 0.04/2) x 0.25 
h —-——- 

0.125775, 

0.2VO25 

h- = h+- 0.2VO25 = 0.025775. 

Using the approximation in Appendix B, we have $(0.125775) = 0.5500 and 
$(0.025775) = 0.5103. The price of a European call option is then 

ct = $80$(0.125775) - $81 exp(-0.02)$(0.025775) = $3.49. 

The price of the stock has to rise by $4.49 for the purchaser of the call option to 
break even. On the other hand, under the same assumptions, the price of a European 

put option is 

pt = $81 exp(—0.02)$(—0.025775) - $80$(-0.125775) 

= $81 exp(—0.02) x 0.48972 - $80 x 0.44996 = $2.89. 

The stock price must fall $1.89 for the purchaser of the put option to break 

even. 

304 

CONTINUOUS-TIME MODELS AND THEIR APPLICATIONS 

6.6.3 Lower Bounds of European Options 

Consider the call option of a nondividend-paying stock. It can be shown that the 
price of a European call option satisfies 

ct > Pt - K exp[-r(T - f)]; 

that is, the lower bound for a European call price is Pt — K exp[—r(T — t)]. This 
result can be verified by considering two portfolios: 

1. Portfolio A. One European call option plus an amount of cash equal to 

K exp[—r(T — ?)]• 

2. Portfolio B. One share of the stock. 

For portfolio A, if the cash is invested at the risk-free interest rate, it will result in 
K at time T. If PT > K, the call option is exercised at time T and the portfolio is 
worth Pt. If Pt < K, the call option expires worthless and the portfolio is worth 
K. Therefore, the value of portfolio is 

max(/57’, K). 

The value of portfolio B is Pt at time T. Hence, portfolio A is always worth more 
than (or, at least, equal to) portfolio B. It follows that portfolio A must be worth 
more than portfolio B today; that is, 

ct + K exp[—r(T — t)] > Pt, or ct > P, — K exp[—r{T — f)]. 

Furthermore, since ct > 0, we have 

ct > max(F, — K exp[—r(T — f)], 0). 

A similar approach can be used to show that the price of a corresponding 

European put option satisfies 

Pt > max{AT exp[-r(T - t)] - Pt, 0}. 

Example 6.7. Suppose that P, = $30, K = $28, r = 6% per annum, and T — 

t = 0.5. In this case, 

P, — K exp[-r(T - /)] = $[30 - 28 exp(-0.06 x 0.5)] ^ $2.83. 

Assume that the European call price of the stock is $2.50, which is less than the 
theoretical minimum of $2.83. An arbitrageur can buy the call option and short the 
stock. This provides a new cash flow of $(30 — 2.50) = $27.50. If invested for 6 
months at the risk-free interest rate, the $27.50 grows to $27.50 exp(0.06 x 0.5) = 
$28.34. At the expiration time, if PT > $28, the arbitrageur exercises the option, 

BLACK-SCHOLES PRICING FORMULAS 

305 

closes out the short position, and makes a profit of $(28.34 — 28) = $0.34. On 
the other hand, if PT < $28, the stock is bought in the market to close the short 
position. The arbitrageur then makes an even greater profit. For illustration, suppose 
that PT = $27.00, then the profit is $(28.34 - 27.00) = $1.34. 

6.6.4 Discussion 

From the formulas, the price of a call or put option depends on five 
variables—namely, the current stock price Pt, the strike price K, the time to 
expiration T — t measured in years, the volatility a per annum, and the interest 
rate r per annum. It pays to study the effects of these five variables on the price 
of an option. 

Marginal Effects 
Consider first the marginal effects of the five variables on the price of a call option 
ct. By marginal effects we mean changing one variable while holding the others 
fixed. The effects on a call option can be summarized as follows: 

1. Current Stock Price Pt. ct is positively related to ln(.P,). In particular, c, -» 0 
as P, -> 0 and ct oo as Pt -* oo. Figure 6.3(a) illustrates the effects with 
K — 80, r = 6% per annum, T — t — 0.25 year, and o = 30% per annum. 

Current stock price Current stock price 

(a) (b) 

Figure 6.3 Marginal effects of current stock price on price of an option with K = 80, T - t = 0.25, 

a = 0.3, and r = 0.06: (a) call option and (b) put option. 

306 

CONTINUOUS-TIME MODELS AND THEIR APPLICATIONS 

2. Strike Price K. ct is negatively related to ln(/0- In particular, c, -a P, as 

K -a 0 and ct 0 as K -a- oo. 

3. Time to Expiration. ct is related to T - t in a complicated manner, but we 

can obtain the limiting results by writing h+ and as 

, \n(Pt/K) , (r + o2/2)VT^1 
h+ = —. -H-, 

Oy/T^t O 

, In (P,/K) {r-o2/l)yfT=t 
h- = —. -1-• 

Oy/T ~ t O 

If Pt < K, then ct -» 0 as (T — t) -> 0. If Pt > K, then ct —>• Pt — K as 
(T - t) -> 0 and c, -a- P, as (T - t) ->• oo. Figure 6.4(a) shows the marginal 
effects of T — t on c, for three different current stock prices. The fixed 
variables are K = 80, r = 6%, and o = 30%. The solid, dotted, and dashed 
lines of the plot are for P, = 70, 80, and 90, respectively. 

4. Volatility a. Rewriting h+ and h- as 

\n(Pt/K) + r(T - t) 

y[T^~t + 2 V 

Figure 6.4 Marginal effects of time to expiration on price of an option with K = 80, a = 0.3, and 

r = 0.06: (a) call option and (b) put option. Solid, dotted, and dashed lines are for current stock price 
P, = 70, 80, and 90, respectively. 

BLACK-SCHOLES PRICING FORMULAS 

307 

In(P,/K) + r(T-t) 

o \/T — t 

ct 

2 

y/T-t, 

we obtain that (a) if In(Pt/K) + r(T — t) < 0, then ct -> 0 as ct —>• 0, and 
(b) if In(Pt/K) + r(T -1) > 0, then ct -> P, - Ke~r(r-^ as a 0 and 
ct —^ P; as (T —> oo. Figure 6.5(a) shows the effects of ct on ct for K = 80, 
T — t = 0.25, r = 0.06, and three different values of Pt. The solid, dotted, 
and dashed lines are for Pt = 70, 80, and 90, respectively. 

5. Interest Rate. ct is positively related to r such that ct —> Pt as r —> oo. 

The marginal effects of the five variables on a put option can be obtained 
similarly. Figures 6.3(b), 6.4(b), and 6.5(b) illustrates the effects for some selected 
cases. 

Some Joint Effects 
Figure 6.6 shows the joint effects of volatility and strike price on a call option, 
where the other variables are fixed at Pt = 80, r = 0.06, and T — t = 0.25. As 
expected, the price of a call option is higher when the volatility is high and the 
strike price is well below the current stock price. Figure 6.7 shows the effects on 

Figure 6.5 Marginal effects of stock volatility on price of an option with K = 80, T — t = 0.25, and 

r = 0.06: (a) call option and (b) put option. Solid, dotted, and dashed lines are for current stock price 

P, = 70, 80, and 90, respectively. 

308 

CONTINUOUS-TIME MODELS AND THEIR APPLICATIONS 

03 
O 
03 

O 
CD 
3 
03 
> 

Figure 6.6 Joint effects of stock volatility and strike price on call option with P, = 80, r = 0.06, and 

T-t = 0.25. 

t
u
p

a

f
o

e
u
l
a
V

Figure 6.7 Joint effects of stock volatility and strike price on put option with K = 80, T — t = 0.25, 

and r = 0.06. 

 
 
 
 
EXTENSION OF ITO’S LEMMA 

309 

a put option under the same conditions. The price of a put option is higher when 
the volatility is high and the strike price is well above the current stock price. 
Furthermore, the plot also shows that the effects of a strike price on the price of a 
put option becomes more linear as the volatility increases. 

6.7 EXTENSION OF ITO’S LEMMA 

In derivative pricing, a derivative may be contingent on multiple securities. When 
the prices of these securities are driven by multiple factors, the price of the deriva¬ 
tive is a function of several stochastic processes. The two-factor model for the term 
structure of interest rate is an example of two stochastic processes. In this section, 
we briefly discuss the extension of Ito’s lemma to the case of several stochastic 
processes. 

Consider a k-dimensional continuous-time process xt = (x\t, ■ ■ ■, XktY, where k 

is a positive integer and xa is a continuous-time stochastic process satisfying 

dxit = ^i(xt)dt + <7j(xt)dwit, i = 1, 

(6.21) 

where Wjt is a Wiener process. It is understood that the drift and volatility functions 
/Xi(xit) and cr, (x,'?) are functions of time index t as well. We omit t from their 
arguments to simplify the notation. For i / j, the Wiener processes wit and wjt 
are different. We assume that the correlation between dwn and dwjt is ptj. This 
means that p;y is the correlation between the two standard normal random variables 
and €j defined by Awit = et At and AWjt = At. Assume that G, = G(xt, t) 
is a function of the stochastic processes xit and time t. The Taylor expansion gives 

The discretized version of Eq. (6.21) is 

Awu = HiiXt) At + Oi(xt) Awit, i = 1.k. 

Using a similar argument as that of Eq. (6.5) in Section 6.3, we can obtain that 

lim (Axu)2 -* cr?(xt) dt, 
Ar-»0 

lim (Axit Ax;-f) Oi(xt)crj(xt)pijdt. 

(6.22) 

(6.23) 

310 

CONTINUOUS-TIME MODELS AND THEIR APPLICATIONS 

Using Eqs. (6.21)—(6.23), taking the limit as At -» 0, and ignoring higher order 
terms of At, we have 

(6.24) 

This is a generalization of Ito’s lemma to the case of multiple stochastic processes. 

6.8 STOCHASTIC INTEGRAL 

We briefly discuss stochastic integration so that the price of an asset can be obtained 
under the assumption that it follows an Ito process. We deduce the integration result 
using Ito’s formula. For a rigorous treatment on the topic, readers may consult 
textbooks on stochastic calculus. First, like the usual integration of a deterministic 
function, integration is the opposite of differentiation so that 

continues to hold for a stochastic process xt. In particular, for the Wiener pro¬ 
cess wt, we have /Qr dws = wt because wq = 0. Next, consider the integration 
/0( wsdws. Using the prior result and taking integration of Eq. (6.7), we have 

Therefore, 

This is different from the usual deterministic integration for which y dy = (>f 

yl)/2. 

Turn to the case that xt is a geometric Brownian motion—that is, xt satisfies 

dxt = jix, dt + axt dwt. 

where /z and a are constant with a > 0; see Eq. (6.8). Applying Ito’s lemma to 
G(xt,t) = ln(jq), we obtain 

JUMP DIFFUSION MODELS 

311 

Performing the integration and using the results obtained before, we have 

J d ln(Xs) = J ds +cr J dws. 

r 2 \ rt 

Consequently, 

and 

ln(xr) = ln(xo) + (/x — ct"/2 )t + awt 

xt = jco exp[(/x — a2/2)t + crwt\. 

Changing the notation xt to P, for the price of an asset, we have a solution for the 
price under the assumption that it is a geometric Brownian motion. The price is 

Pt = P0 exp[(/x — a2/2)t + crwt\. (6.25) 

6.9 JUMP DIFFUSION MODELS 

Empirical studies have found that the stochastic diffusion model based on Brownian 
motion fails to explain some characteristics of asset returns and the prices of their 
derivatives [e.g., the “volatility smile” of implied volatilities; see Bakshi, Cao, and 
Chen (1997) and the references therein]. Volatility smile is referred to as the convex 
function between the implied volatility and strike price of an option. Both out-of- 
the-money and in-the-money options tend to have higher implied volatilities than 
at-the-money options especially in the foreign exchange markets. Volatility smile 
is less pronounced for equity options. The inadequacy of the standard stochastic 
diffusion model has led to the developments of alternative continuous-time models. 
For example, jump diffusion and stochastic volatility models have been proposed 
in the literature to overcome the inadequacy; see Merton (1976) and Duffie (1995). 
Jumps in stock prices are often assumed to follow a probability law. For example, 
the jumps may follow a Poisson process, which is a continuous-time discrete pro¬ 
cess. For a given time t, let Xt be the number of times a special event occurs 
during the time period [0, t]. Then X, is a Poisson process if 

■\m tm 

Pr(Xt = m) —-— exp(-At), A > 0. 

ml 

That is, Xt follows a Poisson distribution with parameter Xt. The parameter X 
governs the occurrence of the special event and is referred to as the rate or intensity 
of the process. A formal definition also requires that Xt be a right-continuous 
homogeneous Markov process with left-hand limit. 

In this section, we discuss a simple jump diffusion model proposed by Kou 
(2002). This simple model enjoys several nice properties. The returns implied 

312 

CONTINUOUS-TIME MODELS AND THEIR APPLICATIONS 

by the model are leptokurtic and asymmetric with respect to zero. In addition, 
the model can reproduce volatility smile and provide analytical formulas for the 
prices of many options. The model consists of two parts, with the first part being 
continuous and following a geometric Brownian motion and the second part being 
a jump process. The occurrences of jump are governed by a Poisson process, and 
the jump size follows a double exponential distribution. Let Pt be the price of an 
asset at time t. The simple jump diffusion model postulates that the price follows 
the stochastic differential equation 

dP, 

Pt 

— Hdt + a dwt + d 

n, 

_i=l 

- » 

(6.26) 

where w, is a Wiener process, n, is a Poisson process with rate X, and {7, } is a 
sequence of independent and identically distributed nonnegative random variables 
such that X = ln(7) has a double exponential distribution with probability density 
function 

/x(i)4rM/,> 0 < 77 < 1. (6.27) 

2 V 

The double exponential distribution is also referred to as the Laplacian distribution. 
In model (6.26), nt, wt, and 7, are independent so that there is no relation between 
the randomness of the model. Notice that nt is the number of jumps in the time 
interval [0, t] and follows a Poisson distribution with parameter Xt, where A. is a 
constant. At the ith jump, the proportion of price jump is 7, -1. 

The double exponential distribution can be written as 

y _ _ £ with probability 0.5, 

-f with probability 0.5, K } 

where £ is an exponential random variable with mean t) and variance rj2. The 
probability density function of £ is 

fix) = —e~x^, 0 < jc < 00. 

Some useful properties of the double exponential distribution are 

E(X) = k , Var(X) = 2i;2, E(ex) = 

1 — r)1 

For finite samples, it is hard to distinguish a double exponential distribution from a 
Student-/ distribution. However, a double exponential distribution is more tractable 
analytically and can generate a higher probability concentration (e.g., higher peak) 
around its mean value. As stated in Chapter 1, histograms of observed asset returns 
tend to have a higher peak than the normal density. Figure 6.8 shows the probability 

JUMP DIFFUSION MODELS 

313 

Figure 6.8 Probability density functions of double exponential and normal random variable with 

mean zero and variance 0.0008. Solid line denotes the double exponential distribution. Dotted line is 
the normal distribution. 

density function of a double exponential random variable in the solid line and that 
of a normal random variable in the dotted line. Both variables have mean zero and 
variance 0.0008. The high peak of the double exponential density is clearly seen. 
Solving the stochastic differential equation in Eq. (6.26), we obtain the dynamics 

of the asset price as 

P, = P0 exp 

A 

t + awt 

nt 

(=i 

(6-29) 

where it is understood that fl/Li = 1- This result is a generalization of Eq. (6.25) 
by including the stochastic jumps. It can be obtained as follows. Let f,- be the time 
of the ith jump. For t e [0, 0), there is no jump and the price is given in Eq. (6.25). 
Consequently, the left-hand price limit at time t\ is 

Pt- = P0 exp[(ii - o2/2)t\ +crivtl]. 

At time t\, the proportion of price jump is J\ — 1 so that the price becomes 

Pt{ = (1 + J\ - 1 )Pt- = J\Pt- = Po exp[(jtt - cr2/2)ti +awh]Ji. 

For t G (t\, t-i), there is no jump in the interval (tj, t] so that 

P, = Ptj exp[(jtt - cr2/2)(t - t\) + cr(wt - wn)]. 

314 

CONTINUOUS-TIME MODELS AND THEIR APPLICATIONS 

Plugging in Pt], we have 

P, = P0 exp[(/Li - a2/2)t + owt]J\. 

Repeating the scheme, we obtain Eq. (6.29). 

From Eq. (6.29), the simple return of the underlying asset in a small time 

increment At becomes 

Pt+At ~ Pt 
pT 

= exp 

At + a(wt + &t - Wt) + X Xi 

- 1, 

«r+Ar 

i=n, +1 

where it is understood that a summation over an empty set is zero and Xj = ln(7,). 
For a small At, we may use the approximation ex ^ l + x + x2/2 and the result 
(Ain,)2 % At discussed in Section 6.3 to obtain 

Pt+At - Pt ^ / _ l2\ At+aAu)t+ £ Xi + l-a2(Awtf 

\ 2 / i=n, + \ 

~ /a At cr €\/At -f- ^ ^ Xj, 

«/+Af 

i=nt + l 

where Awt = wt+At ~ wt and e is a standard normal random variable. 

Under the assumption of a Poisson process, the probability of having one jump 
in the time interval (t, t + At] is X At and that of having more than one jump is 
o(At), where the symbol o(At) means that if we divide this term by At then its 
value tends to zero as At tends to zero. Therefore, for a small At, by ignoring 
multiple jumps, we have 

nl+At 

E * 

i=nt+l 

Xnt+\ with probability XAt, 
0 with probability 1 — X At. 

Combining the prior results, we see that the simple return of the underlying asset 
is approximately distributed as 

Pt+At ~ Pt 

Pt 

~ 1+ At + o€sf~At + / x X, 

(6.30) 

where I is a Bernoulli random variable with Pr(7 = 1) = X At and Pr(7 = 0) = 
1 — X At, and X is a double exponential random variable defined in Eq. (6.28). 
Equation (6.30) reduces to that of a geometric Brownian motion without jumps. 
Let G = /i At + a€\Z~At + 7 x X be the random variable on the right-hand 
side of Eq. (6.30). Using the independence between the exponential and normal 

JUMP DIFFUSION MODELS 

315 

distributions used in the model, Kou (2002) obtains the probability density function 
of G as 

g(x) = 

XAt  ,^2a t/(2n2) 
2 rj 

fan} — ct2 AA ; ,m/„t (m + <r2 AtV 

\ or\sTKt / \ or]J~At / _ 

+ (1 - A Af)- 

/ 

a 

\/a7 

x — fx At 

o\Ta1 

(6.31) 

where co = x — /x At — k, and /(•) and $(•) are, respectively, the probability den¬ 
sity and cumulative distribution functions of the standard normal random variable. 
Furthermore, 

E{G) — jx At + kX At, Var(G) = a2 At + X At[2r]2 + /c2( 1 — X At)]. 

Figure 6.9 shows some comparisons between probability density functions of a 
normal distribution and the distribution of Eq. (6.31). Both distributions have mean 
zero and variance 2.0572 x 10-4. The mean and variance are obtained by assuming 
that the return of the underlying asset satisfies /x = 20% per annum, o = 20% per 
annum, At = 1 day = 1/252 year, X — 10, k = —0.02, and r\ — 0.02. In other 
words, we assume that there are about 10 daily jumps per year with average jump 
size —2%, and the jump size standard error is 2%. These values are reasonable 
for a U.S. stock. From the plots, the leptokurtic feature of the distribution derived 
from the jump diffusion process in Eq. (6.26) is clearly shown. The distribution 
has a higher peak and fatter tails than the corresponding normal distribution. 

6.9.1 Option Pricing under Jump Diffusion 

In the presence of random jumps, the market becomes incomplete. In this case, the 
standard hedging arguments are not applicable to price an option. But we can still 
derive an option pricing formula that does not depend on attitudes toward risk by 
assuming that the number of securities available is very large so that the risk of the 
sudden jumps is diversifiable and the market will therefore pay no risk premium 
over the risk-free rate for bearing this risk. Alternatively, for a given set of risk 
premiums, one can consider a risk-neutral measure P* such that 

dPt 

[r — XE(J — 1 )]dt + cr dwt + d 

nt 
E« - » 

(r — X\jx) dt + o dwt + d 

n t 

E« - 
_l=1 

where r is the risk-free interest rate, J = exp(A) such that X follows the double 
exponential distribution of Eq. (6.27), ifr = eK/(I — r]2) — 1, 0 < r] < 1, and the 
parameters K,rj,\jx, and a become risk-neutral parameters taking consideration of 

316 

CONTINUOUS-TIME MODELS AND THEIR APPLICATIONS 

Figure 6.9 Density comparisons between normal distribution and distribution of Eq. (6.31). Dotted 

line denotes the normal distribution. Both distributions have mean zero and variance 2.0572 xlO-4. 

(a) Overall comparison, (b) comparison of peaks, (c) left tails, and (d) right tails. 

the risk premiums; see Kou (2002) for more details. The unique solution of the 
prior equation is given by 

Pt = P0 exp 

r —-— Xx/r ) t + owt 

n, 
n-L 

i=i 

To price a European option in the jump diffusion model, it remains to compute the 
expectation, under the measure P*, of the discounted final payoff of the option. In 
particular, the price of a European call option at time t is given by 

ct = £*[e_r(r_f)(Pr - K)+] 

= E* I p-r(r-f)  Pt exp 

r — cr- 

nj 

+ oy/T - te j Y\ Ji ~ 

K 

i=1 

(6.32) 

where T is the expiration time, (T — t) is the time to expiration measured in 
years, K is the strike price, (y)+ = max(0, y), and e is a standard normal random 

JUMP DIFFUSION MODELS 

317 

variable. Kou (2002) shows that ct is analytically tractable as 

« = ££■ 
n=1 7=1 

-X(T _t)Xn(T-ty V ( 2n-j-\ 
2ln 

n — 1 

n\ 

(^1 ,n,j + tn,j + A^nj) 

+ [Pte-^{T-t)<$>(h+) - Ke-r(T-t)^>(h-)], 

(6.33) 

where <&(•) is the CDF of the standard normal random variable, 

AhnJ = i>g-W-0+™I |"...... 1 + 1 

2 |_ (1 — rf)j (l + rj)j 

<t>(b+)-e-r(T-t)K<t>(b^), 

1 

Al,n. 

] 2' 

-r(T-t)-co/ri+cr2(T-t)/(2r)2) K 

j-1 r 

1 

- £ 
1=0 

.(1 - 

a VT^lY l 

Y2n 

Hhi(c_), 

^3,n, 

1 

X 

-r(r-o+<u/'?+o-2(:r-0/(2Jy2) ^ 

;-l r 

£ 

1=0 u 

1 - 

1 

cr YT^l\ l 

(1 + r/£- 

v 

Y2tt 

Hhi(c+), 

b± = 

h± = 

c± — 

\n(Pt/K) + (r ± cr2/2 — A,x/s)(T — t) + nK 

o s/T — t 

ln(Pt/K) + (r± a2/2 - \f)(T - t) 

OyjT — t 

o s/T — t co 

V 

cr 

y/T=t' 

co = In ( + k\fr(T - t) - (r - y ) (T - t) - nic. 

I //■ 

1, 

and the Hhi(-) functions are defined as 

Hhn(x) = — / (s — x)ne~s I2 ds, 

1 2 

n — 0, 1, ..., 

(6.34) 

Jx 

and Hh-x(x) — exp(—x2/2), which is *J2nf(x) with f(x) being the probability 
density function of a standard normal random variable; see Abramowitz and Stegun 

318 

CONTINUOUS-TIME MODELS AND THEIR APPLICATIONS 

(1972). The Hhn(x) functions satisfy the recursion 

nHhn(x) = Hhn-2(x) -xHhn-i(x), n > 1, (6.35) 

with starting values Hh^\{x) — e~x ^ and Hho(x) = y/2n^(—a). 

The pricing formula involves an infinite series, but its numerical value can be 
approximated quickly and accurately through truncation (e.g., the first 10 terms). 
Also, if X = 0 (i.e., there are no jumps), then it is easily seen that c, reduces to the 
Black-Scholes formula for a call option discussed before. 

Finally, the price of a European put option under the jump diffusion model 

considered can be obtained by using the put/-call parity; that is, 

Pt =ct + Ke-r{-T~t) - Pt. 

Pricing formulas for other options under the jump diffusion model in Eq. (6.26) 

can be found in Kou (2002). 

Example 6.8. Consider the stock of Example 6.6, which has a current price 
of $80. As before, assume that the strike price of a European option is K = $81 
and other parameters are r — 0.08 and T — t = 0.25. In addition, assume that the 
price of the stock follows the jump diffusion model in Eq. (6.26) with parameters 
A, = 10, k — —0.02, and rj = 0.02. In other words, there are about 10 jumps per 
year with average jump size —2% and jump size standard error of 2%. Using the 
formula in Eq. (6.33), we obtain ct = $3.92, which is higher than the $3.49 of 
Example 6.6 when there are no jumps. The corresponding put option assumes the 
value pt = $3.31, which is also higher than what we had before. As expected, 
adding the jumps while keeping the other parameters fixed increases the prices of 
both European options. Keep in mind, however, that adding the jump process to 
the stock price in a real application often leads to different estimates for the stock 
volatility a. 

6.10 ESTIMATION OF CONTINUOUS-TIME MODELS 

Next, we consider the problem of estimating directly the diffusion equation (i.e., 
Ito process) from discretely sampled data. Here the drift and volatility functions 
p,(xt,t) and a{xt,t) are time varying and may not follow a specific paramet¬ 
ric form. This is a topic of considerable interest in recent years. Details of the 
available methods are beyond the scope of this chapter. Hence, we only outline 
the approaches proposed in the literature. Interested readers can consult the corre¬ 
sponding references and Lo (1988). 

There are several approaches available for estimating a diffusion equation. The 
first approach is the quasi-maximum-likelihood approach, which makes use of the 
fact that for a small time interval dwt is normally distributed; see Kessler (1997) 
and the references therein. The second approach uses methods of moments; see 

APPENDIX a: integration of black-scholes formula 

319 

Conley, Hansen, Luttmer, and Scheinkman (1997) and the references therein. The 
third approach uses nonparametric methods; see Ait-Sahalia (1996, 2002). The 
fourth approach uses semiparametric and reprojection methods; see Gallant and 
Long (1997) and Gallant and Tauchen (1997). Recently, many researchers have 
applied Markov chain Monte Carlo methods to estimate the diffusion equation; see 
Eraker (2001) and Elerian, Chib, and Shephard (2001). 

APPENDIX A: INTEGRATION OF BLACK-SCHOLES FORMULA 

In this appendix, we derive the price of a European call option given in Eq. (6.19). 
Let x = In(Pj). By changing variable and using g(PT)dPr = f(x)dx, where 
/(x) is the probability density function of x, we have 

ct = exp[—r(T  -*)] r 
Jk 

(PT — K)g(Pr) dP, 

POO 

/ (ex~K)f{x)dx 
7ln(X) 

POO POO 

= e-^-* 

/ exf{x)dx — K I f{x)dx 
J]n(K) Jln(K) 

Ly ln(X) 

(6.36) 

Because x = ln(/Y) ~ A[ln(P?) + (r — a2/2){T — t), o2{T — f)], the integration 
of the second term of Eq. (6.36) reduces to 

f /(x) dx = 1 - f 
Jln(K) J- 

ln (K) 

f (x) dx 

= 1 - CDF[ln(A")] 

= 1 - d>(-/?_) = 4>(fc_), 

where CDF[ln(/f)] is the cumulative distribution function (CDF) of x = ln(/Y) 
evaluated at ln(X), <!>(•) is the CDF of the standard normal random variable, and 

ln(K)-ln(Pt)-(r-a2/2)(T-t) 

—- 

as/T — t 

~ ln(Pf/K) — (r — o2/2)(T — t) 

o*jT — t 

The integration of the first term of Eq. (6.36) can be written as 

1 

l In(AT) V2n^cr2(T — t) 

exp 

[x-\n{Pt)-{r-a2/2)(T-t)Y\ 

x-- ' 

320 

CONTINUOUS-TIME MODELS AND THEIR APPLICATIONS 

where the exponent can be simplified to 

{x - [In(Pt) + (r - o2/2){T - Q]}2 

2a2(T — t) 

{x — [ln(Pf) + (r + o2/2){T — Q]}2 

2o2(T - t) 

+ ln (Pt) + r{T-t). 

Consequently, the first integration becomes 

/»00 

/ exf(x)dx = Pte 
Jln(AT) 

r(T-t) L 

i 

ln(AT) \/2TC^/o2{T — t) 

x exp 

[x — [ln(Pf) + (r + <y2/2){T — t)]}2 

2o2(T -t) 

dx, 

which involves the CDF of a normal distribution with mean ln(Pf) + (r + 
a2/2){T - t) and variance cr2(T - t). By using the same techniques as those of 
the second integration shown before, we have 

exf(x)dx = PteriT-t]<t>(h+), 

where h+ is given by 

h. 

\n(Pt/K) + (r+a2/2)(T-t) 

a s/T — t 

Putting the two integration results together, we have 

ct = e-r{T-t)[Pter{T-t)<$>(h+) - K<S>(h_)] = P,Q(h+) - Ke~r{T-t] 

APPENDIX B: APPROXIMATION TO STANDARD NORMAL 
PROBABILITY 

The CDF ^(x) of a standard normal random variable can be approximated by 

J 1 — f(x)[c\k + C2&2 + C3A:3 + C4&4 + C5/:5] if x > 0, 
CPU) ~ \ l- O(-x) if x < 0, 

where /(x) = exy(-x2/2)/j2n, k = 1/(1 + 0.2316419x), ci = 0.319381530, 
c2 = -0.356563782, c3 = 1.781477937, c4 = -1.821255978, and c5 = 
1.330274429. 

For illustration, using the earlier approximation, we obtain <t>(1.96) = 0.975002, 
0(0.82) = 0.793892, and <t>(—0.61) = 0.270931. These probabilities are very 
close to that obtained from a typical normal probability table. 

EXERCISES 

EXERCISES 

321 

6.1. Assume that the log price pt = ln(P() follows a stochastic differential 

equation 

dpt = y dt + a dwt, 

where wt is a Wiener process. Derive the stochastic equation for the price Pt. 

6.2. Considering the forward price F of a nondividend-paying stock, we have 

FtJ = PteriT-'\ 

where r is the risk-free interest rate, which is constant, and Pt is the cur¬ 
rent stock price. Suppose Pt follows the geometric Brownian motion dPt = 
pcP, dt + cr Pt dwt. Derive a stochastic diffusion equation for Ftj. 

6.3. Assume that the price of IBM stock follows the Ito process 

dPt — pPt dt + a P, dwt, 

where p and a are constant and wt is a standard Brownian motion. Consider 
the daily log returns of IBM stock in 1997. The average return and the sample 
standard deviation are 0.00131 and 0.02215, respectively. Use the data to 
estimate the parameters p and a assuming that there were 252 trading days 
in 1997. 

6.4. Suppose that the current price of a stock is $120 per share with volatility 
cr = 50% per annum. Suppose further that the risk-free interest rate is 7% per 
annum and the stock pays no dividend, (a) What is the price of a European 
call option contingent on the stock with a strike price of $125 that will expire 
in 3 months? (b) What is the price of a European put option on the same stock 
with a strike price of $118 that will expire in 3 months? If the volatility o 
is increased to 80% per annum, then what are the prices of the two options? 

6.5. Derive the limiting marginal effects of the five variables K, Pr, T — t, cr, 

and r on a European put option contingent on a stock. 

6.6. A stock price is currently $60 per share and follows the geometric Brownian 
motion dPt = pP, dt + a P, dt. Assume that the expected return p from the 
stock is 20% per annum and its volatility is 40% per annum. What is the 
probability distribution for the stock price in 2 years? Obtain the mean and 
standard deviation of the distribution and construct a 95% confidence interval 
for the stock price. 

6.7. A stock price is currently $60 per share and follows the geometric Brownian 
motion dPt = pP,dt +a Ptdt. Assume that the expected return p from 
the stock is 20% per annum and its volatility is 40% per annum. What is 
the probability distribution for the continuously compounded rate of return 
of the stock over 2 years? Obtain the mean and standard deviation of the 

distribution. 

322 

CONTINUOUS-TIME MODELS AND THEIR APPLICATIONS 

6.8. Suppose that the current price of stock A is $70 per share and the price 
follows the jump diffusion model in Eq. (6.26). Assume that the risk-free 
interest rate is 8% per annum, the stock pays no dividend, and its volatility 
(a) is 30% per annum. In addition, the price on average has about 15 jumps 
per year with average jump size -2% and jump standard error 3%. What is 
the price of a European call option with strike price $75 that will expire in 
3 months? What is the price of the corresponding European put option? 

6.9. Consider the European call option of a nondividend-paying stock. Suppose 
that P, — $20, K = $18, r = 6% per annum, and T - t =0.5 year. If the 
price of a European call option of the stock is $2.10, what opportunities are 

there for an arbitrageur? 

6.10. Consider the put option of a nondividend-paying stock. Suppose that Pt = 
$44, K = $47, r — 6% per annum, and T — t = 0.5 year. If the European 
put option of the stock is selling at $1.00, what opportunities are there for 
an arbitrageur? 

REFERENCES 

Abramowitz, M. and Stegun, I. A. (1972). Handbook of Mathematical Functions, 10th ed. 

U.S. National Bureau of Standards, Washington, DC. 

Ait-Sahalia, Y. (1996). Testing continuous-time models for the spot interest rate. Review of 

Financial Studies 9: 385—426. 

Ait-Sahalia, Y. (2002). Maximum likelihood estimation of discretely sampled diffusions: A 

closed-form approach. Econometrica 70: 223-262. 

Bakshi, G., Cao, C., and Chen, Z. (1997). Empirical performance of alternative option pricing 

models. Journal of Finance 52: 2003-2049. 

Billingsley, P. (1968). Convergence of Probability Measures. Wiley, New York. 

Billingsley, P. (1986). Probability and Measure, 2nd ed. Wiley, New York. 

Black, F. and Scholes, M. (1973). The pricing of options and corporate liabilities. Journal 

of Political Economy 81: 637-654. 

Conley, T. G., Hansen, L. P., Luttmer, E. G. J., and Scheinkman, J. A. (1997). Short-term 
interest rates as subordinated diffusions. Review of Financial Studies 10: 525-577. 

Cox, J. C. and Rubinstein, M. (1985). Options Markets. Prentice Hall, Englewood Cliffs, 

NJ. 

Donsker, M. (1951). An invariance principle for certain probability limit theorems. Memoirs 

American Mathematical Society No. 6. 

Duffie, D. (1995). Dynamic Asset Pricing Theory, 2nd ed. Princeton University Press, Prince¬ 

ton, NJ. 

Elerian, 0„ Chib, S., and Shephard, N. (2001). Likelihood inference for discretely observed 

non-linear diffusions. Econometrica 69: 959-993. 

Eraker, B. (2001). MCMC analysis of diffusion models with application to finance. Journal 

of Business & Economic Statistics 19: 177-191. 

Gallant, A. R. and Long, J. R. (1997). Estimating stochastic diffusion equations efficiently 

by minimum chi-squared. Biometrika 84: 125-141. 

REFERENCES 

323 

Gallant, A. R. and Tauchen, G. (1997). The relative efficiency of method of moments 
estimators. Working paper, Economics Department, University of North Carolina. 

Hull, J. C. (2007). Options, Futures, and Other Derivatives with Derivagem CD, 7th ed. 

Prentice Hall, Upper Saddle River, NJ. 

Kessler, M. (1997). Estimation of an ergodic diffusion from discrete observations. Scandi¬ 

navian Journal of Statistics 24: 1-19. 

Kou, S. (2002). A jump diffusion model for option pricing. Management Science 48: 

1086-1101. 

Lo, A. W. (1988). Maximum likelihood estimation of generalized Ito’s processes with dis¬ 

cretely sampled data. Econometric Theory 4: 231-247. 

Merton, R. C. (1976). Option pricing when the underlying stock returns are discontinuous. 

Journal of Financial Economics 5: 125-144. 

■ 

CHAPTER 7 

Extreme Values, Quantiles, 
and Value at Risk 

Extreme price movements in the financial markets are rare but important. The stock 
market crash on Wall Street in October 1987 and other big financial crises such as 
the Long-Term Capital Management and the bankruptcy of Lehman Brothers have 
attracted a great deal of attention among investors, practitioners, and researchers. 
The recent worldwide financial crisis characterized by the substantial increase in 
market volatility, for example, the volatility index (VIX) of the Chicago Board 
Options Exchange index, and the big drops in market indices has further gener¬ 
ated discussions on market risk and margin setting for financial institutions. As a 
result, value at risk (VaR) has become the standard measure of market risk in risk 
management. Its usefulness and weaknesses are widely discussed. 

In this chapter, we discuss various methods for calculating VaR and the statistical 
theories behind these methods. In particular, we consider the extreme value theory 
developed in the statistical literature for studying rare (or extraordinary) events 
and its application to VaR. Both unconditional and conditional concepts of extreme 
values are discussed. The unconditional approach to VaR calculation for a financial 
position uses the historical returns of the instruments involved to compute VaR. 
On the other hand, a conditional approach uses the historical data and explanatory 
variables to calculate VaR. The explanatory variables may include macroeconomic 
variables of an economy and accounting variables of companies involved. 

Other approaches to VaR calculation discussed in the chapter are RiskMetrics, 
econometric modeling using volatility models, and empirical quantile. We use daily 
log returns of IBM stock to illustrate the actual calculation of all the methods 
discussed. The results obtained can therefore be used to compare the performance 
of different methods. Figure 7.1 shows the time plot of daily log returns of IBM 
stock from July 3, 1962, to December 31, 1998, for 9190 observations. 

VaR is a point estimate of potential financial loss. It contains a certain degree 
of uncertainty. It also has a tendency to underestimate the actual loss if an extreme 

Analysis of Financial Time Series, Third Edition, By Ruey S. Tsay 
Copyright © 2010 John Wiley & Sons, Inc. 

325 

EXTREME VALUES, QUANTILES, AND VALUE AT RISK 

326 

o 

oj _ 
o . 
-i o ■ l 

CM 

o • 
I 

-1-1-———-1-r 

1970 1980 1990 2000 

Year 

Figure 7.1 Time plot of daily log returns of IBM stock from July 3, 1962, to December 31, 1998. 

event actually occurs. To overcome the weaknesses of VaR, we discuss other risk 
measures such as expected shortfalls and the loss distribution of a financial position 
in the chapter. 

7.1 VALUE AT RISK 

There are several types of risk in financial markets. Credit risk, operational risk, 
and market risk are the three main categories of financial risk. Value at risk (VaR) 
is mainly concerned with market risk, but the concept is also applicable to other 
types of risk. VaR is a single estimate of the amount by which an institution’s 
position in a risk category could decline due to general market movements during 
a given holding period; see Duffie and Pan (1997) and Jorion (2006) for a general 
exposition of VaR. The measure can be used by financial institutions to assess 
their risks or by a regulatory committee to set margin requirements. In either case, 
VaR is used to ensure that the financial institutions can still be in business after 
a catastrophic event. From the viewpoint of a financial institution, VaR can be 
defined as the maximal loss of a financial position during a given time period for a 
given probability. In this view, one treats VaR as a measure of loss associated with 
a rare (or extraordinary) event under normal market conditions. Alternatively, from 
the viewpoint of a regulatory committee, VaR can be defined as the minimal loss 
under extraordinary market circumstances. Both definitions will lead to the same 
VaR measure, even though the concepts appear to be different. 

VALUE AT RISK 

327 

In what follows, we define VaR under a probabilistic framework. Suppose that 
at the time index t we are interested in the risk of a financial position for the 
next i periods. Let A V (t) be the change in value of the underlying assets of the 
financial position from time t to t +1 and L(l) be the associated loss function. 
These two quantities are measured in dollars and are random variables at the time 
index t. L(t) is a positive or negative function of AV(£) depending on the position 
being short or long. Denote the cumulative distribution function (CDF) of L(i) by 
Fi(x). We define the VaR of a financial position over the time horizon l with tail 
probability p as 

p = Pr[L(l) > VaR] = 1 - Pr[L(£) < VaR], (7.1) 

From the definition, the probability that the position holder would encounter a loss 
greater than or equal to VaR over the time horizon t is p. Alternatively, VaR 
can be interpreted as follows. With probability (1 — p), the potential loss encoun¬ 
tered by the holder of the financial position over the time horizon l is less than 
VaR. 

The previous definition shows that VaR is concerned with the upper tail behavior 
of the loss CDF Fi{x). For any univariate CDF Ff{x) and probability q, such that 
0 < q < 1, the quantity 

xq = mf{x\Ft(x) > q) 

is called the qXh quantile of F) (v), where inf denotes the smallest real number 
x satisfying Fi(x) > q. If the random variable L(i) of Fi(x) is continuous, then 
q=Pr[L(i)<xq]. 

If the CDF Fi(x) of Eq. (7.1) is known, then 1 — p = Pr[L(l) < VaR] so that 
VaR is simply the (1 — p)th quantile of the CDF of the loss function L(l) (i.e., 
VaR = x\-p). Sometimes, VaR is referred to as the upper pth quantile because 
p is the upper tail probability of the loss distribution. The CDF is unknown in 
practice, however. Studies of VaR are essentially concerned with estimation of the 
CDF and/or its quantile, especially the upper tail behavior of the loss CDF. 

In real applications, calculation of VaR involves several factors: 

1. The probability of interest p, such as p = 0.01 for risk management and 

p — 0.001 in stress testing. 

2. The time horizon i. It might be set by a regulatory committee, such as 1 day 

or 10 days for market risk and 1 year or 5 years for credit risk. 

3. The frequency of the data, which might not be the same as the time horizon 

t. Daily observations are often used in market risk analysis. 

4. The CDF Fi(x) or its quantiles. 

5. The amount of the financial position or the mark-to-market value of the 

portfolio. 

328 

EXTREME VALUES, QUANTILES, AND VALUE AT RISK 

Among these factors, the CDF Fe(x) is the focus of econometric modeling. 
Different methods for estimating the CDF give rise to different approaches to VaR 

calculation. 

Remark. The definition of VaR in Eq. (7.1) is based on the upper tail of 
a loss function. For a long financial position, loss occurs when the returns are 
negative. Therefore, we shall use negative returns in data analysis for a long 
financial position. Furthermore, the VaR defined in Eq. (7.1) is in dollar amount. 
Since log returns correspond approximately to percentage changes in value of 
a financial asset, we use log returns r, in data analysis. The VaR calculated 
from the upper quantile of the distribution of rt+1 given information available 
at time t is therefore in percentage. The dollar amount of VaR is then the cash 
value of the financial position times the VaR of the log return series. That is, 
VaR = Value x VaR(of log returns). If necessary, one can also use the approxima¬ 
tion VaR = Value x [exp(VaR of log returns) — 1]. □ 

Remark. VaR is a prediction concerning possible loss of a portfolio in a 
given time horizon. It should be computed using the predictive distribution of 
future returns of the financial position. For example, the VaR for a 1-day hori¬ 
zon of a portfolio using daily returns rt should be calculated using the predictive 
distribution of rt+1 given information available at time t. From a statistical view¬ 
point, predictive distribution takes into account the parameter uncertainty in a 
properly specified model. However, predictive distribution is hard to obtain, and 
most of the available methods for VaR calculation ignore the effects of parameter 
uncertainty. □ 

Remark. From the prior discussion, VaR is just a quantile of the loss function. 
It does not fully describe the upper tail behavior of the loss function. In practice, 
two assets may have the same VaR yet encounter different losses when the VaR is 
exceeded. Furthermore, the VaR does not satisfy the sub-additivity property, which 
states that a risk measure for two portfolios after they have been merged should be 
no greater than the sum of their risk measures before they were merged. Therefore, 
care must be exercised in using VaR to measure risk. We discuss the concept of 
expected shortfall later as an alternative to measuring risk. The expected shortfall 
is also known as the conditional value at risk (CVaR). □ 

7.2 RISKMETRICS 

J. P. Morgan developed the RiskMetrics methodology to VaR calculation; see 
Longerstaey and More (1995). In its simple form, RiskMetrics assumes that the 
continuously compounded daily return of a portfolio follows a conditional normal 
distribution. Denote the daily log return by rt and the information set available at 
time r — 1 by Ft-\. RiskMetrics assumes that rt\Ft_\ ~ N(p,t, oj), where /z, is the 
conditional mean and o} is the conditional variance of r,. In addition, the method 

RISKMETRICS 

329 

assumes that the two quantities evolve over time according to the simple model: 

Mr = 0, of = atr^j + (1 - a)rf_lt 1 > a > 0. (7.2) 

Therefore, the method assumes that the logarithm of the daily price, pt = ln(P,), 
of the portfolio satisfies the difference equation pt — pt_\ = at, where at = crt€t 
is an IGARCH(1,1) process without drift. The value of a is often in the interval 
(0.9, 1) with a typical value of 0.94. 

A nice property of such a special random-walk IGARCH model is that the 
conditional distribution of a multiperiod return is easily available. Specifically, for 
a k-period horizon, the log return from time t + 1 to time t + k (inclusive) is 
rt[k] = rt+1 +-b rt+k-1 + A+fc- We use the square bracket [k] to denote a k- 
horizon return. Under the special IGARCH(1,1) model in Eq. (7.2), the conditional 
distribution rt[k\\Ft is normal with mean zero and variance of[k], where of\k\ 
can be computed using the forecasting method discussed in Chapter 3. Using the 
independence assumption of et and model (7.2), we have 

k 

of M = Var(r,[fc]|Ff) = Var(flt+i|F,), 

i=1 

where Var(a/+,|/y) = E(af+i\Ft) can be obtained recursively. Using rt-\ = at~\ = 
crf_i6j_i, we can rewrite the volatility equation of the IGARCH(1,1) model in Eq. 
(7.2) as 

of = of_! + (1 

«)orf2_1(fr2_1 

1) for all t. 

In particular, we have 

o'. t-\-i  — or, t+i-  i + (1 - a)o-2-_i (e2+!-_, - 1) for i = 2,..., k. 

Since E(ef+i_l — l|Fr) = 0 for i > 2, the prior equation shows that 

E{of+i\Ft) = E(af+i_x\Ft) for i = 2,..., k. 

(7.3) 

For the 1-step-ahead volatility forecast, Eq. (7.2) shows that a 2 
ao} + (1 - 
a)rf. Therefore, Eq. (7.3) shows that \ax(rt+i\Ft) =crf,l for i > 1 and, hence, 
o', [T ] = kai 2 j. The results show that r,[k]\Ft ~ A^(0, kaf+l). Consequently, under 
the special IGARCH(1,1) model in Eq. (7.2) the conditional variance of rt[k] is 
proportional to the time horizon k. The conditional standard deviation of a k-period 
horizon log return is then Vkcrt+1, which is \fk times at+\. 

f+1 

2 

Given a tail probability, RiskMetrics uses the result rt[k]\Ft ~ A(0, kcr2+1) 
to calculate VaR for the log return. If the tail probability is set to 5%, then 
VaR — 1.65cr/+i for the next trading day. This is the upper 5% quantile (or the 
95th percentile) of a normal distribution with mean zero and standard devia¬ 
tion at+]. For the next k trading days, VaR[k] = 1.65</feo>+i, which is the 95th 

330 

EXTREME VALUES, QUANTILES, AND VALUE AT RISK 

percentile of N(0, k<y^+ j). Similarly, if the tail probability is 1%, then VaR — 

2.326o>+i for the next trading day and VaR[&] = 2.326\/&<7(+i for the next k 

trading days. 

Consider the case of 1% tail probability. The VaR for the portfolio under Risk- 

Metrics is then 

VaR = Amount of position x 2.326a;+i, 

for the next trading day and that of a k-day horizon is 

VaR(k) = Amount of position x 2.326\/&orf+i, 

where the argument (k) of VaR is used to denote the time horizon and the portfolio 
value is measured in dollars. Consequently, under RiskMetrics, we have 

VaR(k) = \fk x VaR. 

This is referred to as the square wot of time rule in VaR calculation under Risk- 
Metrics. 

If the log returns are in percentages, then the 1% VaR for the next trading day 
is VaR = Amount of position x 2326ot+\/\00, where ot+\ is the volatility of the 
percentage log returns. 

Note that because RiskMetrics assumes log returns are normally distributed with 
mean zero, the loss function is symmetric and VaR are the same for long and short 
financial positions. 

Example 7.1. The sample standard deviation of the continuously compounded 
daily return of the German mark/U.S. dollar exchange rate was about 0.53% 
in June 1997. Suppose that an investor was long in $10 million worth of 
mark/dollar exchange rate contract. Then the 5% VaR for a 1-day horizon of the 
investor is 

$10,000,000 x (1.65 x 0.0053) = $87,450. 

The corresponding VaR for 10-day horizon is 

$10,000,000 x (TlO x 1.65 x 0.0053) ^ $276,541. 

Example 7.2. Consider the daily IBM log returns of Figure 7.1. As mentioned 
in Chapter 1, the sample mean of the returns is significantly different from zero. 
However, for demonstration of VaR calculation using RiskMetrics, we assume in 
this example that the conditional mean is zero and the volatility of the returns 
follows an IGARCH(1,1) model without drift. The fitted model is 

n = at, 

at — crt€t, 

of = 0.9396o^{ + (1 - 0.9396)a2_j, 

(7.4) 

RISKMETRICS 

331 

where {et} is a standard Gaussian white noise series. As expected, this model is 
rejected by the Q statistics. For instance, we have a highly significant statistic 
<2(10) = 56.19 for the squared standardized residuals. 

From the data and the fitted model, we have r^o = —0.0128 and a}n 90 = 
0.0003472. Therefore, the 1-step-ahead volatility forecast is a92190(l) = 0.000336. 

The 95% quantile of the conditional distribution r9i9i 12^9190 is 1.65 x V0.000336 = 
0.03025. Consequently, the 1-day horizon 5% VaR of a long position of $10 mil¬ 
lions is 

VaR = $10,000,000 x 0.03025 = $302,500. 

The 99% quantile is 2.326 x \/0.000336 = 0.04265, and the corresponding 1% 
VaR for the same long position is $426,500. 

Remark. To implement RiskMetrics in S-Plus, one can use ewmal (expo¬ 
nentially weighted moving average of order 1) under the mgarch (multivariate 
GARCH) command to obtain the estimate of 1 — a. Then, use the command pre¬ 
dict to obtain volatility forecasts. For the IBM data used, the estimate of a is 
1 — 0.036 = 0.964 and the 1-step-ahead volatility forecast is <79190(1) = 0.01888. 
Please see the demonstration below. This leads to VaR = $10,000,000 x (1.65 x 
0.01888) = $311,520 and VaR = $439,187 for p = 0.05 and 0.01, respectively. 
These two values are slightly higher than those of Example 7.2, which are based 
on estimates of the RATS package. □ 

S-Plus Demonstration 
The following output has been simplified: 

> ibm.risk=mgarch(ibm~-l, “ewmal) 

> ibm.risk 

ALPHA 0.036 

> predict(ibm.risk,2) 

$sigma.pred 0.01888 

7.2.1 Discussion 

An advantage of RiskMetrics is simplicity. It is easy to understand and apply. 
Another advantage is that it makes risk more transparent in the financial markets. 
However, as security returns tend to have heavy tails (or fat tails), the normality 
assumption used often results in underestimation of VaR. Other approaches to VaR 
calculation avoid making such an assumption. 

The square root of time rule is a consequence of the special model used by 
RiskMetrics. If either the zero mean assumption or the special IGARCH(1,1) 
model assumption of the log returns fails, then the rule is invalid. Consider the 
simple model 

rt = /x + at, at=crt€t, 

0, 

a2 = aaf_x + (1 - a)a2_1, 

332 

EXTREME VALUES, QUANTILES, AND VALUE AT RISK 

where {e,} is a standard Gaussian white noise series. The assumption that /x^O 
holds for returns of many heavily traded stocks on the NYSE; see Chapter 1. For 
this simple model, the distribution of rt+\ given Ft is crf+x). The 95% quantile 
used to calculate the 1-period horizon VaR becomes /x + 1.65cr/+1. For a ^-period 
horizon, the distribution of rt[k\ given Ft is N(kp, kcrf+i), where as before rt[k] = 
rt+i H-h rt+k- The 95% quantile used in the ^-period horizon VaR calculation 
is kpt + 1.65VXcr/+i = Vk(Vk/A + 1.65crf+i). Consequently, VaR(&) ^ Vk x VaR 
when the mean return is not zero. It is also easy to show that the rule fails when 
the volatility model of the return is not an IGARCH(1,1) model without drift. 

7.2.2 Multiple Positions 

In some applications, an investor may hold multiple positions and needs to com¬ 
pute the overall VaR of the positions. RiskMetrics adopts a simple approach for 
doing such a calculation under the assumption that daily log returns of each 
position follow a random-walk IGARCH(1,1) model. The additional quantities 
needed are the cross-correlation coefficients between the returns. Consider the 
case of two positions. Fet VaRi and VaR2 be the VaR for the two positions 
and p\2 be the cross-correlation coefficient between the two returns—that is, 
pn = Cov(rif, r2f)/[Var(>i,)Var(r2f)]0-5. Then the overall VaR of the investor is 

VaR = y VaRj + VaRj + 2pi2VaRi VaR2- 

The generalization of VaR to a position consisting of m instruments is straightfor¬ 
ward as 

m m 

VaR = ]T VaRf + 2^ Pij VaR,- VaR,, 

\| i=l i<j 

where p,; is the cross-correlation coefficient between returns of the i th and j th 
instruments and VaR, is the VaR of the /th instrument. 

The prior formula is obtained using the assumption that the joint distribution 
of the log returns of assets involved in the portfolio is multivariate normal with 
mean zero and covariance matrix E/+i. Under such an assumption, the log return 
of the portfolio is normal with mean zero and finite variance; see Appendix B of 
Chapter 8 for properties of multivariate normal variables. 

7.2.3 Expected Shortfall 

Given a tail probability p, VaR is simply the (1 — p)th quantile of the loss func¬ 
tion. In practice, the actual loss, if it occurs, can be greater than VaR. In this 
sense, VaR may underestimate the actual loss. To have a better assessment of the 
potential loss, one can consider the expected value of the loss function if the VaR 
is exceeded. This consideration leads to the concept of expected shortfall (ES). 

ECONOMETRIC APPROACH TO VAR CALCULATION 

333 

Under RiskMetrics, the loss function is normally distributed so that the conditional 
distribution of the loss function given that a VaR is exceeded is a truncated (from 
below) normal distribution. Properties such as mean and variance of a truncated 
normal distribution have been well-studied in the statistical literature. We can use 
the mean of the distribution to calculate expected shortfall. Specifically, consider 
the standard normal distribution X ~ N(0, 1). For a given upper tail probability p, 
let q = 1 — p and VaR, be the associated VaR, that is, VaR, is the qth quantile of 
X. Then the expectation of X given X > VaR, is E(X\X > VaR,) = f(VaRq)/p, 
where f (x) = (l/V/jr) exp(—x1 /2) is the pdf of X. The expected shortfall for a 
log return rt with conditional distribution N(0, o}) is then 

™ /(VaR,) 
ES, =-ot  or ESi_ 

i-p 

/(vm-P) 

Of 

For example, if p = 0.05, then VaRo.95 ~ 1.645 and f(VaRq)/p = /(1.645)/0.05 
= 2.0627 so that the expected shortfall under RiskMetrics is ES0.95 = 2.0627o>. If 
p = 0.01, then ES0.99 = 2.6652crr. 

7.3 ECONOMETRIC APPROACH TO VAR CALCULATION 

A general approach to VaR calculation is to use the time series econometric models 
of Chapters 2-4. For a log return series, the time series models of Chapter 2 can 
be used to model the mean equation, and the conditional heteroscedastic models 
of Chapter 3 or 4 are used to handle the volatility. For simplicity, we use GARCH 
models in our discussion and refer to the approach as an econometric approach to 
VaR calculation. Other volatility models, including the nonlinear ones in Chapter 4, 
can also be used. 

Consider the log return rt of an asset. A general time series model for rt can be 

written as p , 

rt = 00 + + at ~ eiat-h (7-5) 

i=i 7=1 

at — otct, 

U V 

°)2 = “0 + aia?-i + J2 /W-7 • (7-6) 

(=1 7=1 

Equations (7.5) and (7.6) are the mean and volatility equations for rt. These two 
equations can be used to obtain 1-step-ahead forecasts of the conditional mean and 
conditional variance of rt assuming that the parameters are known. Specifically, 
we have p q 

/(l) = 0o + - ^2eJat+i-j’ 

(=1 7=1 

U V 

/,2(i) = «o + J2aia?+i -i+(W+w 

i—1 7=1 

334 

EXTREME VALUES, QUANTILES, AND VALUE AT RISK 

If one further assumes that et is Gaussian, then the conditional distribution of 
rt+1 given the information available at time ! is V[r?(l), <r2( 1)]. Quantiles of this 
conditional distribution can easily be obtained for VaR calculation. For example, the 
95% quantile is r,(l) + 1.65S>(1). If one assumes that e, is a standardized Student-! 
distribution with v degrees of freedom, then the quantile is rt{ 1) + !*( 1 — p)d>(l), 
where !*(1 — p) is the (1 — p)th quantile of a standardized Student-! distribution 

with v degrees of freedom. 

The relationship between quantiles of a Student-! distribution with v degrees 
of freedom, denoted by tv, and those of its standardized distribution, denoted by 
!*, is 

P = Pr(!„ < q) — Pr 

6; ^ Q 

_s/v/(v - 2) ~ Vv/(v - 2). 

— Pr 

" * <7 
t* < --- 
L” ~ Vv/(v — 2) ] 

where v > 2. That is, if q is the pth quantile of a Student-! distribution with 
v degrees of freedom, then q/«Jv/(v — 2) is the pth quantile of a standardized 
Student-! distribution with v degrees of freedom. Therefore, if et of the GARCH 
model in Eq. (7.6) is a standardized Student-! distribution with v degrees of freedom 
and the upper tail probability is p, then the (1 — p)th quantile used to calculate 
the 1-period horizon VaR at time index ! is 

n( D + 

!«(1 ~ P)<V(1) 

Vv/(v - 2) ’ 

where !„(1 — p) is the (1 — p)th quantile of a Student-! distribution with v degrees 
of freedom. 

Example 7.3. Consider again the daily IBM log returns of Example 7.2. We 
use two volatility models to calculate VaR of 1-day horizon at ! = 9190 for a long 
position of $10 million. These econometric models are reasonable based on the 
modeling techniques of Chapters 2 and 3. 

Because the position is long, we use rt — —rtc, where rf is the usual log return 

of IBM stock shown in Figure 7.1. 

CASE 1. Assume that e, is standard normal. The fitted model is 

rt = —0.00066 — 0.0247r(_2 + at, at — atet, 

a} = 0.00000389 + 0.0799ar2_1 + 0.9073cri.2_1. 

From the data, we have r9ig9 = 0.00201, 1-9190 = 0.0128, and cr92190 = 0.00033455. 
Consequently, the prior AR(2)-GARCH(1,1) model produces 1-step-ahead fore¬ 
casts as 

f9i9o(l) = -0.00071 and a9219o(l) = 0.0003211. 

ECONOMETRIC APPROACH TO VAR CALCULATION 

335 

The 95% quantile is then 

-0.00071 + 1.6449 x V0.0003211 = 0.02877. 

The VaR for a long position of $10 million with probability 0.05 is 
VaR = $10,000,000 x 0.02877 = $287,700. The result shows that, with proba¬ 
bility 95%, the potential loss of holding that position next day is $287, 200 or 
less assuming that the AR(2)-GARCH(1,1) model holds. If the tail probability is 
0.01, then the 99% quantile is 

-0.00071 + 2.3262 x V0.0003211 = 0.0409738. 

The VaR for the position becomes $409, 738. 

CA$E 2. Assume that et is a standardized Student-! distribution with 5 degrees 

of freedom. The fitted model is 

rt = —0.0003 — 0.0335rf_2 + at, at = atet, 

a2 = 0.000003 + 0.0559a2_, + 0.9350a 2_,. 

From the data, we have 1-9189 = 0.00201, r9i9o = 0.0128, and a9190 = 0.000349. 
Consequently, the prior Student-! AR(2)-GARCH(1,1) model produces 1-step- 
ahead forecasts 

f9i9o(l) = -0.000367 and a92190(l) = 0.0003386. 

The 95% quantile of a Student-! distribution with 5 degrees of freedom is 2.015 
and that of its standardized distribution is 2.015/^5/3 = 1.5608. Therefore, the 
95% quantile of the conditional distribution of /-9191 given F9190 is 

-0.000367 + 1.5608V0.0003386 = 0.028354. 

The VaR for a long position of $10 million is 

VaR = $10,000,000 x 0.028352 = $283,520, 

which is essentially the same as that obtained under the normality assumption. The 
99% quantile of the conditional distribution is 

-0.000367 + (3.3649/^5/3) VO.0003386 = 0.0475943. 

The corresponding VaR is $475, 943. Comparing with that of Case 1, we see the 
heavy-tail effect of using a Student-! distribution with 5 degrees of freedom; it 
increases the VaR when the tail probability becomes smaller. In R and S-Plus, the 
quantile of a Student-! distribution with m degrees of freedom can be obtained 
by the command qt(p, m), for example, xp = qt (0.99,5.23) for the 99th per¬ 
centile of a Student-! distribution with 5.23 degrees of freedom. 

336 

EXTREME VALUES, QUANTILES, AND VALUE AT RISK 

7.3.1 Multiple Periods 

Suppose that at time h we want to compute the /c-horizon VaR of an asset whose 
log return is rt. The variable of interest is the ^-period log return at the forecast 
origin h (i.e., rh[k] = rh+1 + • • • + rh+k). If the return r, follows the time series 
model in Eqs. (7.5) and (7.6), then the conditional mean and variance of r/,[fc] 
given the information set Fk can be obtained by the forecasting methods discussed 

in Chapters 2 and 3. 

Expected Return and Forecast Error 
The conditional mean E(rk[k]\Fh) can be obtained by the forecasting method of 
ARMA models in Chapter 2. Specifically, we have 

h[k] = rh{ 1) H-1- rh(k), 

where rh (l) is the l-step-ahead forecast of the return at the forecast origin h. These 
forecasts can be computed recursively as discussed in Section 2.6.4. Using the MA 
representation 

rt = /x + at + f\at-\ + V^2«r-2 H- 

of the ARMA model in Eq. (7.5), we can write the l-step-ahead forecast error at 
the forecast origin h as 

Chit) = rh+t ~ rh(£) — ah+t + ficih+t-i + • • • + ^i-\Clh+\! 

see Eq. (2.33) and the associated forecast error. The forecast error of the expected 
^-period return r/, [A] is the sum of 1-step to k-step forecast errors of rt at the 
forecast origin h and can be written as 

eh[k] = eh{\) + eh(2) -t-b eh(k) 

— &h+1 + (ah+2 + faah+i) + • • • + xj/jah+k-i 

(=0 

(7.7) 

where yjrQ = \ . 

Expected Volatility 
The volatility forecast of the Uperiod return at the forecast origin h is the condi¬ 
tional variance of eh[k] given F*. Using the independent assumption of et+i for 

ECONOMETRIC APPROACH TO VAR CALCULATION 

337 

i = l,... ,k, where a/+(- = o>+I-cf+!-, we have 

Vh(eh[k]) — Vh(ah+k) + (1 + ^\)~yh{ah+k-\) + * * ' + Vh(®h +1) 

— °f (^) + (1 + Vofof^ “ 1) +-°f(l)» (7-8) 

where Vh (z) denotes the conditional variance of z given Fh and of(£) is the l- 
step-ahead volatility forecast at the forecast origin h. If the volatility model is 
the GARCH model in Eq. (7.6), then these volatility forecasts can be obtained 
recursively by the methods discussed in Chapter 3. 

As an illustration, consider the special time series model 

rt = ii + at, at = cr,et, 

of = a0 + a i a^_x + y8i°f-i- 

Then we have 0,- = 0 for all i > 0. The point forecast of the ^-period return at the 
forecast origin h is r^[k] — k/i and the associated forecast error is 

eh[k] = dh+k + dh+k-l + • • • + a^+i. 

Consequently, the volatility forecast for the ^-period return at the forecast origin 

h is 

k 

Var(.eh[k]\Fh) = '£,*&)- 

t=l 

Using the forecasting method of GARCH(1,1) models in Section 3.5, we have 

of (1) = ao + oqcf + /hof > 

ot(l) = aQ + (ci{+px)al(l-\), l = 2,...,k. (7.9) 

Using Eq. (7.9), we obtain that for the case of t/r,- = 0 for i > 0, 

\ax(eh[k]\Fh) = (k - l—+ ^^a,2(l), (7-10) 

where 0 = a\ + fi\ < 1. If 0,- ^ 0 for some i > 0, then one should use the general 
formula of Var(eh[k]\Fh) in Eq. (7.8). If et is Gaussian, then the conditional distri¬ 
bution of rh[k] given F/, is normal with mean k/jb and variance Vai(eh[k]\Fh). The 

338 

EXTREME VALUES, QUANTILES, AND VALUE AT RISK 

quantiles needed in VaR calculations are readily available. If the conditional dis¬ 
tribution of at is not Gaussian (e.g., a Student-t or generalized error distribution), 
simulation can be used to obtain the multiperiod VaR. 

Example 7.3 (Continued). Consider the Gaussian AR(2)-GARCH(1,1) 
model of Example 7.3 for the daily log returns of IBM stock. Suppose that we 
are interested in the VaR of a 15-day horizon starting at the forecast origin 9190 
(i.e., December 31, 1998). We can use the fitted model to compute the conditional 
mean and variance for the 15-day log return via z*9i9o[15] = ^-=1 ^190+; 
given E9190. The conditional mean is —0.00998 and the conditional variance is 
0.0047948, which is obtained by the recursion in Eq. (7.9). The 95% quantile of 
the conditional distribution is then —0.00998 + 1.6449V0.0047948 = 0.1039191. 
Consequently, the 5% 15-day horizon VaR for a long position of $10 million 
is VaR = $10,000,000 x 0.1039191 = $1,039,191. This amount is smaller than 
$287,700 x 0^15 = $1,114,257. This example further demonstrates that the 
square root of time rule used by RiskMetrics holds only for the special white 
noise IGARCH(1,1) model used. When the conditional mean is not zero, proper 
steps must be taken to compute the ^-horizon VaR. 

7.3.2 Expected Shortfall under Conditional Normality 

We can use the result of Section 7.2.3 to calculate the ES when the conditional 
distribution of the log return is of). The result is 

cc , fixq) 
ESq = !xt 4-ot, 

P 

where q = 1 — p and xq is the qth quantile of the standard normal distribution. 
For instance, if p = 0.01, then ES0.99 = /x, + 2.6652ot. 

7.4 QUANTILE ESTIMATION 

Quantile estimation provides a nonparametric approach to VaR calculation. It makes 
no specific distributional assumption on the return of a portfolio except that the 
distribution continues to hold within the prediction period. There are two types of 
quantile methods. The first method is to use empirical quantile directly, and the 
second method uses quantile regression. 

7.4.1 Quantile and Order Statistics 

Assuming that the distribution of return in the prediction period is the same as that 
in the sample period, one can use the empirical quantile of the return rt to calculate 
VaR. Let r\, ..., rn be the returns of a portfolio in the sample period. The order 

QUANTILE ESTIMATION 

339 

statistics of the sample are these values arranged in increasing order. We use the 
notation 

''(I) < r(2) < < rw 

to denote the arrangement and refer to ry) as the ith order statistic of the sample. 
In particular, r(i) is the sample minimum and r^n) the sample maximum. 

Assume that the returns are independent and identically distributed random vari¬ 
ables that have a continuous distribution with probability density function (pdf) 
/(x) and CDF F(x). Then we have the following asymptotic result from the sta¬ 
tistical literature [e.g., Cox and Hinkley (1974), Appendix 2], for the order statistic 
r(£), where l = np with 0 < p < 1. 

Result. Let xp be the pth quantile of F(x), that is, xp = F~1(p). Assume that 
the pdf f(x) is not zero at xp [i.e., f{xp) ^ 0]. Then the order statistic r(Q is 
asymptotically normal with mean xp and variance p(\ — p)/[nf2(xp)]. That is, 

rW 

p{\ - 

Xp’ n[f(xp)]2 

i = np. 

(7.11) 

Based on the prior result, one can use to estimate the quantile xp, where 
l = np. In practice, the probability of interest p may not satisfy that np is a positive 
integer. In this case, one can use simple interpolation to obtain quantile estimates. 
More specifically, for noninteger np, let t\ and £2 be the two neighboring positive 
integers such that i\ < np < f2. Define pt = li/n. The previous result shows that 
/•(4) is a consistent estimate of the quantile xPi. From the definition, p\ < p < P2. 
Therefore, the quantile xp can be estimated by 

xp = 

P2 ~ P 

P2 - Pi 

r(h) + 

P- Pi 

P2 ~ Pi 

rih)‘ 

(7.12) 

In practice, sample quantiles can easily be obtained from most statistical packages, 
including R and S-Plus. A demonstration is given after the examples. 

Example 7.4. Consider the daily log returns of Intel stock from December 15, 
1972, to December 31, 2008. There are 9096 observations. For a long position in the 
Intel stock, we consider the negative log returns. Since 9096 x 0.95 = 8641.2, we 
have l\ = 8641, i2 = 8642, p\ = 8641/9096, and p2 = 8642/9096. The empirical 
95% quantile of the negative log returns can be obtained as 

*0.95 = 0.8r(g64i) + 0.2r(8642) = 4.2952%, 

r(!) is the /th order statistic of the negative log returns. In this particular instance, 

(8641) = 4.2951% and r(8642) = 4.2954%. 

340 

EXTREME VALUES, QUANTILES, AND VALUE AT RISK 

R Demonstration 

> da=read.table("d-intc72 08.txt",header=T) 

> intc=log(da[,2]+1) 

> nintc=-intc 

> quantile(nintc,0.95) 

95% 

0.04295213 

> quantile(rtn,.05) % An alternative 

5% 

-0.04295213 

Example 7.5. Consider again the daily log returns of IBM stock from July 3, 
1962, to December 31, 1998. Using all 9190 observations, the empirical 95% quan¬ 
tile of the negative log returns can be obtained as (r(g730) + ^*(873i))/2 = 0.021603, 
where r(i) is the z'th order statistic and np = 9190 x 0.95 = 8730.5. The VaR of 
a long position of $10 million is $216,030, which is much smaller than those 
obtained by the econometric approach discussed before. Because the sample size is 
9190, we have 9098 < 9190 x 0.99 < 9099. Let px = 9198/9190 = 0.98999 and 
P2 = 9099/9190 = 0.9901. The empirical 99% quantile can be obtained as 

„ p2 — 0.99 0.99 — pi 
*0.99 = -^(9098) H-r(9099) 
P2 ~ P1 P2~ P1 

(3.627) + 

0.00001 

0.00011 

(3.657) 

0.0001 

0.00011 
3.630. 

The 1% 1-day horizon VaR of the long position is $363, 000. Again this amount 
is lower than those obtained before by other methods. 

Discussion. Advantages of using the empirical quantile method to VaR cal¬ 
culation include (a) simplicity and (b) using no specific distributional assumption. 
However, the approach has several drawbacks. First, it assumes that the distribution 
of the return r, remains unchanged from the sample period to the prediction period. 
Given that VaR is concerned mainly with tail probability, this assumption implies 
that the predicted loss cannot be greater than that of the historical loss. It is defi¬ 
nitely not so in practice. Second, when the tail probability p is small, the empirical 
quantile is not an efficient estimate of the theoretical quantile. Third, the direct 
quantile estimation fails to take into account the effect of explanatory variables 
that are relevant to the portfolio under study. In real application, VaR obtained by 
the empirical quantile can serve as a lower bound for the actual VaR. □ 

The expected shortfall can also be estimated directly from the sample returns. 
Let xq be the empirical qth quantile, where q = 1 - p with p being the upper tail 

QUANTILE ESTIMATION 

probability. We have 

ES, 

341 

where /[•] = 1 if X(i) > xq and = 0, otherwise, and Nq denotes the number of x-t 
greater than xq. For illustration, consider the negative IBM daily log returns. If 
p = 0.01, we have xo.99 = 3.630. Therefore, ES0.99 = 5.097. 

R Demonstration 

> da=read.table("d-ibm6 2 9 8.txt",header=T) 

> ibm=log(da[,2]+1)*100 

> nibm=-ibm 

> q99=quantile(nibm,0.99) 

> q99 

99% 

[1] 3.630295 

> idx=c(1:length(nibm))[nibm>q99] % locate the exceedances 

> es=mean(nibm[idx]) 

> es 

[1] 5.097222 

7.4.2 Quantile Regression 

In real application, one often has explanatory variables available that are important 
to the problem under study. For example, the action taken by Federal Reserve 
Banks on interest rates could have important impacts on the returns of U.S. 
stocks. It is then more appropriate to consider the distribution function rt+\\Ft, 
where Ft includes the explanatory variables. In other words, we are interested 
in the quantiles of the distribution function of rt+\ given Ft. Such a quantile 
is referred to as a regression quantile in the literature; see Koenker and Bassett 

(1978). 

To understand regression quantile, it is helpful to cast the empirical quantile of 
the previous subsection as an estimation problem. For a given probability p, the 

pth quantile of {rt} is obtained by 

n 

xp = argmin^ ^ wp(n - ft), 

/=! 

where wp(z) is defined by 

wp{z) 

pz if z > 0, 
(p - l)z if z < 0. 

Regression quantile is a generalization of such an estimate. 

342 

EXTREME VALUES, QUANTILES, AND VALUE AT RISK 

To see the generalization, suppose that we have the linear regression 

rt = f}'xt+at, (7.13) 

where /? is a ^-dimensional vector of parameters and x, is a vector of predictors that 
are elements of Ft-\. The conditional distribution of rt given Ft-i is a translation 
of the distribution of a, because P'xt is known. Viewing the problem this way, 
Koenker and Bassett (1978) suggest estimating the conditional quantile xp\Ft-\ of 
r, given Ft-\ as 

xp\Ft-x * mf{fi'0x\Rptf0) = min}, (7.14) 

where “Rp(fi0) = min” means that (i0 is obtained by 

n 

p0 = arguing ^ wp(rt - fi'xt), 

t=l 

where wp(-) is defined as before. A computer program to obtain such an estimated 
quantile can be found in Koenker and D’Orey (1987). The package quantreg of 
R performs quantile regression analysis. 

7.5 EXTREME VALUE THEORY 

In this section, we review some extreme value theory in the statistical literature. 
Denote the return of an asset, measured in a fixed time interval such as daily, 
by rt. Consider the collection of n returns, {/q,..., rn). The minimum return of 
the collection is r(i), that is, the smallest order statistic, whereas the maximum 
return is r(n), the maximum order statistic. Specifically, r(i) = mini<y<„{ry} and 
r(„) = maxi<y<n{r,-}. Following the literature and using the loss function in VaR 
calculation, we focus on properties of the maximum return r(„). However, the theory 
discussed also applies to the minimum return of an asset over a given time period 
because properties of the minimum return can be obtained from those of the max¬ 
imum by a simple sign change. Specifically, we have r(1) = - maxi<7<n{—rj) — 
~r(n)> where rct = -rt with the superscript c denoting sign change. The minimum 
return is relevant to holding a long financial position. As before, we shall use neg¬ 
ative log returns, instead of the log returns, to perform VaR calculation for a long 
position. 

7.5.1 Review of Extreme Value Theory 

Assume that the returns r, are serially independent with a common cumulative 
distribution function F(x) and that the range of the return r, is [/, u]. For log 

EXTREME VALUE THEORY 

343 

returns, we have 1 = —oo and u = oo. Then the CDF of r(„), denoted by Fn<n(x), 
is given by 

Fn,n(x) = Pr[r(n) < x] 

= Pr(ri < x, r2 < xrn < x) (by definition of maximum) 

n 

= J~[Pr(rj < x) (by independence) 

7=1 

n 

^Y\F(x) = [F(x)]n. (7.15) 

7=1 

In practice, the CDF F(x) of rt is unknown and, hence, FnM(x) of r(„) is unknown. 
However, as n increases to infinity, F„t„(x) becomes degenerated—namely, 
Fn,n00 —> 0 if x < u and Fn n{x) -a 1 if x > u as n goes to infinity. This 
degenerated CDF has no practical value. Therefore, the extreme value theory is 
concerned with finding two sequences {/?„} and {an}, where a„ > 0, such that the 
distribution of r(„*) = (r(„) — /3n)/<Xn converges to a nondegenerate distribution as 
n goes to infinity. The sequence {/!„} is a location series and {an} is a series of 
scaling factors. Under the independent assumption, the limiting distribution of the 
normalized minimum r(„*) is given by 

j expt-d + ^r1^] if £ 7^ 0, 
[ exp[— exp(—x)] if £ = 0 

for x < — 1 /£ if £ < 0 and for x > — 1 /£ if £ > 0, where the subscript * signifies 
the maximum. The case of £ = 0 is taken as the limit when £ -* 0. The parameter 
£ is referred to as the shape parameter that governs the tail behavior of the limiting 
distribution. The parameter a = l/£ is called the tail index of the distribution. 

The limiting distribution in Eq. (7.16) is the generalized extreme value (GEV) 
distribution of Jenkinson (1955) for the maximum. It encompasses the three types 
of limiting distribution of Gnedenko (1943): 

• Type I: £ = 0, the Gumbel family. The CDF is 

F*(x) = exp[—exp(—x)], —oo < x < oo. 

(7.17) 

• Type II: £ > 0, the Frechet family. The CDF is 

| exp[—(1 + £x) 1/?] if x > —1/£, 
I 0 otherwise. 

(7.18) 

• Type III: £ < 0, the Weibull family. The CDF here is 

p1 ( \_ exp[—(1 +£x)-1/?] if x < —1/£, 

~ 1 otherwise. 

344 

EXTREME VALUES, QUANTILES, AND VALUE AT RISK 

Figure 7.2 Probability density functions of extreme value distributions for maximum. Solid line is 
for Gumbel distribution, dotted line is for Weibull distribution with £ = —0.5, and dashed line is for 
Frechet distribution with = 0.9. 

Gnedenko (1943) gave necessary and sufficient conditions for the CDF F(x) of rt 
to be associated with one of the three types of limiting distribution. Briefly speak¬ 
ing, the tail behavior of F(x) determines the limiting distribution F*(x) of the 
maximum. The right tail of the distribution declines exponentially for the Gumbel 
family, by a power function for the Frechet family, and is finite for the Weibull 
family (Figure 7.2). Readers are referred to Embrechts, Kuppelberg, and Mikosch 
(1997) for a comprehensive treatment of the extreme value theory. For risk man¬ 
agement, we are mainly interested in the Frechet family, which includes stable 
and Student-t distributions. The Gumbel family consists of thin-tailed distributions 
such as normal and lognormal distributions. The probability density function (pdf) 
of the generalized limiting distribution in Eq. (7.16) can be obtained easily by 
differentiation: 

(1+fx) 1/? 1 exp[-(l + ijx) 1/?] if £ ^ 0, 
exp[—x - exp(-x)] if £ = 0, 

(7.19) 

where -oo < x < oo for £ = 0, and x < -1 /£ for £ < 0, and x > -1 /£ for £ > 0. 
The aforementioned extreme value theory has two important implications. First, 
the tail behavior of the CDF F(x) of rt, not the specific distribution, determines 
the limiting distribution F*(x) of the (normalized) maximum. Thus, the theory is 
generally applicable to a wide range of distributions for the return rt. The sequences 

EXTREME VALUE THEORY 

345 

{/3„} and {a„}, however, may depend on the CDF F(x). Second, Feller (1971, 
p. 279) shows that the tail index £ does not depend on the time interval of rt. 
That is, the tail index (or equivalently the shape parameter) is invariant under time 
aggregation. This second feature of the limiting distribution becomes handy in the 
VaR calculation. 

The extreme value theory has been extended to serially dependent observa¬ 
tions {rr}”=1 provided that the dependence is weak. Berman (1964) shows that the 
same form of the limiting extreme value distribution holds for stationary normal 
sequences provided that the autocorrelation function of rt is squared summable 
(i.e., Pf < °°)> where p, is the lag-/ autocorrelation function of rt. For fur¬ 
ther results concerning the effect of serial dependence on the extreme value theory, 
readers are referred to Leadbetter, Lindgren, and Rootzen (1983, Chapter 3). We 
shall discuss extremal index for a strictly stationary time series later in Section 7.8. 

7.5.2 Empirical Estimation 

The extreme value distribution contains three parameters—£, /3n, and an. These 
parameters are referred to as the shape, location, and scale parameters, respec¬ 
tively. They can be estimated by using either parametric or nonparametric methods. 
We review some of the estimation methods. 

For a given sample, there is only a single minimum or maximum, and we cannot 
estimate the three parameters with only an extreme observation. Alternative ideas 
must be used. One of the ideas used in the literature is to divide the sample into 
subsamples and apply the extreme value theory to the subsamples. Assume that 
there are T returns {r; }J=1 available. We divide the sample into g nonoverlapping 
subsamples each with n observations, assuming for simplicity that T = ng. In other 
words, we divide the data as 

{r\, . . . , rn\r'n+1! • • • j Yin \f2n+l > • • ■ > r3n I ' ‘ ' \^(g— l)n+l« • • • > rng\ 

and write the observed returns as rin+j, where 1 < j < n and i = 0, ..., g — 1. 
Note that each subsample corresponds to a subperiod of the data span. When n is 
sufficiently large, we hope that the extreme value theory applies to each subsample. 
In application, the choice of n can be guided by practical considerations. For 
example, for daily returns, n = 21 corresponds approximately to the number of 
trading days in a month and n = 63 denotes the number of trading days in a 

quarter. 

Let rnj be the maximum of the ith subsample (i.e., rnj is the largest return of the 
zth subsample), where the subscript n is used to denote the size of the subsample. 
When n is sufficiently large, xn<i = (rn>i - pn)/an should follow an extreme value 
distribution, and the collection of subsample maxima {rnj\i = 1,..., g} can then 
be regarded as a sample of g observations from that extreme value distribution. 

Specifically, we define 

rn,i — max {r}, 

1 <j<n 

i = l,...,g. 

(7.20) 

346 

EXTREME VALUES, QUANTILES, AND VALUE AT RISK 

The collection of subsample maxima [rnj] is the data we use to estimate the 
unknown parameters of the extreme value distribution. Clearly, the estimates 
obtained may depend on the choice of subperiod length n. 

Remark. When T is not a multiple of the subsample size n, several methods 
have been used to deal with this issue. First, one can allow the last subsample to 
have a smaller size. Second, one can ignore the first few observations so that each 
subsample has size n. □ 

The Parametric Approach 
Two parametric approaches are available. They are the maximum-likelihood and 
regression methods. 

Maximum-Likelihood Method 
Assuming that the subperiod maxima {/■„,,-} follow a generalized extreme value 
distribution such that the pdf of Xj = (rnj — fin)/an is given in Eq. (7.19), we can 
obtain the pdf of rni by a simple transformation as 

±[‘ + exp [- (i + 

/ (rn,i)~ 

Xexp[-^-exp(-^)] 

if Hn 7^ 0. 

if Hn = 0, 

where it is understood that 1 + %n(rnj - fin)/an > 0 if ^ 0. The subscript n is 
added to the shape parameter £ to signify that its estimate depends on the choice 
of n. Under the independence assumption, the likelihood function of the subperiod 
maxima is 

f (Ml’ • • ■ i rn,g\Hni fin) — j [ f (rn,i)- 

( = 1 

8 

Nonlinear estimation procedures can then be used to obtain maximum-likelihood 
estimates of fin, and an. These estimates are unbiased, asymptotically normal, 
and of minimum variance under proper assumptions. See Embrechts et al. (1997) 
and Coles (2001) for details. We apply this approach to some stock return series 
later. 

Regression Method 

This method assumes that [rn,i}8i=x is a random sample from the generalized extreme 
value distribution in Eq. (7.16) and makes use of properties of order statistics; see 
Gumbel (1958). Denote the order statistics of the subperiod maxima {rnj}8i=l as 

Ml) < M2) <••• < rn(g). 

EXTREME VALUE THEORY 

347 

Using properties of order statistics (e.g., Cox and Hinkley, 1974, p. 467), 
we have 

E{F*[r,.(/)]} = —— 
8 + 1 

(7.21) 

For simplicity, we separate the discussion into two cases depending on the value 
of First, consider the case of £ ^ 0. From Eq. (7.16), we have 

F*[rn(i)] = exp 

rn(i) - Pn 

Un 

(7.22) 

Consequently, using Eqs. (7.21) and (7.22) and approximating expectation by an 
observed value, we have 

= exp 

g + 1 

1 /&. 

rn(i) — Pn 

an 

i = 1,..., g- 

Taking natural logarithm twice, the prior equation gives 

In 

rn{i) 

Pn 

OLn 

i = 1, • • •, g- 

In practice, letting e,- be the deviation between the previous two quantities and 
assuming that the series {et} is not serially correlated, we have a regression setup 

In  -In 

g + 1 

Aln(x+t:J^h 
Hn V art 

+ £/> 

i = l,...,g. (7.23) 

The least-squares estimates of %n, Pn, and an can be obtained by minimizing the 

sum of squares of 

When = 0, the regression setup reduces to 

In  -In 

-1 Pn 
-rn(i) H-- + ei, 
din Oin 

1, 

8- 

The least-squares estimates are consistent but less efficient than the likelihood 

estimates. We use the likelihood estimates in this chapter. 

348 

EXTREME VALUES, QUANTILES, AND VALUE AT RISK 

The Nonparametric Approach 
The shape parameter £ can be estimated using some nonparametric methods. We 
mention two such methods here. These two methods are proposed by Hill (1975) 
and Pickands (1975) and are referred to as the Hill estimator and Pickands estimator, 
respectively. Both estimators apply directly to the returns {rt}J=l. Thus, there is no 
need to consider subsamples. Denote the order statistics of the sample as 

f(l) < T(2) < • • • < r(7). 

Let q be a positive integer. The two estimators of £ are defined as 

q < 774, (7.24) 

(7.25) 

where the argument (q) is used to emphasize that the estimators depend on q and the 
subscripts p and h denote Pickands and Hill estimators, respectively. The choice of 
q differs between Hill and Pickands estimators. It has been investigated by several 
researchers, but there is no general consensus on the best choice available. Dekkers 
and De Haan (1989) show that %p(q) is consistent if q increases at a properly chosen 
pace with the sample size T. In addition, *Jq[%p(q) — £] is asymptotically normal 
with mean zero and variance £2(22^+1 + l)/[2(2? - 1) ln(2)]2. The Hill estimator is 
applicable to the Frechet distribution only, but it is more efficient than the Pickands 
estimator when applicable. Goldie and Smith (1987) show that y/q[^h(q) ~ £] is 
asymptotically normal with mean zero and variance £2. In practice, one may plot the 
Hill estimator £/,(<?) against q and find a proper q such that the estimate appears to 
be stable. The estimated tail index a = 1 /£/,(<?) can then be used to obtain extreme 
quantiles of the return series; see Zivot and Wang (2003). 

7.5.3 Application to Stock Returns 

We apply the extreme value theory to the daily log returns of IBM stock from July 
3, 1962, to December 31, 1998. The returns are measured in percentages, and the 
sample size is 9190 (i.e., T = 9190). Figure 7.3 shows the time plots of extreme 
daily log returns when the length of the subperiod is 21 days, which corresponds 
approximately to a month. The October 1987 crash is clearly seen from the plot. 
Excluding the 1987 crash, the range of extreme daily log returns is between 0.5 
and 13%. 

Table 7.1 summarizes some estimation results of the shape parameter £ via 
the Hill estimator. Three choices of q are reported in the table, and the results 
are stable. To provide an overall picture of the performance of the Hill estimator, 
Figure 7.4 shows the scatterplots of the Hill estimator %h(q) and its pointwise 95% 
confidence interval against q. For both positive and negative extreme daily log 

EXTREME VALUE THEORY 

349 

C\J 
1— 
O 

o 

in 
I 
o 

c I 

o 
(M 

LO 
CM 
I 

1970 1980 1990 2000 

Year 

(a) 

1970 

1980 

Year 

(b) 

1990 

2000 

Figure 7.3 Maximum and minimum daily log returns of IBM stock when subperiod is 21 trading days. 

Data span is from July 3, 1962, to December 31, 1998: (a) positive returns and (b) negative returns. 

TABLE 7.1 Results of Hill Estimator for Daily Log Returns of IBM Stock from July 
3, 1962, to December 31, 1998" 

q 

n 
-rt 

190 

200 

210 

0.300(0.022) 

0.290(0.021) 

0.299(0.021) 

0.292(0.021) 

0.305(0.021) 

0.289(0.020) 

"Standard errors are  in parentheses. 

returns, the estimator is stable except for cases when q is small. The estimated 
shape parameters are about 0.30 and are significantly different from zero at the 
asymptotic 5% level. The plots also indicate that the shape parameter £ appears to 
be larger for the negative extremes, indicating that the daily log return may have a 
heavier left tail. Overall, the result indicates that the distribution of daily log returns 
of IBM stock belongs to the Frechet family. The analysis thus rejects the normality 
assumption commonly used in practice. Such a conclusion is in agreement with 
that of Longin (1996), who used a U.S. stock market index series. R and S-Plus 
commands used to perform the analysis are given in the demonstration below. 

Next, we apply the maximum-likelihood method to estimate parameters of the 
generalized extreme value distribution for IBM daily log returns. Table 7.2 summa¬ 
rizes the estimation results for different choices of the length of subperiods ranging 

350 

EXTREME VALUES, QUANTILES, AND VALUE AT RISK 

0.0636 0.0428 0.0341 0.0306 0.0286 0.0265 0.0249 0.0234 

Threshold 

Order statistics 

Threshold 

0.0552 0.0377 0.0315 0.0283 0.0258 0.0243 0.0228 0.0213 

Figure 7.4 Scatterplots of Hill estimator for daily log returns of IBM stock. Sample period is from 

July 3, 1962, to December 31, 1998: upper plot is for positive returns and lower one for negative 
returns. 

Order statistics 

TABLE 7.2 Maximum-Likelihood Estimates of Extreme Value Distribution for Daily 
Log Returns of IBM Stock from July 3, 1962 to December 31, 1998" 

Length of Subperiod 

Scale an 

Location 

Shape Par. 

Minimal Returns 

1 mon. (n = 21, g — 437) 
1 qur (n — 63, g = 145) 
6 mon. (n = 126, g = 72) 
1 year in = 252, g = 36) 

0.823(0.035) 
0.945(0.077) 
1.147(0.131) 
1.542(0.242) 

Maximal Returns 

1 mon. (n = 21, g = 437) 
1 qur (n = 63, g — 145) 
6 mon. (n = 126, g — 72) 
1 year (n = 252, g = 36) 

0.931(0.039) 
1.157(0.087) 
1.292(0.158) 
1.624(0.271) 

Standard errors are in parentheses. 

1.902(0.044) 
2.583(0.090) 
3.141(0.153) 
3.761(0.285) 

2.184(0.050) 
3.012(0.108) 
3.471(0.181) 
4.475(0.325) 

0.197(0.036) 
0.335(0.076) 
0.330(0.101) 
0.322(0.127) 

0.168(0.036) 
0.217(0.066) 
0.349(0.130) 
0.264(0.186) 

EXTREME VALUE THEORY 

351 

from 1 month (n = 21) to 1 year (n — 252). From the table, we make the following 
observations: 

• Estimates of the location and scale parameters fn and an increase in modulus 
as n increases. This is expected as magnitudes of the subperiod minimum and 
maximum are nondecreasing functions of n. 

• Estimates of the shape parameter (or equivalently the tail index) are stable for 

the negative extremes when n > 63 and are approximately 0.33. 

• Estimates of the shape parameter are less stable for the positive extremes. The 
estimates are smaller in magnitude but remain significantly different from zero. 

• The results for n = 252 have higher variabilities as the number of subperiods 

g is relatively small. 

Again the conclusion obtained is similar to that of Longin (1996), who provided a 
good illustration of applying the extreme value theory to stock market returns. 

The results of Table 7.2 were obtained using a Fortran program developed by 
Richard Smith and modified by the author. The package evir of R performs similar 
estimation. S-Plus is also based on the evir package. I demonstrate below the 
commands used. Note that the package uses subgroup maxima in the estimation so 
that negative log returns are used for holding long financial positions. Furthermore, 
xi, sigma, mu in the package corresponds to (£„, a„, /3„) of the table. The estimates 
obtained by R and S-Plus are close to those in Table 7.2. A source of minor 
difference is that in Table 7.2 I dropped some data points at the beginning when 
the sample size T is not a multiple of the subgroup size n. Consequently, results 
of the R package have one more subgroup than that of Table 7.2. 

R Demonstration for Extreme Value Analysis 
The series is daily IBM log returns from 1962 to 1998. The following output was 

edited: 

> library(evir) 

> help(hill) 

> da=read.table("d-ibm6298.txt",header=T) 

> ibm=log(da[,2]+1)*100 

> nibm=-ibm 

> par(mfcol=c(2,1)) <== Obtain plots 

> hill(ibm,option=c("xi"),end=500) 

> hill(nibm,option=c("xi"),end=500) 

# A simple R program to compute Hill estimate 

> source("Hill.R") 

> Hill 

function(x,q){ 

# Compute the Hill estimate of the shape parameter. 

# x: data and q: the number of order statistics used. 

sx=sort(x) 

T=length(x) 

352 

EXTREME VALUES, QUANTILES, AND VALUE AT RISK 

ist=T-q 

y=log(sx[ist:T]) 

hill=sum(y[2:length(y)])/q 

hill=hill-y[1] 

sd=sqrt (hill/'2/q) 

cat("Hill estimate & std-err:",c(hill,sd),"\n") 

} 
> ml=Hill(ibm,190) 

Hill estimate & std-err: 0.3000144 0.02176533 

> ml=Hill(nibm,190) 

Hill estimate & std-err: 0.2903796 0.02106635 

> ml=gev(nibm,block=21) 

> ml 

$n.all 

[1] 9190 

$n 

[1] 438 

$data 

[1] 3.2884827 3.6186920 3.9936970 ... 

$block 
[1] 21 
$par.ests 

xi sigma mu 

0.1954537 0.8240286 1.9033817 

$par.ses 

xi sigma 

mu 

0.03553259 0.03477151 

0.04413856 

$varcov 

[, 1] 

[,2] [,3] 

[1, ] 

1.262565e-03 

-2 

831235e-05 -0.0004336771 

[2, ]  -2.831235e-05 

[3,]  -4.336771e-04 

1 

8 

209058e-03 0.0008477562 

477562e-04 0.0019482125 

> names(ml) 

[1] "n.all" "n" "data" "block" "par.ests" 

[6] "par.ses" "varcov" "converged" "nllh.final" 

> plot(ml) 

Make a plot selection (or 0 to exit): 

1: plot: Scatterplot of Residuals 

2: plot: QQplot of Residuals 

Selection: 1 

Define the residuals of a GEV distribution fit as 

u>« 

EXTREME VALUE APPROACH TO VAR 

353 

V) 
CO 
=3 
~a 
C/3 
CD 
tr 

Figure 7.5 Residual plots from fitting GEV distribution to daily negative IBM log returns, in percent¬ 

age, for data from July 3, 1962, to December 31, 1998, with subperiod length of 21 days. 

Using the pdf of the GEV distribution and transformation of variables, one can 
easily show that {uy} should form an iid random sample of exponentially distributed 
random variables if the fitted model is correctly specified. Figure 7.5 shows the 
residual plots of the GEV distribution fit to the daily negative IBM log returns 
with subperiod length of 21 days. The left panel gives the residuals and the right 
panel shows a quantile-to-quantile (QQ) plot against an exponential distribution. 
The plots indicate that the fit is reasonable. 

Remark. Besides evir, several other packages are also available in R to per¬ 

form extreme value analysis. They are evd, POT, and extRemes. □ 

7.6 EXTREME VALUE APPROACH TO VAR 

In this section, we discuss an approach to VaR calculation using the extreme 
value theory. The approach is similar to that of Longin (1999a,b), who pro¬ 
posed an eight-step procedure for the same purpose. We divide the discussion 
into two parts. The first part is concerned with parameter estimation using the 
method discussed in the previous subsections. The second part focuses on VaR 
calculation by relating the probabilities of interest associated with different time 

intervals. 

354 

EXTREME VALUES, QUANTILES, AND VALUE AT RISK 

Part I 
Assume that there are T observations of an asset return available in the sample 
period. We partition the sample period into g nonoverlapping subperiods of length 
n such that T — ng. If T = ng + m with 1 < m < n, then we delete the first m 
observations from the sample. The extreme value theory discussed in the previous 
section enables us to obtain estimates of the location, scale, and shape parameters 
Pn,un, and for the subperiod maxima {rnj}. Plugging the maximum-likelihood 
estimates into the CDF in Eq. (7.16) with x = (r — /3n)/an, we can obtain the 
quantile of a given probability of the generalized extreme value distribution. Let 
p* be a small upper tail probability that indicates the potential loss and r* be 
the (1 — p*)th quantile of the subperiod maxima under the limiting generalized 
extreme value distribution. Then we have 

equation as 

exp  f r, ttr-A.ii 
L an 
exp [-«p(-^)] 
that  1 + !n(r* — Pn)/an 

if In ^ 0, 

if In — 0, 

)= •  -[l+*•«-«] 

-1 !Sn 

exp ( \f") 

if In 7^ 0, 

if In = 0, 

we obtain the quantile as 

r * = 
n 

Pn- fn {l-[-ln(l-/>*)] ?n} if£„^0, 

Pn - Un ln[— ln(l - p*)] if = 0. 

(7.26) 

In financial applications, the case of ^ 0 is of major interest. 

Part II 
For a given upper tail probability p*, the quantile r* of Eq. (7.26) is the VaR 
based on the extreme value theory for the subperiod maximum. The next step is to 
make explicit the relationship between subperiod maxima and the observed return 
rt series. 

Because most asset returns are either serially uncorrelated or have weak serial 

correlations, we may use the relationship in Eq. (7.15) and obtain 

1 ~P* = P(rnti < r*) = [P(rt < r*)]n. (7.27) 

This relationship between probabilities allows us to obtain VaR for the original 
asset return series rt. More precisely, for a specified small upper probability p, 
the (1 - p)th quantile of rt is r* if the upper tail probability p* of the subperiod 

EXTREME VALUE APPROACH TO VAR 

355 

maximum is chosen based on Eq. (7.27), where P(rt < r*) = 1 — p. Consequently, 
for a given small upper tail probability p, the VaR of a financial position with log 
return rt is 

Pn-%{l-[-nln(l-p)] ?' 

1 lf ^ ° } (7.28) 

Pn — an ln[—/? ln(l - p)] 

if — 0, ) 

where n is the length of the subperiod. 

Summary 
We summarize the approach of applying the traditional extreme value theory to 
VaR calculation as follows: 

1. Select the length of the subperiod n and obtain subperiod maxima {rnj}, 

i = where g = [T/n\. 

2. Obtain the maximum-likelihood estimates of /3n,an, and £„. 

3. Check the adequacy of the fitted extreme value model; see the next section 

for some methods of model checking. 

4. If the extreme value model is adequate, apply Eq. (7.28) to calculate VaR. 

Remark. Since we focus on loss function so that maxima of log returns are 
used in the derivation. Keep in mind that for a long financial position, the return 
series used in loss function is the negative log returns, not the traditional log 
returns. □ 

Example 7.6. Consider the daily log return, in percentage, of IBM stock from 
July 3, 1962, to December 31, 1998. From Table 7.2, we have an = 0.945, j3n = 
2.583, and fn = 0.335 for n = 63. Therefore, for the left-tail probability p = 0.01, 
the corresponding VaR is 

VaR = 2.583 - {1 - [-63 ln(l - 0.01 )]-°'335} 

= 3.04969. 

Thus, for daily negative log returns of the stock, the upper 1% quantile is 3.04969. 
If one holds a long position on the stock worth $10 million, then the estimated VaR 
with probability 1% is $10,000,000 x 0.0304969 = $304, 969. If the probability is 

0.05, then the corresponding VaR is $166, 641. 

If we chose n = 21 (i.e., approximately 1 month), then an = 0.823, pn — 1.902, 
and f„ = 0.197. The upper 1% quantile of the negative log returns based on the 

extreme value distribution is 

VaR = 1.902- 

^^{1 - [-21 ln(l - 0.01)r°197} = 3.40013. 
0.1971 

356 

EXTREME VALUES, QUANTILES, AND VALUE AT RISK 

Therefore, for a long position of $10,000,000, the corresponding 1-day horizon VaR 
is $340, 013 at the 1% risk level. If the probability is 0.05, then the corresponding 
VaR is $184, 127. In this particular case, the choice of n = 21 gives higher VaR 

values. 

It is somewhat surprising to see that the VaR values obtained in Example 7.6 
using the extreme value theory are smaller than those of Example 7.3 that uses a 
GARCH(1,1) model. In fact, the VaR values of Example 7.6 are even smaller than 
those based on the empirical quantile in Example 7.5. This is due in part to the 
choice of probability 0.05. If one chooses probability 0.001 =0.1% and consid¬ 
ers the same financial position, then we have VaR = $546,641 for the Gaussian 
AR(2)-GARCH(1,1) model and VaR = $666,590 for the extreme value theory 
with n — 21. Furthermore, the VaR obtained here via the traditional extreme value 
theory may not be adequate because the independent assumption of daily log 
returns is often rejected by statistical testings. Finally, the use of subperiod max¬ 
ima overlooks the fact of volatility clustering in the daily log returns. The new 
approach of extreme value theory discussed in the next section overcomes these 
weaknesses. 

Remark. As shown by the results of Example 7.6, the VaR calculation based 
on the traditional extreme value theory depends on the choice of n, which is the 
length of subperiods. For the limiting extreme value distribution to hold, one would 
prefer a large n. But a larger n means a smaller g when the sample size T is fixed, 
where g is the effective sample size used in estimating the three parameters an, /3n, 
and £„. Therefore, some compromise between the choices of n and g is needed. A 
proper choice may depend on the returns of the asset under study. We recommend 
that one should check the stability of the resulting VaR in applying the traditional 
extreme value theory. □ 

7.6.1 Discussion 

We have applied various methods of VaR calculation to the daily log returns of 
IBM stock for a long position of $10 million. Consider the VaR of the position for 
the next trading day. If the probability is 5%, which means that with probability 
0.95 the loss will be less than or equal to the VaR for the next trading day, then 
the results obtained are 

1. $302, 500 for the RiskMetrics 

2. $287, 200 for a Gaussian AR(2)-GARCH(1,1) model 

3. $283, 520 for an AR(2)-GARCH(1,1) model with a standardized Student-! 

distribution with 5 degrees of freedom 

4. $216, 030 for using the empirical quantile 

5. $184, 127 for applying the traditional extreme value theory using monthly 
minima (i.e., subperiod length n = 21) of the log returns (or maxima of the 
negative log returns) 

EXTREME VALUE APPROACH TO VAR 

357 

If the probability is 1%, then the VaR is 

1. $426, 500 for the RiskMetrics 

2. $409, 738 for a Gaussian AR(2)-GARCH(1,1) model 

3. $475, 943 for an AR(2)-GARCH(1,1) model with a standardized Student-? 

distribution with 5 degrees of freedom 

4. $365, 709 for using the empirical quantile 

5. $340, 013 for applying the traditional extreme value theory using monthly 

minima (i.e., subperiod length n = 21) 

If the probability is 0.1%, then the VaR becomes 

1. $566, 443 for the RiskMetrics 

2. $546, 641 for a Gaussian AR(2)-GARCH(1,1) model 

3. $836, 341 for an AR(2)-GARCH(1,1) model with a standardized Student-? 

distribution with 5 degrees of freedom 

4. $780, 712 for using the empirical quantile 

5. $666, 590 for applying the traditional extreme value theory using monthly 

minima (i.e., subperiod length n = 21) 

There are substantial differences among different approaches. This is not sur¬ 
prising because there exists substantial uncertainty in estimating tail behavior of a 
statistical distribution. Since there is no true VaR available to compare the accuracy 
of different approaches, we recommend that one applies several methods to gain 
insight into the range of VaR. 

The choice of tail probability also plays an important role in VaR calculation. For 
the daily IBM stock returns, the sample size is 9190 so that the empirical quantiles 
of 5 and 1% are decent estimates of the quantiles of the return distribution. In 
this case, we can treat the results based on empirical quantiles as conservative 
estimates of the true VaR (i.e., lower bounds). In this view, the approach based 
on the traditional extreme value theory seems to underestimate the VaR for the 
daily log returns of IBM stock. The conditional approach of extreme value theory 
discussed in the next section overcomes this weakness. 

When the tail probability is small (e.g., 0.1%), the empirical quantile is a less 
reliable estimate of the true quantile. The VaR based on empirical quantiles can 
no longer serve as a lower bound of the true VaR. Finally, the earlier results show 
clearly the effects of using a heavy-tail distribution in VaR calculation when the 
tail probability is small. The VaR based on either a Student-? distribution with 5 
degrees of freedom or the extreme value distribution is greater than that based on 
the normal assumption when the probability is 0.1%. 

7.6.2 Multiperiod VaR 

The square root of time rule of the RiskMetrics methodology becomes a special 
case under the extreme value theory. The proper relationship between f-day and 

358 

EXTREME VALUES, QUANTILES, AND VALUE AT RISK 

1-day horizons is 

VaR(£) = £1/aVaR = f^VaR, 

where a is the tail index and £ is the shape parameter of the extreme value distri¬ 
bution; see Danielsson and de Vries (1997a). This relationship is referred to as the 
a root of time rule. Here a — l/£, not the scale parameter an. 

For illustration, consider the daily log returns of IBM stock in Example 7.6. If 

we use p = 0.01 and the results of n = 63, then for a 30-day horizon we have 

VaR(30) = (30)°'335VaR = 3.125 x $304,969 = $952,997. 

Because f0335 < f0-5, the a root of time rule produces lower f-day horizon VaR 
than the square root of time rule does. 

7.6.3 Return Level 

Another risk measure based on the extreme values of subperiods is the return level. 
The g n-subperiod return level, Ln<g, is defined as the level that is exceeded in one 
out of every g subperiods of length n. That is, 

P(fn,i -> Ln,g) — > 
g 

where rnj denotes subperiod maximum. The subperiod in which the return level 
is exceeded is called a stress period. If the subperiod length n is sufficiently large 
so that normalized rnj follows the GEV distribution, then the return level is 

provided that ^ 0. Note that this is precisely the quantile of extreme value 
distribution given in Eq. (7.26) with tail probability p* = l/g, even though we 
write it in a slightly different way. Thus, return level applies to the subperiod 
maximum, not to the underlying returns. This marks the difference between VaR 
and return level. 

For the daily negative IBM log returns with subperiod length of 21 days, we can 
use the fitted model to obtain the return level for 12 such subperiods (i.e., g = 12). 
The return level is 4.4835%. 

R and S-Plus Commands for Obtaining Return Level 

> ml=gev(nibm,block=21) 

# S-Plus output 

> rl.21.12=rlevel.gev(ml, k.blocks=12, type='profile') 

> class(rl.21.12) 

[1] "list" 

NEW APPROACH BASED ON THE EXTREME VALUE THEORY 

359 

> names(rl.21.12) 
[1] "Range" "rlevel" 
> rl.21.12$rlevel 

[1] 4.483506 
# R output 
> rl.21.12=rlevel.gev(ml,k.blocks=12) 
> rl.21.12 

[1] 4.177923 4.481976 4.858102 

In the prior demonstration, the number of subperiods is denoted by k.blocks 

and the subcommand, type = 'profile', produces a plot of the profile log- 

likelihood confidence interval for the return level. The plot is not shown here. 

7.7 NEW APPROACH BASED ON THE EXTREME VALUE THEORY 

The aforementioned approach to VaR calculation using the extreme value theory 
encounters some difficulties. First, the choice of subperiod length n is not clearly 
defined. Second, the approach is unconditional and, hence, does not take into con¬ 
sideration effects of other explanatory variables. To overcome these difficulties, a 
modem approach to extreme value theory has been proposed in the statistical liter¬ 
ature; see Davison and Smith (1990) and Smith (1989). Instead of focusing on the 
extremes (maximum or minimum), the new approach focuses on exceedances of 
the measurement over some high threshold and the times at which the exceedances 
occur. Thus, this new approach is also referred to as peaks over thresholds (POT). 
For illustration, consider the daily returns of IBM stock used in this chapter and 
a long position on the stock. Denote the negative daily log return by rt. Let x] 
be a prespecified high threshold. We may choose r] = 2.5%. Suppose that the zth 
exceedance occurs at day (i.e., rti < x]). Then the new approach focuses on the 
data (ti, rtj — rj). Here rti — r) is the exceedance over the threshold x] and is the 
time at which the zth exceedance occurs. Similarly, for a short position, we may 
choose r\ — 2% and focus on the data (tt, rti - rj) for which rtj > rj. 

In practice, the occurrence times {fi} provide useful information about the inten¬ 
sity of the occurrence of important “rare events” (e.g., less than the threshold x] for 
a long position). A cluster of indicates a period of large market declines. The 
exceeding amount (or exceedance) rn - x] is also of importance as it provides the 

actual quantity of interest. 

Based on the prior introduction, the new approach does not require the choice 
of a subperiod length n, but it requires the specification of threshold x]. Different 
choices of the threshold xj lead to different estimates of the shape parameter k 
(and hence the tail index l/£). In the literature, some researchers believe that 
the choice of x] is a statistical problem as well as a financial one, and it cannot 
be determined based purely on statistical theory. For example, different financial 
institutions (or investors) have different risk tolerances. As such, they may select 
different thresholds even for an identical financial position. For the daily log returns 

360 

EXTREME VALUES, QUANTILES, AND VALUE AT RISK 

of IBM stock considered in this chapter, the calculated VaR is not sensitive to the 
choice of 17. 

The choice of threshold 17 also depends on the observed log returns. For a 
stable return series, 17 = 2.5% may fare well for a long position. For a volatile 
return series (e.g., daily returns of a dot-com stock), 17 may be as high as 10%. 
Limited experience shows that 17 can be chosen so that the number of exceedances 
is sufficiently large (e.g., about 5% of the sample). For a more formal study on the 
choice of 17, see Danielsson and de Vries (1997b). 

7.7.1 Statistical Theory 

Again consider the log return r, of an asset. Suppose that the ith exceedance 
occurs at t,-. Focusing on the exceedance rt — 17 and exceeding time t,- results in a 
fundamental change in statistical thinking. Instead of using the marginal distribution 
(e.g., the limiting distribution of the minimum or maximum), the new approach 
employs a conditional distribution to handle the magnitude of exceedance given 
that the measurement exceeds a threshold. The chance of exceeding the threshold 
is governed by a probability law. In other words, the new approach considers the 
conditional distribution of x = rt — 17 given rt < 17 for a long position. Occurrence 
of the event {r, < 17} follows a point process (e.g., a Poisson process). See Section 
6.9 for the definition of a Poisson process. In particular, if the intensity parameter 
k of the process is time invariant, then the Poisson process is homogeneous. If k is 
time variant, then the process is nonhomogeneous. The concept of Poisson process 
can be generalized to the multivariate case. 

The basic theory of the new approach is to consider the conditional distribution 
of r = x + ?7 given r > 17 for the limiting distribution of the maximum given in 
Eq. (7.16). Since there is no need to choose the subperiod length n, we do not use 
it as a subscript of the parameters. Then the conditional distribution of r < x + 17 
given r > 17 is 

Pr(r < x + r]\r > 17) = 

Pr(fi < r < x + 17) 

Pr(r < x + 17) — Pr(r < 17) 

Pr(r > 17) 

1 — Pr(r < 17) 

(7.29) 

Using the CDF F*(-) of Eq. (7.16) and the approximation e y % 1 — y and after 
some algebra, we obtain that 

Pr(r < x + r\\r > 17) = 

F*(x + 17) F*(ri) 

1 - F*(V) 

exp I - I" 1 + 

{- [l + } -expj- [l + 

l-exp{-[l + Mpj 

1 + 

<x + %(V-P). 

-l/$ 

(7.30) 

NEW APPROACH BASED ON THE EXTREME VALUE THEORY 

361 

where x > 0 and 1 + £(77 — P)/a > 0. As is seen later, this approximation makes 
explicit the connection of the new approach to the traditional extreme value theory. 
The case of £ = 0 is taken as the limit of £ -» 0 so that 

Pr(r < x + r]\r > rj) & 1 — exp(—x/a). 

The distribution with cumulative distribution function 

‘-[i + fcP for^0' 
1 — exp[—x/\jr(r})] for £ = 0, 

(7.31) 

where i/s(rj) > 0, x > 0 when £ > 0, and 0 < r < —xfr(v)/t; when £ < 0, is called 
the generalized Pareto distribution (GPD). Thus, the result of Eq. (7.30) shows that 
the conditional distribution of r given r > ij is well approximated by a GPD with 
parameters £ and ^r(rj) = a + £(77 — /3). See Embrechts et al. (1997) for further 
information. An important property of the GPD is as follows. Suppose that the 
excess distribution of r given a threshold rj0 is a GPD with shape parameter £ 
and scale parameter r4r{j]0). Then, for an arbitrary threshold rj > rj0, the excess 
distribution over the threshold rj is also a GPD with shape parameter £ and scale 

parameter f{rj) = 1J/(rj0) + ~ Vo)- 

When £ = 0, the GPD in Eq. (7.31) reduces to an exponential distribution. This 
result motivates the use of a QQ plot of excess returns over a threshold against 
exponential distribution to infer the tail behavior of the returns. If £ = 0, then the 
QQ plot should be linear. Figure 7.6(a) shows the QQ plot of daily negative IBM 
log returns used in this chapter with threshold 0.025. The nonlinear feature of the 
plot clearly shows that the left tail of the daily IBM log returns is heavier than that 
of a normal distribution, that is, £ ^ 0. 

R and S-Plus Commands Used to Produce Figure 7.6 

> par(mfcol=c(2,1) ) 

> qplot(-ibm,threshold=0.025,main='Negative daily IBM 

log returns') 

> meplot(-ibm) 

> title(main='Mean excess plot') 

7.7.2 Mean Excess Function 

Given a high threshold rja, suppose that the excess r — r\0 follows a GPD with 
parameter £ and ir{rj0), where 0 < £ < 1. Then the mean excess over the threshold 

Vo is 

E(r - rj0\r > rj0) 

f(Vo) 

W 

362 

EXTREME VALUES, QUANTILES, AND VALUE AT RISK 

(a) 

(b) 

Figure 7.6 Plots for daily negative IBM log returns from July 3, 1962, to December 31, 1998. (a) 

QQ plot of excess returns over threshold 2.5% and (b) mean excess plot. 

For any r] > rj0, define the mean excess function e{r]) as 

e(rj) = E(r - rj\r > ty) = 

f(r]o) + g(q - rj0) 

1-? 

In other words, for any y > 0, 

e(Vo + y) = - (no + y)|r > rjo + j] =-----• 

I — ? 

Thus, for a fixed the mean excess function is a linear function of y = r/ — ij0. 
This result leads to a simple graphical method to infer the appropriate threshold 
value rj0 for the GPD. Define the empirical mean excess function as 

1 , \ 

eT(v) = — ~ ('7-32) 

^ i=i 

where Nv is the number of returns that exceed r) and rtj are the values of the 
corresponding returns. See the next subsection for more information on the notation. 
The scatterplot of er(rj) against r] is called the mean excess plot, which should be 
linear in r) for r] > r}0 under the GPD. The plot is also called mean residual life plot. 

NEW APPROACH BASED ON THE EXTREME VALUE THEORY 

363 

Figure 7.6(b) shows the mean excess plot of the daily negative IBM log returns. It 
shows that, among others, a threshold of about 3% is reasonable for the negative 
return series. In the evir package of R and S-Plus, the command for mean excess 
plot is meplot. 

7.7.3 New Approach to Modeling Extreme Values 

Using the statistical result in Eq. (7.30) and considering jointly the exceedances 
and exceeding times, Smith (1989) proposes a two-dimensional Poisson process 
to model (r,, rtj). This approach was used by Tsay (1999) to study VaR in risk 
management. We follow the same approach. 

Assume that the baseline time interval is D, which is typically a year. In the 
United States, D = 252 is used as there are typically 252 trading days in a year. 
Let t be the time interval of the data points (e.g., daily) and denote the data span by 
t = 1, 2,..., T, where T is the total number of data points. For a given threshold 
r], the exceeding times over the threshold are denoted by {r,, i = 1,..., Nv} and 
the observed log return at t{ is rtj. Consequently, we focus on modeling {(/,-, rti)} 
for i = 1,..., Nrj, where Nv depends on the threshold tj. 

The new approach to applying the extreme value theory is to postulate that 
the exceeding times and the associated returns [i.e., (?;, rti)\ jointly form a two- 
dimensional Poisson process with intensity measure given by 

A[(D2, DO x (r, oo)] = °2 DDlS(r; £, a, 0), 

(7.33) 

where 

0 < D\ < Do < T, r > rj, a >0, (5, and £ are parameters, and the notation [x]+ 
is defined as [x]+ = max(x, 0). This intensity measure says that the occurrence of 
exceeding the threshold is proportional to the length of the time interval [D\, D2] 
and the probability is governed by a survival function similar to the exponent of 
the CDF F*(r) in Eq. (7.16). A survival function of a random variable X is defined 
as S(x) = Pr(X > x) = 1 — Pr(X < x) = 1 — CDF(x). When £ = 0, the intensity 
measure is taken as the limit of £ —► 0; that is, 

"-0' - PY 
a 

In Eq. (7.33), the length of time interval is measured with respect to the baseline 

interval D. 

364 

EXTREME VALUES, QUANTILES, AND VALUE AT RISK 

The idea of using the intensity measure in Eq. (7.33) becomes clear when one 
considers its implied conditional probability of r — x + rj given r > rj over the time 
interval [0, D], where x > 0, 

A[(0, D) x (a + rj, oo)] 

'1 +£(* + rj - p)/a  -i/? 

A[(0, D) x (t), oo)] 

1 +%(ri-P)/(x _ 

1 + 

a + HV~P). 

which is precisely the survival function of the conditional distribution given in 
Eq. (7.30). This survival function is obtained from the extreme limiting distribution 
for maximum in Eq. (7.16). We use survival function here because it denotes the 
probability of exceedance. 

The relationship between the limiting extreme value distribution in Eq. (7.16) 
and the intensity measure in Eq. (7.33) directly connects the new approach of 
extreme value theory to the traditional one. 

Mathematically, the intensity measure in Eq. (7.33) can be written as an integral 

of an intensity function: 

A[(£>2, Di) x (r, oo)] = 

k(t, z; £, a, ft) dzdt, 

where the intensity function k(t, z; a, P) is defined as 

\{t, z; £, a, P) = — g(z; £, a, P), 

1 

(7.34) 

where 

1 \l I 
a |_ ' a. 

g(z;£,a,/3) = 

if ^ # 0, 

if $ = 0. 

Using the results of a Poisson process, we can write down the likelihood function 
for the observed exceeding times and their corresponding returns {(fi, r(;)} over the 
two-dimensional space [0, T] x (rj, oo) as 

L($,a,P) 

1 Y\^8(rti;^,a,P) 

exp 

i=l u 

T 

~D 

S{r); a, P) 

(7.35) 

The parameters £, a, and p can then be estimated by maximizing the logarithm of 
this likelihood function. Since the scale parameter a is nonnegative, we use ln(a) 
in the estimation. 

Example 7.7. Consider again the daily log returns of IBM stock from July 3, 
1962, to December 31, 1998. There are 9190 daily returns. Table 7.3 gives some 

NEW APPROACH BASED ON THE EXTREME VALUE THEORY 

365 

TABLE 7.3 Estimation Results of a Two-Dimensional Homogeneous Poisson Model 
for Daily Negative Log Returns of IBM Stock from July 3, 1962 to December 31, 
1998" 

Thr. 

Exc. 

Shape Parameter £ 

Log(Scale) ln(a) 

Location /I 

Original Log Returns 

3.0% 
2.5% 
2.0% 

3.0% 
2.5% 
2.0% 

175 
310 
554 

184 
334 
590 

0.30697(0.09015) 
0.26418(0.06501) 
0.18751(0.04394) 

0.30699(0.12380) 
0.31529(0.11277) 
0.27655(0.09867) 

4.69204(0.19058) 
4.74062(0.18041) 
4.81003(0.17209) 

Removing the Sample Mean 

0.30516(0.08824) 
0.28179(0.06737) 
0.19260(0.04357) 

0.30807(0.12395) 
0.31968(0.12065) 
0.27917(0.09913) 

4.73804(0.19151) 
4.76808(0.18533) 
4.84859(0.17255) 

“The baseline time interval is 252 (i.e., 1 year). The numbers in parentheses are standard errors, where 
Thr. and Exc. stand for threshold and the number of exceedings. 

estimation results of the parameters a, and f5 for three choices of the threshold 
when the negative series {—rt] is used. As mentioned before, we use the negative 
series {—rt}, instead of {rt} because we focus on holding a long financial position. 
The table also shows the number of exceeding times for a given threshold. It is seen 
that the chance of dropping 2.5% or more in a day for IBM stock occurred with 
probability 310/9190 ~ 3.4%. Because the sample mean of IBM stock returns 
is not zero, we also consider the case when the sample mean is removed from 
the original daily log returns. From the table, removing the sample mean has 
little impact on the parameter estimates. These parameter estimates are used next 
to calculate VaR, keeping in mind that in a real application one needs to check 
carefully the adequacy of a fitted Poisson model. We discuss methods of model 
checking in the next section. 

7.7.4 VaR Calculation Based on the New Approach 

As shown in Eq. (7.30), the two-dimensional Poisson process model used, which 
employs the intensity measure in Eq. (7.33), has the same parameters as those 
of the extreme value distribution in Eq. (7.16). Therefore, one can use the same 
formula as that of Eq. (7.28) to calculate VaR of the new approach. More specifi¬ 
cally, for a given upper tail probability p, the (1 — p)th quantile of the log return 

rt is 

= { P-f{l-[-Dln(l-p)] * 

if £ 7^ 0, 

VaR 

P — a ln[—D ln(l — p)] 

if £ = 0, 

(7.36) 

where D is the baseline time interval used in estimation. In the United States, one 
typically uses D = 252, which is approximately the number of trading days in a 

year. 

366 

EXTREME VALUES, QUANTILES, AND VALUE AT RISK 

Example 7.8. Consider again the case of holding a long position of IBM stock 
valued at $10 million. We use the estimation results of Table 7.3 to calculate 1-day 
horizon VaR for the tail probabilities of 0.05 and 0.01. 

• Case I: Use the original daily log returns. The three choices of threshold ij 

result in the following VaR values: 

1. rj = 3.0%: VaR(5%) = $228,239, VaR(l%) = $359,303. 

2. 77 = 2.5%: VaR(5%) = $219,106, VaR(l%) = $361,119. 

3. rj = 2.0%: VaR(5%) = $212,981, VaR(l%) = $368,552. 

• Case II: The sample mean of the daily log returns is removed. The three 

choices of threshold r? result in the following VaR values: 

1. rj = 3.0%: VaR(5%) = $232,094, VaR(l%) = $363,697. 

2. r) = 2.5%: VaR(5%) = $225,782, VaR(l%) = $364,254. 

3. 77 = 2.0%: VaR(5%) = $217,740, VaR(l%) = $372,372. 

As expected, removing the sample mean, which is positive, slightly increases the 
VaR. However, the VaR is rather stable among the three threshold values used. In 
practice, we recommend that one removes the sample mean first before applying 
this new approach to VaR calculation. 

Discussion. Compared with the VaR of Example 7.6 that uses the traditional 
extreme value theory, the new approach provides a more stable VaR calcula¬ 
tion. The traditional approach is rather sensitive to the choice of the subperiod 
length n. □ 

The command pot of the R package evir can be used to perform the estimation 
of the POT model. We demonstrate it below using the negative log returns of IBM 
stock. As expected, the results are very close to those obtained before. 

R Demonstration Using POT Command 

> library(evir) 

> m3=pot(nibm,0.025) 

> m3 

$n 

[1] 9190 

$period 

[1] 1 9190 

$data 

[1] 0.03288483 0.02648772 0.02817316 . 

$span 

[1] 9189 

$threshold 

NEW APPROACH BASED ON THE EXTREME VALUE THEORY 

367 

[1] 0.025 

$p.less.thresh 

[1] 0.9662677 

$n.exceed 

[1] 310 

$par.ests 

xi sigma mu beta 

0.264078835 0.003182365 0.007557534 0.007788551 

$par.ses 

xi sigma mu 

0.0229175739 0.0001808472 0.0007675515 

$varcov 

[,1] [,2] [ ,3] 

[1,] 5.252152e-04 -2.873160e-06 -6.970497e-07 

[2,] -2.873160e-06 3.270571e-08 -7.907532e-08 

[3,] -6.970497e-07 -7.907532e-08 5.891353e-07 

$intensity %intensity function of exceeding the threshold 

[1] 0.03373599 

> plot(m3) % model checking 

Make a plot selection (or 0 to exit): 

1:  plot:  Point Process of Exceedances 

2 :  plot:  Scatterplot of Gaps 

3 :  plot:  Qplot of Gaps 

4:  plot:  ACF of Gaps 

5 :  plot :  Scatterplot of Residuals 

6 :  plot:  Qplot of Residuals 

7 :  plot:  ACF of Residuals 

8 :  plot:  Go to GPD Plots 

Selection: 

> riskmeasures(m3,0(0.95,0.99,0.999)) 

p quantile sfall 

[1,] 0.950 0.02208860 0.03162728 

[2,] 0.990 0.03616686 0.05075740 

[3,] 0.999 0.07019419 0.09699513 

7.7.5 Alternative Parameterization 

As mentioned before, for a given threshold ij, the GPD can also be parameterized 
by the shape parameter £ and the scale parameter i/jiv) This 
is the parameterization used in the evir package of R and S-Plus. Specifically, 
(xi, beta) of R and S-Plus corresponds to [£, ^(fi)] of this chapter. The command 
for estimating a GPD model in R and S-Plus is gpd. The output format for S-Plus 
is slightly different from that of R. For illustration, consider the daily negative IBM 
log return series from 1962 to 1998. The results of R are given below. 

368 

EXTREME VALUES, QUANTILES, AND VALUE AT RISK 

R Demonstration 
Data are negative IBM log returns. The following output was edited: 

> library(evir) 

> mgpd=gpd(nibm,threshold=0.025) 

> names(mgpd) 

[1]  "n" 
[5]  "n.exceed" 

"data" 

"threshold" 

"p.less.thresh 

"method" 

"par.ests" 

"par.ses" 

[9]  "varcov" 

"information " "converged"  "nllh.final" 

> mgpd 

$n 

[1] 9190 

$data 

[1] 0.03288483 0.02648772 0.02817316 0.03618692 .... 

$threshold 

[1] 0.025 

$p.less.thresh %Percentage of data below the threshold. 

[1] 0.9662677 

$n.exceed % Number of exceedances 

[1] 310 

$method 

[1] "ml" 

$par.ests 

xi beta 

0.264184649 0.007786063 

$par.ses 

xi beta 

0.0662137508 0.0006427826 

$varcov 

[, 1] [ < 2 ] 
[1,] 4.384261e-03 -2.461142e-05 

[2,] -2.461142e-05 4.131694e-07 

> par(mfcol=c(2,2)) %Plots for residual analysis 

> plot(mgpd) 

Make a plot selection (or 0 to exit): 

1: plot: Excess Distribution 

2: plot: Tail of Underlying Distribution 

3: plot: Scatterplot of Residuals 

4: plot: QQplot of Residuals 

Selection: 

Note that the results are very close to those in Table 7.3, where percentage log 
returns are used. The estimates of £ and iare 0.26418 and a + §(?? — /3) = 
exp(0.31529) + (0.26418)(2.5 — 4.7406) = 0.77873, respectively, in Table 7.3. In 
terms of log returns, the estimate of x/r(r]) is 0.007787, which is the same as the R 
and S-Plus estimate. 

NEW APPROACH BASED ON THE EXTREME VALUE THEORY 

369 

Ordering 

Figure 7.7 Diagnostic plots for GPD fit to daily negative log returns of IBM stock from July 3, 1962, 
to December 31, 1998. 

Figure 7.7 shows the diagnostic plots for the GPD fit to the daily negative log 
returns of IBM stock. The QQ plot (lower right panel) and the tail probability 
estimate (in log scale and in the lower left panel) show some minor deviation from 
a straight line, indicating further improvement is possible. 

From the conditional distributions in Eqs. (7.29) and (7.30) and the GPD in Eq. 

(7.31), we have 

F(y) - F(rj) 

1 - F(V) 

** Gr,Mn)(x), 

where y = x + r/ with x > 0. If we estimate the CDF F(rj) of the returns by the 
empirical CDF, then 

T-Nv 

Hv) = 

where Nv is the number of exceedances of the threshold ij and T is the sample 
size. Consequently, by Eq. (7.31), 

F(y) = F{rj) + G(x)[l — 

1-^ 
T 

1 +  Z(y - v) 

370 

EXTREME VALUES, QUANTILES, AND VALUE AT RISK 

This leads to an alternative estimate of the quantile of F(y) for use in VaR calcu¬ 
lation. Specifically, for a small upper tail probability p, let q = 1 - p. Then, by 
solving for y, we can estimate the q\h quantile of F(y), denoted by VaR^, by 

VaR^ — q — ——— -  1 -  — (1 — q) 

xl/(q) 

r t 
iiyv J 

N 

where, as before, q is the threshold, T is the sample size, Nv is the number of 
exceedances, and \/f(q) and % are the scale and shape parameters of the GPD 
distribution. This method to VaR calculation is used in R and S-Plus. 

As mentioned before in Section 7.2.3, expected shortfall (ES) associated with a 
given VaR is a useful risk measure. It is defined as the expected loss given that the 
VaR is exceeded. For generalized Pareto distribution, ES assumes a simple form. 
Specifically, for a given tail probability p, let q — 1 — p and denote the value at 
risk by VaR^. Then, the expected shortfall is defined by 

ESq = E{r\r > VaR?) = VaR? + E(r - VaR^ |r > VaRg). (7.38) 

Using properties of the GPD, it can be shown that 

E(r — VaR^ |r > VaR^) 

If in) + f(VaR0 - q) 

provided that 0 < £ < 1. Consequently, we have 

vaRg , f(q)~Hq 

9 1-1 1-1 

To illustrate the new method to VaR and ES calculations, we again use the daily 
negative log returns of IBM stock with threshold 2.5%. In the evir package of 
R and S-Plus, the command to compute VaR and ES via the peak over threshold 
method is riskmeasures: 

> riskmeasures(mgpd,c(0.95,0.99,0.999)) 

p quantile stall 

[1,] 0.950 0.02208959 0.03162619 

[2,] 0.990 0.03616405 0.05075390 

[3,] 0.999 0.07018944 0.09699565 

From the output, the VaR values for the financial position of $10 million are 
$220, 889 and $361,661, respectively, for tail probability of 0.05 and 0.01. These 
two values are rather close to those given in Example 7.8 that are based on the 
method of the previous section. The expected shortfalls for the financial position 
are $316, 272 and $507, 576, respectively, for tail probability of 0.05 and 0.01. 

NEW APPROACH BASED ON THE EXTREME VALUE THEORY 

371 

7.7.6 Use of Explanatory Variables 

The two-dimensional Poisson process model discussed earlier is homogeneous 
because the three parameters £, a, and fi are constant over time. In practice, such 
a model may not be adequate. Furthermore, some explanatory variables are often 
available that may influence the behavior of the log returns rt. A nice feature of 
the new extreme value theory approach to VaR calculation is that it can easily 
take explanatory variables into consideration. We discuss such a framework in this 
section. In addition, we also discuss methods that can be used to check the adequacy 
of a fitted two-dimensional Poisson process model. 

Suppose that x, = (xi,, ..., xvt)' is a vector of v explanatory variables that are 
available prior to time t. For asset returns, the volatility oj of rt discussed in 
Chapter 3 is an example of explanatory variables. Another example of explanatory 
variables in the U.S. equity markets is an indicator variable denoting the meetings 
of the Federal Open Market Committee. A simple way to make use of explanatory 
variables is to postulate that the three parameters £, a., and fi are time varying and 
are linear functions of the explanatory variables. Specifically, when explanatory 
variables xt are available, we assume that 

%t = Yb + Y\x\t H-f YvXvt = Yo + y'xt, 

\n{at) —- 5o +• &\X\t + • • • + Svxvt — 8q -f- 8'xt, 

(7.39) 

fit — $o + Q\X\t + • • • + 0vxvt = 9q + 0xt. 

If y = 0, then the shape parameter = yo, which is time invariant. Thus, testing the 
significance of y can provide information about the contribution of the explanatory 
variables to the shape parameter. Similar methods apply to the scale and location 
parameters. In Eq. (7.39), we use the same explanatory variables for all three 
parameters ln(o;(), and fit. In an application, different explanatory variables may 
be used for different parameters. 

When the three parameters of the extreme value distribution are time varying, 

we have an inhomogeneous Poisson process. The intensity measure becomes 

The likelihood function of the exceeding times and returns {(t,-, rti)} becomes 

^(r-fit)Tmt 

at J + 

r > rj. (7.40) 

l= n ~Dg(n‘; ^ ^ > exp 

1 

i=t 

which reduces to 

T 

(7.41) 

372 

EXTREME VALUES, QUANTILES, AND VALUE AT RISK 

if one assumes that the parameters at, and fi, are constant within each trading 
day, where g(z\ fit) and 5(/?; £(, at, 0t) are given in Eqs. (7.34) and (7.33), 
respectively. For given observations {rt, x,\t — 1, ..., T}, the baseline time interval 
D, and the threshold rj, the parameters in Eq. (7.39) can be estimated by maximizing 
the logarithm of the likelihood function in Eq. (7.41). Again we use ln(af) to satisfy 

the positive constraint of at. 

Remark. The parameterization in Eq. (7.39) is similar to that of the volatility 
models of Chapter 3 in the sense that the three parameters are exact functions of the 
available information at time t. Other functions can be used if necessary. □ 

7.7.7 Model Checking 

Checking an entertained two-dimensional Poisson process model for exceedance 
times and excesses involves examining three key features of the model. The 
first feature is to verify the adequacy of the exceedance rate, the second 
feature is to examine the distribution of exceedances, and the final feature is 
to check the independence assumption of the model. We discuss briefly some 
statistics that are useful for checking these three features. These statistics are 
based on some basic statistical theory concerning distributions and stochastic 
processes. 

Exceedance Rate 
A fundamental property of univariate Poisson processes is that the time durations 
between two consecutive events are independent and exponentially distributed. To 
exploit a similar property for checking a two-dimensional process model, Smith 
and Shively (1995) propose examining the time durations between consecutive 
exceedances. If the two-dimensional Poisson process model is appropriate for 
the exceedance times and excesses, the time duration between the ith and (/ — l)th 
exceedances should follow an exponential distribution. More specifically, letting 
to = 0, we expect that 

are iid as a standard exponential distribution. Because daily returns are discrete-time 
observations, we employ the time durations 

t—fi-i+i 

(7.42) 

and use the QQ plot to check the validity of the iid standard exponential distribution. 
If the model is adequate, the QQ plot should show a straight line through the origin 
with unit slope. 

NEW APPROACH BASED ON THE EXTREME VALUE THEORY 

373 

Distribution of Excesses 

Under the two-dimensional Poisson process model considered, the conditional dis¬ 
tribution of the excess xt = r, — rj over the threshold rj is a GPD with shape 
parameter and scale parameter f, = a, + £,(77 — f}t). Therefore, we can make 
use of the relationship between a standard exponential distribution and GPD, and 
define 

rti ~ r) 

if = 0. 

(7.43) 

If the model is adequate, {wtj} are independent and exponentially distributed with 
mean 1; see also Smith (1999). We can then apply the QQ plot to check the validity 
of the GPD assumption for excesses. 

Independence 
A simple way to check the independence assumption, after adjusting for the effects 
of explanatory variables, is to examine the sample autocorrelation functions of z,t; 
and wti. Under the independence assumption, we expect that both ztt and wti have 
no serial correlations. 

7.7.8 An Illustration 

In this section, we apply a two-dimensional inhomogeneous Poisson process model 
to the daily log returns, in percentages, of IBM stock from July 3, 1962, to 
December 31, 1998. We focus on holding a long position of $10 million. The 
analysis enables us to compare the results with those obtained before by using 
other approaches to calculating VaR. 

We begin by pointing out that the two-dimensional homogeneous model of 
Example 7.7 needs further refinements because the fitted model fails to pass the 
model checking statistics of the previous section. Figures 7.8(a) and 7.8(b) show 
the autocorrelation functions of the statistics zti and wti, defined in Eqs. (7.42) and 
(7.43), of the homogeneous model when the threshold is rj = 2.5%. The horizontal 
lines in the plots denote asymptotic limits of two standard errors. It is seen that both 
Zt. and wti series have some significant serial correlations. Figures 7.9(a) and 7.9(b) 
show the QQ plots of the zti and wti series. The straight line in each plot is the 
theoretical line, which passes through the origin and has a unit slope under the 
assumption of a standard exponential distribution. The QQ plot of zti shows some 

discrepancy. 

To refine the model, we use the mean-corrected log return series 

374 

EXTREME VALUES, QUANTILES, AND VALUE AT RISK 

123456789 10 

123456789 10 

Lag 

(a) 

Lag 
(c) 

1 23456789 10 

123456789 10 

Lag 

(b) 

Lag 

(d) 

Figure 7.8 Sample autocorrelation functions of the z and w measures for two-dimensional Poisson 

models. Parts (a) and (b) are for homogeneous model and parts (c) and (d) are for inhomogeneous 

model. Data are daily mean-corrected log returns, in percentages, of IBM stock from July 3, 1962, to 
December 31, 1998, and the threshold is 2.5%. A long financial position is used. 

where r, is the daily log return in percentages, and employ the following explana¬ 
tory variables: 

1. xu\ an indicator variable for October, November, and December. That is, 
x\t = 1 if t is in October, November, or December. This variable is chosen 
to take care of the fourth-quarter effect (or year-end effect), if any, on the 
daily IBM stock returns. 

2. X2t: an indicator variable for the behavior of the previous trading day. Specif¬ 
ically, X2t = 1 if and only if the log return rr°_, < -2.5%. Since we focus on 
holding a long position with threshold 2.5%, an exceedance occurs when the 
daily price drops over 2.5%. Therefore, x2r is used to capture the possibility 
of panic selling when the price of IBM stock dropped 2.5% or more on the 
previous trading day. 

3. x3,: a qualitative measurement of volatility, which is the number of days 
between t - 1 and t - 5 (inclusive) that has a log return with magnitude 
exceeding the threshold. In our case, x3r is the number of r°_t satisfying 
|r°_t | > 2.5% for i = 1,..., 5. 

4. x4t: an annual trend defined as x4t = (year of time t - 1961)/38. This vari¬ 
able is used to detect any trend in the behavior of extreme returns of IBM 
stock. 

NEW APPROACH BASED ON THE EXTREME VALUE THEORY 

375 

5. x$t: a volatility series based on a Gaussian GARCH(1,1) model for the 
mean-corrected series r°. Specifically, = o>, where af is the conditional 
variance of the GARCH(1,1) model 

r° = at, at = otet, €t ~ iV(0, 1), 

crf2 = 0.04565 + OMOlaf^ + 0.9031 af_x. 

These five explanatory variables are all available at time t — 1. We use two volatil¬ 
ity measures (*3, and X5,) to study the effect of market volatility on VaR. As 
shown in Example 7.3 by the fitted AR(2)-GARCH(1,1) model, the serial corre¬ 
lations in rt are weak so that we do not entertain any ARMA model for the mean 
equation. 

Using the prior five explanatory variables and deleting insignificant parameters, 
we obtain the estimation results shown in Table 7.4. Figures 7.8(c) and 7.8(d) 
and Figures 7.9(c) and 7.9(d) show the model checking statistics for the fitted 
two-dimensional inhomogeneous Poisson process model when the threshold is 
7) = 2.5%. All autocorrelation functions of ztt and wti are within the asymptotic 
two standard error limits. The QQ plots also show marked improvements as they 

CO 

co 

Figure 7.9 Quantile-to-quantile plot of z and w 

measures for two-dimensional Poisson models. Parts 

(a) and (b) are for homogeneous model and parts 

(c) and (d) are for inhomogeneous model. Data are 

daily mean-corrected log returns, in percentages, 

of IBM stock from July 3, 1962, to December 31, 

1998, and the threshold is 2.5%. A long financial 

position is used. 

(d) 

376 

EXTREME VALUES, QUANTILES, AND VALUE AT RISK 

TABLE 7.4 Estimation Results of Two-Dimensional Inhomogeneous Poisson Process 
Model for Daily Log Returns, in Percentages, of IBM Stock from July 3, 1962 to 
December 31, 1998" 

Parameter 

Constant 

Coefficient of X3, 

Coefficient of x^t 

Coefficient of x$t 

(Std.err) 

ln(Q!/) 
(Std.err) 

(Std.err) 

& 
(Std.err) 

ln(ar) 
(Std.err) 

Ht 
(Std.err) 

0.3202 
(0.3387) 
-0.8119 
(0.1798) 
0.1805 
(0.1290) 

1.1569 
(0.4082) 
-0.0316 
(0.1201) 
0.6008 
(0.1454) 

Threshold 2.5% with 334 Exceedances 

0.3305 
(0.0826) 
0.2118 
(0.0580) 

1.4772 
(0.3222) 
1.0324 
(0.2619) 
0.3551 
(0.1503) 

Threshold 3.0% with 184 Exceedances 

0.3336 
(0.0861) 
0.2480 
(0.0731) 

2.1991 
(0.2450) 

-0.2602 
(0.0461) 

2.1918 
(0.2909) 

-0.3175 
(0.0685) 

“Four explanatory variables defined in the text are used. The model is for holding a long position on 

IBM stock. The sample mean of the log returns is removed from the data. 

indicate no model inadequacy. Based on these checking results, the inhomogeneous 
model seems adequate. 

Consider the case of threshold 2.5%. The estimation results show the following: 

1. All three parameters of the intensity function depend significantly on the 
annual time trend. In particular, the shape parameter has a negative annual 
trend, indicating that the log returns of IBM stock are moving farther away 
from normality as time passes. Both the location and scale parameters 
increase over time. 

2. Indicators for the fourth quarter, xu, and for panic selling, x2t, are not sig¬ 

nificant for all three parameters. 

3. The location and shape parameters are positively affected by the volatility of 
the GARCH(1,1) model; see the coefficients of x5t. This is understandable 
because the variability of log returns increases when the volatility is high. 
Consequently, the dependence of log returns on the tail index is reduced. 

4. The scale and shape parameters depend significantly on the qualitative mea¬ 

sure of volatility. Signs of the estimates are also plausible. 

The explanatory variables for December 31, 1998, assumed the values jc3i9i9o = 
0* -£4,9190 = 0.9737, and .£5,9190 = 1.9766. Using these values and the fitted model 

THE EXTREMAL INDEX 

in Table 7.4, we obtain 

377 

£9190 = 0.01195, ln(or9i9o) = 0.19331, #,190 = 6.105. 

Assume that the tail probability is 0.05. The VaR quantile shown in Eq. (7.36) gives 
VaR = 3.03756%. Consequently, for a long position of $10 million, we have 

VaR = $10,000,000 x 0.0303756 = $303,756. 

If the tail probability is 0.01, the VaR is $497, 425. The 5% VaR is slightly larger 
than that of Example 7.3, which uses a Gaussian AR(2)-GARCH(1,1) model. The 
1% VaR is larger than that of Case 1 of Example 7.3. Again, as expected, the 
effect of extreme values (i.e., heavy tails) on VaR is more pronounced when the 
tail probability used is small. 

An advantage of using explanatory variables is that the parameters are adaptive 
to the change in market conditions. For example, the explanatory variables for 
December 30, 1998, assumed the values *3,9189 = 1> *4,9189 = 0.97 37, and *5,9189 = 
1.8757. In this case, we have 

^9189 = 0.2500, ln(a9189) = 0.52385, #189 = 5.8834. 

The 95% quantile (i.e., the tail probability is 5%) then becomes 2.69139%. Con¬ 
sequently, the VaR is 

VaR = $10,000,000 x 0.0269139 = $269,139. 

If the tail probability is 0.01, then VaR becomes $448, 323. Based on this example, 
the homogeneous Poisson model shown in Example 7.8 seems to underestimate 

the VaR. 

7.8 THE EXTREMAL INDEX 

So far our discussions of extreme values are based on the assumption that the 
data are iid random variables. However, in reality extremal events tend to occur in 
clusters because of the serial dependence in the data. For instance, we often observe 
large returns (both positive and negative) of an asset after some news event. In this 
section we extend the theory and applications of extreme values to cases in which 
the data form a strictly stationary time series. The basic concept of the extension 
is extremal index, which allows one to characterize the relationship between the 
dependence structure of the data and their extremal behavior. Our discussion will 
be brief. Interested readers are referred to Beirlant et al. (2004, Chapter 10) and 

Embrechts et al. (1997). 

Let*i, *2, • • • be a strictly stationary sequence of random variables with marginal 

distribution function F(x). Consider the case of n observations {*#' = 

378 

EXTREME VALUES, QUANTILES, AND VALUE AT RISK 

As before, let X(n) be the maximum of the data, that is, X(„) = maxjx/}. We seek 
the limiting distribution of (x(„) — fin)/an for some suitably chosen normalizing 
constants an > 0 and f}„. If {jc, } were iid, Section 7.5 shows that the only possi¬ 
ble nondegenerate limits are the extreme value distributions. What is the limiting 
distribution when {x,} are serially dependent? 

To answer this question, we start with a heuristic argument. Suppose that the 
serial dependence of the stationary series x,- decays quickly so that x,- and x;+£ are 
essentially independent when i is sufficiently large. In other words, assume that the 
long-range dependence of x; vanishes quickly. Now divide the data into disjoint 
blocks of size k. Specifically, let g = [n/k] be the largest integer less than or equal 
to n/k. The /th block of the data is then [xj\j = (i — 1) * k + 1,..., i * k], where 
it is understood that the (g + 1 )th block may contain less than k observations. 
Let Xkj be the maximum of the ith block, that is, Xkj = max{xj \j = (i — l) * k + 
1, ...,/* k}. The collection of block maxima is [xkj\i = 1,..., g + 1}. From the 
definitions, it is easy to see that 

x(„) = max xkj. (7.44) 

i=i,...,g+l 

That is, the sample maximum is also the maximum of the block maxima. If the 
block size k is sufficiently large and the block maximum Xkj does not occur near 
the end of the /th block, then Xkj and Xkj+i are sufficiently far apart and essen¬ 
tially independent under the assumption of weak long-range dependence in {x, }. 
Consequently, {xkj\i = 1,..., g + 1} can be regarded as a sample of iid random 
variables, and the limiting distribution of its maximum, which is X(„), should be 
the extreme value distribution. The prior discussion shows that, under some proper 
condition, the limiting distribution of the maximum of a strictly stationary time 
series is also the extreme value distribution. 

The proper condition needed for the maximum x(„) of a strictly stationary time 
series to have the extreme value limiting distribution is obtained by Leadbetter 
(1974) and known as the D(un) condition. Details are given in the next section. 
The prior heuristic argument also suggests that, even though the limiting distribu¬ 
tion of X(„) is also the extreme value distribution, the parameters associated with 
the limiting distribution, however, will not be the same as those when {x,} are iid 
random samples because the limiting distribution depends on the marginal distri¬ 
bution of the underlying sequences. For the iid sequences, the marginal distribution 
is F(x), but for a stationary series the underlying sequences are the block max¬ 
ima xkj whose marginal distribution is not F(x). The marginal distribution of xkj 
depends on k and the strength of serial dependence in {x;}. 

7.8.1 The D(un) Condition 

Consider the sample xi, X2,..., x„. To place limits on the long-range dependence 
of {xt}, let u„ be a sequence of thresholds increasing at a rate for which the expected 
number of exceedances of x,- over un remains bounded. Mathematically, this says 
that limsup«[l — F{un)] < oo, where F(-) is the marginal cumulative distribution 

THE EXTREMAL INDEX 

379 

function of xFor any positive integers p and q, suppose that iv (v — 1,..., p) 
and j, (t = 1, ..., q) are arbitrary integers satisfying 

1 < it < h < • • • < iP < j\ < • • • < jq < n, 

where j\ — ip > 4, where 4 is a function of the sample size n such that ln/n —> 0 
as«-Aoo. Let A\ = {ii, *2, ..., 4) ar>d A2 = {jt, h, • • •, jq) be two sets of time 
indices. From the prior condition, elements in A\ and A2 are separated by at least 
4 time periods. The condition D(un) is satisfied if 

P( max Xi < un) - P(maxx,- < un)P(maxxi < un)\ < 4,4. (7.45) 

i'eAiUA2 ieAi ieA2 

where 4,4 -* 0 as n —> 00. This condition says that any two events of the form 
{max/e/4i Xi < un] and {max,£A2 Xi < un} can become asymptotically independent 
as the sample size n increases when the index subsets A\ and Aj of {1, 2, ..., n] 
are separated by a distance 4 which satisfies ln/n -> 0 as n -* 00. The D(un) 
condition looks complicated, but it is relatively weak. For instance, consider Gaus¬ 
sian sequences with autocorrelation pn for lag n. The D{un) condition is satisfied 
if pn ln(n) -» 0 as n —► 00; see Berman (1964). 

Leadbetter’s Theorem 1. Suppose that {xt\i = 1,...,«} is a strictly station¬ 
ary time series for which there exist sequences of constants an > 0 and 4 and a 
nondegenerate distribution function F*(-) such that 

X(n) fin 

an 

< X 

F*(x), 

00, 

where —^ denotes convergence in distribution. If D(un) holds with un = anx + fin 
for each x such that F*(x) > 0, then F*(x) is an extreme value distribution function. 
The prior theorem shows that the possible limiting distributions for the maxima 
of strictly stationary time series satisfying the D(un) condition are also the extreme 
value distributions. As noted before, the dependence can affect the limiting distri¬ 
bution, however. The effect of the dependence appears in the marginal distribution 
of the block maxima xkj. To state the effect more precisely, let {ii, x2, .xn} 
be a sequence of iid random variables such that the marginal distribution of xt is 
the same as that of the stationary time series X,. Let x(n) be the maximum of {i, }. 
Leadbetter (1983) establishes the following result. 

Leadbetter’s Theorem 2. If there exist sequences of constants an > 0 and fin 

and a nondegenerate distribution function F*(x) such that 

X(n) fin 

C4 

< X 

F*(x), 

00, 

380 

EXTREME VALUES, QUANTILES, AND VALUE AT RISK 

if the condition D(un) holds with un = anx + fin for each x such that F*(x) > 0, 
and if P[(x(„) — /3n)/an < x] converges for some a, then 

•^-(/i) fin 
an 

< X 

F*(x) 

FeXx), 

n 

oo, 

for some constant 0 e (0, 1]. 

The constant 6 is called the extremal index. It plays an important role in deter¬ 
mining the limiting distribution F*(x) for the maximum of a strictly stationary time 
series. To see this, we provide some simple derivations for the case of £ ± 0. From 
the result of Eq. (7.16), F*(x) is the generalized extreme value distribution and 
assumes the form 

F*(x) = exp 

x — fi 

a 

where £ 0 and 1 + £(x — fi)/a > 0. In other words, we assume that for the iid 
sequence {x,}, the limiting extreme distribution of X(„) has parameters £, fi and a. 
Based on Theorem 2 of Leadbetter (1983), we have 

F*(x) = F* (x) = exp 

x — fi 

-i/r 

d l + £ 

a 

-i/r 

= exp 

= exp 

= exp 

= exp 

1 0* ^ a6* 
A , hx-P + <x/% -a9*/%\ 
~{l + t--J 

“I1 

-i/? 

x - - |(1 -0?)]x _1/l 

l+£- 

a/£ +x - fi  -i/f 

adt 

-i/I. 

(7.46) 

exp 

- 1+?, 

X - fi* 

a* 

where §* = £, a* = ad%, and fi* = fi — or(l — 0^)/£. Therefore, for a stationary 
time series {x,} satisfying the D(un) condition, the limiting distribution of the 
sample maximum is the generalized extreme value distribution with the shape 
parameter £, which is the same as that of the iid sequences. On the other hand, the 
location and scale parameters are affected by the extremal index 9. Specifically, 
a* = ad% and fi* = fi — a{\ — 0^)/£. Results for the case of £ = 0 can be derived 
via the same approach and we have a* = a and fi* — fi + a ln(0). 

THE EXTREMAL INDEX 

381 

A formal definition of the extremal index is as follows: Let {x,} be a strictly 
stationary time series with marginal cumulative distribution function F(x) and 9 
a nonnegative number. Assume that for every r > 0 there exists a sequence of 
thresholds un such that 

lim n[ 1 - F(un)] = r, (7.47) 
n—>-00 

lim P(xrn) < u„) — exp(—6x). (7.48) 
n-* oo 

Then 0 is called the extremal index of the time series {x,-}. See Embrechts et al. 
(1997). Note that, for the corresponding iid sequence {x,}, under the assumption 
that Eq. (7.47) holds, we have 

lim P{xin) < un) = lim [F(un)]n = lim 11-n[ 1 — F(un 
n-+oo n—>oo n—>oo I fi 

)] 

exp(-r), 

where we have used the property lim„_>00(l — y/n)n = exp(—y). Thus, the defi¬ 
nition also highlights the role played by the extremal index 9. 

7.8.2 Estimation of the Extremal Index 

There are several ways to estimate the extremal index 9 of a strictly stationary 
time series {x, }. Each estimation method is associated with an interpretation of the 
extremal index. In what follows, we discuss some of the estimation methods. 

The Blocks Method 
From the definition of the extremal index 9, we have, for a large n, that 

P(X(„) < U„) ~ P6 {X(n) < Un) = [F {un)f°, 

provided that n[ 1 — F{un)] —»■ r > 0. Hence 

In P(x(w) < un) 
lim - 
n-yoo n In F(un) 

= 9. 

(7.49) 

This limiting relationship suggests a method to estimate 9. The denominator can 
be estimated by the sample quantile, namely 

1 " 1 A N(u„) 

F(un) = - 7 7(x; < un) — 1->/ (Xi >un) = 1-, 

n ' n ' n 

i=i i'=i 

where / (C) = 1 if the augment C holds and = 0 otherwise, that is, 1(C) is the indi¬ 
cator variable for the statement C, and N(un) denotes the number of exceedances 
of the sample over the threshold un. The numerator P(x(n) < un) is harder to esti¬ 
mate. One possibility is to use the block maxima. Specifically, let k = k(n) be a 

382 

EXTREME VALUES, QUANTILES, AND VALUE AT RISK 

properly chosen block size that depends on the sample size n and, as before, let 
g = [n/k] be the integer part of n/k. For simplicity, assume that n — gk. The z'th 
block consists of {xj\j = (z — 1) * k + 1,..., i * k) and let xkj be the maximum 
of the z'th block. Using Eq. (7.44) and the approximate independence of block 

maxima, we have 

P(xM < un) = P( max xkj < un) ^ [P(xkj < un)]8. 

!<<<g 

The probability P{xkj < un) can be estimated from the block maxima, that is, 

Is Is 
P{xk,i 5 un) = — ^ 7 (xkj < Un) = 1 ^ I (.Xk,i ■> m/j) = 1 

G(un) 

8 

where G(un) is the number of blocks such that the block maximum exceeds the 
threshold un. Combining the estimators for numerator and denominator, we obtain 

^(l) _ g Ml - G(un)/g] __ 1 ln[l - G(un)/g] 

n\n[l -N{un)/n] k ln[l — TV(un)/n] ’ 

where the subscript b signifies the blocks method. Note that N(un) is the number 
of exceedances of the sample {jq} over the threshold un and G(un) is the number 
of blocks with one or more exceedances. Using approximation based on Taylor 
expansion of ln(l — x), we obtain a second estimator: 

0(2) _ 1 G(un)/g _ G(un) 

b k N(«„)/n N(un) 

Based on the results of Hsing et al. (1988), this estimator can also be interpreted as 
the reciprocal of the mean cluster size of the limiting compound Poisson process 
N (un). 

The Runs Method 
O’Brien (1987) proved, under certain weak mixing condition, that 

lim P(x*(n) < un\xx >un) = 6, 

where x*n) — max2</<i x,-, where i is a function of the sample size n satisfying 
some growth conditions, including s oo and s/n -a 0 as n —>• oo. See Beirlant 
et al. (2004) and Embrechts et al. (1997) for details. This result has been used to 
construct an estimator of 0 based on runs'. 

go) = T.mKA,.„) = Efei* /(A>) 

ELl 1 (*Z > un) N{un) 

THE EXTREMAL INDEX 

383 

where N{un) is the number of exceedances of the sample {jc, } over the threshold 
un, k is a function of n, and Ai<n — {> un, x,-+i <un,xi+k < m„). Note that 
A, „ denotes the event that an exceedance is followed by a run of & observations 
below the threshold. Since k/n —0 as n —> oo, we can write the runs estimator as 

0(3) ^ ("-*)-* Efaf 7(A,,„) 

r n~xN{un) 

Finally, other estimators of 0 are available in the literature. See, for instance, the 
methods discussed in Beirlant et al. (2004). For demonstration, we consider, again, 
the negative daily log returns of IBM stock from July 3, 1962, to December 31, 
1998. Figure 7.10 shows the estimates of the extremal index for various thresholds 
when the block size k = 10. We chose k = 10 because the daily log returns have 
weak serial dependence. The estimates are based on the blocks method, that is, 
0{bX\ From the plot, we see that 0(bX ~ 0. S2- for threshold 0.025. Indeed, a simple 

direct calculation using k = 10 and threshold 0.025 gives 6[V) = 0.823. The plot 

also shows that the estimate db of the extremal index might be sensitive to the 
choices of threshold and block size k. 

0.08520 0.02960 0.02280 0.01840 0.01460 0.01200 0.00716 

Threshold 

Figure 7.10 Estimates of extremal index for negative daily log returns of IBM stock from July 3, 
1962, to December 31, 1998. Block size is k = 10 and lower horizontal axis of plot K denotes number 

of blocks whose maximum exceeds threshold. 

K 

384 

EXTREME VALUES, QUANTILES, AND VALUE AT RISK 

7.8.3 Value at Risk for a Stationary Time Series 

The relationship between F*(x) of the maximum of a stationary time series and 
F*(x) of its iid counterpart established in Theorem 2 of Leadbetter (1983) can be 
used to calculate the VaR of a financial position when the associated log returns 
form a stationary time series. Specifically, from P(x(n) < un) ~ [F(.x)]"0, the (1 — 
p)th quantile of F(x) is the (1 - p)ndth quantile of the limiting extreme value 
distribution of X(n). Consequently, the VaR of Eq. (7.28) based on the extreme 
value theory becomes 

' Pn~%\l-[-n9\n(\-p)] *"} if^O 

VaR = 

Pn ~ oin In[~nQ ln(l - p)] if = 0, 

(7.51) 

where n is the length of the subperiod. From the formula, we risk underestimating 
the VaR if the extremal index is overlooked. 

As an illustration, again consider the negative daily log returns of IBM stock 
from July 3, 1962, to December 31, 1998. Using 6^ = 0.823, the 1% VaR for the 
long position of $10 millions on the stock for the next trading day becomes 3.2714 
for the case of choosing n = 63 days in parameter estimation. As expected, this is 
higher than the 3.0497 of Example 7.6 when the extremal index is neglected. 

R Demonstration 

> library(evir) 
> help(exindex) 
> ml=exindex(nibm,10) %Estimate the extremal index 

of Figure 7.10. 

> % VaR calculation. 
> 2.583 - ( .945/.335)*(1-(-63*.823*log(.99))A-.335) 
[1] 3.271388 

EXERCISES 

7.1. Consider the daily returns of GE stock from January 2, 1998, to December 31, 
2008. The data can be obtained from CRSP or the file d-ge9808 . txt. Convert 
the simple returns into log returns. Suppose that you hold a long position on 
the stock valued at $1 million. Use the tail probability 0.01. Compute the 
value at risk of your position for 1-day horizon and 15-day horizon using the 
following methods: 

(a) The RiskMetrics method. 

(b) A Gaussian ARMA-GARCH model. 

(c) An ARMA-GARCH model with a Student-? distribution. You should also 

estimate the degrees of freedom. 

(d) The traditional extreme value theory with subperiod length n = 21. 

EXERCISES 

385 

7.2. The file d-csco9 808 . txt contains the daily simple returns of Cisco Systems 
stock from 1998 to 2008 with 2767 observations. Transform the simple returns 
to log returns. Suppose that you hold a long position of Cisco stock valued 
at $ 1 million. Compute the value at risk of your position for the next trading 
day using probability p = 0.01. 

(a) Use the RiskMetrics method. 

(b) Use a GARCH model with a conditional Gaussian distribution. 

(c) Use a GARCH model with a Student-r distribution. You may also estimate 

the degrees of freedom. 

(d) Use the unconditional sample quantile. 

(e) Use a two-dimensional homogeneous Poisson process with threshold 2%, 
that is, focusing on the exceeding times and exceedances that the daily 
stock price drops 2% or more. Check the fitted model. 

(f) Use a two-dimensional nonhomogeneous Poisson process with threshold 
2%. The explanatory variables are (1) an annual time trend, (2) a dummy 
variable for October, November, and December, and (3) a fitted volatility 
based on a Gaussian GARCH(1,1) model. Perform a diagnostic check on 
the fitted model. 

(g) Repeat the prior two-dimensional nonhomogeneous Poisson process with 

threshold 2.5 or 3%. Comment on the selection of threshold. 

7.3. Use Hill’s estimator and the data d-csco9808 . txt to estimate the tail index 

for daily log returns of Cisco stock. 

7.4. The file d-hpq3dx9808.txt contains dates and the daily simple returns of 
Hewlett-Packard, the CRSP value-weighted index, equal-weighted index, and 
the S&P 500 index from 1998 to 2008. The returns include dividend dis¬ 
tributions. Transform the simple returns to log returns. Assume that the tail 
probability of interest is 0.01. Calculate value at risk for the following financial 
positions for the first trading day of year 2009. 

(a) Long on Hewlett-Packard stock of $1 million and S&P 500 index of $1 
million using RiskMetrics. The a coefficient of the IGARCH(1,1) model 
for each series should be estimated. 

(b) The same position as part (a) but using a univariate ARMA-GARCH 

model for each return series. 

(c) A long position on Hewlett-Packard stock of $1 million using a two- 
dimensional nonhomogeneous Poisson model with the following explana¬ 
tory variables: (1) an annual time trend, (2) a fitted volatility based on a 
Gaussian GARCH model for Hewlett-Packard stock, (3) a fitted volatility 
based on a Gaussian GARCH model for the S&P 500 index returns, and 
(4) a fitted volatility based on a Gaussian GARCH model for the value- 
weighted index return. Perform a diagnostic check for the fitted models. 
Are the market volatility as measured by the S&P 500 index and value- 
weighted index returns helpful in determining the tail behavior of stock 
returns of Hewlett-Packard? You may choose several thresholds. 

386 

EXTREME VALUES, QUANTILES, AND VALUE AT RISK 

7.5. Consider the daily returns of Alcoa (AA) stock and the S&P 500 composite 
index (SPX) from 1998 to 2008. The simple returns and dates are in the file 
d-aaspx9808 . txt. Transform the simple returns to log returns and focus on 
the daily negative log returns of AA stock. 

(a) Fit the generalized extreme value distribution to the negative AA log 
returns, in percentages, with subperiods of 21 trading days. Write down 
the parameter estimates and their standard errors. Obtain a scatterplot and 
a QQ plot of the residuals. 

(b) What is the return level of the prior fitted model when 24 subperiods of 

21 days are used? 

(c) Obtain a QQ plot (against exponential distribution) of the negative log 

returns with threshold 2.5% and a mean excess plot of the returns. 

(d) Fit a generalize Pareto distribution to the negative log returns with thresh¬ 

old 3.5%. Write down the parameter estimates and their standard errors. 

(e) Obtain (i) a plot of excess distribution, (ii) a plot of the tail of the under¬ 
lying distribution, (iii) a scatterplot of residuals, and (iv) a QQ plot of the 
residuals for the fitted GPD. 

(f) Based on the fitted GPD model, compute the VaR and expected shortfall 

for probabilities q = 0.99 and 0.999. 

7.6. Consider, again, the daily log returns of Alcoa (AA) stock in Exercise 7.5. 
Focus now on the daily positive log returns. Answer the same questions as in 
Exercise 7.5. However, use threshold 3% in fitting the GPD model. 

7.7. Consider the daily returns of SPX in d-aaspx9808.txt. Transform the 

returns into log returns and focus on the daily negative log returns. 

(a) Fit the generalized extreme value distribution to the negative SPX log 
returns, in percentage, with subperiods of 21 trading days. Write down the 
parameter estimates and their standard errors. Obtain a scatterplot and a 
QQ plot of the residuals. 

(b) What is the return level of the prior fitted model when 24 subperiods of 

21 days are used? 

(c) Obtain a QQ plot (against exponential distribution) of the negative log 

returns with threshold 2.5% and a mean excess plot of the returns. 

(d) Fit a generalize Pareto distribution to the negative log returns with thresh¬ 

old 2.5%. Write down the parameter estimates and their standard errors. 

(e) Obtain (i) a plot of excess distribution, (ii) a plot of the tail of the under¬ 
lying distribution, (iii) a scatterplot of residuals, and (iv) a QQ plot of the 
residuals for the fitted GPD. 

(f) Based on the fitted GPD model, compute the VaR and expected shortfall 

for probabilities q = 0.99 and 0.999. 

7.8. Consider the daily log returns of the GE stock of Exercise 7.1. Obtain esti¬ 

mates 9(bl} and #/3) of the extremal index of (a) the positive return series 
and (b) the negative return series, using block sizes k — 5 and 10 and 
threshold 2.5%. 

REFERENCES 

REFERENCES 

387 

Beirlant, J., Goegebeur, Y., Segers, J., and Teugels, J. (2004). Statistics of Extremes: Theory 

and Applications. Wiley, Hoboken, NJ. 

Berman, S. M. (1964). Limiting theorems for the maximum term in stationary sequences. 

Annals of Mathematical Statistics 35: 502-516. 

Coles, S. (2001). An Introduction to Statistical Modeling of Extreme Values. Springer, New 

York. 

Cox, D. R. and Hinkley, D. V. (1974). Theoretical Statistics. Chapman and Hall, London. 

Danielsson, J. and De Vries, C. G. (1997a). Value at risk and extreme returns. Working 

paper, London School of Economics, London, UK. 

Danielsson, J. and De Vries, C. G. (1997b). Tail index and quantile estimation with very 

high frequency data. Journal of Empirical Finance 4: 241-257. 

Davison, A. C. and Smith, R. L. (1990). Models for exceedances over high thresholds (with 

discussion). Journal of the Royal Statistical Society Series B 52: 393-442. 

Dekkers, A. L. M. and De Haan, L. (1989). On the estimation of extreme value index and 

large quantile estimation. Annals of Statistics 17: 1795-1832. 

Duffie, D. and Pan, J. (1997). An overview of value at risk. Journal of Derivatives Spring: 

7-48. 

Embrechts, P., Kuppelberg, C., and Mikosch, T. (1997). Modelling Extremal Events. 

Springer, Berlin. 

Feller, W. (1971). An Introduction to Probability Theory and Its Applications, Vol. 2. Wiley, 

New York. 

Goldie, C. M. and Smith, R. L. (1987). Slow variation with remainder: Theory and appli¬ 

cations. Quarterly Journal of Mathematics 38: 45-71. 

Gnedenko, B. V. (1943). Sur la distribution limite du terme maximum of d’une serie 

Aleatorie. Annals of Mathematics 44: 423-453. 

Gumbel, E. J. (1958). Statistics of Extremes. Columbia University Press, New York. 

Hill, B. M. (1975). A simple general approach to inference about the tail of a distribution. 

Annals of Statistics 3: 1163-1173. 

Hsing, T., Hiisler, J. and Leadbetter, M. R. (1988). On the exceedance point process for a 

stationary sequence. Probability Theory and Related Fields 78: 97-112. 

Jenkinson, A. F. (1955). The frequency distribution of the annual maximum (or minimum) 
of meteorological elements. Quarterly Journal of the Royal Meteorological Society 81: 

158-171. 

Jorion, P. (2006). Value at Risk: The New Benchmark for Managing Financial Risk, 3rd ed. 

McGraw-Hill, Chicago. 

Koenker, R. W. and Bassett, G. W. (1978). Regression quantiles. Econometrica 46: 33-50. 

Koenker, R. W. and D’Orey, V. (1987). Computing regression quantiles. Applied Statistics 

36: 383-393. 

Leadbetter, M. R. (1974). On extreme values in stationary sequences. Zeitschrift fiir 

Wahrscheinlichkeitsthorie und Verwandte Gebiete 28: 289-303. 

Leadbetter, M. R. (1983). Extremes and local dependence in stationary sequences. Zeitschrift 

fiir Wahrscheinlichkeitsthorie und Verwandte Gebiete 65: 291-306. 

388 

EXTREME VALUES, QUANTILES, AND VALUE AT RISK 

Leadbetter, M. R., Lindgren, G., and Rootzen, H. (1983). Extremes and Related Properties 

of Random Sequences and Processes. Springer, New York. 

Longerstaey, J. and More, L. (1995). Introduction to RiskMetrics™, 4th ed. Morgan Guar¬ 

anty Trust Company, New York. 

Longin, F. M. (1996). The asymptotic distribution of extreme stock market returns. Journal 

of Business 69: 383-408. 

Longin, F. M. (1999a). Optimal margin level in futures markets: Extreme price movements. 

Journal of Futures Markets 19: 127-152. 

Longin, F. M. (1999b). From value at risk to stress testing: The extreme value approach. 

Working paper. Centre for Economic Policy Research, London. 

O’Brien, G. L. (1987). Extreme values for stationary and Markov sequences. Annals of 

Probability 15: 281-291. 

Pickands, J. (1975). Statistical inference using extreme order statistics. Annals of Statistics 

3: 119-131. 

Smith, R. L. (1989). Extreme value analysis of environmental time series: An application to 
trend detection in ground-level ozone (with discussion). Statistical Science 4: 367-393. 

Smith, R. L. (1999). Measuring risk with extreme value theory. Working paper, Department 

of Statistics, University of North Carolina at Chapel Hill. 

Smith, R. L. and Shively, T. S. (1995). A point process approach to modeling trends in 

troposheric ozone. Atmospheric Environment 29: 3489-3499. 

Tsay, R. S. (1999). Extreme value analysis of financial data. Working paper, Graduate School 

of Business, University of Chicago. 

Zivot, E. and Wang, J. (2003). Modeling Financial Time Series with S-Plus. Springer, New 

York. 

CHAPTER 8 

Multivariate Time Series Analysis 
and Its Applications 

Economic globalization and Internet communication have accelerated the integra¬ 
tion of world financial markets in recent years. Price movements in one market can 
spread easily and instantly to another market. For this reason, financial markets 
are more dependent on each other than ever before, and one must consider them 
jointly to better understand the dynamic structure of the global finance. One market 
may lead the other market under some circumstances, yet the relationship may be 
reversed under other circumstances. Consequently, knowing how the markets are 
interrelated is of great importance in finance. Similarly, for an investor or a finan¬ 
cial institution holding multiple assets, the dynamic relationships between returns 
of the assets play an important role in decision making. In this and the next two 
chapters, we introduce econometric models and methods useful for studying jointly 
multiple return series. In the statistical literature, these models and methods belong 
to vector or multivariate time series analysis. 

A multivariate time series consists of multiple single series referred to as com¬ 
ponents. As such, concepts of vector and matrix are useful in understanding 
multivariate time series analysis. We use boldface notation to indicate vectors 
and matrices. If necessary, readers may consult Appendix A of this chapter for 
some basic operations and properties of vectors and matrices. Appendix B pro¬ 
vides some results of multivariate normal distribution, which is widely used in 
multivariate statistical analysis (e.g., Johnson and Wichern, 1998). 

Let rt = (r\t, r2t,.fkt)' be the log returns of k assets at time t, where a' 
denotes the transpose of a. For example, an investor holding stocks of IBM, 
Microsoft, Exxon Mobil, General Motors, and Wal-Mart may consider the five¬ 
dimensional daily log returns of these companies. Here r\t denotes the daily log 
return of IBM stock, r2t is that of Microsoft, and so on. As a second example, 
an investor who is interested in global investment may consider the return series 
of the S&P 500 index of the United States, the FTSE 100 index of the United 

Analysis of Financial Time Series, Third Edition, By Ruey S. Tsay 
Copyright © 2010 John Wiley & Sons, Inc. 

389 

390 

MULTIVARIATE TIME SERIES ANALYSIS AND ITS APPLICATIONS 

Kingdom, and the Nikkei 225 index of Japan. Here the series is three-dimensional, 
with r\t denoting the return of the S&P 500 index, r2t the return of the Financial 
Times Stock Exchange (FTSE) 100 index, and r3, the return of the Nikkei 225. 
The goals of this chapter are (a) to explore the basic properties of rt and (b) to 
study econometric models for analyzing the multivariate data {rt\t = \,... ,T}. 

Many of the models and methods discussed in previous chapters can be gen¬ 
eralized directly to the multivariate case. But there are situations in which the 
generalization requires some attention. In some situations, one needs new models 
and methods to handle the complicated relationships between multiple series. In 
this chapter, we discuss these issues with emphasis on intuition and applications. 
For statistical theory of multivariate time series analysis, readers are referred to 
Liitkepohl (2005) and Reinsel (1993). 

8.1 WEAK STATIONARITY AND CROSS-CORRELATION MATRICES 

Consider a ^-dimensional time series rt = {r\t,..., rkt)'. The series rt is weakly 
stationary if its first and second moments are time invariant. In particular, the 
mean vector and covariance matrix of a weakly stationary series are constant over 
time. Unless stated explicitly to the contrary, we assume that the return series of 
financial assets are weakly stationary. 

For a weakly stationary time series rt, we define its mean vector and covariance 

matrix as 

fi = E(rt), T0 = E[{rt - fi)(rt - fi)'], (8.1) 

where the expectation is taken element by element over the joint distribution of rt. 
The mean fi is a ^-dimensional vector consisting of the unconditional expectations 
of the components of rt. The covariance matrix To is a k x k matrix. The z'th 
diagonal element of T0 is the variance of rit, whereas the (/, y)th element of To is 
the covariance between rit and rjt. We write fi = (iau ..., iik)r and T0 = |T;7(0)] 
when the elements are needed. 

8.1.1 Cross-Correlation Matrices 

Let D be a k x k diagonal matrix consisting of the standard deviations of rit for 
i = 1.k. In other words, D = diaglVTn (0),..., ^Fkk(0)}. The concurrent, 
or lag-zero, cross-correlation matrix of r, is defined as 

Po = [Pi;'(0)] = D_1r0D_1. 

More specifically, the (/, ;')th element of p0 is 

Pijio) = , r'7(0> = 

>/r«/(0)r;;(0) std(rIf)std(r;()’ 

WEAK STATIONARITY AND CROSS-CORRELATION MATRICES 

391 

which is the correlation coefficient between r,-f and rjt. In time series analysis, 
such a correlation coefficient is referred to as a concurrent, or contemporaneous, 
correlation coefficient because it is the correlation of the two series at time t. It is 
easy to see that p,;(0) = Pji(0), —1 < p,y(0) < 1, and pa(0) = 1 for 1 < i, j < k. 
Thus, p(0) is a symmetric matrix with unit diagonal elements. 

An important topic in multivariate time series analysis is the lead-lag relation¬ 
ships between component series. To this end, the cross-correlation matrices are 
used to measure the strength of linear dependence between time series. The lag-l 
cross-covariance matrix of r, is defined as 

Tt = [Tijd)] = E[(rt - _t - /*)'], (8.2) 

where ft is the mean vector of rt. Therefore, the (i, j)th element of T£ is the covari¬ 
ance between r,-r and r^t-n. For a weakly stationary series, the cross-covariance 
matrix Y i is a function of l, not the time index t. 

The \ag-l cross-correlation matrix (CCM) of rt is defined as 

pe = [pij(l)] = D-1TiD-\ (8.3) 

where, as before, D is the diagonal matrix of standard deviations of the individual 
series r,r. From the definition, 

Pi jit) 

jruioWjjiO) 

std(r^)std(rjt) ’ 

Cov(rit, rj't-i) 

(8.4) 

which is the correlation coefficient between rit and When i > 0, this corre¬ 
lation coefficient measures the linear dependence of on which occurred 
prior to time t. Consequently, if pij (£) ^ 0 and l > 0, we say that the series rjt 
leads the series rat lag t. Similarly, Pji(t) measures the linear dependence of rjt 
and rij-i, and we say that the series r,-, leads the series rjt at lag t if Pji(t) 7^ 0 
and i > 0. Equation (8.4) also shows that the diagonal element pu(l) is simply the 
lag-^ autocorrelation coefficient of rit. 

Based on this discussion, we obtain some important properties of the cross 
correlations when i > 0. First, in general, pij(£) 7^ Pjiit) for i 7^ j because the 
two correlation coefficients measure different linear relationships between {rit} and 
{rjt}. Therefore, Yi and pt are in general not symmetric. Second, using Cov(x, y) 
= Cov(y, x) and the weak stationarity assumption, we have 

Com (j u, rjtt-i) = Com (rj't-i, rit) = Co M(rjt, ritt+l) = Co M(rjt, r;-,f_(_^), 

so that Vij (£) = Tji(—l). Because Tji(—l) is the (j, i)th element of the matrix T_£ 
and the equality holds for 1 < i, j < k, we have and pt = p'_v Conse¬ 
quently, unlike the univariate case, pt 7^ p_t for a general vector time series when 
l > 0. Because pt = p'_v it suffices in practice to consider the cross-correlation 

matrices pt for l > 0. 

392 

MULTIVARIATE TIME SERIES ANALYSIS AND ITS APPLICATIONS 

8.1.2 Linear Dependence 

Considered jointly, the cross-correlation matrices {p(\t =0, 1,...} of a weakly 
stationary vector time series contain the following information: 

1. The diagonal elements {pu(l)\t — 0, 1,...} are the autocorrelation function 

of rit. 

2. The off-diagonal element p,y(0) measures the concurrent linear relationship 

between r,r and rjt. 

3. For l > 0, the off-diagonal element pij(l) measures the linear dependence of 

rn on the past value rjj-i. 

Therefore, if Pij (£) = 0 for all l > 0, then rlt does not depend linearly on any 

past value rjyt-i of the rjt series. 

In general, the linear relationship between two time series {rit} and {rjt} can be 

summarized as follows: 

1. rit and rjt have no linear relationship if Pij{l) = Pji (£) = 0 for all i > 0. 

2. rit and rjt are concurrently correlated if pxj (0) 0. 

3. rit and rj, have no lead-lag relationship if pij(£) = 0 and pji(£) = 0 for all 

t > 0. In this case, we say the two series are uncoupled. 

4. There is a unidirectional relationship from rlt to rjt if p,y (£) — 0 for all 
l > 0, but pji (v) ^ 0 for some v > 0. In this case, r,-r does not depend on 
any past value of rjt, but rjt depends on some past values of r(f. 

5. There is a feedback relationship between rit and rJt if Pij(i) / 0 for some 

£ > 0 and pji (v) # 0 for some v > 0. 

The conditions stated earlier are sufficient conditions. A more informative approach 
to study the relationship between time series is to build a multivariate model for 
the series because a properly specified model considers simultaneously the serial 
and cross correlations among the series. 

8.1.3 Sample Cross-Correlation Matrices 

Given the data {rt\t = 1,...,T}, the cross-covariance matrix can be esti¬ 
mated by 

t=e+1 

(8.5) 

where r = rt)/T is the vector of sample means. The cross-correlation matrix 
pe is estimated by 

Pi = D \ l >0, 

(8.6) 

WEAK STATIONARITY AND CROSS-CORRELATION MATRICES 

393 

where D is the k x k diagonal matrix of the sample standard deviations of the 
component series. 

Similar to the univariate case, asymptotic properties of the sample cross¬ 
correlation matrix 'pl> have been investigated under various assumptions; see, for 
instance, Fuller (1976, Chapter 6). The estimate is consistent but is biased in a 
finite sample. For asset return series, the finite sample distribution of is rather 
complicated partly because of the presence of conditional heteroscedasticity and 
high kurtosis. If the finite-sample distribution of cross correlations is needed, 
we recommend that proper bootstrap resampling methods be used to obtain 
an approximate estimate of the distribution. For many applications, a crude 
approximation of the variance of Pij(l) is sufficient. 

Example 8.1. Consider the monthly log returns of IBM stock and the S&P 500 
index from January 1926 to December 2008 with 996 observations. The returns 
include dividend payments and are in percentages. Denote the returns of IBM 
stock and the S&P 500 index by r\t and rj_u respectively. These two returns form a 
bivariate time series r, = (rit, r^)'. Figure 8.1 shows the time plots of rt. Figure 8.2 
shows some scatterplots of the two series. The plots show that the two return series 
are concurrently correlated. Indeed, the sample concurrent correlation coefficient 
between the two returns is 0.65, which is statistically significant at the 5% level. 
However, the cross correlations at lag 1 are weak if any. 

o 

1940 1960 1980 2000 

Year 

(a) 

Figure 8.1 Time plots of monthly log returns, in percentages, for (a) IBM stock and (b) the S&P 500 

index from January 1926 to December 2008. 

394 

MULTIVARIATE TIME SERIES ANALYSIS AND ITS APPLICATIONS 

o 
o 
in 
Q_ 
°3 
C0 

-30 -10 0 10 20 30 40 

-30 -10 0 10 20 30 

IBM 1 

(b) 

S&P 500—Lag 1 

(d) 

Figure 8.2 Some scatterplots for monthly log returns of IBM stock and S&P 500 index: (a) concurrent 
plot of IBM vs. S&P 500, (b) S&P 500 vs. lag-1 IBM, (c) IBM vs. lag-1 S&P 500, and (d) S&P 500 
vs. lag-1 S&P 500. 

Table 8.1 provides some summary statistics and cross-correlation matrices of the 
two series. For a bivariate series, each CCM is a 2 x 2 matrix with four correlations. 
Empirical experience indicates that it is rather hard to absorb simultaneously many 
cross-correlation matrices, especially when the dimension k is greater than 3. To 
overcome this difficulty, we use the simplifying notation of Tiao and Box (1981) 
and define a simplified cross-correlation matrix consisting of three symbols “+,” 

and where they have the following meaning: 

1. Plus sign (+) means that the corresponding correlation coefficient is greater 

than or equal to 2/y/T. 

2. Minus sign (—) means that the corresponding correlation coefficient is less 

than or equal to —2/y/T. 

3. Period (.) means that the corresponding correlation coefficient is between 

-2/Vf and 2/Vf. 

And l/Vf is the asymptotic 5% critical value of the sample correlation under the 
assumption that rt is a white noise series. 

WEAK STATIONARITY AND CROSS-CORRELATION MATRICES 

395 

TABLE 8.1 Summary Statistics and Cross-Correlation Matrices of Monthly Log 
Returns of IBM Stock and S&P 500 Index: January 1926 to December 2008 

(a) Summary Statistics 

Ticker 

Mean 

Standard Excess 
Skewness 

Error 

Kurtosis 

Minimum 

Maximum 

IBM 
SP5 

1.089 
0.430 

7.033 
5.537 

-0.068 
-0.521 

2.622 
7.927 

-30.37 
-35.59 

38.57 
35.22 

(b) Cross-Correlation Matrices 

Lag 1 Lag 2 Lag 3 Lag 4 Lag 5 

0.04 0.10 0.00 -0.08 -0.01 -0.06 -0.03 -0.03 0.02 0.08 
0.04 0.08 0.02 -0.02 -0.06 -0.10 0.04 0.03 0.00 0.09 

(c) Simplified notation 

'• +‘ 

• + 

• — 

’■ +" 

• + 

Table 8.1(c) shows the simplified CCM for the monthly log returns of IBM 
stock and the S&P 500 index. It is easily seen that significant cross correlations at 
the approximate 5% level appear mainly at lags 1 and 3. An examination of the 
sample CCMs at these two lags indicates that (a) S&P 500 index returns have some 
marginal autocorrelations at lags 1, 2, 3, and 5 and (b) IBM stock returns depend 
weakly on the previous returns of the S&P 500 index. The latter observation is 
based on the significance of cross correlations at the (1, 2)th element of lag-1, lag-2 
and lag-5 CCMs. 

Figure 8.3 shows the sample autocorrelations and cross correlations of the two 
series. The upper-left plot is the sample ACF of IBM stock returns and the upper- 
right plot shows the dependence of IBM stock returns on the lagged S&P 500 index 
returns. The dashed lines in the plots are the asymptotic two standard error limits 
of the sample auto- and cross-correlation coefficients. From the plots, the dynamic 
relationship is weak between the two return series, but their contemporaneous 
correlation is statistically significant. 

Example 8.2. Consider the simple returns of monthly indexes of U.S. gov¬ 
ernment bonds with maturities in 30 years, 20 years, 10 years, 5 years, and 1 year. 
The data obtained from the CRSP database have 696 observations starting from 
January 1942 to December 1999. Let r, = (ru, ..., r5t)f be the return series with 
decreasing time to maturity. Figure 8.4 shows the time plots of rt on the same scale. 
The variability of the 1-year bond returns is much smaller than that of returns with 
longer maturities. The sample means and standard deviations of the data are fit — 
10-2(0.43, 0.45, 0.45, 0.46, 0.44)' and a = 10“2(2.53, 2.43, 1.97, 1.39, 0.53)'. 

396 

MULTIVARIATE TIME SERIES ANALYSIS AND ITS APPLICATIONS 

6 

8 10 12 

Lag 

6 

8 10 12 

Lag 

LO 
o 

8 o 

LO 
o 

8 o 

o 
7 

6 

8 10 12 

Lag 

6 

8 10 12 

Lag 

Figure 8.3 Sample auto- and cross-correlation functions (CCF) of two monthly log return series: 
(a) sample ACF of IBM stock returns, (b) cross-correlations between S&P 500 index and lagged IBM 

stock returns (lower left), (c) cross correlations between IBM stock and lagged S&P 500 index returns, 
and (d) sample ACF of S&P 500 index returns. Dashed lines denote 95% limits. 

The concurrent correlation  matrix  of the  series  is 

- 

Po = 

1.00  0.98  0.92  0.85  0.63 
0.98 
1.00  0.91  0.86  0.64 
0.92  0.91 
1.00  0.90  0.68 
0.85  0.86  0.90 
1.00  0.82 
0.63  0.64  0.68  0.82 

1.00 

It is not surprising that (a) the series have high concurrent correlations, and (b) the 
correlations between long-term bonds are higher than those between short-term 
bonds. 

Table 8.2 gives the lag-1 and lag-2 cross-correlation matrices of rt and the 
corresponding simplified matrices. Most of the significant cross correlations are at 
lag 1, and the five return series appear to be intercorrelated. In addition, lag-1 and 
lag-2 sample ACFs of the 1-year bond returns are substantially higher than those 
of other series with longer maturities. 

WEAK STATIONARITY AND CROSS-CORRELATION MATRICES 

397 

'1940 

1950 

1960 

1970 

1980 

1990 

2000 

Year 

(a) 

'1940 

1950 

1960 

1970 

1980 

1990 

2000 

Year 
(b) 

'1940 

1950 

1960 

1970 

1980 

1990 

2000 

Year 

(c) 

Q ° 

CC o 

T1940 1950 1960 1970 1980 1990 2000 

Year 

(d) 

'1940 1950 1960 1970 1980 1990 2000 

Year 

(e) 

Figure 8.4 Time plots of monthly simple returns of five indexes of U.S. government bonds with 
maturities in (a) 30 years, (b) 20 years, (c) 10 years, (d) 5 years, and (e) 1 year. Sample period is from 

January 1942 to December 1999. 

8.1.4 Multivariate Portmanteau Tests 

The univariate Ljung-Box statistic Q(m) has been generalized to the multivariate 
case by Hosking (1980, 1981) and Li and McLeod (1981). For a multivariate 
series, the null hypothesis of the test statistic is Ho : px = • • • = pm = 0, and the 
alternative hypothesis Ha : pt ^ 0 for some i 6 {1,..., m}. Thus, the statistic is 
used to test that there are no auto- and cross correlations in the vector series rt. 

The test statistic assumes the form 

m 1 

Qt(m) = T2 J2 —tr(f',f „‘fTo"'), (8.7) 

where T is the sample size, k is the dimension of rt, and tr(A) is the trace of 
the matrix A, which is the sum of the diagonal elements of A. Under the null 

398 

MULTIVARIATE TIME SERIES ANALYSIS AND ITS APPLICATIONS 

TABLE 8.2 Sample Cross-Correlation Matrices of Monthly Simple Returns of Five 
Indexes of U.S. Government Bonds: January 1942 to December 1999 

Lag 1 Lag 2 

Cross-Correlations 

0.10  0.08 

0.10  0.08 

0.09  0.08 

0.14  0.12 

0.17  0.15 

0.11 

0.12 

0.09 

0.15 

0.21 

0.12 

0.14 

0.13 

0.14 

0.22 

0.16 

0.17 

0.18 

0.22 

0.40 

-0.01 

-0.01 

0.01 

-0.02 

-0.02 

0.00 

0.00 

0.01 

-0.01 

0.00 

0.00 

0.00 

0.01 

0.00 

0.02 

-0.03 

-0.04 

-0.02 

-0.04 

0.02 

0.03 

0.02 

0.07 

0.07 

0.22 

Simplified Cross-Correlation Matrices 

+  +  +  +  + 
+  +  +  +  + 
+  +  +  +  + 
+  +  +  +  + 

hypothesis and some regularity conditions, Qk{m) follows asymptotically a chi- 
squared distribution with k2m degrees of freedom. 

Remark. The Qk(m) statistics can be rewritten in terms of the sample cross¬ 
correlation matrices fie. Using the Kronecker product ® and vectorization of matri¬ 
ces discussed in Appendix A of this chapter, we have 

m j 

Qk(m) = ®p^l)be, 

t= l 1 

where bt = vec(^). The test statistic proposed by Li and McLeod (1981) is 

Qt(m) = + -2"l(m + 

1= 1 11 

which is asymptotically equivalent to Qk{m). □ 

Applying the Qk(m) statistics to the bivariate monthly log returns of IBM stock 
and the S&P 500 index of Example 8.1, we have £>2(1) = 9.81, £>2(5) = 47.06, 
and £>2 (10) = 71.65. Based on asymptotic chi-squared distributions with degrees 
of freedom 4, 20, and 40, the p values of these £)2(m) statistics are 0.044, 0.001, 
and 0.002, respectively. The portmanteau tests thus confirm the existence of serial 
dependence in the bivariate return series at the 5% significance level. For the five¬ 
dimensional monthly simple returns of bond indexes in Example 8.2, we have 
£>5(5) = 1065.63, which is highly significant compared with a chi-squared distri¬ 
bution with 125 degrees of freedom. 

VECTOR AUTOREGRESSIVE MODELS 

399 

The Qk(m) statistic is a joint test for checking the first m cross-correlation matri¬ 
ces of rt being zero. If it rejects the null hypothesis, then we build a multivariate 
model for the series to study the lead-lag relationships between the component 
series. In what follows, we discuss some simple vector models useful for modeling 
the linear dynamic structure of a multivariate financial time series. 

8.2 VECTOR AUTOREGRESSIVE MODELS 

A simple vector model useful in modeling asset returns is the vector autoregressive 
(VAR) model. A multivariate time series rt is a VAR process of order 1, or VAR(l) 
for short, if it follows the model 

tt — 0o T t—i + ati (8-8) 

where 0O is a k-dimensional vector, O is a k x k matrix, and {at} is a sequence of 
serially uncorrelated random vectors with mean zero and covariance matrix E. In 
application, the covariance matrix E is required to be positive definite; otherwise, 
the dimension of r, can be reduced. In the literature, it is often assumed that at is 

multivariate normal. 

Consider the bivariate case [i.e., k = 2, rt = (r\t, r2tf, and at = (a\t, a^f]- The 

VAR(l) model consists of the following two equations: 

r\t = 010 + ^lin.f-i + ^12^2,t-i + a\t> 

r2t — 020 + <&2\lr\, t-\ + ®22r2,t-l + a2t, 

where is the (i, y')th element of $ and 0(O is the i th element of 0O. Based on 
the first equation, 4>i2 denotes the linear dependence of r\t on r2,f—i in the presence 
of Therefore, d>i2 is the conditional effect of r2>/_i on ri, given If 
<fi12 = 0, then r\t does not depend on r2jf_i, and the model shows that r\t only 
depends on its own past. Similarly, if d>2i = 0, then the second equation shows 
that r2r does not depend on when r2:f_i is given. 

Consider the two equations jointly. If <&i2 = 0 and <fi2i ^ 0, then there is a 
unidirectional relationship from r\t to r2f. If = <t,2i = 0, then r\t and r2, are 
uncoupled. If <$l2 ^ 0 and $>2\ ^ 0, then there is a feedback relationship between 

the two series. 

8.2.1 Reduced and Structural Forms 

In general, the coefficient matrix $ of Eq. (8.8) measures the dynamic dependence 
of rt. The concurrent relationship between r\, and r2f is shown by the off-diagonal 
element cri2 of the covariance matrix D of at. If cri2 = 0, then there is no con¬ 
current linear relationship between the two component series. In the econometric 
literature, the VAR(l) model in Eq. (8.8) is called a reduced-form model because it 

400 

MULTIVARIATE TIME SERIES ANALYSIS AND ITS APPLICATIONS 

does not show explicitly the concurrent dependence between the component series. 
If necessary, an explicit expression involving the concurrent relationship can be 
deduced from the reduced-form model by a simple linear transformation. Because 
E is positive definite, there exists a lower triangular matrix L with unit diago¬ 
nal elements and a diagonal matrix G such that X = LGL'\ see Appendix A on 
Cholesky decomposition. Therefore, L~lT(L')~l = G. 

Define bt = (b\t,..., bktY = L~]at. Then 

E(bt) = L~lE(at) = 0, Coy(bt) = = L~XyL(L')-x = G. 

Since G is a diagonal matrix, the components of bt are uncorrelated. Multiplying 
L_1 from the left to model (8.8), we obtain 

L~]rt = L~'0o + + L~la, = 0q + **>*-1 + bt, (8.9) 

where 0q = L~l<t>0 is a ^-dimensional vector and 4>* = L x is a k x k matrix. 
Because of the special matrix structure, the kth row of L-1 is in the form 
(u>k\, Wk2, ..., Wk,k-\, 1). Consequently, the &th equation of model (8.9) is 

k— 1 k 

rkt + 'S^J ^ki^it = 4>k,0 + ®kiri,t-1 + ^kt, (8.10) 

i=l i'=l 

where 0£o is the A;th element of 0q and <J>^. is the (k, i)th element of $*. Because 
bkt is uncorrelated with bit for 1 < i < k, Eq. (8.10) shows explicitly the concurrent 
linear dependence of rkt on rit, where 1 < i < k - 1. This equation is referred to 
as a structural equation for rkt in the econometric literature. 

For any other component rit of rt, we can rearrange the VAR(l) model so that 
fit becomes the last component of rt. The prior transformation method can then be 
applied to obtain a structural equation for rit. Therefore, the reduced-form model 
(8.8) is equivalent to the structural form used in the econometric literature. In time 
series analysis, the reduced-form model is commonly used for two reasons. The 
first reason is ease in estimation. The second and main reason is that the concurrent 
correlations cannot be used in forecasting. 

Example 8,3. To illustrate the transformation from a reduced-form model to 

structural equations, consider the bivariate AR(1) model 

f ir 

fit 

"0.2" 

_0-4 

+ 

0.2  0.3" 
!.!_ 

-0.6 

r l ,r— 1 

f2,t-\ 

1 
1 

For this particular covariance matrix X, the lower triangular matrix 

L1 

1.0 0.0 
-0.5 1.0 

VECTOR AUTOREGRESSIVE MODELS 

401 

provides a Cholesky decomposition [i.e., L X'L(L') 1 is a diagonal matrix]. Pre¬ 
multiplying L_1 to the previous bivariate AR(1) model, we obtain 

1.0  0.0 " 

r\t 

" 0.2 ' 

-0.5 

1.0 

. r2« 

. °-3 . 

+ 

0.2  0.3 

r\)t-1 
-0.7  0.95 _  . r2T-l 

bu 

+ 

where G = Cov(bt). The second equation of this transformed model gives 

r2t - 0.3 + 0.5ru - 0.7ri,f_i + 0.95r2,,-i + b2t, 

which shows explicitly the linear dependence of r2t on r\t. 

Rearranging the order of elements in ru the bivariate AR(1) model becomes 

r2t 

ru 

0.4 
0.2 

+ 

1.1 -0.6 
0.3 0.2 

r2,t-i 

n,t-i 

z = 

l 1 
1 2 

The lower triangular matrix needed in the Cholesky decomposition of X becomes 

L-1 

1.0 0.0 
-1.0 1.0 

Premultiplying L 1 to the earlier rearranged VAR(l) model, we obtain 

1.0 0.0 
-1.0 1.0 

r2t 
r\t 

.4 
0.2 

1.1 -0.6 
-0.8 0.8 

r2,t-i 
r\,t-1 

G = 

1 0 
0 1 ’ 

where G = Cov(cr). The second equation now gives 

r\t = —0.2 + \-0r2t — 0.8r2,j-i + 0.8rii?_i + c2t. 

Again this equation shows explicitly the concurrent linear dependence of r\t on r2t. 

8.2.2 Stationarity Condition and Moments of a VAR(l) Model 

Assume that the VAR(l) model in Eq. (8.8) is weakly stationary. Taking expectation 
of the model and using E{at) = 0, we obtain 

E(rt) = 0o + <&£(rr_i). 

Since E(rt) is time invariant, we have 

fi = E(rt) = (/ - o 

402 

MULTIVARIATE TIME SERIES ANALYSIS AND ITS APPLICATIONS 

provided that the matrix / — 4> is nonsingular, where / is the k x k identity matrix. 

Using 0O = (/ — &)fi, the VAR(l) model in Eq. (8.8) can be written as 

(rt ~li) = *(rt-1 - /i)+at. 

Let r, = rt — fi be the mean-corrected time series. Then the VAR(l) model 
becomes 

rt = + a,. (8.11) 

This model can be used to derive properties of a VAR(l) model. By repeated 
substitutions, we can rewrite Eq. (8.11) as 

ft = at + i + &2at-2 + $3ar-3 H-. 

This expression shows several characteristics of a VAR(l) process. First, since 
a, is serially uncorrelated, it follows that Cov(ar,r?_i) = 0. In fact, at is not 
correlated with rf_£ for all l > 0. For this reason, a, is referred to as the shock or 
innovation of the series at time t. It turns out that, similar to the univariate case, at 
is uncorrelated with the past value rt_j (j > 0) for all time series models. Second, 
postmultiplying the expression by a', taking expectation, and using the fact of no 
serial correlations in the a, process, we obtain Cov(r,, at) = E. Third, for a VAR(l) 
model, r, depends on the past innovation at-j with coefficient matrix OU For such 
dependence to be meaningful, must converge to zero as j -> 00. This means 
that the k eigenvalues of must be less than 1 in modulus; otherwise, <bj will 
either explode or converge to a nonzero matrix as j —► 00. As a matter of fact, the 
requirement that all eigenvalues of $ are less than 1 in modulus is the necessary 
and sufficient condition for weak stationarity of rt provided that the covariance 
matrix of at exists. Notice that this stationarity condition reduces to that of the 
univariate AR(1) case in which the condition is |0| < 1. Furthermore, because 

|A./-*| = A* | / - $- I, 

X 

the eigenvalues of $ are the inverses of the zeros of the determinant \I - 
Thus, an equivalent sufficient and necessary condition for stationarity of r, is that all 
zeros of the determinant |4>(5)| are greater than one in modulus; that is, all zeros are 
outside the unit circle in the complex plane. Fourth, using the expression, we have 

Cov(r,) = T0 = T + + $2S($2)' -|-= ^2 I!($')', 

OO 

1=0 

where it is understood that 4>° = /, the k x k identity matrix. 

VECTOR AUTOREGRESSIVE MODELS 

403 

Postmultiplying r't_e to Eq. (8.11), taking expectation, and using the result 

Cov(af, rt-j) = E(atr't_j) = 0 for j > 0, we obtain 

E(rtf't_f) = $E(rf_ir,_*)\ i > 0. 

Therefore, 

r/ = $IVi, £ > 0, (8.12) 

where Tj is the lag-j cross-covariance matrix of rt. Again this result is a general¬ 
ization of that of a univariate AR(1) process. By repeated substitutions, Eq. (8.12) 

shows that 

Ye = $%, for £>0. 

Pre- and postmultiplying Eq. (8.12) by we obtain 

Pe = D~1/2*rt-iD-l/2 = ZK1/2$D1/2zr1/2iVi£r1/2 = r pt_lt 

where T = D~1/2$Dl/2. Consequently, the CCM of a VAR(l) model satisfies 

pt = Te p0, for £>0. 

8.2.3 Vector AR(p) Models 

The generalization of VAR(l) to VAR(p) models is straightforward. The time series 

rt follows a VAR(p) model if it satisfies 

rt = 0O + 4>ir;_i -|-4>prt-p+at, p>0, (8.13) 

where 0O and at are defined as before, and <t>; are k x k matrices. Using the 
back-shift operator B, the VAR(p) model can be written as 

-*PBp)rt =0o + flf, 

where / is the k x k identity matrix. This representation can be written in a compact 

form as 

<b(B)rt = 0o + fl/, 

where <S>(B) = / - 0>\B-®pBp is a matrix polynomial. If rt is weakly 
stationary, then we have 

p = E(rt) = (/-$i-$P)-10 o = [*(l)]-10o 

404 

MULTIVARIATE TIME SERIES ANALYSIS AND ITS APPLICATIONS 

provided that the inverse exists. Let rt — rt— ft. The VAR(p) model becomes 

ft = H-\-&prt-p + at. (8.14) 

Using this equation and the same techniques as those for VAR(l) models, we 
obtain that 

• Cov(r;, a,) = X, the covariance matrix of at. 

• Cov(r,_£, at) = 0 for i > 0. 

• r£ = 4>1r,£_IH-1- QpTt-p for £>0. 

The last property is called the moment equations of a VAR(p) model. It is a 
multivariate version of the Yule-Walker equation of a univariate AR(p) model. In 
terms of CCM, the moment equations become 

pe — T\pt_x + ■ • • + YpPi_p for i>0, 

where Y, = D1/24>; D1/2. 

A simple approach to understanding properties of the VAR(p) model in Eq. 
(8.13) is to make use of the results of the VAR(l) model in Eq. (8.8). This can be 
achieved by transforming the VAR(p) model of rt into a k^-dimensional VAR(l) 
model. Specifically, let x, = (rt_p+l, r,_p+2,..., rt)' and bt = (0, ..., 0, a't)' be 
two kp-dimensional processes. The mean of bt is zero and the covariance matrix 
of bt is a kp x kp matrix with zero everywhere except for the lower right corner, 
which is X. The VAR(p) model for r, can then be written in the form 

x, = +bt, (8.15) 

where is a kp x kp matrix given by 

<r = 

" 0 

0 

0 

$ 

L 

/ 

0 

0 

0 

I 

0 

0 

0 

0 

0 

0 

/ 

*p-' 

4* p—2  ®p-3 • 

_ 

where 0 and I are the k x k zero  matrix  and identity  matrix, respectively. In 
the literature, <t> is called the companion matrix of the matrix polynomial 
*(B). 

Equation (8.15) is a VAR(l) model for x,, which contains r, as its last k com¬ 
ponents. The results of a VAR(l) model shown in the previous section can now be 
used to derive properties of the VAR(p) model via Eq. (8.15). For example, from 
the definition, x, is weakly stationary if and only if r, is weakly stationary. There¬ 
fore, the necessary and sufficient condition of weak stationarity for the VAR(p) 

VECTOR AUTOREGRESSIVE MODELS 

405 

model in Eq. (8.13) is that all eigenvalues of 4>* in Eq. (8.15) are less than 1 in 
modulus. It is easy to show that |7 — 4>*fi| = |4>(fi)|. Therefore, similar to the 
VAR(l) case, the necessary and sufficient condition is equivalent to all zeros of 
the determinant |4>(5)| being outside the unit circle. 

Of particular relevance to financial time series analysis is the structure of the 
coefficient matrices 4>£ of a VAR(/?) model. For instance, if the (z, j)th element 
of is zero for all i, then rit does not depend on the past values of rjt. The 
structure of the coefficient matrices 4>/ thus provides information on the lead-lag 

relationship between the components of rt. 

8.2.4 Building a VAR(/>) Model 

We continue to use the iterative procedure of order specification, estimation, and 
model checking to build a vector AR model for a given time series. The concept of 
partial autocorrelation function of a univariate series can be generalized to specify 
the order p of a vector series. Consider the following consecutive VAR models: 

rt — 0o + 4>iff-i + at 

rt =<t> o + 4>irf_i + 2 + at 

rt = 0o + 4>i/y_i +-b <birt-i + a, 

(8.16) 

Parameters of these models can be estimated by the ordinary least-squares (OLS) 
method. This is called the multivariate linear regression estimation in multivariate 

statistical analysis; see Johnson and Wichern (1998). 

For the z'th equation in Eq. (8.16), let be the OLS estimate of 4>y and 0O' 
be the estimate of </>0, where the superscript (z) is used to denote that the estimates 

are for a VAR(z) model. Then the residual is 

For i = 0, the residual is defined as rj0) = rt - r, where r is the sample mean of 

rt. The residual covariance matrix is defined as 

(8.17) 

To specify the order p, one can test the hypothesis H0 : 4>e = 0 versus the alter¬ 
native hypothesis Ha : ± 0 sequentially for l = 1, 2,.... For example, using 

406 

MULTIVARIATE TIME SERIES ANALYSIS AND ITS APPLICATIONS 

the first equation in Eq. (8.16), we can test the hypothesis Hq : 4>i =0 versus the 
alternative hypothesis Ha : 4>i 7^ 0. The test statistic is 

where £,- is defined in Eq. (8.17) and |A| denotes the determinant of the matrix 
A. Under some regularity conditions, the test statistic M{ 1) is asymptotically a 
chi-squared distribution with k2 degrees of freedom; see Tiao and Box (1981). 

In general, we use the ith and (i - l)th equations in Eq. (8.16) to test H0 : 4>, = 
0 versus Ha : 4>, ^ 0; that is, testing a VAR(i) model versus a VAR(/ — 1) model. 
The test statistic is 

(8.18) 

Asymptotically, M(/') is distributed as a chi-squared distribution with k2 degrees 
of freedom. 

Alternatively, one can use the Akaike information criterion (AIC) or its variants 
to select the order p. Assume that at is multivariate normal and consider the /th 
equation in Eq. (8.16). One can estimate the model by the maximum-likelihood 
(ML) method. For AR models, the OLS estimates 0O and 4>; are equivalent to the 
(conditional) ML estimates. However, there are differences between the estimates 
of £. The ML estimate of £ is 

t=i+\ 

(8.19) 

The AIC of a VAR(/) model under the normality assumption is defined as 

9 k2i 
AIC(0 = ln(|£,-|) -|—j~ 

For a given vector time series, one selects the AR order p such that AIC(p) = 
mino<i<Po AIC(/), where p0 is a prespecified positive integer. 
Other information criteria available for VAR(i) models are 

The HQ criterion is proposed by Hannan and Quinn (1979). 

VECTOR AUTOREGRESSIVE MODELS 

407 

Example 8.4. Assuming that the bivariate series of monthly log returns of IBM 
stock and the S&P 500 index discussed in Example 8.1 follows a VAR model, we 
apply the M(i) statistics and AIC to the data. Table 8.3 shows the results of these 
statistics. Both statistics indicate that a VAR(5) model might be adequate for the 
data. The M(i) statistics are marginally significant at lags 1, 3, and 5 at the 5% 
level. The minimum of AIC occurs at order 5. For this particular instance, the M(i) 
statistic is only marginally significant at the 1% level when i = 2, confirming the 
previous observation that the dynamic linear dependence between the two return 

series is weak. 

Estimation and Model Checking 
For a specified VAR model, one can estimate the parameters using either the OFS 
method or the MF method. The two methods are asymptotically equivalent. Under 
some regularity conditions, the estimates are asymptotically normal; see Reinsel 
(1993). A fitted model should then be checked carefully for any possible inad¬ 
equacy. The Qk(m) statistic can be applied to the residual series to check the 
assumption that there are no serial or cross correlations in the residuals. For a 
fitted VAR(p) model, the Qk(m) statistic of the residuals is asymptotically a chi- 
squared distribution with k2m — g degrees of freedom, where g is the number of 
estimated parameters in the AR coefficient matrices; see Fiitkepohl (2005). 

Example 8.4 (Continued). Table 8.4(a) shows the estimation results of a 
VAR(5) model for the bivariate series of monthly log returns of IBM stock and the 

S&P 500 index. The specified model is in the form 

rt = 00 + ®\rt-\ + ®2rt-2 + ®3r t-3 + f—5 + at. (8.20) 

where the first component of rt denotes IBM stock returns. For this particular 
instance, we do not use AR coefficient matrix at lag 4 because of the weak serial 
dependence of the data. In general, when the M(i) statistics and the AIC criterion 
specify a VAR(5) model, all five AR lags should be used. Table 8.4(b) shows the 
estimation results after some statistically insignificant parameters are set to zero. 
The Qk(m) statistics of the residual series for the fitted model in Table 8.4(b) 
give Q2(4) = 16.64 and Q2(S) = 31.55. Since the fitted VAR(5) model has six 
parameters in the AR coefficient matrices, these two Qk(m) statistics are distributed 
asymptotically as a chi-squared distribution with degrees of freedom 10 and 26, 

TABLE 8.3 Order Specification Statistics for Monthly Log Returns of IBM Stock 

and S&P 500 Index from January 1926 to December 2008° 

Order 

M{i) 
AIC 

1 

10.76 

6.795 

2 

13.41 

6.789 

3 

10.34 

6.786 

4 

7.78 

6.786 

5 

12.07 

6.782 

6 

1.93 

6.788 

“The 5% and 1% critical values of a chi-squared distribution with 4 degrees of freedom are 9.5 and 13.3. 

408 

MULTIVARIATE TIME SERIES ANALYSIS AND ITS APPLICATIONS 

respectively. The p-values of the test statistics are 0.083 and 0.208, and hence the 
fitted model is adequate at the 5% significance level. As shown by the univariate 
analysis, the return series are likely to have conditional heteroscedasticity. We 
discuss multivariate volatility in Chapter 10. 

From the fitted model in Table 8.4(b), we make the following observations: 
(a) The concurrent con-elation coefficient between the two innovational series is 
24/a/48 x 30 = 0.63, which, as expected, is close to the sample correlation coeffi¬ 
cient between r\t and r2t■ (b) The two log return series have positive and significant 
means, implying that the log prices of the two series had an upward trend over the 
data span, (c) The model shows that 

IBM, = 1.0 + 0.13SP5,-! — 0.09SP5,_2 + 0.09SP5,_5 + a\t, 

SP5, = 0.4 + 0.08SP5,_i - 0.06SP5,_3 + 0.09SP5,_s + a2t. 

Consequently, at the 5% significance level, there is a unidirectional dynamic rela¬ 
tionship from the monthly S&P 500 index return to the IBM return. If the S&P 
500 index represents the U.S. stock market, then IBM return is affected by the past 
movements of the market. However, past movements of IBM stock returns do not 
significantly affect the U.S. market, even though the two returns have substantial 
concurrent correlation. Finally, the fitted model can be written as 

IBM, 
SP5, 

1.0 
0.4 _ 

+ 

0.13 

. °-08 . 

SP5,_! - 

' 0.09 " 
0 

SP5r_2 

0 
0.06 

SP5,_3 

+ 

0.09 
0.09 

SP5,_5 + 

1

_

i

<
3

Q

r
q

1

-

indicating that SP5, is the driving factor of the bivariate series. 

TABLE 8.4 Estimation Results of a VAR(5) Model for the Monthly Log Returns, in 
Percentages, of IBM Stock and S&P 500 Index from January 1926 to December 2008 

Parameter 

00 

4*2 

4*3 

4*5 

I 

(a) Full Model 

-0.03 

0.15 

0.10 

-0.17 

0.05 

-0.11 

-0.06  0.14 

-0.03 

0.11 

0.04 

-0.04 

0.02 

-0.11 

-0.07 

0.04  0.05 

0.03 

0.04 

0.04 

0.03 

0.05 

0.04 

0.04 

0.03 

0.05 

0.04 

(b) Simplified Model 

0 

0 

— 

— 

0.13 

0.08 

0.04 

0.03 

0 

0 

— 

— 

-0.09 

0 

0.03 

— 

0 

0 

— 

- 

0 

-0.06 

— 

-0.06 

48 

24 

24 

30 

48 

24 

24 

30 

0.15 

0.05 

0.04 

0.03 

0.04 

0 

0 

— 

— 

0.09 

0.09 

0.04 

0.03 

Estimate 

Standard 
error 

Estimate 

Standard 
error 

1.0 

0.4 

0.23 

0.18 

1.0 

0.4 

0.22 

0.18 

 
 
 
 
 
VECTOR AUTOREGRESSIVE MODELS 

409 

Forecasting 

Treating a properly built model as the true model, one can apply the same techniques 
as those in the univariate analysis to produce forecasts and standard deviations of 
the associated forecast errors. For a VAR(p) model, the 1-step-ahead forecast at the 
time origin h is r/,( 1) = 0O + ®irh+i-i, and the associated forecast error is 
€h( 1) = fl/i+i- The covariance matrix of the forecast error is X. For 2-step-ahead 
forecasts, we substitute rh+i by its forecast to obtain 

rh (2) = 00 + $1^(1) + E i f h+2—i i 

i—2 

p 

and the associated forecast error is 

eh(2) = ah+2 + $i[rf - rh( 1)] = ah+2 + 4*iah+i. 

The covariance matrix of the forecast error is X + i X<J>j. If rt is weakly sta¬ 
tionary, then the Ustep-ahead forecast r^(£) converges to its mean vector fi as 
the forecast horizon t increases and the covariance matrix of its forecast error 
converges to the covariance matrix of rt. 

Table 8.5 provides 1-step- to 6-step-ahead forecasts of the monthly log returns, 
in percentages, of IBM stock and the S&P 500 index at the forecast origin h = 
996. These forecasts are obtained by the refined VAR(5) model in Table 8.4(b). 
As expected, the standard errors of the forecasts converge to the sample standard 
errors 7.03 and 5.53, respectively, for the two log return series. 

In summary, building a VAR model involves three steps: (a) Use the test statistic 
M(i) or some information criterion to identify the order, (b) estimate the specified 
model by using the least-squares method and, if necessary, reestimate the model 
by removing statistically insignificant parameters, and (c) use the Qk{m) statistic 
of the residuals to check the adequacy of a fitted model. Other characteristics of 
the residual series, such as conditional heteroscedasticity and outliers, can also be 
checked. If the fitted model is adequate, then it can be used to obtain forecasts and 
make inference concerning the dynamic relationship between the variables. 

We used SCA to perform the analysis in this section. The commands used include 
miden, mtsm, mest, and mf ore, where the prefix m stands for multivariate. Details 
of the commands and output are shown below. 

TABLE 8.5 Forecasts of a VAR(5) Model for Monthly Log Returns, in Percentages, 
of IBM Stock and S&P 500 Index: Forecast Origin Is December 2008 

Step 

IBM forecast 

Standard error 

SP forecast 

Standard error 

1 

1.95 

6.95 

1.70 

5.48 

2 

0.30 

6.99 

0.17 

5.50 

3 

-0.82 

7.00 

-1.26 

5.50 

4 

0.14 

7.00 

-0.49 

5.51 

5 

1.16 

7.00 

0.41 

5.51 

6 

1.29 

7.00 

0.65 

5.53 

410 

MULTIVARIATE TIME SERIES ANALYSIS AND ITS APPLICATIONS 

SCA Demonstration 
Output has been edited and % denotes explanation in the following: 

input date, ibm, sp5. file 'm-ibmsp2608.txt'. 

--% compute percentage log returns. 

ibm=ln(ibm+1)*100 

sp5=ln(sp5+l)*100 

--% model identification 

miden ibm,sp5. arfits 1 to 12. 

TIME PERIOD ANALYZED.1 TO 99 6 

EFFECTIVE NUMBER OF OBSERVATIONS (NOBE). . . 996 

SERIES NAME MEAN STD. ERROR 

1 IBM 1.0891 7.0298 

2 SP5 0.4301 5.5346 

NOTE: THE APPROX. STD. ERROR FOR THE ESTIMATED CORRELA¬ 

TIONS BELOW 

IS (1/NOBE* *.5) = 0.03169 

SAMPLE CORRELATION MATRIX OF THE SERIES 

1.00 

0.65 1.00 

SUMMARIES OF CROSS CORRELATION MATRICES USING WHERE 

+ DENOTES A VALUE GREATER THAN 2/SQRT(NOBE) 

- DENOTES A VALUE LESS THAN -2/SQRT(NOBE) 

. DENOTES A NON-SIGNIFICANT VALUE BASED ON THE ABOVE 

CRITERION 

CROSS CORRELATION MATRICES IN TERMS OF . 

LAGS 

LAGS 

6 

12 

1 THROUGH 
. + . - 
. + . . 
7 THROUGH 
. . + . 
. . + . 

. ~ 

. + 
. + 

. + . . 

STEPWISE AUTOREGRESSION SUMMARY 

I  RESIDUAL I  EIGENVAL. . I  CHI-SQ 

LAG  I  VARIANCESI  OF SIGMA  I  TEST 

I 

I 

I SIGN. 

AIC I PAR. 

1  I 

.492E+02 I 

.133E+02  I 

10.76  I 

6.795 I . + 

I 

.306E+02 I 

.665E+02  I 

I 

I . + 

2 I .486E+02 I .133E+02 I 

13.41 I 

6.789 I + - 

VECTOR AUTOREGRESSIVE MODELS 

411 

I 

. 306E+02  I 

. 659E+02  I 

1 

I 

. 

3  I 

.484E+02  I 

. 132E+02  I 

10.34  1 

6.786  I 

. 

. 

. 

I 

.303E+02  I 

. 655E+02  I 

I 

I 

.  - 

4  I 

.484E+02  I 

. 131E+02  I 

7.78  I 

6.786  I 

I 

.3 02E+02  I 

. 655E+02  I 

I 

I 

. 
. 

, 
. 

5  I 

.480E+02  I 

.131E+02  I 

12.07  I 

6.782  I 

.  + 

I 

. 299E+02  I 

.648E+02  I 

I 

I  -  + 

- + - 

6  I 

. 47 9E+02  I 

.131E+02  I 

1.93  I 

6.788  I 

I 

. 298E+02  I 

.647E+02  I 

I 

- + - 

I 

- + - 

7  I 

.479E+02  I 

.13 0E+02  I 

2.68  I 

6.793  I 

I 

.298E+02  I 

.647E+02  I 

I 

I 

8  I 

.477E+02  I 

.13 0E+02  I 

7.09  I 

6.794  I 

I 

. 296E+02  I 

.643E+02  I 

I 

I 

- + - 

9  I 

. 476E+02  I 

.13 0E+02  I 

5.23  I 

6.797  I 

I 

. 295E+02  I 

.642E+02  I 

I 

I 

- + " 

-H- 

10  I 

. 476E+02  I 

.13 0E+02  i 

1.43  I 

6.803  I 

I 

. 295E+02  I 

.641E+02  i 

I 

- + - 

I 

- + - 

11  I 

. 475E+02  I 

.13 0E+02  i 

1.81  I 

6.809  I 

I 

. 294E+02  I 

.640E+02  i 

I 

I 

- + - 

- + - 

12  I 

. 475E+02  I 

.12 9E+02  i 

1.88  I 

6.815  I 

I 

.2 94E+02  I 

.640E+02  i 

I 

-4-- 

I 

- + ~ 

NOTE:CHI-SQUARED CRITICAL VALUES WITH 4 DEGREES OF FREEDOM ARE 

5 PERCENT: 9.5 1 PERCENT: 13.3 

-- % model specification of a VAR(5) model without lag 4. 

mtsm ml. series ibm, sp5. model @ 

(i-p2*b-p2*b* *2-p3 *b* *3-p5*b* *5)series=c+noise. 

-- % estimation 

mestim ml. hold resi(rl,r2). 

-- % demonstration of setting zero constraint 

p2(2,2)=0 

cp2(2,2)=1 

p3(1,2)=0 

cp3(1,2)=1 

mestim ml. hold resi(rl,r2) 

412 

MULTIVARIATE TIME SERIES ANALYSIS AND ITS APPLICATIONS 

FINAL MODEL SUMMARY WITH CONDITIONAL LIKELIHOOD PAR. EST. 

- CONSTANT VECTOR (STD ERROR) - 

1.039 ( 0.223 ) 

0.390 ( 

0.176 

) 

- PHI MATRICES - 

ESTIMATES OF 

PHI ( 

1 )  MATRIX  AND  SIGNIFICANCE 

. 000 

. 000 

STANDARD ERRORS 

-- 

-- 

129 

080 

040 

031 

. + 

. + 

ESTIMATES OF 

PHI ( 

2 )  MATRIX  AND  SIGNIFICANCE 

.000 

.000 

STANDARD ERRORS 

090 

000 

031 

. - 

. 

ESTIMATES OF 

PHI ( 

3 )  MATRIX  AND  SIGNIFICANCE 

.000 

.000 

000 

061 

. 

. - 

STANDARD ERRORS 

-- 

024 

ESTIMATES OF 

PHI ( 

5 )  MATRIX  AND  SIGNIFICANCE 

.000 

. 000 

STANDARD ERRORS 

-- 

— 

093 

087 

040 

032 

. + 

. + 

ERROR COVARIANCE MATRIX 

1 2 

1 48.328570 

2 24.361464 30.027406 

-- % compute residual cross-correlation matrices 

miden rl,r2. maxi 12. 

-- % prediction 

mfore ml. nofs 6. 

6 FORECASTS, BEGINNING AT ORIGIN = 996 

SERIES: IBM SP5 

TIME FORECAST STD ERR FORECAST STD ERR 

997 1.954 6.952 1.698 5.480 

VECTOR AUTOREGRESSIVE MODELS 413 

998 

999 

1000 

1001 

1002 

0.304 

-0.815 

0.138 

1.162 

1.294 

6.988 

7.001 

7.001 

7.002 

7.022 

0.173 

-1.263 

-0.494 

0.408 

0.649 

5.497 

5.497 

5.507 

5.508 

5.528 

8.2.5 Impulse Response Function 

Similar to the univariate case, a VAR(p) model can be written as a linear function 
of the past innovations, that is. 

rt — f1 + at + ^\at-\ + ^2at-2 + • ■ •, (8.21) 

where p = [4>(l)]~'0o provided that the inverse exists, and the coefficient matrices 

can be obtained by equating the coefficients of Bl in the equation 

(/ - 4»,B-<I>PBp)(I + *iB + V2B2 + •••) = /, 

where I is the identity matrix. This is a moving-average representation of rt with 
the coefficient matrix being the impact of the past innovation on rt. Equiv¬ 
alently, is the effect of at on the future observation rt+i. Therefore, ty, is often 
referred to as the impulse response function of r,. However, since the components 
of at are often correlated, the interpretation of elements in of Eq. (8.21) is 
not straightforward. To aid interpretation, one can use the Cholesky decomposition 
mentioned earlier to transform the innovations so that the resulting components 
are uncorrelated. Specifically, there exists a lower triangular matrix L such that 
X = LGL', where G is a diagonal matrix and the diagonal elements of L are 
unity. See Eq. (8.9). Let bt = L~xat. Then, Cov(2>,.) = G so that the elements bp 
are uncorrelated. Rewrite Eq. (8.21) as 

rt ■= p at -\- \dt-\ ^2#r-2 + • • • 

= p + LL~lat + \LL~Xat-\ + ^2LL~lat-2 H- 

= p + %bt + *\bt-1 + **bt. 2 + • • •, (8.22) 

where Wq = L and = ^/L. The coefficient matrices are called the impulse 
response function of rt with respect to the orthogonal innovations bt. Specifically, 
the (/, y')th element of that is, fi/* (^), is the impact of bjj on the future 
observation r!>r+^. In practice, one can further normalize the orthogonal innovation 
b, such that the variance of bit is one. A weakness of the above orthogonalization 
is that the result depends on the ordering of the components of rt. In particular, 
b\t = ci\t so that au is not transformed. Different orderings of the components of 
rt may lead to different impulse response functions. Interpretation of the impulse 
response function is, therefore, associated with the innovation series bt. 

Both SCA and S-Plus enable one to obtain the impulse response function of a 
fitted VAR model. To demonstrate analysis of VAR models in S-Plus, we again use 

414 

MULTIVARIATE TIME SERIES ANALYSIS AND ITS APPLICATIONS 

the monthly log return series of IBM stock and the S&P 500 index of Example 
8.1. For details of S-Plus commands, see Zivot and Wang (2003). 

S-Plus Demonstration 
The following output has been edited and % denotes explanation: 

> module(finmetrics) 

> da=read.table("m-ibmsp2608.txt",header=T) % Load data 

> ibm=log(da[,2]+1)*100 % Compute percentage log returns 

> sp5=log(da[,3]+1)*100 

> y=cbind(ibm,sp5) % Create a vector series 

> yl=data.frame(y) % Crate a data frame 

> ord.choice=VAR(yl,max.ar=10) % Order selection using BIC 

> names(ord.choice) 

[1] "R" "coef" "fitted" "residuals" "Sigma" "df.resid" 

[7] "rank" "call" "ar.order" "n.na" "terms" "Y0" 

[13] "info" 

> ord.choice$ar.order % selected order 

[1] 1 
> ord.choice$info 

ar(l) ar(2) ar(3) ar(4) ar(5) ar(6) 

BIC 12325.41 12339.42 12356.58 12376.28 12391.57 12417.2 

ar(7) ar(8) ar(9) ar(10) 

BIC 12442.03 12462.5212484.78 12510.91 

> ord=VAR(yl,max.ar=10,criterion='AIC') % Using AIC 

> ord$ar.order 

[11 5 

> ord$info 

ar(l) ar(2) ar(3) ar(4) ar(5) ar(6) 

AIC 12296.04 12290.48 12288.07 12288.2 12283.91 12289.96 

ar(7) ar(8) 

ar(9) ar(10) 

AIC 12295.22 12296.13 

12298.82 12305.37 

The AIC selects a VAR(5) model as before, but BIC selects a VAR(l) model. 
For simplicity, we shall use VAR(l) specification in the demonstration. Note that 
different normalizations are used between the two packages so that the values of 
information criteria appear to be different; see the AIC in Table 8.3. This is not 
important because normalization does not affect order selection. Turn to estimation. 

> varl. f it=VAR (y~ar (1) ) % Estimate a VAR(l) model 

> summary(varl.fit) 

Call: 

VAR(formula = y ~ ar(l)) 

Coefficients: 

ibm sp5 

(Intercept) 1.0614 0.4087 

(std.err) 0.2249 0.1773 

VECTOR AUTOREGRESSIVE MODELS 

415 

; t.stat) 4.7198 2.3053 

ibm.lagl 

■0.0320 -0.0223 

(std.err) 

0.0413 0.0326 

(t.stat) 

-0.7728 -0.6855 

sp5.lagl 

0.1503 0.1020 

[std.err) 

0.0525 0.0414 

(t.stat) 

2.8612 2.4637 

Regression Diagnostics: 

ibm sp5 

R-squared 0.0101 0.0075 

Adj. R-squared 0.0081 0.0055 

Resid. Scale 7.0078 5.5247 

Information Criteria: 

logL AIC BIC HQ 

-6193.988 12399.977 12429.393 12411.159 

Degree of freedom: 995 992 

total residual 

> plot(varl.fit) 

Make a plot selection (or 0 to exit): 

1: plot: All 

2: plot: Response and Fitted Values 

3: plot: Residuals 

8: plot: PACF of Squared Residuals 

Selection: 3 

The fitted model is 

IBM, = 1.06 — 0.03IBM,_i + 0.15SP5t_i + alt, 

SP5, =0.41 — 0.02IBM,_i +0.10SP5,_i +a2t. 

Based on t statistics of the coefficient estimates, only the lagged variable SP5,_i is 
informative in both equations. Figure 8.5 shows the time plots of the two residual 
series, where the two horizontal lines indicate the two standard error limits. As 
expected, there exist clusters of outlying observations. 

Next, we compute 1-step- to 6-step-ahead forecasts and the impulse response 
function of the fitted VAR(l) model when the IBM stock return is the first com¬ 
ponent of r,. Compared with those of a VAR(5) model in Table 8.5, the forecasts 
of the VAR(l) model converge faster to the sample mean of the series. 

> varl,pred=predict(varl.fit,n.predict=6) % Compute forecasts 

> summary(varl.pred) 

416 

MULTIVARIATE TIME SERIES ANALYSIS AND ITS APPLICATIONS 

Residuals vs.Time 

0 200 400 600 800 1000 

Figure 8.5 Residual plots of fitting a VAR(l) model to the monthly log returns, in percentages, of 

IBM stock and S&P 500 index. Sample period is from January 1926 to December 2008. 

Predicted Values with Standard Errors: 

1- step-ahead 

(std.err) 

2- step-ahead 

(std.err) 

3- step-ahead 

(std.err) 

ibm sp5 
1.0798 0.4192 
7.0078 5.5247 
1.0899 0.4274 
7.0434 5.5453 
1.0908 0.4280 
7.0436 5.5454 

6-step-ahead 1.0909 0.4280 

(std.err) 7.0436 5.5454 

> plot(varl.pred,y,n.old=12) % Obtain forecast plot 

% Below is to compute the impulse response function 
> varl. irf = impRes (varl. f it, period=6 , std. err=/asymptotic/) 
> summary(varl.irf) 

Impulse Response Function: 

(with responses in rows, and innovations in columns) 
/ /  lag. 0 

ibm 
sp5 
ibm  6 . . 9973  0 . .0000 
(std . err)  0 . . 1569  0 . . 0000 
sp5  3 . . 5432  4 , .2280 
(std . err)  0 . . 1558  0 , . 0948 

/ /  lag. 1 

sp5 
ibm 
ibm  0 , .3088  0. . 6353 
(std . err)  0 , .2217  0 , .2221 

VECTOR MOVING-AVERAGE MODELS 

417 

985 990 995 1000 

Figure 8.6 Forecasting plots of fitted VAR(l) model to monthly log returns, in percentages, of IBM 

stock and S&P 500 index. Sample period is from January 1926 to December 2008. 

sp5 0.2050 0.4312 

(std.err) 0.1746 0.1750 

> plot(varl.irf) 

Figure 8.6 shows the forecasts and their pointwise 95% confidence intervals 
along with the last 12 data points of the series. Figure 8.7 shows the impulse 
response functions of the fitted VAR(l) model where the IBM stock return is the 
first component of rt. Since the dynamic dependence of the returns is weak, the 
impulse response functions exhibit simple patterns and decay quickly. 

8.3 VECTOR MOVING-AVERAGE MODELS 

A vector moving-average model of order q, or VMA(g), is in the form 

rt = 0o + at - ©iar_i-®qat-q or r, = d0 + G(B)at, (8.23) 

where 6q is a ^-dimensional vector, 0, are k x k matrices, and 0(5) = / — 
0i B — ■■■ — ©q Bq is the MA matrix polynomial in the back-shift operator B. 
Similar to the univariate case, VMA(g) processes are weakly stationary provided 

418 

MULTIVARIATE TIME SERIES ANALYSIS AND ITS APPLICATIONS 

0 1 2 3 4 5 

CD 
(I) 
C 
o 
Q. 
(/) 
CD 

CD 

Q. 

Figure 8.7 Plots of impulse response functions of orthogonal innovations for fitted VAR(l) model to 

monthly log returns, in percentages, of IBM stock and S&P 500 index. Sample period is from January 
1926 to December 2008. 

Steps 

that the covariance matrix X of at exists. Taking expectation of Eq. (8.23), we 
obtain that fi = E(r,) = 0q. Thus, the constant vector Oq is the mean vector of rt 
for a VMA model. 

Let r, = rt — 0o be the mean-corrected VAR(g) process. Then using Eq. (8.23) 

and the fact that {at} has no serial correlations, we have 

1. Cov(rf, at) = X. 

2. ro-i: + 0i5:0/1 + --- + 0,/i:0;. 

3. r£=0if£>g. 

4. r£ = J2C'j=i Qj'EQ'j-e if 1 < & < q, where 0o = —I. 

Since r^ = 0 for i > q, the cross-correlation matrices (CCMs) of a VMAO?) pro¬ 
cess rt satisfy 

Pi = 0, £>q. (8.24) 

Therefore, similar to the univariate case, the sample CCMs can be used to identify 
the order of a VMA process. 

VECTOR MOVING-AVERAGE MODELS 

419 

To better understand the VMA processes, let us consider the bivariate MA( 1) 

model 

rt = 0o + at - Oa,_i = fi + at - 0ar_i, (8.25) 

where, for simplicity, the subscript of 0i is removed. This model can be written 
explicitly as 

r\t  _  " til 

. r2? 

. 

+ 

a\t 

a2t 

011 

©21 

@12 

@22 

<32,7-1 

(8.26) 

It says that the current return series rt only depends on the current and past shocks. 
Therefore, the model is a finite-memory model. 

Consider the equation for r\t in Eq. (8.26). The parameter ©12 denotes the linear 
dependence of r\t on <22,7-1 in the presence of a\j-\. If ©12 = 0, then rif does not 
depend on the lagged values of <227 and, hence, the lagged values of r2t. Similarly, 
if ©2i = 0, then r2t does not depend on the past values of r\t. The off-diagonal 
elements of 0 thus show the dynamic dependence between the component series. 
For this simple VMA(l) model, we can classify the relationships between rif and 
r2t as follows: 

1. They are uncoupled series if ©12 = ©21 =0. 

2. There is a unidirectional dynamic relationship from r\t to r2, if Q'12 = 0, 
but @21 7^ 0. The opposite unidirectional relationship holds if ©21 = 0, but 

012 7^0. 

3. There is a feedback relationship between r\t and r2t if ©12 7^ 0 and ©21 7^ 0. 

Finally, the concurrent correlation between rlf is the same as that between alt. The 
previous classification can be generalized to a VMA(g) model. 

Estimation 
Unlike the VAR models, estimation of VMA models is much more involved; 
see Hillmer and Tiao (1979), Fiitkepohl (2005), and the references therein. 
For the likelihood approach, there are two methods available. The first is the 
conditional-likelihood method that assumes that at — 0 for t < 0. The second is 
the exact-likelihood method that treats a, with t < 0 as additional parameters of 
the model. To gain some insight into the problem of estimation, we consider the 
VMA(l) model in Eq. (8.25). Suppose that the data are [rt\t = 1,..., T} and a, 
is multivariate normal. For a VMA(l) model, the data depend on a0. 

Conditional MLE 
The conditional-likelihood method assumes that ao = 0. Under such an assumption 
and rewriting the model as at = rt -90 + 0af_i, we can compute the shock a, 

recursively as 

«i = n - 0o, 

a2 = r2- 0o + 0ifli, 

420 

MULTIVARIATE TIME SERIES ANALYSIS AND ITS APPLICATIONS 

Consequently, the likelihood function of the data becomes 

/(n,..., rr 100,01,2) = f] (27r)*/2|j;|i/2 exp (~2a' 

Y 

T 

which can be evaluated to obtain the parameter estimates. 

Exact MLE 
For the exact-likelihood method, ao is an unknown vector that must be estimated 
from the data to evaluate the likelihood function. For simplicity, let rt = rt — 0o 
be the mean-corrected series. Using rt and Eq. (8.25), we have 

a, =rt + 0«(_i. 

(8.27) 

By repeated substitutions, «o is related to all rt as 

a\=r i + 0ao, 

a2 = r 2 + 0a j = r2 + 0f\ + 02fl o, 

: (8.28) 

aT =rT + 0rT_\ + • • • + 0T~lr\ 4- 0Ta^. 

Thus, ao is a linear function of the data if Oo and 0 are given. This result enables 
us to estimate ao using the data and initial estimates of 0q and 0. More specifically, 
given 6o, 0, and the data, we can define 

r* = r, + 0r/_i-I-\-0‘~lru for t = 1,2, 

Equation (8.28) can then be rewritten as 

r*x = -0ao + a\, 

r 2 = -02ao + a2, 

r*T - —0Tao + aj. 

This is in the form of a multiple linear regression with parameter vector ao, even 
though the covariance matrix D of at may not be a diagonal matrix. If initial 
estimate of X is also available, one can premultiply each equation of the prior 
system by X"1/2, which is the square root matrix of Y. The resulting system is 
indeed a multiple linear regression, and the ordinary least-squares method can be 
used to obtain an estimate of ao- Denote the estimate by ao. 

VECTOR MOVING-AVERAGE MODELS 

421 

Using the estimate oq, we can compute the shocks at recursively as 

«i = r\- do + &a0, a2 = r2 - 00 + Oai, 

This recursion is a linear transformation from (ao, ri, ..., rj) to (ao, a\, ..., aj), 
from which we can (a) obtain the joint distribution of ao and the data, and (2) 
integrate out ao to derive the exact-likelihood function of the data. The resulting 
likelihood function can then be evaluated to obtain the exact ML estimates. For 
details, see Hillmer and Tiao (1979). 

In summary, the exact-likelihood method works as follows. Given initial esti¬ 
mates of 9q, 0, and X, one uses Eq. (8.28) to derive an estimate of ao- This 
estimate is in turn used to compute a, recursively using Eq. (8.27) and starting 
with a\ =f\ + 0oo. The resulting [at}J=i are then used to evaluate the exact- 
likelihood function of the data to update the estimates of 6q, 0, and X. The whole 
process is then repeated until the estimates converge. This iterative method to 
evaluate the exact-likelihood function applies to the general VMA(g) models. 

From the previous discussion, the exact-likelihood method requires more inten¬ 
sive computation than the conditional-likelihood approach does. But it provides 
more accurate parameter estimates, especially when some eigenvalues of 0 are 
close to 1 in modulus. Hillmer and Tiao (1979) provide some comparison between 
the conditional- and exact-likelihood estimations of VMA models. In multivariate 
time series analysis, the exact maximum-likelihood method becomes important if 
one suspects that the data might have been overdifferenced. Overdifferencing may 
occur in many situations (e.g., differencing individual components of a cointegrated 
system; see discussion later on cointegration). 

In summary, building a VMA model involves three steps: (a) Use the sample 
cross-correlation matrices to specify the order q—for a VMA(g) model, pe = 0 for 
l>q; (b) estimate the specified model by using either the conditional- or exact- 
likelihood method—the exact method is preferred when the sample size is not 
large; and (c) the fitted model should be checked for adequacy [e.g., applying the 
Qk(m) statistics to the residual series]. Finally, forecasts of a VMA model can be 
obtained by using the same procedure as a univariate MA model. 

Example 8.5. Consider again the bivariate series of monthly log returns in 
percentages of IBM stock and the S&P 500 index from January 1926 to Decem¬ 
ber 2008. Since significant cross correlations occur mainly at lags 1, 2, 3 and 5, 

we employ the VMA(5) model 

rt = 0q + at — 0i«r-i — 02«r-2 — ®3«/-3 — ©5«/-5 (8.29) 

for the data. Table 8.6 shows the estimation results of the model. The Qk{m) 
statistics for the residuals of the simplified model give <22(4) = 16.00 and Q2($) = 
29.46. Compared with chi-squared distributions with 10 and 26 degrees of freedom, 
the p values of these statistics are 0.10 and 0.291, respectively. Thus, the model 

is adequate at the 5% significance level. 

422 

MULTIVARIATE TIME SERIES ANALYSIS AND ITS APPLICATIONS 

TABLE 8.6 Estimation Results for Monthly Log Returns of IBM Stock and S&P 
500 Index Using the Vector Moving-Average Model in Eq. (8.29)" 

Parameter 

Oo 

0! 

02 

03 

05 

Estimate 

Standard error 

Estimate 

Standard error 

(a) Full Model with Conditional-Likelihood Method 

1.1 

0.4 

0.24 

0.19 

1.1 

0.4 

0.24 

0.19 

0.02 

0.02 

0.04 

0.03 

-0.15 

-0.09 

-0.10 

-0.04 

0.05 

0.04 

0.04 

0.03 

0.15 

0.04 

0.05 

0.04 

-0.05 

-0.01 

0.11 

0.11 

0.04 

0.05 

0.03 

0.04 

(b) Full Model with Exact-Likelihood Method 

0.02 

0.02 

0.04 

0.03 

-0.05 

-0.09 

-0.10 

-0.04 

0.05 

0.04 

0.04 

0.03 

0.15 

0.04 

0.05 

0.04 

-0.05 

-0.01 

0.04 

0.03 

0.11 

0.11 

0.05 

0.04 

(c) Simplified Model with Exact-Likelihood Method 

0.05 

0.07 

0.04 

0.03 

0.05 

0.07 

0.04 

0.03 

Estimate 

1.1 

0.4 

0.0 

0.0 

Standard error 

0.24  — 

0.19  — 

-0.13 

-0.09 

0.04 

0.03 

0.0 

0.0 

— 

— 

0.08 

0.0 

0.03 

— 

0.0 

0.0 

— 

— 

0.0 

0.07 

0.0 

0.0 

—  — 

0.02  — 

-0.15 

-0.15 

0.05 

0.04 

-0.15 

-0.15 

0.05 

0.04 

-0.10 

-0.09 

0.04 

0.03 

"The sample period is from January 1926 to December 2008. The residual covariance matrix is not 

shown as it is similar to that in Table 8.4 

From Table 8.6, we make the following observations: 

1. The difference between conditional- and exact-likelihood estimates is small 
for this particular example. This is not surprising because the sample size is 
not small and, more important, the dynamic structure of the data is weak. 

2. The VMA(5) model provides essentially the same dynamic relationship for 
the series as that of the VAR(5) model in Example 8.4. The monthly log 
return of IBM stock depends on the previous returns of the S&P 500 index. 
The market return, in contrast, does not depend on lagged returns of IBM 
stock. In other words, the dynamic structure of the data is driven by the 
market return, not by the IBM return. The concurrent correlation between 
the two returns remains strong, however. 

8.4 VECTOR ARMA MODELS 

Univariate ARMA models can also be generalized to handle vector time series. The 
resulting models are called VARMA models. The generalization, however, encoun¬ 
ters some new issues that do not occur in developing VAR and VMA models. One 
of the issues is the identifiability problem. Unlike the univariate ARMA models, 
VARMA models may not be uniquely defined. For example, the VMA(l) model 

r\t 

_ r2t 

ait 

a2t 

' 0  2 ' 
_ 0  0 

a\,t-\ 

a2,t-l 

VECTOR ARMA MODELS 

423 

is identical to the VAR(l) model 

fit 

. r2> 

' 0  -2 ' 

L 0 

0 

f\,t-1 
. r2a-i 

a\t 

a2t 

The equivalence of the two models can easily be seen by examining their compo¬ 
nent models. For the VMA(l) model, we have 

fit = a\t— 2a2,t-i, f2t — &2t- 

For the VAR(l) model, the equations are 

fit + 2r2,f-i = a u, r2t = a2t. 

From the model for r2t, we have r2,t-\ = a2,t-i- Therefore, the models for r\t are 
identical. This type of identifiability problem is harmless because either model can 
be used in a real application. 

Another type of identifiability problem is more troublesome. Consider the 

VARMA(1,1) model 

fit 

_ r2‘ 

" 0.8 
0 

-2 ' 
0 

fi,t-1 
. r2,t—l 

ait 

a2t 

' -0.5  0 ' 
0 _ 

0 

ai,t-i 

a2,t—l 

This model is identical to the VARMA(1,1) model 

r\t 

fit 

0.8  -2 + r] 
0 

(O 

fi,t-1 

_ r2,t~l 

an 

. a2< 

' -0.5 
0 

V 

(!) 

ai,t-i 

for any nonzero co and rj. In this particular instance, the equivalence occurs 
because we have r2t = a2t in both models. The effects of the parameters co and 
T] on the system cancel out between AR and MA parts of the second model. 
Such an identifiability problem is serious because, without proper constraints, the 
likelihood function of a vector ARMA(1,1) model for the data is not uniquely 
defined, resulting in a situation similar to the exact multicollinearity in a regression 
analysis. This type of identifiability problem can occur in a vector model even if 

none of the components is a white noise series. 

These two simple examples highlight the new issues involved in the general¬ 
ization to VARMA models. Building a VARMA model for a given data set thus 
requires some attention. In the time series literature, methods of structural specifi¬ 
cation have been proposed to overcome the identifiability problem; see Tiao and 
Tsay (1989), Tsay (1991), and the references therein. We do not discuss the detail of 
structural specification here because VAR and VMA models are sufficient in most 
financial applications. When VARMA models are used, only lower order models 
are entertained [e.g., a VARMA(1,1) or VARMA(2,1) model] especially when the 

time series involved are not seasonal. 

424 

MULTIVARIATE TIME SERIES ANALYSIS AND ITS APPLICATIONS 

A VARMA(p, q) model can be written as 

$(5)rf = 0o + 

where $(5) = / — $15 — • • • — $pBp and 0(5) = / - 0)B-0^5^ are 
two k x k matrix polynomials. We assume that the two matrix polynomials have 
no left common factors; otherwise, the model can be simplified. The necessary 
and sufficient condition of weak stationarity for rt is the same as that for the 
VAR(p) model with matrix polynomial $(5). For v > 0, the (z, y)th elements 
of the coefficient matrices $„ and 0„ measure the linear dependence of r\t on 
rjj-v and ajj-v, respectively. If the (i, y)th element is zero for all AR and MA 
coefficient matrices, then ru does not depend on the lagged values of rjt. However, 
the converse proposition does not hold in a VARMA model. In other words, nonzero 
coefficients at the (z, j)th position of AR and MA matrices may exist even when 
r,7 does not depend on any lagged value of r jJt. 

To illustrate, consider the following bivariate model 

' $, i (5)  $12(5) ' 
. 02i(5)  $22(5) 

ru 

. r21 

' 0n(5)  0,2(5) ‘ 
. ©21(5) 

022 (5) 

a\t 

Here the necessary and sufficient conditions for the existence of a unidirectional 
dynamic relationship from r\t to are 

$22(5)012(5) — $12(5)022(5) = 0, 

but 

$n(5)02i(5) - $2i(5)0„(5) ^ 0. (8.30) 

These conditions can be obtained as follows. Letting 

£2(5) = |$(5)| = $n(5)$22(5) - $,2(5)$2,(5) 

be the determinant of the AR matrix polynomial and premultiplying the model by 
the matrix 

$22(5) —$12(5) 

.-$21(5) $11(5) J ’ 

we can rewrite the bivariate model as 

r2t_ 

'$22(5)011(5) - $12(5)021(5) $22(5)0,2(5) - $12(5)022(5) 
$11(5)02,(5) - $21(5)011(5) $ii(5)022(5) - $2,(5)012(5) 

x 

a\t 

ait 

VECTOR ARMA MODELS 

425 

Figure 8.8 Time plots of log U.S. monthly interest rates from April 1953 to January 2001. Solid line 

denotes 1-year Treasury constant maturity rate and dashed line denotes 3-year rate. 

Consider the equation for r\t. The first condition in Eq. (8.30) shows that r\t does 
not depend on any past value of a2t or r2t. From the equation for r2t, the second 
condition in Eq. (8.30) implies that r2t indeed depends on some past values of 
a\t. Based on Eq. (8.30), @i2(5) = = 0 is a sufficient, but not necessary, 
condition for the unidirectional relationship from r\t to r2t. 

Estimation of a VARMA model can be carried out by either the conditional or 
exact maximum-likelihood method. The Qkipi) statistic continues to apply to the 
residual series of a fitted model, but the degrees of freedom of its asymptotic chi- 
squared distribution are k2m - g, where g is the number of estimated parameters 
in both the AR and MA coefficient matrices. 

Example 8.6. To demonstrate VARMA modeling, we consider two U.S. 
monthly interest rate series. The first series is the 1-year Treasury constant maturity 
rate, and the second series is the 3-year Treasury constant maturity rate. The data 
are obtained from the Federal Reserve Bank of St. Louis, and the sampling period 
is from April 1953 to January 2001. There are 574 observations. To ensure the 
positiveness of U.S. interest rates, we analyze the log series. Figure 8.8 shows the 
time plots of the two log interest rate series. The solid line denotes the 1-year 
maturity rate. The two series moved closely in the sampling period. 

The M(i) statistics and AIC criterion specify a VAR(4) model for the data. How¬ 
ever, we employ a VARMA(2,1) model because the two models provide similar 

426 

MULTIVARIATE TIME SERIES ANALYSIS AND ITS APPLICATIONS 

TABLE 8.7 Parameter Estimates of VARMA(2,1) Model for Two Monthly U.S. 
Interest Rate Series Based on Exact-Likelihood Method 

Parameter 

Estimate 

4>i 
1.82 - -0.97 
— 

$2 

-0.84 

00 
0.90 
0.028 
0.025  — 

0.98 
— 

0! 

I X  103 

-1.66 
-0.47 

3.58 
2.50 

2.50 
2.19 

0.99  — 

Standard error 

0.03 
— 

0.08 
0.01 

0.03 
— 

0.08 
— 

0.03 
0.014 
0.011  — 

0.10 
0.04 

fits. Table 8.7 shows the parameter estimates of the VARMA(2,1) model obtained 
by the exact-likelihood method. We removed the insignificant parameters and rees¬ 
timated the simplified model. The residual series of the fitted model has some minor 
serial and cross correlations at lags 7 and 11. Figure 8.9 shows the residual plots 
and indicates the existence of some outlying data points. The model can be further 
improved, but it seems to capture the dynamic structure of the data reasonably well. 
The final VARMA(2,1) model shows some interesting characteristics of the 
data. First, the interest rate series are highly contemporaneously correlated. The 
concurrent correlation coefficient is 2.5/V3.58 x 2.19 = 0.893. Second, there is a 

(J) 
03 Z3 
~o 
CO 
CD 
CE 

CM 
o 

CvJ 
o 
l 

<n 
ca 
■g 
CD 
cc 

o I 

1960 

1970 

1980 

1990 

2000 

Year 

(a) 

1960 

1970 

1980 

1990 

2000 

Year 

(b) 

Figure 8.9 Residual plots for log U.S. monthly interest rate series of Example 8.6. Fitted model is 
VARMA(2,1): (a) 1-year rate and (b) 3-year rate. 

VECTOR ARMA MODELS 

427 

unidirectional linear relationship from the 3-year rate to the 1-year rate because the 
(2, l)th elements of all AR and MA matrices are zero, but some (1, 2)th element 
is not zero. As a matter of fact, the model in Table 8.7 shows that 

r-n = 0.025 + 0.99r3>f_i + a3t + OAla^j-i, 

r\t = 0.028 + 1.82ri,,_i - 0.84n,*-2 - 0.97r3if_i + 0.98r3tf_2 

+ ci\t — 0.90fli,;_i + 1.66a3jf_i, 

where rit is the log series of i-year interest rate and an is the corresponding shock 
series. Therefore, the 3-year interest rate does not depend on the past values of 
the 1-year rate, but the 1-year rate depends on the past values of the 3-year rate. 
Third, the two interest rate series appear to be unit-root nonstationary. Using the 
back-shift operator B, the model can be rewritten approximately as 

(1 — 5)r3r = 0.03 + (1 + 0.475)a3f, 

(1 - 5)( 1 - 0.82fl)ru = 0.03 - 0.975(1 - B)r3,t + (1 - 0.9B)au + 1.66Ba3,t- 

Finally, the SCA commands used in the analysis are given in Appendix C. 

8.4.1 Marginal Models of Components 

Given a vector model for rt, the implied univariate models for the components rlt 
are the marginal models. For a ^-dimensional ARMA(p, q) model, the marginal 
models are AKMA[kp, (k — 1 )p + q]. This result can be obtained in two steps. 
First, the marginal model of a VMA(g) model is univariate MA(g). Assume that 
rt is a VMA(g) process. Because the cross-correlation matrix of rt vanishes after 
lag q (i.e., pt — 0 for l > q), the ACF of rlt is zero beyond lag q. Therefore, rit is 
an MA process and its univariate model is in the form rit = 0(>o + Y^j=i 9i,jbi,t-j, 
where {£/,} is a sequence of uncorrelated random variables with mean zero and 
variance crfb. The parameters 0tj and alb are functions of the parameters of the 
VMA model for rt. 

The second step to obtain the result is to diagonalize the AR matrix polynomial 

of a VARMA(p, q) model. For illustration, consider the bivariate AR(1) model 

' 1 - On 5 

— $12# 

— $2i5 

1 — $22^ 

rit 

. r2f 

a it 

a2t 

Premultiplying the model by the matrix polynomial 

1 — <t>22^ 012B 

$21B 1 — 4>nB ’ 

we obtain 

[(1 - 4>n5)(l - $22B) - $12$2252] 

ru 
rjt 

1 — $22 B 
— $21 B 

— $12$ 

1 - $n5 

a\t 

«21 

428 

MULTIVARIATE TIME SERIES ANALYSIS AND ITS APPLICATIONS 

The left-hand side of the prior equation shows that the univariate AR polynomials 
for r-lt are of order 2. In contrast, the right-hand side of the equation is in a VMA(l) 
form. Using the result of VMA models in step 1, we show that the univariate 
model for rit is ARMA(2,1). The technique generalizes easily to the ^-dimensional 
VAR(l) model, and the marginal models are ARMA(&, k — 1). More generally, for 
a ^-dimensional VAR(p) model, the marginal models are ARMA[&p, (k — 1 )p]. 
The result for VARMA models follows directly from those of VMA and VAR 
models. 

The order [kp, (k — \)p + q] is the maximum order (i.e., the upper bound) for 

the marginal models. The actual marginal order of rit can be much lower. 

8.5 UNIT-ROOT NONSTATIONARITY AND COINTEGRATION 

When modeling several unit-root nonstationary time series jointly, one may 
encounter the case of cointegration. Consider the bivariate ARMA(1,1) model 

x\t 

*21_ 

0.5 
-0.25 

-1.0'  X\,t-1 
_X2,t-1. 

0.5 

ait 

_a2,_ 

0.2 
-0.1 

-0.4" 

0.2_  _a2,t-l_ 

where the covariance matrix £ of the shock a, is positive definite. This is not a 
weakly stationary model because the two eigenvalues of the AR coefficient matrix 
are 0 and 1. Figure 8.10 shows the time plots of a simulated series of the model with 
200 data points and X = /, whereas Figure 8.11 shows the sample autocorrelations 
of the two component series xit. It is easy to see that the two series have high 
autocorrelations and exhibit features of unit-root nonstationarity. The two marginal 
models of xt are indeed unit-root nonstationary. Rewrite the model as 

1 -0.55 
0.255 

5 
1 -0.55 

Xlt 

x2t 

1 - 0.25 
0.15 

0.45 
1 -0.25 

au 

«2t 

Premultiplying the above equation by 

1-0.55 -5 
-0.255 1-0.55 J’ 

we obtain the result 

'1-5 0 

0 1-5 

Xu 

. X2t 

1 -0.75 
-0.155 

-0.65 
1 -0.75 

au 

a2t 

Therefore, each component Xu of the model is unit-root nonstationary and follows 
an ARIMA(0,1,1) model. 

UNIT-ROOT NONSTATIONARITY AND COINTEGRATION 

429 

CNJ X 

Figure 8.10 Time plots of simulated series based on model (8.31) with identity covariance matrix for 

shocks. 

However, we can consider a linear transformation by defining 

i

~y\t 
yit_ 

~b\t 
b2t 

 q

1

(

N

o

0.5 1.0 

X\t 

X2t_ 

"1.0 -2.0"  a\t 
0.5 1.0 _ 

o
t

_
S
_
l

The VARMA model of the transformed series yt can be obtained as follows: 

Lxt = L4>x,_i + Lat - L0a(_i 

= L<t>Ll Lxt-\ + Lat — L0L-1 Lat-\ 

= L<f>L~l(Lxt-\) +bt — LQL~lbt-i. 

Thus, the model for yt is 

’ yu 
yit 

' 1.0  0 '  yu-i 

0 

0 

bit 
b2t 

' 0.4  0 ' 

0 

0 

bi,t-i 
b2,t-i 

. (8.32) 

From the prior model, we see that (a) y\t and y2t are uncoupled series with con¬ 
current correlation equal to that between the shocks b\t and b2t, (b) y\t follows a 
univariate ARIMA(0,1,1) model, and (c) y2t is a white noise series (i.e., y2t = b2t). 

 
 
 
 
 
 
430 

MULTIVARIATE TIME SERIES ANALYSIS AND ITS APPLICATIONS 

Lag 

Lag 

Figure 8.11 Sample autocorrelation functions of two simulated component series. There are 200 

observations, and model is given by Eq. (8.31) with identity covariance matrix for shocks. 

In particular, the model in Eq. (8.32) shows that there is only a single unit root in 
the system. Consequently, the unit roots of x\t and *2r are introduced by the unit 
root of y\t. In the literature, y\t is referred to as the common trend of x\t and xjt- 
The phenomenon that both x\t and xit are unit-root nonstationary, but there is 
only a single unit root in the vector series, is referred to as cointegration in the 
econometric and time series literature. Another way to define cointegration is to 
focus on linear transformations of unit-root nonstationary series. For the simulated 
example of model (8.31), the transformation shows that the linear combination 
y2t = 0.5x\, + X2t does not have a unit root. Consequently, x\t and X2t are coin¬ 
tegrated if (a) both of them are unit-root nonstationary, and (b) they have a linear 
combination that is unit-root stationary. 

Generally speaking, for a k-dimensional unit-root nonstationary time series, coin¬ 
tegration exists if there are less than k unit roots in the system. Let h be the number 
of unit roots in the ^-dimensional series xt. Cointegration exists if 0 < h < k, and 
the quantity k — h is called the number of cointegrating factors. Alternatively, the 
number of cointegrating factors is the number of different linear combinations 
that are unit-root stationary. The linear combinations are called the cointegrating 
vectors. For the prior simulated example, y2t = (0.5, l)x, so that (0.5, l)7 is a 
cointegrating vector for the system. For more discussions on cointegration and 
cointegration tests, see Box and Tiao (1977), Engle and Granger (1987), Stock 
and Watson (1988), and Johansen (1988). We discuss cointegrated VAR models in 
Section 8.6. 

The concept of cointegration is interesting and has attracted a lot of attention in 
the literature. However, there are difficulties in testing for cointegration in a real 

UNIT-ROOT NONSTATIONARITY AND COINTEGRATION 

431 

application. The main source of difficulties is that cointegration tests overlook the 
scaling effects of the component series. Interested readers are referred to Cochrane 
(1988) and Tiao, Tsay, and Wang (1993) for further discussion. 

While I have some misgivings on the practical value of cointegration tests, the 
idea of cointegration is highly relevant in financial study. For example, consider the 
stock of Finnish Nokia Corporation. Its price on the Helsinki Stock Market must 
move in unison with the price of its American Depositary Receipts on the New York 
Stock Exchange; otherwise there exists some arbitrage opportunity for investors. 
If the stock price has a unit root, then the two price series must be cointegrated. 
In practice, such a cointegration can exist after adjusting for transaction costs and 
exchange rate risk. We discuss issues like this later in Section 8.7. 

8.5.1 An Error Correction Form 

Because there are more unit-root nonstationary components than the number of 
unit roots in a cointegrated system, differencing individual components to achieve 
stationarity results in overdifferencing. Overdifferencing leads to the problem of 
unit roots in the MA matrix polynomial, which in turn may encounter difficulties in 
parameter estimation. If the MA matrix polynomial contains unit roots, the vector 
time series is said to be noninvertible. 

Engle and Granger (1987) discuss an error correction representation for a coin¬ 
tegrated system that overcomes the difficulty of estimating noninvertible VARMA 
models. Consider the cointegrated system in Eq. (8.31). Let Ax, = xt — xt-\ be 
the differenced series. Subtracting x,-\ from both sides of the equation, we obtain 
a model for Ax, as 

Axn 

Ax2t 

-0.5 
-0.25 

-1.0 
-0.5 _ 

I

H

—

r

7

H

(

N

7

1

+ 

a\t 

a2t 

— 

O

<
N

1

o

1

O

t

—

;

o

c
n

-1 
-0.5 _ 

[0.5, 1.0] 

. X2,f-1 

+ 

a\t 

a2t 

i

— 

i

p

o
t

1

o

 o

—

i

o

c
n

1

-
1
1
-
1

a\,t-1 

<31,1-1 

<32,f—I 

This is a stationary model because both Ax, and [0.5, 1.0]x, = y2t are unit-root 
stationary. Because xt-\ is used on the right-hand side of the previous equation, 
the MA matrix polynomial is the same as before and, hence, the model does not 
encounter the problem of noninvertibility. Such a formulation is referred to as an 
error correction model for Ax,, and it can be extended to the general cointegrated 
VARMA model. For a cointegrated VARMA(p, q) model with m cointegrating 
factors (m < k), an error correction representation is 

p-i <7 

Ax, = ajS Xf-i + Y - Y 
i=l 7 = 1 

(8.33) 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
432 

MULTIVARIATE TIME SERIES ANALYSIS AND ITS APPLICATIONS 

where a and /? are k x m full-rank matrices. The AR coefficient matrices 4>* are 
functions of the original coefficient matrices 4>y. Specifically, we have 

p 

i=j+1 

ap' = 4>p + 4>p-1 4-b 4>i - I = 

(8.34) 

These results can be obtained by equating coefficient matrices of the AR matrix 
polynomials. The time series fi'xt is unit-root stationary, and the columns of /? are 
the cointegrating vectors of xt. 

Existence of the stationary series j}'xt-\ in the error correction representation 
(8.33) is natural. It can be regarded as a “compensation” term for the overdif¬ 
ferenced system Axt. The stationarity of /?’xt-\ can be justified as follows. The 
theory of unit-root time series shows that the sample correlation coefficient between 
a unit-root nonstationary series and a stationary series converges to zero as the sam¬ 
ple size goes to infinity; see Tsay and Tiao (1990) and the references therein. In an 
error correction representation, xt-\ is unit-root nonstationary, but Axt is station¬ 
ary. Therefore, the only way that Axt can relate meaningfully to xt-i is through 
a stationary series P’xt-\. 

Remark. Our discussion of cointegration assumes that all unit roots are of 
multiplicity 1, but the concept can be extended to cases in which the unit roots have 
different multiplicities. Also, if the number of cointegrating factors m is given, then 
the eiTor correction model in Eq. (8.33) can be estimated by likelihood methods. 
We discuss the simple case of cointegrated VAR models in the next section. Finally, 
there are many ways to construct an error correction representation. In fact, one 
can use any aP'xt^v for 1 < v < p in Eq. (8.33) with some modifications to the 
AR coefficient matrices <I>*. □ 

8.6 COINTEGRATED VAR MODELS 

To better understand cointegration, we focus on VAR models for their simplicity 
in estimation. Consider a ^-dimensional VAR(p) time series x, with possible time 
trend so that the model is 

= lit + + • • • + ®pXt-p + at, 

(8.35) 

where the innovation a, is assumed to be Gaussian and fit = /l0 + fi{t, where fi0 
and n, are ^-dimensional constant vectors. Write 4>(5) = / — &\B — ■ • • — 0>pBp. 
Recall that if all zeros of the determinant |4>(5)| are outside the unit circle, then 
x, is unit-root stationary. In the literature, a unit-root stationary series is said to 
be an /(0) process; that is, it is not integrated. If |4>(1)| = 0, then jc, is unit-root 
nonstationary. For simplicity, we assume that xt is at most an integrated process of 

COINTEGRATED VAR MODELS 

433 

order 1; that is, an 7(1) process. This means that (1 — B)xit is unit-root stationary 
if xu itself is not. 

An error correction model (ECM) for the VAR(p) process xt is 

Ax, = fi, "T T- 4>*Axy_i + • • • + 4>*_jAx,~p+\ + a,, (8.36) 

where <l>* are defined in Eq. (8.34) and II = oc/T = — <&(1). We refer to the term 
IIjcr_i of Eq. (8.36) as the error correction term, which plays a key role in coin¬ 
tegration study. Notice that 4>, can be recovered from the ECM representation via 

<D! =/ + n + $t, 

<*>, i = 2,..., p, 

where <&* = 0, the zero matrix. Based on the assumption that xt is at most 7(1), 
Ax, of Eq. (8.36) is an 7(0) process. 

If xt contains unit roots, then |4>(1)| = 0 so that II = —<&(1) is singular. There¬ 

fore, three cases are of interest in considering the ECM in Eq. (8.36): 

1. Rank(II) = 0. This implies II = 0 and xt is not cointegrated. The ECM of 

Eq. (8.36) reduces to 

Axt = pt + <&*Ax,_i H-f Axt-p+\ + at, 

so that Ax, follows a VAR(/? — 1) model with deterministic trend fit. 

2. Rank(II) = k. This implies that |4>(1)| ^ 0 and x, contains no unit roots; 
that is, xt is 7(0). The ECM model is not informative and one studies x, 

directly. 

3. 0 < Rank(II) = m < k. In this case, one can write II as 

n = a/?', (8.37) 

where a and /? are k x m matrices with Rank(a) = Rank(/?) = m. The ECM 

of Eq. (8.36) becomes 

Ajcf = fi, + aP'xt-i + 4>i Ajc;_i H-h ^*_j Axt-p+\ + a,. (8.38) 

This means that xt is cointegrated with m linearly independent cointegrat¬ 
ing vectors, w, = /3'x,, and has k - m unit roots that give k — m common 

stochastic trends of x,. 

If x, is cointegrated with Rank(II) = m, then a simple way to obtain a presen¬ 
tation of the k - m common trends is to obtain an orthogonal complement matrix 
a± of a; that is, a± is a k x (k - m) matrix such that a'±a — 0, a (k - m) x m 
zero matrix, and use yt = a x,. To see this, one can premultiply the ECM by a'± 

434 

MULTIVARIATE TIME SERIES ANALYSIS AND ITS APPLICATIONS 

and use II = aft' to see that there would be no error correction term in the result¬ 
ing equation. Consequently, the (k — m)-dimensional series yt should have k — m 
unit roots. For illustration, consider the bivariate example of Section 8.5.1. For this 
special series, a = (—1, -0.5)' and a_L = (1, —2)'. Therefore, yt — (1, -2)xt = 
x\t — 2x2r, which is precisely the unit-root nonstationary series y\t in Eq. (8.32). 

Note that the factorization in Eq. (8.37) is not unique because for any m x m 

orthogonal matrix £2 satisfying £2£2' = /, we have a 

ap' = a£2£2'/T = (a£2)(0£2)' = 

where both a* and ft* are also of rank m. Additional constraints are needed to 
uniquely identify a and ft. It is common to require that /?' = [Im, fi\], where Im 
is the m x m identity matrix and /?, is a (k — m) x m matrix. In practice, this may 
require reordering of the elements of x, such that the first m components all have 
a unit root. The elements of a and /? must also satisfy other constraints for the 
process wt = P'xt to be unit-root stationary. For example, consider the case of a 
bivariate VAR(l) model with one cointegrating vector. Here k = 2, m = 1, and the 
ECM is 

Axt = fit + 

ai 

Oil 

[1, p\]xt-i +at. 

Premultiplying the prior equation by /T, using wt-i = P'xt-i, and moving wt-\ to 
the right-hand side of the equation, we obtain 

wt — ft’ fit + (1 + a i + + bt, 

where bt = P'at. This implies that wt is a stationary AR(1) process. Consequently, 
di and P\ must satisfy the stationarity constraint |1 +ai + < 1. 

The prior discussion shows that the rank of II in the ECM of Eq. (8.36) is the 
number of cointegrating vectors. Thus, to test for cointegration, one can examine 
the rank of II. This is the approach taken by Johansen (1988, 1995) and Reinsel 
and Ahn (1992). 

8.6.1 Specification of the Deterministic Function 

Similar to the univariate case, the limiting distributions of cointegration tests depend 
on the deterministic function /if. In this section, we discuss some specifications of 
fi, that have been proposed in the literature. To understand some of the statements 
made below, keep in mind that provides a presentation for the common 
stochastic trends of xt if it is cointegrated. 

1. fit — 0: In this case, all the component series of x, are 1(1) without drift 

and the stationary series wt = (i'xt has mean zero. 

COINTEGRATED VAR MODELS 

435 

2. fit = fi0 = aco, where co is an m-dimensional nonzero constant vector. The 

ECM becomes 

Ax, = + Co) + 4*1 Ax,_i + • • • + 4>*_i Ax,_p+i + at, 

so that the components of x, are 7(1) without drift, but wt have a nonzero 
mean — cq. This is referred to as the case of restricted constant. 

3. fit = (Lq, which is nonzero. Here the component series of xt are 7(1) with 

drift fi0 and wt may have a nonzero mean. 

4. fit = /jL0 + ac\t, where c\ is a nonzero vector. The ECM becomes 

A xt = fiQ + a(P'xt-1 + C\t) + $*Axf_i + • • • + Axf_p+i + at, 

so that the components of xt are 7(1) with drift /,c0 and wt has a linear time 
trend related to c\t. This is the case of restricted trend. 

5. fit = fi0 + /i^t, where /z(- are nonzero. Here both the constant and trend are 
unrestricted. The components of xt are 7(1) and have a quadratic time trend 
and wt have a linear trend. 

Obviously, the last case is not common in empirical work. The first case is not 
common for economic time series but may represent the log price series of some 
assets. The third case is also useful in modeling asset prices. 

8.6,2 Maximum-Likelihood Estimation 

In this section, we briefly outline the maximum-likelihood estimation (MLE) of a 
cointegrated VAR(p) model. Suppose that the data are {xt\t = 1, ..., T). Without 
loss of generality, we write /it = fidt, where dt = [1, t]', and it is understood that 
fit depends on the specification of the previous section. For a given m, which is 
the rank of n, the ECM model becomes 

Ax, = fidt + O'/Ex,-! + 4>*Ax,_i H-1- 4>*_j Ax,_p+i +at, (8.39) 

where t = p + l, ... ,T. A key step in the estimation is to concentrate the likeli¬ 
hood function with respect to the deterministic term and the stationary effects. This 
is done by considering the following two multivariate linear regressions: 

Ax, = y0dt + S2x Ax,_i + • • • + Slp-\ Ax,_p+i + ut, (8.40) 

x,_i = y\dt + Si Ax,_i + • • • + Sp_i Ax,_p+i + vt. (8.41) 

Let Uj and vt be the residuals of Eqs. (8.40) and (8.41), respectively. Define the 

sample covariance matrices 

>oo; 

T 

i 1 
- utut, 301 = 

t=p+1 

T 

\ ' A / 0 
> , Utvt, S 

it: 

t=p+1 

1 

E Vtv'r 

t=p+l 

436 

MULTIVARIATE TIME SERIES ANALYSIS AND ITS APPLICATIONS 

Next, compute the eigenvalues and eigenvectors of SioSqqSoi with respect to Sn. 
This amounts to solving the eigenvalue problem 

|ASn — Sio-SqJSoiI = 0- 

Denote the eigenvalue and eigenvector pairs by (A.;, e,), where Ai > A2 > • • • > A*. 
Here the eigenvectors are normalized so that e'S\\e = /, where e = [e\, ..., e^] 
is the matrix of eigenvectors. 

/V A A -A 

The unnormalized MLE of the cointegrating vector (i is (l = [e \,... ,em] and 
from which we can obtain an MLE for /} that satisfies the identifying constraint 
and normalization condition. Denote the resulting estimate by f$c with the subscript 
c signifying constraints. The MLE of other parameters can then be obtained by the 
multivariate linear regression 

Ax, = [id, + aPcx,-\ + <&{ Ax,_i + • • • + <&*_[ At,_p+i + a,. 

The maximized value of the likelihood function based on m cointegrating vectors is 

m 

^max ^ l^ool J~[(1 — ^;)- 

i=l 

This value is used in the maximum-likelihood ratio test for testing Rank(II) = m. 
Finally, estimates of the orthogonal complements of a and /? can be obtained using 

a± — S00 Sn[^m_f.i, ..., ek], = 5n[em+i, ..., e/,]. 

8.6.3 Cointegration Test 

For a specified deterministic term fi„ we now discuss the maximum-likelihood test 
for testing the rank of the II matrix in Eq. (8.36). Let H(m) be the null hypothesis 
that the rank of IT is m. For example, under H{0), Rank(II) = 0 so that II = 0 
and there is no cointegration. The hypotheses of interest are 

H(0) C ••• C Him) C ••• C H{k). 

For testing purpose, the ECM in Eq. (8.39) becomes 

Axf = lid, + n*?_, + Axf_i + • • • + Axf_p+1 + a,, 

where t = p + 1,..., T. Our goal is to test the rank of II. Mathematically, the 
rank of II is the number of nonzero eigenvalues of II, which can be obtained if a 
consistent estimate of II is available. Based on the prior equation, which is in the 
form of a multivariate linear regression, we see that II is related to the covariance 
matrix between x?_j and Ax, after adjusting for the effects of d, and Ax,_f for i 

COINTEGRATED VAR MODELS 

437 

= l,p — 1. The necessary adjustments can be achieved by the techniques of 
multivariate linear regression shown in the previous section. Indeed, the adjusted 
series of xt-\ and Ax, are v, and u,, respectively. The equation of interest for the 
cointegration test then becomes 

u, = ni, + a,. 

Under the normality assumption, the likelihood ratio test for testing the rank of 
II in the prior equation can be done by using the canonical correlation analysis 
between u, and vt. See Johnson and Wichem (1998) for information on canonical 
correlation analysis. The associated canonical correlations are the partial canonical 
correlations between A;c,_i and xt-\ because the effects of d, and Ajchave 
been adjusted. The quantities {k, } are the squared canonical correlations between 
u, and vt. 

Consider the hypotheses 

Ho : Rank(Il) = m versus Ha : Rank(II)>m. 

Johansen (1988) proposes the likelihood ratio (LR) statistic 

k 

LR&(m) = —(T - p) J2 ln(! - *i) (8.42) 

i=m+1 

to perform the test. If Rank(II) = m, then X, should be small for i > m and hence 
LRtr(m) should be small. This test is referred to as the trace cointegration test. Due 
the presence of unit roots, the asymptotic distribution of LRtr(m) is not chi squared 
but a function of standard Brownian motions. Thus, critical values of LRti-fra) must 
be obtained via simulation. 

Johansen (1988) also considers a sequential procedure to determine the number 

of cointegrating vectors. Specifically, the hypotheses of interest are 

H0 : Rank(II) = m versus Ha : Rank(II) = m -f 1. 

The LR ratio test statistic, called the maximum eigenvalue statistic, is 

LRmax(m) = -(T - p) ln(l - Xm+i). 

Again, critical values of the test statistics are nonstandard and must be evaluated 
via simulation. 

8.6.4 Forecasting of Cointegrated VAR Models 

The fitted ECM can be used to produce forecasts. First, conditioned on the estimated 
parameters, the ECM equation can be used to produce forecasts of the differenced 
series Axt. Such forecasts can in turn be used to obtain forecasts of xt. A difference 
between ECM forecasts and the traditional VAR forecasts is that the ECM approach 
imposes the cointegration relationships in producing the forecasts. 

438 

MULTIVARIATE TIME SERIES ANALYSIS AND ITS APPLICATIONS 

(a) 

(b) 

Figure 8.12 Time plots of weekly U.S. interest rate from December 12, 1958, to August 6, 2004. 
(a) The 3-month Treasury bill rate and (b) 6-month Treasury bill rate. Rates are from secondary market. 

8.6.5 An Example 

To demonstrate the analysis of cointegrated VAR models, we consider two weekly 
U.S. short-term interest rates. The series are the 3-month Treasury bill (TB) rate 
and 6-month Treasury bill rate from December 12, 1958, to August 6, 2004, for 
2383 observations. The TB rates are from the secondary market and obtained from 
the Federal Reserve Bank of St. Loius. Figure 8.12 shows the time plots of the 
interest rates. As expected, the two series move closely together. 

Our analysis uses the S-Plus software with commands VAR for VAR analy¬ 
sis, coint for cointegration test, and VECM for vector error correction estima¬ 
tion. Denote the two series by tb3m and tb6m and define the vector series x, = 
(tb3mr, tb6mr)'. The augmented Dickey-Fuller unit-root tests fail to reject the 
hypothesis of a unit root in the individual series; see Chapter 2. Indeed, the test 
statistics are -2.34 and -2.33 with p value about 0.16 for the 3-month and 6-month 
interest rate when an AR(3) model is used. Thus, we proceed to VAR modeling. 

For the bivariate series xt, the BIC criterion selects a VAR(3) model: 

> x=cbind(tb3m,tb6m) 

> y=data.frame(x) 

> ord.choice$ar.order 

[1] 3 

COINTEGRATED VAR MODELS 

439 

To perform a cointegration test, we choose a restricted constant for /jl, because 
there is no reason a priori to believe the existence of a drift in the U.S. interest 
rate. Both Johansen’s tests confirm that the two series are cointegrated with one 
cointegrating vector when a VAR(3) model is entertained. 

> cointst.rc=coint(x,trend='rc', lags=2) % lags = p-1. 

> cointst.rc 

Call: 

coint(Y = x, lags = 2, trend = "rc") 

Trend Specification: 

HI*(r): Restricted constant 

Trace tests sign, at the 5% level are flagged by ' +'. 

Trace tests sign, at the 1% level are flagged by '++'. 

Max Eig. tests sign, at the 5% level are flagged by ' *'. 

Max Eig. tests sign, at the 1% level are flagged by '**'. 

Tests for Cointegration Rank: 

Eigenvalue Trace Stat 

95% CV 99% CV 

H ( 0)+ + * * 0.0322 83.2712 

19.96 24.60 

H (1) 0.0023 5.4936 

9.24 12.97 

Max Stat 95% CV 

H(0)++* * 77.7776 15.67 

99% CV 
20.20 

H (1) 5.4936 9.24 

12.97 

Next, we perform the maximum-likelihood estimation of the specified cointe¬ 

grated VAR(3) model using an ECM presentation. The results are as follows: 

> vecm.fit=VECM(cointst.rc) 

> summary(vecm.fit) 

Call: 

VECM(test = cointst.rc) 

Cointegrating Vectors: 

coint.1 
1.0000 

tb6m 

-1.0124 

(std.err) 

0.0086 

(t.stat)  -118.2799 

Intercept* 

(std.err) 

(t.stat) 

0.2254 

0.0545 

4.1382 

VECM Coefficients: 

440 

MULTIVARIATE TIME SERIES ANALYSIS AND ITS APPLICATIONS 

tb3m 

tb6m 

coint. 1  -0.0949  -0.0211 

(std.err) 

0.0199  0.0179 

(t.stat)  -4.7590  -1.1775 

tb3m.lagl  0.0466  -0.0419 

(std.err) 

0.0480  0.0432 

(t.stat) 

0.9696  -0.9699 

tb6m.lagl  0.2650  0.3164 

(std.err) 

0.0538  0.0484 

(t.stat) 

4.9263 

6.5385 

tb3m.Iag2  -0.2067  -0.0346 

(std.err) 

0.0481  0.0433 

(t.stat)  -4.2984  -0.8005 

tb6m.Iag2  0.2547 

0.0994 

(std.err) 

0.0543 

0.0488 

(t.stat) 

4.6936  2.0356 

Regression Diagnostics: 

tb3m tb6m 

R-squared 0.1081 0.0913 

Adj. R-squared 0.1066 0.0898 

Resid. Scale 0.2009 0.1807 

> plot(vecm.fit) 

Make a plot selection (or 0 to exit): 

1: plot: All 

2: plot: Response and Fitted Values 

3: plot: Residuals 

13: plot: PACF of Squared Cointegrating Residuals 

Selection: 

As expected, the output shows that the stationary series is wt ~ tb3m, — tb6mf 

and the mean of wt is about —0.225. The fitted ECM is 

Axt 

-0.09 
-0.02 

(u>t~ i + 0.23) + 

0.05 0.27 
-0.04 0.32 

+ 

-0.21 0.25 
-0.03 0.10 

Ax,-2 + at. 

and the estimated standard errors of ait are 0.20 and 0.18, respectively. Ade¬ 
quacy of the fitted ECM can be examined via various plots. For illustration, 
Figure 8.13 shows the cointegrating residuals. Some large residuals are shown 

COINTEGRATED VAR MODELS 

441 

Figure 8.13 Time plot of cointegrating residuals for an ECM fit to weekly U.S. interest rate series. 

Data span is from December 12, 1958, to August 6, 2004. 

in the plot, which occurred in the early 1980s when the interest rates were high 

and volatile. 

Finally, we use the fitted ECM to produce 1-step- to 10-step-ahead forecasts for 

both Ax, and xt. The forecast origin is August 6, 2004. 

> vecm.fst=predict(vecm.fit, n.predict=10) 

> summary(vecm.fst) 

Predicted Values with Standard Errors: 

tb3m tb6m 

1- step-ahead -0.0378 -0.0642 

(std.err) 0.2009 0.1807 

2- step-ahead -0.0870 -0.0864 

(std.err) 0.3222 0.2927 

10-step-ahead -0.2276 -0.1314 

(std.err) 0.8460 0.8157 

> plot(vecm.fst,xold=diff(x),n.old=12) 

> vecm.fit.level=VECM(cointst.rc,levels=T) 

> vecm.fst.level=predict(vecm.fit.level, n.predict=10) 

> summary(vecm.fst.level) 

Predicted Values with Standard Errors: 

1-step-ahead 1.4501 1.7057 

tb3m tb6m 

442 

MULTIVARIATE TIME SERIES ANALYSIS AND ITS APPLICATIONS 

2375 2380 2385 2390 

Figure 8.14 Forecasting plots of fitted ECM model for weekly U.S. interest rate series. Forecasts are 
for differenced series and forecast origin is August 6, 2004. 

(std.err) 0.2009 0.1807 

2-step-ahead 1.4420 1.7017 

(std.err) 0.3222 0.2927 

10-step-ahead 1.4722 1.7078 

(std.err) 0.8460 0.8157 

> plot(vecm.fst.level, xold=x, n.old=50) 

The forecasts are shown in Figures 8.14 and 8.15 for the differenced data and the 
original series, respectively, along with some observed data points. The dashed 
lines in the plots are pointwise 95% confidence intervals. Because of unit-root 
nonstationarity, the intervals are wide and not informative. 

Remark. The package urea of R can be used to perform Johansen’s co¬ 
integration test. The command is ca. jo. It requires specification of some sub¬ 
commands. See the section of pairs trading for demonstration. □ 

8.7 THRESHOLD COINTEGRATION AND ARBITRAGE 

In this section, we focus on detecting arbitrage opportunities in index trading by 
using multivariate time series methods. We also demonstrate that simple univariate 

THRESHOLD COINTEGRATION AND ARBITRAGE 

443 

2340 2350 2360 2370 2380 2390 

Figure 8.15 Forecasting plots of fitted ECM model for weekly U.S. interest rate series. Forecasts are 

for interest rates and forecast origin is August 6, 2004. 

Index 

nonlinear models of Chapter 4. can be extended naturally to the multivariate case 

in conjunction with the idea of cointegration. 

Our study considers the relationship between the price of the S&P 500 index 
futures and the price of the shares underlying the index on the cash market. Let 
ft l be the log price of the index futures at time t with maturity t, and let st be 
the log price of the shares underlying the index on the cash market at time t. A 
version of the cost-of-carry model in the finance literature states 

ft,i ~ st = 0,,£ - — t) + z*, (8.43) 

where r,x is the risk-free interest rate, qt%i is the dividend yield with respect to 
the cash price at time t, and (i - t) is the time to maturity of the futures contract; 
see Brenner and Kroner (1995), Dwyer, Locke, and Yu (1996), and the references 

therein. 

The z* process of model (8.43) must be unit-root stationary; otherwise there 
exist persistent arbitrage opportunities. Here an arbitrage trading consists of simul¬ 
taneously buying (short-selling) the security index and selling (buying) the index 
futures whenever the log prices diverge by more than the cost of carrying the index 
over time until maturity of the futures contract. Under the weak stationarity of z*, 

444 

MULTIVARIATE TIME SERIES ANALYSIS AND ITS APPLICATIONS 

for arbitrage to be profitable, z* must exceed a certain value in modulus determined 
by transaction costs and other economic and risk factors. 

It is commonly believed that the ft^ and st series of the S&P 500 index contain 
a unit root, but Eq. (8.43) indicates that they are cointegrated after adjusting for 
the effect of interest rate and dividend yield. The cointegrating vector is (1,-1) 
after the adjustment, and the cointegrated series is z*. Therefore, one should use 
an error correction form to model the return series r, = (A f,, Ast)', where Aft = 

ft,i — ft-i,e and Ast = st — st~i, where for ease in notation we drop the maturity 
time t from the subscript of Aft. 

8.7,1 Multivariate Threshold Model 

In practice, arbitrage tradings affect the dynamic of the market, and hence the 
model for rt may vary over time depending on the presence or absence of arbitrage 
tradings. Consequently, the prior discussions lead naturally to the following model: 

c> + ELi + 0iz<-i 
C2 + EL, + fc-, + a'2) 
C3 + ZI, *J3)r,-,+/J3z,_,+a<3> 

if zt-1 < Yu 
if Y\ < zt-1 < Y2, 
if Y2 < zt-1, 

(8.44) 

where zt — 100z*, y\ < 0 < y2 are two real numbers, and (a)0} are sequences 
of two-dimensional white noises and are independent of each other. Here we use 
zt = 100z* because the actual value of z* is relatively small. 

The model in Eq. (8.44) is referred to as a multivariate threshold model with 
three regimes. The two real numbers y\ and y2 are the thresholds and zt-\ is 
the threshold variable. The threshold variable zt-i is supported by the data; see 
Tsay (1998). In general, one can select zt-d as a threshold variable by considering 
d e {1, ... , do}, where do is a prespecified positive integer. 

Model (8.44) is a generalization of the threshold autoregressive model of 
Chapter 4. It is also a generalization of the error correlation model of Eq. 
(8.33). As mentioned earlier, an arbitrage trading is profitable only when z* or, 
equivalently, zt is large in modulus. Therefore, arbitrage tradings only occurred in 
regimes 1 and 3 of model (8.44). As such, the dynamic relationship between ft t 
and s, in regime 2 is determined mainly by the normal market force, and hence 
the two series behave more or less like a random walk. In other words, the two 
log prices in the middle regime should be free from arbitrage effects and, hence, 
free from the cointegration constraint. From an econometric viewpoint, this means 
that the estimate of /?2 in the middle regime should be insignificant. 

In summary, we expect that the cointegration effects between the log price of 
the futures and the log price of security index on the cash market are significant 
in regimes 1 and 3, but insignificant in regime 2. This phenomenon is referred to 
as a threshold cointegration; see Balke and Fomby (1997). 

THRESHOLD COINTEGRATION AND ARBITRAGE 

445 

0 2000 4000 6000 

Time index 

(b) 

Figure 8.16 Time plots of 1-minute log returns of S&P 500 index futures and cash prices and asso¬ 
ciated threshold variable in May 1993: (a) log returns of index futures, (b) log returns of index cash 

prices, and (c) zr series. 

8.7.2 The Data 

The data used in this case study are the intraday transaction data of the S&P 500 
index in May 1993 and its June futures contract traded at the Chicago Mercantile 
Exchange; see Forbes, Kalb, and Kofman (1999), who used the data to construct a 
minute-by-minute bivariate price series with 7060 observations. To avoid the undue 
influence of unusual returns, I replaced 10 extreme values (5 on each side) by the 
simple average of their two nearest neighbors. This step does not affect the qual¬ 
itative conclusion of the analysis but may affect the conditional heteroscedasticity 
in the data. For simplicity, we do not consider conditional heteroscedasticity in the 
study. Figure 8.16 shows the time plots of the log returns of the index futures and 
cash prices and the associated threshold variable zt = 100z* of model (8.43). 

8.7.3 Estimation 

A formal specification of the multivariate threshold model in Eq. (8.44) includes 
selecting the threshold variable, determining the number of regimes, and choosing 

446 

MULTIVARIATE TIME SERIES ANALYSIS AND ITS APPLICATIONS 

the order p for each regime. Interested readers are referred to Tsay (1998) and 
Forbes, Kalb, and Kofman (1999). The thresholds y\ and yi can be estimated 
by using some information criteria [e.g., the Akaike information criterion (AIC) 
or the sum of squares of residuals]. Assuming p = 8, d e {1,2, 3,4], y\ e 
[-0.15,-0.02], and yi € [0.025,0.145], and using a grid search method with 
300 points on each of the two intervals, the AIC selects Zt-i as the threshold 
variable with thresholds y\ = —0.0226 and p2 = 0.0377. Details of the parameter 
estimates are given in Table 8.8. 

From Table 8.8, we make the following observations. First, the t ratios of /?2 in 
the middle regime show that, as expected, the estimates are insignificant at the 5% 
level, confirming that there is no cointegration between the two log prices in the 
absence of arbitrage opportunities. Second, Aft depends negatively on A/r_i in all 
three regimes. This is in agreement with the bid-ask bounce discussed in Chapter 5. 
Third, past log returns of the index futures seem to be more informative than the 
past log returns of the cash prices because there are more significant t ratios in 
Aft-i than in Ast-{. This is reasonable because futures series are in general more 
liquid. For more information on index arbitrage, see Dwyer, Locke, and Yu (1996). 

8.8 PAIRS TRADING 

Pairs trading is a market-neutral trading strategy. There are several versions of pairs 
trading in the equity markets. In this section, we focus on the statistical arbitrage 
pairs trading, which makes use of the ideas of cointegration and error correction 
model discussed in the chapter. Our discussion will be brief. For more information 
concerning pairs trading and statistical arbitrage, see Vidyamurthy (2004) and Pole 
(2007). 

The general theme for trading in the equity markets is to buy undervalued stocks 
and sell overvalued ones. However, the true price of a stock is hard to assess. 
Pairs trading attempts to resolve this difficulty using the idea of relative pricing. 
Based on the arbitrage pricing theory (APT) in finance, if two stocks have similar 
characteristics, then the prices of both stocks must be more or less the same. If 
the prices differ, then it is likely that one of the stocks is overpriced and the other 
underpriced. Pairs trading involves selling the higher priced stock and buying the 
lower priced stock with the hope that the mispricing will correct itself in the future. 
Note that the true prices of the two stocks are not important. The observed prices 
may be wrong. What is important is that the observed prices be the same. The 
gap (properly scaled) between the two observed prices is called the spread. For 
pairs trading, the greater the spread, the larger the magnitude of mispricing and the 
greater the profit potential. Before discussing a trading strategy, we first introduce 
the theoretical framework. 

8.8.1 Theoretical Framework 

Consider two stocks. Let Pit be the observed price of stock i at time t and pit = 
In(Pn) be the corresponding log price. As mentioned in earlier chapters, it is 

PAIRS TRADING 

447 

TABLE 8.8 Least-Squares Estimates and Their t Ratios of Multivariate Threshold 
Model in Eq. (8.43) for S&P 500 Index Data in May 1993° 

Regime 1 

Regime 2 

Regime 3 

Af, 

A st 

Afr 

As, 

Af, 

As, 

00 
t 

A/,-i 
t 

A/,-2 
t 

1

m

<

t 

A/,-4 
t 

A/,-5 
t 

Aft-6 
t 

>

i

t 

<

1

o
o

t 
As,-\ 
t 

Aj(_2 
t 

Ast-3 

t 
As,-A 
t 

As,-5 

t 

Av,_6 
t 

Ast-i 

t 

Ast-s 

t 

Zf-1 
t 

0.00002 
(1.47) 
-0.08468 
(-3.83) 
-0.00450 
(-0.20) 
0.02274 
(0.95) 
0.02429 
(0.99) 
0.00340 
(0.14) 
0.00098 
(0.04) 
-0.00372 
(-0.15) 
0.00043 
(0.02) 
-0.08419 
(-2.01) 
-0.05103 
(-1.18) 
0.07275 
(1.65) 
0.04706 
(1.03) 
0.08118 
(1.77) 
0.04390 
(0.96) 
-0.03033 
(-0.70) 
-0.02920 
(-0.68) 
0.00024 
(1.34) 

0.00005 
(7.64) 
0.07098 
(6.15) 
0.15899 
(13.36) 
0.11911 
(9.53) 
0.08141 
(6.35) 
0.08936 
(7.10) 
0.07291 
(5.64) 
0.05201 
(4.01) 
0.00954 
(0.76) 
0.00264 
(0.12) 
0.00256 
(0.11) 
-0.03631 
(-1.58) 
0.01438 
(0.60) 
0.02111 
(0.88) 
0.04569 
(1.92) 
0.02051 
(0.91) 
0.03018 
(1.34) 
0.00097 
(10.47) 

0.00000 
(-0.07) 
-0.03861 
(-1.53) 
0.04478 
(1.85) 
0.07251 
(3.08) 
0.01418 
(0.60) 
0.01185 
(0.51) 
0.01251 
(0.54) 
0.02989 
(1.34) 
0.01812 
(0.85) 
-0.07618 
(-1.70) 
-0.10920 
(-2.59) 
-0.00504 
(-0.12) 
0.02751 
(0.71) 
0.03943 
(0.97) 
0.01690 
(0.44) 
-0.08647 

(-2.09) 
0.01887 
(0.49) 
-0.00010 
(-0.30) 

0.00000 
(0.53) 
0.04037 
(3.98) 
0.08621 
(8.88) 
0.09752 
(10.32) 
0.06827 
(7.24) 
0.04831 
(5.13) 
0.03580 
(3.84) 
0.04837 
(5.42) 
0.02196 
(2.57) 
-0.05633 
(-3.14) 
-0.01521 
(-0.90) 
0.01174 

(0.71) 
0.01490 
(0.96) 
0.02330 
(1.43) 
0.01919 
(1.25) 
0.00270 
(0.16) 
-0.00213 
(-0.14) 
0.00012 
(0.86) 

-0.00001 
(-0.74) 
-0.04102 
(-1.72) 
-0.02069 
(-0.87) 
0.00365 
(0.15) 
-0.02759 

(-1.13) 
-0.00638 
(-0.26) 
-0.03941 
(-1.62) 
-0.02031 
(-0.85) 
-0.04422 
(-1.90) 
0.06664 
(1.49) 
0.04099 
(0.92) 
-0.01948 
(-0.44) 
0.01646 
(0.37) 
-0.03430 
(-0.83) 
0.06084 
(1.45) 
-0.00491 
(-0.13) 
0.00030 
(0.01) 
0.00025 

(1.41) 

-0.00005 
(-6.37) 
0.02305 

(1.96) 
0.09898 
(8.45) 
0.08455 
(7.02) 
0.07699 
(6.37) 
0.05004 
(4.07) 
0.02615 
(2.18) 
0.02293 
(1.95) 
0.00462 
(0.40) 
0.11143 
(5.05) 
-0.01179 
(-0.53) 
-0.01829 
(-0.84) 
0.00367 
(0.17) 
-0.00462 
(-0.23) 
-0.00392 
(-0.19) 
0.03597 
(1.90) 
0.02171 
(1.14) 
0.00086 
(9.75) 

“The numbers of data points for the three regimes are 2234, 2410, and 2408, respectively. 

 
 
 
 
 
 
 
 
448 

MULTIVARIATE TIME SERIES ANALYSIS AND ITS APPLICATIONS 

reasonable to assume that pit is unit-root nonstationary and follows a random-walk 
model; that is, plt = Pi,t-i + rit, where {rit} is the return and forms a sequence 
of uncorrelated innovations. If the two stocks have similar risk factors, then they 
should have similar returns based on APT. Therefore, p\t and p2t are likely to be 
driven by a common component and are cointegrated. In other words, there exists a 
linear combination wt = p\t — yp2t, which is unit-root stationary and, hence, mean 
reverting. The two price series {p\t} and {p2t} thus assume an error correction form 

Pit - Pi,t-1 
P2t ~ P2,t-l 

(wt-1 ~ fiw) + 

(8.45) 

where /iw = E(wt) denotes the mean of w,. The four parameters y, p,w, ol\, and 
ot2 can be estimated, for instance, by the maximum-likelihood or least-squares 
methods; see Section 8.6.2. We refer to the stationary series wt as the spread 
between the two log stock prices. 

The left-hand side of Eq. (8.45) consists the log returns of the two stocks. The 
equation says that the returns depend on wt-\, which is the stationary. Specifically, 
wt-1 — pw denotes the deviation from the log-run equilibrium between the two 
stocks. Equation (8.45) shows that, for cointegrated stocks, the returns depend on 
the past deviation from equilibrium. The coefficients a\ and a.2 show the effect of 
past deviation on the returns r\t and /*&, respectively. In practice, a\ and 012 should 
have opposite signs, indicating reversion to the equilibrium. 

Next, consider a portfolio with long one share of stock 1 and short y shares of 

stock 2. The return of the portfolio for a given time period i is 

rp,t+i = (pi,t+i - Pit) - y(P2j+i - Pit) 

= (pilt+i - yp2,t+i) - (pu - ypit) 

= wt+i - wt. 

Therefore, the return rpJ+i of the portfolio is the increment of the spread in the 
time period. As expected, the return of the portfolio does not depend on the mean 
of wt. 

8.8.2 Trading Strategy 

The idea behind a pairs-trading strategy is to trade on the oscillations about the 
equilibrium value of the spread. The oscillations in spread occur because the spread 
is mean reverting. Since the equilibrium value is the mean of wt, that is, pw, we 
can put on a trade when wt deviates substantially from its mean and unwind the 
trade when the equilibrium is restored. In practice, how big the deviation needs 
to be in order for the trading to be profitable depends on several factors. Trading 
costs, marginal interest rates, and bid-ask spreads of the two stocks are three 
obvious factors. Mathematically, let r/ be the cost involved in carrying out a pairs 
trading. Let A be a target deviation of wt from its mean p,w for pairs trading. Then, 
conditioned on 2A > rj, a simple trading strategy is as follows: 

PAIRS TRADING 

449 

• Buy a share of stock 1 and short y shares of stock 2 at time t if wt = 

Pit ~ YPit = P>w~ A- 

• Unwind the position at time t + i (i > 0) if wt+i = Pij+i — YPz,t+i — P'w + 

A. 

One can identify the time point t so long as A is not too large compared with 
the standard deviation of wt. The time point t + i will occur because of the mean 
reverting of the spread series. In this particular instance, the return of the portfolio 
wt+i — wt = 2 A and the net profit of the trade is 2 A — rj > 0. 

Discussion. The aforementioned trading strategy is just one of many possibil¬ 
ities. For instance, if A > rj, one can unwind the position when wt+i = p,w. The 
net profit of the pairs trading then is A — r] > 0. This may result in more trans¬ 
actions and trading costs, but it shortens the holding period of the portfolio. If A 
is negative, then one can short one share of stock 1 and buy y shares of stock 2 
to make a net profit -2A - rj. The quantity rj is the threshold for trading and is 
likely to depend on several factors such as transaction fees and bid-ask spreads of 

the two stocks. □ 

8.8.3 Simple Illustration 

To demonstrate pairs trading, we consider two stocks traded on the New York 
Stock Exchange. The two companies are the Billiton Ltd. of Australia and the 
Vale S.A. of Brazil with stock symbols BHP and VALE, respectively. BHP of 
Australia is a natural resources company with business in Australia, the Americans, 
and Southern Africa. Vale of Brazil is a worldwide metals and mining company. 
Thus, both multinational companies belong to the natural resources industry and 
encounter similar risk factors. The daily prices of the two stocks were downloaded 
from Yahoo Finance, and we employ adjusted closing prices from July 1, 2002, to 

March 31, 2006, in our study. 

Figure 8.17 shows the time plots of the daily log prices of the two stocks 
(adjusted closing prices). The upper plot is for the BHP stock. From the plots, the 
prices of the two stocks exhibit certain characteristics of comovement. Let pu and 
p2t be the daily log closing prices of BHP and VALE, respectively. We analyze 
the series using both the least-squares and maximum-likelihood methods. 

Least-Squares Estimation 
A simple way to verify that the two stocks are suitable for pairs trading is to check 
the cointegration of their log stock prices. To this end, we consider the simple 
linear regression p\t = /3q + ji\P2t + RY where wt denotes the residual series. For 

the BHP and VALE stocks, we have 

p\t = 1.823 + O.lllpj, + wt, 

ow = 0.044. 

450 

MULTIVARIATE TIME SERIES ANALYSIS AND ITS APPLICATIONS 

Figure 8.17 Daily log (adjusted) closing prices of BHP and VALE stocks from July 1, 2002, to March 
31, 2006. Upper plot is for BHP stock. 

Figure 8.18(a) shows the time plot of the residual series w,. The plot shows that the 
residual series has certain characteristics of a stationary time series. In particular, it 
has mean zero and fluctuates around its mean within a fixed range. Figure 8.18(b) 
gives the sample ACF of wt. The ACFs decay exponentially, supporting that wt 
is indeed stationary. To further confirm the stationarity assertion, we fit an AR(2) 
model to wt and obtain 

(1 - 0.8055 - 0A22B2)w, = at, aa = 0.018. 

Following the discussion of Chapter 2, we can obtain the two characteristic 
roots of the fitted AR(2) model. Indeed, the model can be rewritten as 
(1 — 0.9355)(1 — 0.1305)u)f =at. Hence, wt is stationary. Finally, we conduct 
an augmented Dickey-Fuller unit-root test on w, using an AR(2) model and find 
that the test statistic is -6.04 with a p value of 0.01. The unit-root hypothesis is 
clearly rejected. 

Maximum-Likelihood Estimation 

A formal approach to verify the cointegration of the two log stock prices is to per¬ 
form a cointegration test. Let x, = (pu, pit)'■ Using information criteria, a VAR(l) 
model is specified for xt. We then conduct cointegration tests with restricted and 
unrestricted constant. Both tests give similar results so that we only report the 
results for the case of restricted constant. 

PAIRS TRADING 

Lfi 

451 

Figure 8.18 Results of least-squares estimation: (a) Time plot of the estimated spread between BHP 

and VALE daily log stock prices, (b) Sample autocorrelation functions of estimated spread. 

Lag 

(b) 

> coint2=coint(xx,trends"rc") 

> coint2 
coint(Y = xt, trend = "rc") 

Trend Specification: 
Hi*(r): Restricted constant 

Trace tests signif. at the 5% level are flagged by ' +'. 
% level are flagged by '++'. 

Trace tests signif. at the 
Max Eigenvalue tests signif 

at the 5% level are 

flagged by ' *'. 

Max Eigenvalue tests signif 

at the 1% level are 

flagged by '* *'. 

Tests for Cointegration Rank: 

Eigenvalue TraceSt 95%-CV 99%-CV Max-St 95%-CV 99%- 

CV 
H(0)++** 0.0415 47.7400 19.960 24.600 39.965 15.670 20.200 
H(1) 0.0082 7.7748 9.240 12.970 7.774 9.240 12.970 

The test confirms that xt is cointegrated. Next, we perform the maximum- 

likelihood estimation of the error correction model. The results are given below: 

> n3=VECM(coint2) 

> summary(n3) 

452 

MULTIVARIATE TIME SERIES ANALYSIS AND ITS APPLICATIONS 

VECM(test =  coint2) 

Cointegrating Vectors 

coint.1 
1.0000 

vale 

-0.7177 

(std.err) 

0.0112 

(t.stat) 

-64.0913 

Intercept* 

-1.8144 

(std.err) 

0.0169 

(t.stat)  -107.0430 

VECM Coefficients: 

bhp 

vale 

coint.1  -0.0671  0.0263 

(std.err) 

0.0145  0.0168 

(t.stat)  -4.6462  1.5659 

bhp.lagl  -0.1119  0.0659 

(std.err) 

0.0366  0.0425 

(t.stat)  -3.0596  1.5516 

vale.lagl  0.0732 

0.0445 

(std.err) 

0.0320  0.0371 

(t.stat) 

2.2920  1.1986 

Regression Diagnostics: 

bhp vale 

R-squared 0.0370 0.0104 

Adj. R-squared 0.0350 0.0083 

Resid. Scale 0.0193 0.0224 

Based on the estimation result, we have the model 

Axt 

-0.067 
0.026 

(wt-i — 1.81) + 

-0.11 0.07 
0.07 0.04 

where the estimated standard errors of ait are 0.019 and 0.022, respectively. In 
addition, the spread series is wt = p\t — 0.718/?2/> which is stationary with mean 
1.81. Clearly, the result is very close to that of the least-squares estimation. In 
particular, the y parameter for the pairs trading is y = 0.718. Also, as expected, 
a\ is negative whereas <*2 is positive. 

Trading Strategy 

Since the standard error of the spread series wt is 0.044, we can select A = 0.045, 
which is slightly greater than one standard error of wt, for pairs trading. This choice 

PAIRS TRADING 

453 

Figure 8.19 Time plot of fitted spread series between daily log prices of BHP and VALE stocks. 
Three horizontal lines denotes [jlw, iiw + 0.045, and /iw — 0.045 with [iw = E(wt). 

of A ensures that the probability for the spread wt to deviate A away from its mean 
is not small. In fact, under the normality assumption, the probability is about 30%. 
Figure 8.19 shows the time plot of the spread series wt of the fitted error correction 
model. Three horizontal lines are imposed on the plot. They are [iw, [iw + 0.045, 
and ixw - 0.045 with the latter two serving as boundaries for pairs trading. Since 
wt varies from the lower boundary to the upper one (or from the upper boundary 
to the lower one) several times, there are many pairs-trading opportunities. From 
the discussion of Section 8.8.2, the log return of each pairs trading is 2A = 0.09, 
which is not small. A more realistic demonstration is to implement the trading in 
a out-of-sample period. However, the example shows that pairs trading is feasible. 
Finally, an important question in pairs trading is to identify the cointegrated 
pairs of stocks. There are some procedures available in the literature. It seems 
reasonable to consider pairs of stocks that have similar risk factors. In other words, 

one should make use of finance theory to guide the selection. 

R Demonstration 
The following output has been edited: 

> library(urea) 
> help(ca.jo) 
> da=read.table("d-bhp0206.txt",header=T) 
> dal=read.table("d-vale0206.txt",header=T) 

> bhp=log(da[,9]) 

454 

MULTIVARIATE TIME SERIES ANALYSIS AND ITS APPLICATIONS 

> vale=log(dal[, 9] ) 
> ml=lm(bhp~vale) 
> summary(ml) 
Call: 
lm(formula = bhp ~ vale) 

Coefficients: 

Estimate Std. Error t value Pr(>|t|) 

(Intercept) 1.822648 0.003662 497.7 >2e-16 *** 
vale 0.716664 0.002354 304.4 >2e-16 *** 

Residual standard error: 0.04421 on 944 degrees of freedom 
Multiple R-squared: 0.9899, Adjusted R-squared: 0.9899 
F-statistic: 9.266e+04 on 1 and 944 DF, p-value: < 2.2e-16 

> wt=ml$residuals 
> m3=arima(wt,order=c(2,0,0),include,mean=F) 
> m3 
Call: 

arima(x = wt, order = c(2, 0, 0), include.mean = F) 

Coefficients: 

arl ar2 
0.8051 0.1219 
s.e. 0.0322 0.0325 

sigma/'2 estimated as 0.0003326: log likelihood=2444.76 
> pl=c(1,-m3$coef) 
> x=polyroot(pi) 
> x 

[1] 1.069100+0i -7.675365-0i 
> l/Mod(x) 
[1] 0.9353661 0.1302870 

> xt=cbind(bhp,vale) 
> mm=ar(xt) 
> mm$order 

[1] 2 

> cot=ca.jo(xt,ecdet="const",type='trace',K=2, 
spec='transitory') 
> summary(cot) 
###################### 

# Johansen-Procedure # 
###################### 

Test type: trace statistic, without linear trend and 

constant in cointegration 

Eigenvalues (lambda): 

PAIRS TRADING 

455 

[1] 4.148282e-02 8.206470e-03 -4.610389e-18 

Values of teststatistic and critical values of test: 

test lOpct 5pct lpct 

r <= 1 | 7.78 7.52 9.24 12.97 
r = 0 j 47.77 17.85 19.96 24.60 

Eigenvectors, normalised to first column: 

(These are the cointegration relations) 

bhp.ll vale.11 constant 

bhp.ll 1.000000 1.0000000 1.000000 

vale.11 -0.717704 -0.7327542 2.047274 

constant -1.828460 -1.5411890 -5.712629 

Weights W: 

(This is the loading matrix) 

bhp.ll vale.11 constant 

bhp.d -0.06731196 0.004568985 9.341093e-18 

vale.d 0.02545606 0.007541565 1.015639e-18 

> col=ca.jo(xt,ecdet="const",type='eigen',K=2, 
spec='transitory') 

> summary(col) 

###################### 
# Johansen-Procedure # 
###################### 

Test type: maximal eigenvalue statistic (lambda max), without 

linear trend and constant in cointegration 

Eigenvalues (lambda): 
[1] 4.148282e-02 8.206470e-03 -4.610389e-18 

Values of teststatistic and critical values of test: 

test lOpct 5pct lpct 

r <= 1 | 7.78 7.52 9.24 12.97 

r = 0 | 40.00 13.75 15.67 20.20 

Eigenvectors, normalised to first column: 

(These are the cointegration relations) 

bhp.ll vale.11 constant 

bhp.ll 1.000000 1.0000000 1.000000 

vale.11 -0.717704 -0.7327542 2.047274 

constant -1.828460 -1.5411890 -5.712629 

456 

Weights W: 

MULTIVARIATE TIME SERIES ANALYSIS AND ITS APPLICATIONS 

(This is the loading matrix) 

bhp.ll vale.11 constant 

bhp.d -0.06731196 0.004568985 9.341093e-18 

vale.d 0.02545606 0.007541565 1.015639e-18 

APPENDIX A: REVIEW OF VECTORS AND MATRICES 

In this appendix, we briefly review some algebra and properties of vectors and 
matrices. No proofs are given as they can be found in standard textbooks on 
matrices (e.g., Graybill, 1969). 

An m x n real-valued matrix is an m x n array of real numbers. For example, 

is a 2 x 3 matrix. This matrix has two rows and three columns. In general, an 
m x n matrix is written as 

a ii 

a2l 

«12 

«22 

<2l,n—1 

<22,n —1 

a\n 
a2n 

A = [aij] = 

<2ml 

<2/n2 

ttmn 

The positive integers m and n are the row dimension and column dimension of A. 
The real number atj is referred to as the (i, j)th element of A. In particular, the 
elements a,, are the diagonal elements of the matrix. 

An m x 1 matrix forms an m-dimensional column vector, and a 1 x n matrix 
is an -dimensional row vector. In the literature, a vector is often meant to be a 
column vector. If m = n, then the matrix is a square matrix. If a;j — 0 for i ^ j 
and m = n, then the matrix A is a diagonal matrix. If atj = 0 for i / j and an = 1 
for all i, then A is the m x m identity matrix, which is commonly denoted by I m 
or simply I if the dimension is clear. 

The n x m matrix 

an 

«12 

<221 • 

«22 ' 

<2m —1,1 

<2 m 1 

<2m —1,2 

<2m2 

A' = 

a\n 

<22 n 

<2m—\,n 

<2 mn 

is the transpose of the matrix A. For example, 

~ 2 -1 " 
5 3 
8 4 

is the transpose of 

2 5 8 " 

-13 4 

APPENDIX A! REVIEW OF VECTORS AND MATRICES 

457 

We use the notation A' = [ak] to denote the transpose of A. From the definition, 
a', - = aji and (A')' = A. If A! = A, then A is a symmetric matrix. 

IJ J 

Basic Operations 

Suppose that A = [a,y]mx?1 and C = [cy]pxq are two matrices with dimensions 
given in the subscript. Let b be a real number. Some basic matrix operations are 
defined next: 

• Addition: A + C = [ay + Cij]mxn if m = p and n = q. 

• Subtraction: A — C = [a,y — Cy]mxn if m = p and n = q. 

• Scalar multiplication: bA = [bajj]mxn. 

• Multiplication: AC = [J2v=i aivCVj]mxq provided that n = p. 

When the dimensions of matrices satisfy the condition for multiplication to 
take place, the two matrices are said to be conformable. An example of matrix 
multiplication is 

' 2 
1 

1 
1 

1 2 

3 " 

-1 2  -4 

‘ 2- 1 - 1•1  2-2+ 1 -2  2-3 - 1-4 ' 
1 -2+ 1 -2 

1 • 1 - 1 • 1 

c
n

^
J
-

1

1 6 2 
0 4-1 

1

Important rules of matrix operations include (a) (AC)' = C'A' and (b) AC f CA 

in general. 

Inverse, Trace, Eigenvalue, and Eigenvector 

A square matrix Amxm is nonsingular or invertible if there exists a unique matrix 
Cmxm such that AC — CA = Im, the m x m identity matrix. In this case, C is 
called the inverse matrix of A and is denoted by C = A-1. 

The trace of Amxm is the sum of its diagonal elements [i.e., tr(A) = Ya=\ 
It is easy to see that (a) tr(A + C) = tr(A) + tr(C), (b) tr(A) = tr(A'), and (c) 
tr(AC) = tr(CA) provided that the two matrices are conformable. 

A number X and an m x 1 vector b, possibly complex valued, are a right eigen¬ 
value and eigenvector pair of the matrix A if Ab — Xb. There are m possible 
eigenvalues for the matrix A. For a real-valued matrix A, complex eigenvalues 
occur in conjugated pairs. The matrix A is nonsingular if and only if all of its 
eigenvalues are nonzero. Denote the eigenvalues by [Xi\i = 1, ..., m}: We have 

tr(A) = YT= 1 V In addition, the determinant of the matrix A can be defined as 

IAI = nr=1 Xi. For a general definition of determinant of a matrix, see a standard 

textbook on matrices (e.g., Graybill, 1969). 

Finally, the rank of the matrix Amxn is the number of nonzero eigenvalues of 

the symmetric matrix AA'. Also, for a nonsingular matrix A, (A-1)' = (A')~'. 

 
 
 
 
458 

MULTIVARIATE TIME SERIES ANALYSIS AND ITS APPLICATIONS 

Positive-Definite Matrix 

A square matrix A (m x m) is a positive-definite matrix if (a) A is symmetric and 
(b) all eigenvalues of A are positive. Alternatively, A is a positive-definite matrix 
if for any nonzero m-dimensional vector b, we have b'Ab > 0. 

Useful properties of a positive-definite matrix A include (a) all eigenvalues of 

A are real and positive, and (b) the matrix can be decomposed as 

A = PAP', 

where A is a diagonal matrix consisting of all eigenvalues of A and P is an m x m 
matrix consisting of the m right eigenvectors of A. It is common to write the 
eigenvalues as X\ > X2 > • • • > Xm and the eigenvectors as e\,... ,em such that 
Ae{ = and e'e,- = 1. In addition, these eigenvectors are orthogonal to each 
other—namely, e\ej = 0 if i j—if the eigenvalues are distinct. The matrix 
P is an orthogonal matrix and the decomposition is referred to as the spectral 
decomposition of the matrix A. Consider, for example, the simple 2x2 matrix 

which is positive definite. Simple calculations show that 

'2 1 ' 

. 1 2 . 

1 ‘ 
1 

= 3 

' 1 ‘ 
1 

5 

'2 1 " 
1 2 

1 

-1  — 

1 
-1 

Therefore, 3 and 1 are eigenvalues of H with normalized eigenvectors 
(l/\/2, l/x/2y and (l/\/2, —1/\/2)', respectively. It is easy to verify that the 
spectral decomposition holds—that is. 

" J_ J_ “ 
\/2 y/2 
X X 
_ s/2 s/2 _ 

l 
sTi 
l 
L s/i 

i * 
-1 
s/2 
V2 . 

" 3 
_ 0 

0 ' 
1 

For a symmetric matrix A, there exists a lower triangular matrix L with diagonal 
elements being 1 and a diagonal matrix G such that A = LGL', see Chapter 1 
of Strang (1980). If A is positive definite, then the diagonal elements of G are 
positive. In this case, we have 

a = lVgVgl' = (lVg)(l7g)', 

where Ls/G is again a lower triangular matrix and the square root is taken element 
by element. Such a decomposition is called the Cholesky decomposition of A. This 
decomposition shows that a positive-definite matrix A can be diagonalized as 

L~lA(L')~l = L~1A(L~1)' = G. 

APPENDIX A: REVIEW OF VECTORS AND MATRICES 

459 

Since L is a lower triangular matrix with unit diagonal elements, L_1 is also lower 
triangular matrix with unit diagonal elements. Consider again the prior 2x2 matrix 

X. It is easy to verify that 

1.0 0.0 
0.5 1.0 

and 

G = 

2.0 0.0 
0.0 1.5 

satisfy T = LGL'. In addition, 

L~l 

1.0 0.0 
-0.5 1.0 

and L-XY(L-1)' = G. 

Vectorization and Kronecker Product 

Writing an m x n matrix A in its columns as A = [«i, ..., an], we define the 
stacking operation as vec(A) = {a\,a'2, ..., a'm)', which is an mn x 1 vector. For 
two matrices Amxn and Cpxq, the Kronecker product between A and C is 

auC 

&2\ C 

anC ••• 
d2jC 

d\nC 

A®C = 

dm i c  Q-mlC 

&mn C  mpxnq 

For example, assume that 

A = 

1 " 
3 ’ 

c = 

4 -1 
-2 5 

3 ‘ 
2 • 

Then vec(A) = (2, -1, 1, 3)', vec(C) = (4, -2, -1, 5, 3, 2)', and 

A <g> C = 

8  -2 
10 
1 

-4 
-4 

4 
6 
4  -2 

-1 
5 
12  -3 
15 

-3 

3 
2 
9 
6 

2  -5  -2  -6 

Assuming that the dimensions are appropriate, we have the following useful prop¬ 

erties for the two operators: 

1. A® C ^ C ® A in general. 

2. (A (8) C)' — A' ® C'. 

3. A®(C + D) = A®C + A®D. 

4. (A ® C)(F ®G) = (AF) ® (CG). 

5. If A and C are invertible, then (A ® C)~l = A 1 ® C 1 

6. For square matrices A and C, tr(A ® C) = tr(A)tr(C). 

460 

MULTIVARIATE TIME SERIES ANALYSIS AND ITS APPLICATIONS 

7. vec(A + C) — vec(A) + vec(C). 

8. vec(ABC) = (C' <g> A) vec(B). 

9. tr(AC) = vec(C/)/vec(A) = vec(A')'vec(C). 

10. tr(ABC) = vec(A/)'(C' <g> I)vec(B) — vec(A')'(I ® R)vec(C) 

= vec(B')'(A' ® /)vec(C) = vec(B')'(I <8> C)vec(A) 

= vec(C')'(B’ ® /)vec(A) = wtc(C')r(I <S> A)vec(R). 

In multivariate statistical analysis, we often deal with symmetric matrices. It 
is therefore convenient to generalize the stacking operation to the half-stacking 
operation, which consists of elements on or below the main diagonal. Specifically, 
for a symmetric square matrix A = [<a/y ]*X/t, define 

vech(A) = (a\ ,a'2^ ...,a'kJ, 

where a\ is the first column of A, and a,* = (an, a,+ii(-,..., am)' is a (k — i + 1)- 
dimensional vector. The dimension of vech(A) is k(k + l)/2. For example, suppose 
that k = 3. Then we have vech(A) = (a\\, <221, <231, <222* <232, <233)', which is a six¬ 
dimensional vector. 

APPENDIX B: MULTIVARIATE NORMAL DISTRIBUTIONS 

A ^-dimensional random vector x = (xi,..., x^)' follows a multivariate normal 
distribution with mean ft = (ix\, ..., /XkY and positive-definite covariance matrix 
Z = [Ojj] if its probability density function (pdf) is 

/(*!**, Z) 

1 
(2^)*/2|Z|1/2 

exp  ~(x-#i)'Z l(x-n) 

(8.47) 

We use the notation x ~ Nifix, Z) to denote that x follows such a distribution. 
This normal distribution plays an important role in multivariate statistical analysis 
and it has several nice properties. Here we consider only those properties that are 
relevant to our study. Interested readers are referred to Johnson and Wichem (1998) 
for details. 

To gain insight into multivariate normal distributions, consider the bivariate case 

(i.e., k = 2). In this case, we have 

z = 

<711 

<712 

<712 

<722 

, i-1- 1 

<722  — <712 

<7ll<722 — ^ 

~<712 

<711 

Using the correlation coefficient p — cr\2/(cr\o2), where er, = is the standard 
deviation of x,-, we have or2 = p^a\Xa22 and |Z| — a \io22(1 - P2)- The pdf of x 
then becomes 

f(xux2\fi, Z) 

2ti<j\(j2tJ\ — p1 

exp 

1 

2(1 - P2) 

[Q(x, ix, Z)] 

APPENDIX C: SOME SCA COMMANDS 

461 

where 

Q(x, ft, X) 

*2 ~ A 2 

02 

Chapter 4 of Johnson and Wichern (1998) contains some plots of this pdf function. 
Let c = (ci,..., Ck)' be a nonzero ^-dimensional vector. Partition the random 
vector as x = (x'l5 x'2y, where Xi = (xi, ..., xp)' and X2 = (xp+\, ..., xk)' with 
1 < p < k. Also partition /jl and X accordingly as 

*i 

*2 

Some properties of x are as follows: 

1. c'x ~ N(c'fL, c'Xc). That is, any nonzero linear combination of x is uni¬ 
variate normal. The inverse of this property also holds. Specifically, if c'x is 
univariate normal for any nonzero vector c, then x is multivariate normal. 

2. The marginal distribution of x; is normal. In fact, x, ~ Nki (ft,, X^-) for i = 

1 and 2, where k\ = p and kj = k — p. 

3. X12 = 0 if and only if X\ and X2 are independent. 

4. The random variable y = (x - /t)'X_1(x - p) follows a chi-squared distri¬ 

bution with m degrees of freedom. 

5. The conditional distribution of xi given x2 = b is also normally distributed 

as 

(Xi|x2 = b) ~ Np[fll + X 12^22 (* - ^2)• XU _ X 12^22 S2l]- 

The last property is useful in many scientific areas. For instance, it forms the 
basis for time series forecasting under the normality assumption and for recursive 

least-squares estimation. 

APPENDIX C: SOME SCA COMMANDS 

The following SCA commands are used in the analysis of Example 8.6: 

input xl,x2. file 'm-gsln3-5301.txt' % Load data 

rl=ln(xl) % Take log transformation 

r2=ln(x2) 

miden rl,r2. no com. arfits 1 to 8. 

-- % Denote the model by v21. 

mtsm v21. series rl,r2. model (i-pl*b-p2*b**2)series= @ 

462 

MULTIVARIATE TIME SERIES ANALYSIS AND ITS APPLICATIONS 

c+(i-tl*b)noise. 

mestim v21. % Initial estimation 

pl(2,l)=0 % Set zero constraints 

cpl(2,1)=1 

p2(2,1)=0 

cp2(2,1)=1 

p2(2,2)=0 

cp2(2,2)=1 

tl(2,1)=0 

ctl(2,1)=1 

-- % Refine estimation and store residuals 

mestim v21. method exact, hold resi(resl,res2) 

miden resl(res2. 

EXERCISES 

8.1. Consider the monthly log stock returns, in percentages and including divi¬ 
dends, of Merck & Company, Johnson & Johnson, General Electric, General 
Motors, Ford Motor Company, and value-weighted index from January 1960 
to December 2008; see the file m-mrk2vw.txt. 

(a) Compute the sample mean, covariance matrix, and correlation matrix of 

the data. 

(b) Test the hypothesis H0 : p{ = • • • = p6 = 0, where /0, is the lag-i cross¬ 
correlation matrix of the data. Draw conclusions based on the 5% signifi¬ 
cance level. 

(c) Is there any lead-lag relationship among the six return series? 

8.2. The Federal Reserve Bank of St. Fouis publishes selected interest rates and 

U.S. financial data on its website: http://research.stlouisfed.org/fred2/. 
Consider the monthly 1-year and 10-year Treasury constant maturity rates from 
April 1953 to October 2009 for 679 observations; see the file m-gslnio . txt. 
The rates are in percentages. 

(a) Fet c, —rt - r,_j be the change series of the monthly interest rate rt. 
Build a bivariate autoregressive model for the two change series. Discuss 
the implications of the model. Transform the model into a structural form. 

EXERCISES 

463 

(b) Build a bivariate moving-average model for the two change series. Discuss 
the implications of the model and compare it with the bivariate AR model 

built earlier. 

8.3. Again consider the monthly 1-year and 10-year Treasury constant maturity 
rates from April 1953 to October 2009. Consider the log series of the data 
and build a VARMA model for the series. Discuss the implications of the 

model obtained. 

8.4. Again consider the monthly 1-year and 10-year Treasury constant maturity 
rates from April 1953 to October 2009. Are the two interest rate series 
threshold cointegrated? Use the interest spread st = rio)f — n,t as the thresh¬ 
old variable, where rit is the /-year Treasury constant maturity rate. If they 
are threshold cointegrated, build a multivariate threshold model for the two 

series. 

8.5. The bivariate AR(4) model xt - 4>4*f-4 = 0O + a, is a special seasonal model 
with periodicity 4, where {at} is a sequence of independent and identically 
distributed normal random vectors with mean zero and covariance matrix £. 
Such a seasonal model may be useful in studying quarterly earnings of a 
company, (a) Assume that x, is weakly stationary. Derive the mean vector 
and covariance matrix of xt. (b) Derive the necessary and sufficient condition 
of weak stationarity for xt. (c) Show that T(_ = <I>4r^_4 for i > 0, where 

is the lag-f autocovariance matrix of xt. 

8.6. The bivariate MA(4) model xt = at - 04<C-4 is another seasonal model with 
periodicity 4, where {at} is a sequence of independent and identically dis¬ 
tributed normal random vectors with mean zero and covariance matrix T,. 
Derive the covariance matrices Tt of xt for l — 0,..., 5. 

8.7. Consider the monthly U.S. 1-year and 3-year Treasury constant maturity rates 
from April 1953 to March 2004. The data can be obtained from the Federal 
Reserve Bank of St. Louis or from the file m-gsln3-5304.txt (1-year, 3- 
year, dates). See also Example 8.6, which uses a shorter data span. Here we 
use the interest rates directly without the log transformation and define xt = 
(x]t, X2t)\ where x\t is the 1-year maturity rate and xit is the 3-year maturity 

rate. 

(a) Identify a VAR model for the bivariate interest rate series. Write down the 

fitted model. 

(b) Compute the impulse response functions of the fitted VAR model. It suf¬ 

fices to use the first 6 lags. 

(c) Use the fitted VAR model to produce 1-step- to 12-step-ahead forecasts 
of the interest rates, assuming that the forecast origin is March 2004. 

(d) Are the two interest rate series cointegrated, when a restricted constant 

term is used? Use 5% significance level to perform the test. 

(e) If the series are cointegrated, build an ECM for the series. Write down 

the fitted model. 

464 

MULTIVARIATE TIME SERIES ANALYSIS AND ITS APPLICATIONS 

(f) Use the fitted ECM to produce 1-step- to 12-step-ahead forecasts of the 

interest rates, assuming that the forecast origin is March 2004. 

(g) Compare the forecasts produced by the VAR model and the ECM. 

REFERENCES 

Balke, N. S. and Fomby, T. B. (1997). Threshold cointegration. International Economic 

Review 38: 627-645. 

Box, G. E. P. and Tiao, G. C. (1977). A canonical analysis of multiple time series. Biometrika 

64: 355-366. 

Brenner, R. J. and Kroner, K. F. (1995). Arbitrage, cointegration, and testing the unbiased¬ 
ness hypothesis in financial markets. Journal of Financial and Quantitative Analysis 
30: 23-42. 

Cochrane, J. H. (1988). How big is the random walk in the GNP? Journal of Political 

Economy 96: 893-920. 

Dwyer, G. P. Jr., Locke, P., and Yu, W. (1996). Index arbitrage and nonlinear dynamics 
between the S&P 500 futures and cash. Review of Financial Studies 9: 301-332. 

Engle, R. F. and Granger, C. W. J. (1987). Cointegration and error correction representation, 

estimation and testing. Econometrica 55: 251-276. 

Forbes, C. S., Kalb, G. R. J., and Kofman, P. (1999). Bayesian arbitrage threshold analysis. 

Journal of Business & Economic Statistics 17: 364-372. 

Fuller, W. A. (1976). Introduction to Statistical Time Series. Wiley, New York. 

Graybill, F. A. (1969). Introduction to Matrices with Applications in Statistics. Wadsworth, 

Belmont, CA. 

Hannan, E. J. and Quinn, B. G. (1979). The determination of the order of an autoregression. 

Journal of the Royal Statistical Society Series B 41: 190-195. 

Hillmer, S. C. and Tiao, G. C. (1979). Likelihood function of stationary multiple autore¬ 
gressive moving average models. Journal of the American Statistical Association 74: 
652-660. 

Hosking, J. R. M. (1980). The multivariate portmanteau statistic. Journal of the American 

Statistical Association 75: 602-608. 

Hosking, J. R. M. (1981). Lagrange-multiplier tests of multivariate time series models. 

Journal of the Royal Statistical Society Series B 43: 219-230. 

Johansen, S. (1988). Statistical analysis of cointegration vectors. Journal of Economic 

Dynamics and Control 12: 231-254. 

Johansen, S. (1995). Likelihood Based Inference in Cointegrated Vector Error Correction 

Models. Oxford University Press, Oxford, UK. 

Johnson, R. A. and Wichem, D. W. (1998). Applied Multivariate Statistical Analysis, 4th 

ed. Prentice Hall, Upper Saddle River, NJ. 

Li, W. K. and McLeod, A. I. (1981). Distribution of the residual autocorrelations in multi¬ 
variate ARMA time series models. Journal of the Royal Statistical Society Series B 43: 
231-239. 

Liitkepohl, H. (2005). New Introduction to Multiple Time Series Analysis. Springer, Berlin. 

Pole, A. (2007). Statistical Arbitrage. Wiley, Hoboken, NJ. 

REFERENCES 

465 

Reinsel, G. C. (1993). Elements of Multivariate Time Series Analysis. Springer, New York. 

Reinsel, G. C. and Ahn, S. K. (1992). Vector autoregressive models with unit roots and 
reduced rank structure: Estimation, likelihood ratio test, and forecasting. Journal of 
Time Series Analysis, 13: 353-375. 

Stock, J. H. and Watson, M. W. (1988). Testing for common trends. Journal of the American 

Statistical Association 83: 1097-1107. 

Strang, G. (1980). Linear Algebra and Its Applications, 2nd ed. Harcourt Brace Jovanovich, 

Chicago. 

Tiao, G. C. and Box, G. E. P. (1981). Modeling multiple time series with applications. 

Journal of the American Statistical Association 76: 802-816. 

Tiao, G. C. and Tsay, R. S. (1989). Model specification in multivariate time series (with 

discussions). Journal of the Royal Statistical Society Series B 51: 157-213. 

Tiao, G. C., Tsay, R. S., and Wang, T. (1993). Usefulness of linear transformations in 

multivariate time series analysis. Empirical Economics 18: 567-593. 

Tsay, R. S. (1991). Two canonical forms for vector ARMA processes. Statistica Sinica 1: 

247-269. 

Tsay, R. S. (1998). Testing and modeling multivariate threshold models. Journal of the 

American Statistical Association 93: 1188-1202. 

Tsay, R. S. and Tiao, G. C. (1990). Asymptotic properties of multivariate nonstationary 
processes with applications to autoregressions. Annals of Statistics 18: 220-250. 

Vidyamurthy, G. (2004). Pairs Trading. Wiley, Hoboken, NJ. 

Zivot, E. and Wang, J. (2003). Modeling Financial Time Series with S-Plus. Springer, New 

York. 

CHAPTER 9 

Principal Component Analysis 
and Factor Models 

Most financial portfolios consist of multiple assets, and their returns depend con¬ 
currently and dynamically on many economic and financial variables. Therefore, it 
is important to use proper multivariate statistical analyses to study the behavior and 
properties of portfolio returns. However, as demonstrated in the previous chapter, 
analysis of multiple asset returns often requires high-dimensional statistical models 
that are complicated and hard to apply. To simplify the task of modeling multiple 
returns, we discuss in this chapter some dimension reduction methods to search 
for the underlying structure of the assets. Principal component analysis (PCA) is 
perhaps the most commonly used statistical method in dimension reduction, and 
we start our discussion with the method. In practice, observed return series often 
exhibit similar characteristics leading to the belief that they might be driven by 
some common sources, often referred to as common factors. To study the common 
pattern in asset returns and to simplify portfolio analysis, various factor models 
have been proposed in the literature to analyze multiple asset returns. The second 
goal of this chapter is to introduce some useful factor models and demonstrate their 

applications in finance. 

Three types of factor models are available for studying asset returns; see Connor 
(1995) and Campbell, Lo, and MacKinlay (1997). The first type is the macroeco¬ 
nomic factor models that use macroeconomic variables such as growth rate of GDP, 
interest rates, inflation rate, and unemployment rate to describe the common behav¬ 
ior of asset returns. Here the factors are observable and the model can be estimated 
via linear regression methods. The second type is the fundamental factor models 
that use firm or asset specific attributes such as firm size, book and market val¬ 
ues, and industrial classification to construct common factors. The third type is the 
statistical factor models that treat the common factors as unobservable or latent 
variables to be estimated from the returns series. In this chapter, we discuss all 

Analysis of Financial Time Series, Third Edition, By Ruey S. Tsay 
Copyright © 2010 John Wiley & Sons, Inc. 

467 

468 

PRINCIPAL COMPONENT ANALYSIS AND FACTOR MODELS 

three types of factor models and their applications in finance. Principal component 
analysis and factor models for asset returns are also discussed in Alexander (2001) 

and Zivot and Wang (2003). 

The chapter is organized as follows. Section 9.1 introduces a general factor 
model for asset returns, and Section 9.2 discusses macroeconomic factor models 
with some simple examples. The fundamental factor model and its applications are 
given in Section 9.3. Section 9.4 introduces principal component analysis that serves 
as the basic method for statistical factor analysis. The PCA can also be used to 
reduce the dimension in multivariate analysis. Section 9.5 discusses the orthogonal 
factor models, including factor rotation and its estimation, and provides several 
examples. Finally, Section 9.6 introduces asymptotic principal component analysis. 

9.1 A FACTOR MODEL 

Suppose that there are k assets and T time periods. Let r,-r be the return of asset i 
in the time period t. A general form for the factor model is 

rit — <Xi + Pilflt + • • • + Pimfmt + %, t=l,...,T‘, i = l,...,k, (9.1) 

where a, is a constant representing the intercept, {fjt \j = 1, ..., m) are m common 
factors, fijj is the factor loading for asset i on the jth factor, and cn is the specific 
factor of asset i. 

For asset returns, the factor f t = (f\t,..., fmt)' is assumed to be an 

m-dimensional stationary process such that 

E(ft) = pf, 

Co v(/r) = £/-, an mxm matrix, 

and the asset specific factor is a white noise series and uncorrelated with the 
common factors fJt and other specific factors. Specifically, we assume that 

E(cu) = 0 for all / and t. 

Cov(fjt, €is) = 0 

for all j, i, t and s, 

Cov(eir, €js) = 

of, if i = j and t = s, 
0, otherwise. 

Thus, the common factors are uncorrelated with the specific factors, and the specific 
factors are uncorrelated among each other. The common factors, however, need not 
be uncorrelated with each other in some factor models. 

In some applications, the number of assets k may be larger than the number 
of time periods T. We discuss an approach to analyze such data in Section 9.6. It 

A FACTOR MODEL 

469 

is also common to assume that the factors, hence rt, are serially uncorrelated in 
factor analysis. In applications, if the observed returns are serially dependent, then 
the models in Chapter 8 can be used to remove the serial dependence. 
In matrix form, the factor model in Eq. (9.1) can be written as 

fit = oti + Pj ft + fit, 

where /?, = (A i, - - -, Pim) is a row vector of loadings, and the joint model for the 
k assets at time t is 

rx — a + P f t + ff, t — 1,..., T (9.2) 

where rt = (r\t,..., rkt)', a = («i,..., otk)', P = [Pij] is a kxm factor¬ 
loading matrix, and et = (e\t,..., €kt)' is the error vector with Cov(e,) = D = 
diagfcTj2, ..., o^2}, a k x k diagonal matrix. The covariance matrix of the return rt 

is then 

Cov(r,) = P’ZfP' + D. 

The model presentation in Eq. (9.2) is in a cross-sectional regression form if the 

factors fjt are observed. 

Treating the factor model in Eq. (9.1) as a time series, we have 

Ri =ailT + FP'i+Ei, (9.3) 

for the ith asset (i = 1, ..., k), where Rt = (rn,..., riT)', 1T is a T-dimensional 
vector of ones, F is a T x m matrix whose tth row is f\, and JE1, = (fn,..., fir)'- 
The covariance matrix of E\ is Cov(£',) = cr2/, a T x T diagonal matrix. 

Finally, we can rewrite Eq. (9.2) as 

ft = $gt+*t, 

where gt = (1, f't)' and § = [a, p], which is a k x (m + 1) matrix. Taking the 
transpose of the prior equation and stacking all data together, we have 

R = G%' + E, (9.4) 

where R is a T x k matrix of returns whose tth row is r\ or, equivalently, whose 
ith column is Ri of Eq. (9.3), G is a T x (m + 1) matrix whose tth row is gt, 
and E is a T x k matrix of specific factors whose tth row is ([. If the common 
factors ft are observed, then Eq. (9.4) is a special form of the multivariate linear 
regression (MLR) model; see Johnson and Wichern (2007). For a general MLR 

model, the covariance matrix of et need not be diagonal. 

470 

PRINCIPAL COMPONENT ANALYSIS AND FACTOR MODELS 

9.2 MACROECONOMETRIC FACTOR MODELS 

For macroeconomic factor models, the factors are observed and we can apply the 
least-squares method to the MLR model in Eq. (9.4) to perform estimation. The 
estimate is 

{G'G)-\G'R), 

from which the estimates of a and P are readily available. The residuals of Eq. 
(9.4) are 

E = R-gJ. 

Based on the model assumption, the covariance matrix of €t is estimated by 

D = diag{of, ..., 

where a;2 is the (i, i)th element of E E/(T — m — 1). Furthermore, the R2 of the 
ith asset of Eq. (9.3) is 

[E E\j 
[R'Rki ’ 

where A; I- denotes the (i, i)th element of the matrix A. 

Note that the aforementioned least-squares estimation does not impose the con¬ 

straint that the specific factors €it are uncorrelated with each other. Consequently, 
the estimates obtained are not efficient in general. However, imposing the orthog- 
onalization constraint requires nontrivial computation and is often ignored. One 
can check the off-diagonal elements of the matrix e'e/(T - m — 1) to verify the 
adequacy of the fitted model. These elements should be close to zero. 

9.2.1 Single-Factor Model 

The best known macroeconomic factor model in finance is the market model, see 
Sharpe (1970). This is a single-factor model and can be written as 

rit=<*i+Pirmt+*u, i = l,...,k; t = l,...,T, (9.5) 

where rit is the excess return of the ith asset, rmt is the excess return of the market, 
and ^ is the well-known p for stock returns. To illustrate, we consider monthly 
returns of 13 stocks and use the return of the S&P 500 index as the market return. 
The stocks used and their tick symbols are given in Table 9.1, and the sample 
period is from January 1990 to December 2003 so that k = 13 and T = 168. We 
use the monthly series of 3-month Treasury bill rates of the secondary market as 

MACROECONOMETRIC FACTOR MODELS 

471 

TABLE 9.1 Stocks Used and Their Tick Symbols in Analysis of Single-Factor Model" 

Tick 

AA 
AGE 
CAT 
F 
FDX 
GM 
HPQ 

Company 

Alcoa 
A.G. Edwards 
Caterpillar 
Ford Motor 
FedEx 
General Motors 
Hewlett-Packard 

r(oy) 
1.09(9.49) 
1.36(10.2) 
1.23(8.71) 
0.97(9.77) 
1.14(9.49) 
0.64(9.28) 
1.37(11.8) 

Tick 

KMB 
MEL 
NYT 
PG 
TRB 
TXN 
SP5 

Company 

Kimberly-Clark 
Mellon Financial 
New York Times 
Procter & Gamble 
Chicago Tribune 
Texas Instrument 
S&P 500 Index 

f{or) 
0.78(6.50) 
1.36(7.80) 
0.81(7.37) 
1.08(6.75) 
0.95(7.84) 
2.19(13.8) 
0.42(4.33) 

“Sample means (standard errors) of excess returns are also given. The sample period is from January 

1990 to December 2003. 

the risk-free interest rate to obtain simple excess returns of the stock and market 

index. The returns are in percentages. 

We use S-Plus to implement the estimation method discussed in the previous 

section. Most of the commands used also apply to the software R. 

> x=read.matrix(''m-fac9003.txt'',header=T) 
> xmtx=cbind(rep(1,168),x[,14]) 

> rtn=x[,1:13] 

> xit.hat=solve(xmtx,rtn) 

> beta,hat=t(xit.hat[2,]) 

> E.hat=rtn-xmtx%*%xit.hat 

> D.hat=diag(crossprod(E.hat)/(168-2)) 

> r.square=l-(168-2)*D.hat/diag(var(rtn,SumSquares=T)) 

The estimates of of, and R2 for the ith asset return are given below: 

> t(rbind(beta.hat,sqrt(D.hat),r.square)) 

beta.hat 

sigma(i) 

r.square 

AA 

AGE 

CAT 

F 

FDX 

GM 

HPQ 

KMB 

MEL 

NYT 

PG 

TRB 

TXN 

1.292 

1.514 

0.941 

1.219 

0.805 

1.046 

1.628 

0.550 

1.123 

0.771 

0.469 

0.718 

1.796 

7.694 

7.808 

7.725 

8.241 

8.854 

8.130 

9.469 

6.070 

6.120 

6.590 

6.459 

7.215 

11.474 

0.347 

0.415 

0.219 

0.292 

0.135 

0.238 

0.358 

0.134 

0.388 

0.205 

0.090 

0.157 

0.316 

Figure 9.1 shows the bar plots of /3; and /?“ of the 13 stocks. The financial 
stocks, AGE and MEL, and the high-tech stocks, HPQ and TXN, seem to have 
higher 0 and R2. On the other hand, KMB and PG have lower & and R2. The R2 

472 

PRINCIPAL COMPONENT ANALYSIS AND FACTOR MODELS 

Figure 9.1 Bar plots of (a) beta and (b) R2 for fitting single-factor market model to monthly excess 

returns of 13 stocks. S&P 500 index excess return is used as market index. Sample period is from 
January 1990 to December 2003. 

ranges from 0.09 to 0.41, indicating that the market return explains less than 50% 
of the variabilities of the individual stocks used. 

The covariance and con-elation matrices of rt under the market model can be 

estimated using the following: 

> cov.r=var(x[,14])*(t(beta.hat)%*%beta.hat)+diag(D.hat) 
> sd.r=sqrt(diag(cov.r)) 
> corr.r=cov.r/outer(sd.r,sd.r) 
> print(corr.r,digits=l,width=2) 

F 

FDX 

AA  AGE  CAT 

GM  HPQ  KMB  MEL  NYT 

PG  TRB  TXN 
AA  1.0  0.4  0.3  0.3  0.2  0.3  0.4  0.2  0.4  0.3  0.2  0.2  0.3 
AGE  0.4  1.0  0.3  0.3  0.2  0.3  0.4  0.2  0.4  0.3  0.2  0.3  0.4 
CAT  0.3  0.3  1.0  0.3  0.2  0.2  0.3  0.2  0.3  0.2  0.1  0.2  0.3 
F  0.3  0.3  0.3  1.0  0.2  0.3  0.3  0.2  0.3  0.2  0.2  0.2  0.3 
FDX  0.2  0.2  0.2  0.2  1.0  0.2  0.2  0.1  0.2  0.2  0.1  0.1  0.2 
GM  0.3  0.3  0.2  0.3  0.2  1.0  0.3  0.2  0.3  0.2  0.1  0.2  0.3 
HPQ  0.4  0.4  0.3  0.3  0.2  0.3  1.0  0.2  0.4  0.3  0.2  0.2  0.3 
KMB  0.2  0.2  0.2  0.2  0.1  0.2  0.2  1.0  0.2  0.2  0.1  0.1  0.2 
MEL  0.4  0.4  0.3  0.3  0.2  0.3  0.4  0.2  1.0  0.3  0.2  0.2  0.3 
NYT  0.3  0.3  0.2  0.2  0.2  0.2  0.3  0.2  0.3  1.0  0.1  0.2  0.3 
PG  0.2  0.2  0.1  0.2  0.1  0.1  0.2  0.1  0.2  0.1  1.0  0.1  0.2 
TRB  0.2  0.3  0.2  0.2  0.1  0.2  0.2  0.1  0.2  0.2  0.1  1.0  0.2 
TXN  0.3  0.4  0.3  0.3  0.2  0.3  0.3  0.2  0.3  0.3  0.2  0.2  1.0 

MACROECONOMETRIC FACTOR MODELS 

473 

We can compare these estimated correlations with the sample correlations of the 

excess returns. 

> print(cor(rtn),digits=l,width=2) 

F 

FDX 

AA  AGE  CAT 

GM  HPQ  KMB  MEL  NYT 

PG  TRB  TXN 
AA  1.0  0.3  0.6  0.5  0.2  0.4  0.5  0.3  0.4  0.4  0.1  0.3  0.5 
AGE  0.3  1.0  0.3  0.3  0.3  0.3  0.3  0.3  0.4  0.4  0.2  0.2  0.3 
CAT  0.6  0.3  1.0  0.4  0.2  0.3  0.2  0.3  0.4  0.3  0.1  0.4  0.3 
F  0.5  0.3  0.4  1.0  0.3  0.6  0.3  0.3  0.4  0.4  0.1  0.3  0.3 
FDX  0.2  0.3  0.2  0.3  1.0  0.2  0.3  0.3  0.2  0.2  0.1  0.3  0.2 
GM  0.4  0.3  0.3  0.6  0.2  1.0  0.3  0.3  0.4  0.2  0.1  0.3  0.3 
HPQ  0.5  0.3  0.2  0.3  0.3  0.3  1.0  0.1  0.3  0.3  0.1  0.2  0.6 
KMB  0.3  0.3  0.3  0.2  0.3  0.3  0.1  1.0  0.3  0.2  0.3  0.3  0.1 
MEL  0.4  0.4  0.4  0.4  0.2  0.4  0.3  0.4  1.0  0.3  0.4  0.3  0.3 
NYT  0.4  0.4  0.3  0.4  0.3  0.2  0.3  0.2  0.3  1.0  0.2  0.5  0.2 
PG  0.1  0.2  0.1  0.1  0.1  0.1  0.1  0.3  0.4  0.2  1.0  0.3  0.1 
TRB  0.3  0.2  0.4  0.3  0.3  0.3  0.2  0.3  0.3  0.5  0.3  1.0  0.2 
TXN  0.5  0.3  0.3  0.3  0.2  0.3  0.6  0.1  0.3  0.2  0.1  0.2  1.0 

In finance, one can use the concept of global minimum variance portfolio 
(GMVP) to compare the covariance matrix implied by a fitted factor model with 
the sample covariance matrix of the returns. For a given covariance matrix X, the 
global minimum variance portfolio is the portfolio to that solves 

min ol= co'2,00 such that of 1 = 1, 

w P'a 

where aj M is the variance of the portfolio. The solution is given by 

X-11 

where 1 is the ^-dimensional vector of ones. 

For the market model considered, the GMVP for the fitted model and the data 

are as follows: 

> w. grain .model = solve (cov. r) %*%rep (1, nrow (cov. r) ) 
> w. gmin.model=w. gmin.model/ sum (w. gmin.model) 

> t(w.gmin.model) 

AA AGE CAT F FDX GM 

[1,] 0.0117 -0.0306 0.0792 0.0225 0.0802 0.0533 

HPQ KMB MEL NYT PG TRB TXN 

[1,] -0.0354 0.2503 0.0703 0.1539 0.2434 0.1400 -0.0388 
> w.gmin.data=solve(var(rtn))%*%rep(1,nrow(cov.r)) 

> w.gmin.data=w.gmin.data/sum(w.gmin.data) 

> t(w.gmin.data) 

474 

PRINCIPAL COMPONENT ANALYSIS AND FACTOR MODELS 

AA AGE CAT F FDX GM 

[1,] -0.0073 -0.0085 0.0866 -0.0232 0.0943 0.0916 

HPQ KMB MEL NYT PG TRB TXN 

[1,] 0.0345 0.2296 0.0495 0.1790 0.2651 0.0168 -0.0080 

Comparing the two GMVPs, the weights assigned to TRB stock differ markedly. 
The two portfolios, however, have larger weights for KMB, NYT, and PG stocks. 
Finally, we examine the residual covariance and correlation matrices to verify 
the assumption that the special factors are not correlated among the 13 stocks. The 
first four columns of the residual correlation matrix are given below and there exist 
some large values in the residual cross correlations, for example, Cor(CAT,AA) = 
0.45 and Cor(GM,F) = 0.48. 

> resi.cov=t(E.hat)%*%E.hat/(168-2) 
> resi.sd=sqrt(diag(resi.cov)) 
> resi.cor=resi.cov/outer(resi.sd,resi.sd) 
> print(resi.cor,digits=l,width=2) 

F 

AA 

CAT 

AGE 
AA  1.00  -0.13  0.45  0.22 
AGE  -0.13  1.00  -0.03  -0.01 
CAT  0.45  -0.03  1.00  0.23 
F  0.22  -0.01  0.23  1.00 
FDX  0.00  0.14  0.05  0.07 
GM  0.14  -0.09  0.15  0.48 
HPQ  0.24  -0.13  -0.07  -0.00 
KMB  0.16  0.06  0.18  0.05 
MEL  -0.02  0.06  0.09  0.10 
NYT  0.13  0.10  0.07  0.19 
PG  -0.15  -0.02  -0.01  -0.07 
TRB  0.12  -0.02  0.25  0.16 
TXN  0.19  -0.17  0.09  -0.02 

9.2.2 Multifactor Models 

Chen, Roll, and Ross (1986) consider a multifactor model for stock returns. The fac¬ 
tors used consist of unexpected changes or surprises of macroeconomic variables. 
Here unexpected changes denote the residuals of the macroeconomic variables after 
removing their dynamic dependence. A simple way to obtain unexpected changes 
is to fit a VAR model of Chapter 8 to the macroeconomic variables. For illustration, 
we consider the following two monthly macroeconomic variables: 

1. Consumer price index (CPI) for all urban consumers: all items and with index 

1982-1984 = 100. 

2. Civilian employment numbers 16 years and over (CE16): measured in thou¬ 

sands. 

MACROECONOMETRIC FACTOR MODELS 

475 

Both CPI and CE16 series are seasonally adjusted, and the data span is from January 
1975 to December 2003. We use a longer period to obtain the surprise series of the 
variables. For both series, we construct the growth rate series by taking the first 
difference of the logged data. The growth rates are in percentages. 

To obtain the surprise series, we use the BIC criterion to identify a VAR(3) 
model. Thus, the two macroeconomic factors used in the factor model are the 
residuals of a VAR(3) model from 1990 to 2003. For the excess returns, we use 
the same 13 stocks as before. Details of the analysis follow: 

> da=read.table('m-cpicel6-dp75 03.txt') ,header=T) 
> cpi=da[,1] 
> cen=da[,2] 
> xl=cbind(cpi,cen) 
> yl=data.frame(xl) 
> ord.choice=VAR(yl,max.ar=13) 
> ord.choice$info 

ar(1) ar(2) ar(3) ar(4) ar(5) ar(6) 
BIC 36.992 38.093 28.234 46.241 60.677 75.810 

ar(7) ar(8) ar(9) ar(10) ar(ll) ar(12) ar(13) 
BIC 86.23 99.294 111.27 125.46 138.01 146.71 166.92 
> var3 . f it=VAR (xTar (3 ) ) 
> res=var3.fit$residuals[166:333,1:2] 
> da=matrix(scan(file='m-fac9003.txt'),14) 
> xmtx = cbind(rep(1,168),res) 
> da=t(da) 
> rtn=da[,1:13] 
> xit.hat=solve(xmtx,rtn) 
> beta.hat=t(xit.hat[2:3,]) 
> E.hat=rtn - xmtx%*%xit.hat 
> D.hat=diag(crossprod(E.hat)/(168-3)) 
> r.square=l-(168-3)*D.hat/diag(var(rtn,SumSquares=T)) 

Figure 9.2 shows the bar plots of the beta estimates and R2 for the 13 stocks. It 
is interesting to see that all excess returns are negatively related to the unexpected 
changes of CPI growth rate. This seems reasonable. However, the R2 of all excess 
returns are low, indicating that the two macroeconomic variables used have very 
little explanatory power in understanding the excess returns of the 13 stocks. 

The estimated covariance and correlation matrices of the two-factor model can 

be obtained using the following: 

> cov.rtn=beta.hat%*%var(res)%*%t(beta.hat)+diag(D.hat) 
> sd.rtn=sqrt(diag(cov.rtn)) 
> cor.rtn = cov.rtn/outer(sd.rtn,sd.rtn) 
> print(cor.rtn,diits=l,width=2) 

476 

PRINCIPAL COMPONENT ANALYSIS AND FACTOR MODELS 

Figure 9.2 Bar plots of betas and R2 for fitting two-factor model to monthly excess returns of 13 
stocks. Sample period is from January 1990 to December 2003. 

The correlation matrix is very close to the identity matrix, indicating that the two- 
factor model used does not fit the excess returns well. Finally, the correlation matrix 
of the residuals of the two-factor model is given by the following: 

> cov.resi=t(E.hat)%*%E.hat/(168-3) 
> sd.resi=sqrt(diag(cov.resi)) 
> cor.resi=cov.resi/outer(sd.resi,sd.resi) 
> print(cor.resi,digits=l,width=2) 

As expected, this correlation matrix is close to that of the original excess returns 
given before and is omitted. 

9.3 FUNDAMENTAL FACTOR MODELS 

Fundamental factor models use observable asset specific fundamentals such as 
industrial classification, market capitalization, book value, and style classification 
(growth or value) to construct common factors that explain the excess returns. 

FUNDAMENTAL FACTOR MODELS 

477 

There are two approaches to fundamental factor models available in the literature. 
The first approach is proposed by Bar Rosenberg, founder of BARRA Inc., and is 
referred to as the BARRA approach; see Grinold and Kahn (2000). In contrast to 
the macroeconomic factor models, this approach treats the observed asset specific 
fundamentals as the factor betas, /?,, and estimates the factors f t at each time 
index t via regression methods. The betas are time invariant, but the realizations 
ft evolve over time. The second approach is the Fama-French approach proposed 
by Fama and French (1992). In this approach, the factor realization fjt for a given 
specific fundamental is obtained by constructing some hedge portfolio based on 
the observed fundamental. We briefly discuss the two approaches in the next two 

sections. 

9.3.1 BARRA Factor Model 

Assume that the excess returns and, hence, the factor realizations are mean cor¬ 
rected. At each time index t, the factor model in Eq. (9.2) reduces to 

rt — ft f t + €t, (9.6) 

where r, denotes the (sample) mean-corrected excess returns and, for simplicity in 
notation, we continue to use f, as factor realizations. Since /? is given, the model 
in Eq. (9.6) is a multiple linear regression with k observations and m unknowns. 
Because the number of common factors m should be less than the number of 
assets k, the regression is estimable. However, the regression is not homogeneous 
because the covariance matrix of €t is D = diag{of, ..., of} with of = Var(eif), 
which depends on the ith asset. Consequently, the factor realization at time index t 
can be estimated by the weighted least-squares (WLS) method using the standard 
errors of the specific factors as the weights. The resulting estimate is 

/, = (fi'D-'py1 (f’D-'f,). (9.7) 

In practice, the covariance matrix D is unknown so that we use a two-step procedure 

to perform the estimation. 

In step one, the ordinary least-squares (OLS) method is used at each time index 

t to obtain a preliminary estimate of /, as 

ft,o = 0B'fi-'V'n), 

where the second subscript o is used to denote the OLS estimate. This estimate of 
factor realization is consistent, but not efficient. The residual of the OLS regres¬ 

sion is 

*t,o = ft-Pft,0- 

478 

PRINCIPAL COMPONENT ANALYSIS AND FACTOR MODELS 

Since the residual covariance matrix is time invariant, we can pool the residuals 
together (for t = 1,..., T) to obtain an estimate of D as 

D0 = diag 

i=i 

In step two, we plug in the estimate D0 to obtain a refined estimate of the factor 
realization 

?,,, = (/>%'(>)-'(pD-0'f), (9.8) 

where the second subscript g denotes the generalized least-squares (GLS) esti¬ 
mate, which is a sample version of the WLS estimate. The residual of the refined 
regression is 

*t,g = ft ~ Pft,g> 

from which we estimate the residual variance matrix as 

De 

diag 

tS>- «<«> 

t=l 

Finally, the covariance matrix of the estimated factor realizations is 

Tf = 

/=l 

where 

1 T - 

f g = t X] f l'g‘ 

1=1 

From Eq. (9.6), the covariance matrix of the excess returns under the BARRA 
approach is 

Cov(r,) = /JE//3' + Dg. 

Industry Factor Model 

For illustration, we consider monthly excess returns of 10 stocks and use 
industrial classification as the specific asset fundamental. The stocks used are 
given in Table 9.2 and can be classified into three industrial sectors—namely, 
financial services, computer and high-tech industry, and other. The sample period 
is again from January 1990 to December 2003. Under the BARRA framework, 

FUNDAMENTAL FACTOR MODELS 

479 

TABLE 9.2 Stocks Used and Their Tick Symbols in Analysis of Industrial Factor 
Model" 
Tick 

Company 

Company 

Tick 

AGE 
C 
MWD 
MER 
DELL 

A.G. Edwards 
Citigroup 
Morgan Stanley 
Merrill Lynch 
Dell Inc. 

r(o,•) 
1.36(10.2) 
2.08(9.60) 
1.87(11.2) 
2.08(10.4) 
4.82(16.4) 

HPQ 
IBM 
AA 
CAT 
PG 

Hewlett-Packard 
Int. Bus. Machines 
Alcoa 
Caterpillar 
Procter & Gamble 

r{pr) 
1.37(11.8) 
1.06(9.47) 
1.09(9.49) 
1.23(8.71) 
1.08(6.75) 

"Sample mean and standard deviation of the excess returns are also given. The sample span is from 

January 1990 to December 2003. 

there are three common factors representing the three industrial sectors and the 
betas are indicators for the three industrial sectors; that is, 

?it = fin fit + fin fit + fin fit i = 1, • • •, 10, (9.9) 

with the betas being 

| 1 if asset i belongs to the j industrial sector, „ „ 

^9 — o otherwise, 

where j = 1, 2, 3 representing the financial, high-tech, and other sectors, respec¬ 
tively. For instance, the beta vector for the IBM stock return is fit = (0, 1, 0/ and 

that for Alcoa stock return is fij = (0, 0, 1)'. 

In Eq. (9.9), f\t is the factor realization of the financial services sector, fzt is 
that of the computer and high-tech sector, and fjt is for the other sector. Because 
the fiij are indicator variables, the OLS estimate of ft is extremely simple. Indeed, 
ft is the vector consisting of the averages of sector excess returns at time t. 

Specifically, 

AGE,+C,+MDW,+MER, 1 

4 
DELL, +HPQ, +IBM, 
3 
AA,+CAT(+PG, 

3 J 

The specific factor of the ith asset is simply the deviation of its excess return 
from its industrial sample average. One can then obtain an estimate of the residual 
variance matrix D to perform the generalized least-squares estimation. We use 
S-Plus to perform the analysis. The commands also apply to R. First, load the 
returns into S-Plus, remove the sample means, create the industrial dummies, and 

compute the sample correlation matrix of the returns. 

> da=read.table('m-barra-9003.txt'),header=T) 

> rm = matrix(apply(da,2,mean),1) 
> rtn = da - matrix(1,168,1)%*%rm 

480 

PRINCIPAL COMPONENT ANALYSIS AND FACTOR MODELS 

> fin = c(rep(1,4),rep(0,6)) 
> tech = c(rep(0,4),rep(1,3),rep(0,3) 
> oth = c (rep ( 0,7) , rep (1,3 ) ) 
> ind.dum = cbind(fin,tech,oth) 
> ind.dum 

fin tech oth 
[1,] 100 
[2,] 100 
[3,] 100 
[4,] 100 
[5, ] 0 1 0 
[6,] 010 
[7,] 010 
[8,] 001 
[9,] 001 
[10,] 001 

> cov.rtn=var(rtn) 
> sd.rtn=sqrt(diag(cov.rtn)) 
> corr.rtn=cov.rtn/outer(sd.rtn,sd.rtn) 
> print(corr.rtn,digits=l,width=2) 

AGE 

C  MWD  MER  DELL  HPQ  IBM  AA  CAT  PG 

AGE  1.0  0.6  0.6  0.6  0.3  0.3  0.3  0.3  0.3  0.2 
0.6  1.0  0.7  0.7  0.2  0.4  0.4  0.4  0.4  0.3 
C 
MWD  0.6  0.7  1.0  0.8  0.3  0.5  0.4  0.4  0.3  0.3 
MER  0.6  0.7  0.8  1.0  0.2  0.5  0.3  0.4  0.3  0.3 
DELL  0.3  0.2  0.3  0.2  1.0  0.5  0.4  0.3  0.1  0.1 
HPQ  0.3  0.4  0.5  0.5  0.4  1.0  0.5  0.5  0.2  0.1 
IBM  0.3  0.4  0.4  0.3  0.4  0.5  1.0  0.4  0.3- ■0.0 
AA 
0.3  0.4  0.4  0.4  0.3  0.5  0.4  1.0  0.6  0.1 
CAT  0.3  0.4  0.3  0.3  0.1  0.2  0.3  0.6  1.0  0.1 
0.2  0.3  0.3  0.3  0.1  0.1- ■0.0  0.1  0.1  1.0 
PG 

The OLS estimates, their residuals, and residual variances are estimated as fol¬ 

lows: 

> F.hat.o = solve(crossprod(ind.dum))%*%t(ind.dum)%*%rtn.rm 
> E.hat.o = rtn.rm - ind.dum%*%F.hat.o 
> diagD.hat.o=rowVars(E.hat.o) 

One can then obtain the generalized least-squares estimates. 

> Dinv.hat = diag(diagD.hat.o^(-1)) 

> Hmtx=solve(t(ind.dum)%*%Dinv.hat%*%ind.dum)%*%t(ind.dum) 

%*%Dinv.hat 

> F.hat.g = Hmtx%*%rtn.rm 
> F.hat.gt=t(F.hat.g) 
> E.hat.g = rtn.rm - ind.dum%*%F.hat.g 
> diagD.hat.g = rowVars(E.hat.g) 
> t(Hmtx) 

FUNDAMENTAL FACTOR MODELS 

481 

[1,  ] 
[2,  ] 
[3,  ] 
[4,  ] 
[5,  1 
[6,  1 
[7,  ] 
[8,  ] 
[9,  1 
10,  ] 

fin 
0 . .1870 
0 . .2548 
0 . .2586 
0 . .2995 
0. . 0000 
0 , .0000 
0 . .0000 
0 , .0000 
0. . 0000 
0  . 0000 

tech 
0 . .0000 
0 . . 0000 
0 , . 0000 
0 . .0000 
0 . .2272 
0 . .4015 
0  .3713 
0  . 0000 
0  .0000 
0  .0000 

oth 
0 . .0000 
0 . .0000 
0 . . 0000 
0 . . 0000 
0 . . 0000 
0 . . 0000 
0 . .0000 
0 . .3319 
0 , .4321 
0 , .2360 

> cov.ind=ind.dum%*%var(F.hat.gt)%*%t(ind.dum) 

+ diag(diagD.hat.g) 

> sd.ind=sqrt(diag(cov.ind)) 
> corr.ind=cov.ind/outer(sd.ind,sd.ind) 
> print(corr.ind,digits=l,width=2) 

AGE  C 

MWD  MER  DELL  HPQ  IBM  AA  CAT  PG 

AGE  1.0  0.7  0.7  0.7  0.3  0.3  0.3  0.3  0.3  0.3 
0.7  1.0  0.8  0.8  0.3  0.4  0.4  0.3  0.3  0.3 
C 
MWD  0.7  0.8  1.0  0.8  0.3  0.4  0.4  0.3  0.4  0.3 
MER  0.7  0.8  0.8  1.0  0.3  0.4  0.4  0.3  0.4  0.3 
DELL  0.3  0.3  0.3  0.3  1.0  0.5  0.5  0.2  0.2  0.2 
HPQ  0.3  0.4  0.4  0.4  0.5  1.0  0.7  0.3  0.3  0.2 
IBM  0.3  0.4  0.4  0.4  0.5  0.7  1.0  0.3  0.3  0.2 
0.3  0.3  0.3  0.3  0.2  0.3  0.3  1.0  0.7  0.5 
AA 
CAT  0.3  0.3  0.4  0.4  0.2  0.3  0.3  0.7  1.0  0.6 
0.3  0.3  0.3  0.3  0.2  0.2  0.2  0.5  0.6  1.0 
PG 

The model-based correlations of stocks within an industrial sector are larger 
than their sample counterparts. For instance, the sample correlation between CAT 
and PG stock returns is only 0.1, but the correlation based on the fitted model is 
0.6. Finally, Figure 9.3 shows the time plots of the factor realizations based on the 
generalized least-squares estimation. 

Factor Mimicking Portfolio 
Consider the special case of BARRA factor models with a single factor. Here the 
WLS estimate of ft in Eq. (9.7) has a nice interpretation. Consider a portfolio 

o = (co\, , (Ok)' of the k assets that solves 

min(\o' Do) such that o' fi = 1. 

It turns out that the solution to this portfolio problem is given by 

o' = {p'D~'p)-\p'D-1). 

Thus, the estimated factor realization is the portfolio return 

ft = u'rt. 

482 

PRINCIPAL COMPONENT ANALYSIS AND FACTOR MODELS 

(a) 

(b) 

(c) 

Figure 9.3 Estimated factor realizations of BARRA industrial factor model for 10 monthly stock 
returns in 3 industrial sectors: (a) factor realizations: financial sector, (b) high-tech sector, and (c) other 
sector. 

If the portfolio oo is normalized such that £*=1 wi = 1, it is referred to as a factor 
mimicking portfolio. For multiple factors, one can apply the idea to each factor 
individually. 

Remark. In practice, the sample mean of an excess return is often not signif¬ 
icantly different from zero. Thus, one may not need to remove the sample mean 
before fitting a BARRA factor model. □ 

9.3.2 Fama-French Approach 

For a given asset fundamental (e.g., ratio of book-to-market value), Fama and 
French (1992) determined factor realizations using a two-step procedure. First, they 
sorted the assets based on the values of the observed fundamental. Then they formed 
a hedge portfolio, which is long in the top quintile (±) of the sorted assets and 
short in the bottom quintile of the sorted assets. The observed return on this hedge 
portfolio at time t is the observed factor realization for the given asset fundamental. 

PRINCIPAL COMPONENT ANALYSIS 

483 

The procedure is repeated for each asset fundamental under consideration. Finally, 
given the observed factor realizations {ft\t = 1, ..., T}, the betas for each asset 
are estimated using a time series regression method. These authors identify three 
observed fundamentals that explain high percentages of variability in excess returns. 
The three fundamentals used by Fama and French are (a) the overall market return 
(market excess return), (b) the performance of small stocks relative to large stocks 
(SMB, small minus big), and (c) the performance of value stocks relative to growth 
stocks (HML, high minus low). The size sorted by market equity and the ratio of 
book equity to market equity is used to define value and growth stocks with value 
stocks having high book equity to market equity ratio. 

Remark. The concepts of factor may differ between factor models. The three 
factors used in the Fama-French approach are three financial fundamentals. One 
can combine the fundamentals to create a new attribute of the stocks and refer to the 
resulting model as a single-factor model. This is particularly so because the model 
used is a linear statistical model. Thus, care must be exercised when one refers to 
the number of factors in a factor model. On the other hand, the number of factors 
is more well defined in statistical factor models, which we discuss next. □ 

9.4 PRINCIPAL COMPONENT ANALYSIS 

An important topic in multivariate time series analysis is the study of the covariance 
(or correlation) structure of the series. For example, the covariance structure of a 
vector return series plays an important role in portfolio selection. In what follows, 
we discuss some statistical methods useful in studying the covariance structure of 

a vector time series. 

Given a k-dimensional random variable r = (ri,..., rk)' with covariance matrix 
X,., a principal component analysis (PCA) is concerned with using a few linear 
combinations of r,- to explain the structure of £r. If r denotes the monthly log 
returns of k assets, then PCA can be used to study the main source of variations 
of these k asset returns. Here the keyword is few so that simplification can be 

achieved in multivariate analysis. 

9.4.1 Theory of PCA 

Principal component analysis applies to either the covariance matrix or the 
correlation matrix pr of r. Since the correlation matrix is the covariance matrix 
of the standardized random vector r* = S~lr, where S is the diagonal matrix 
of standard deviations of the components of r, we use covariance matrix in our 
theoretical discussion. Let w{ = (wn,..., wik)' be a k-dimensional real-valued 

vector, where i = 1,... ,k. Then 

k 

484 

PRINCIPAL COMPONENT ANALYSIS AND FACTOR MODELS 

is a linear combination of the random vector r. If r consists of the simple returns 
of k stocks, then yt is the return of a portfolio that assigns weight Wij to the 
;th stock. Since multiplying a constant to u;, does not affect the proportion 
of allocation assigned to the y'th stock, we standardize the vector u>; so that 

w>i=E*=i wfj = L 

Using properties of a linear combination of random variables, we have 

\ax(yi)— w-'ZrWi, i = (9.11) 

Cov(yi,yj) = w'j'ZrWj, i,j = l,...,k. (9.12) 

The idea of PC A is to find linear combinations w,- such that y,- and yj are uncorre¬ 
lated for i ^ j and the variances of y,- are as large as possible. More specifically: 

1. The first principal component of r is the linear combination yi = w\r that 

maximizes Var(yi) subject to the constraint w\ W\ = 1. 

2. The second principal component of r is the linear combination yj = w2r 
that maximizes Var(y2) subject to the constraints w2W2 = 1 and Cov(y2, yi) 
= 0. 

3. The i th principal component of r is the linear combination y; = w-r that 
maximizes Var(y,) subject to the constraints wjwj = 1 and Cov(y, , yj) = 0 
for j — 1,..., i — 1. 

Since the covariance matrix X,- is nonnegative definite, it has a spectral 
decomposition; see Appendix A of Chapter 8. Let (Aj, ei), ..., (A*, e*) be 
the eigenvalue-eigenvector pairs of T,r, where > X2 > • • • > > 0 and 
= (^il, • • •, eik)', which is properly normalized. We have the following 

statistical result. 

Result 9.1. The ith principal component of r is yt = e\r = eijrj f°r 

i = l,... ,k. Moreover, 

Var(y/) = e\ =kt, i = 1,..., k, 

Cov(y,-, yj) = e\Y.rej =0, 1 # j. 

If some eigenvalues Aare equal, the choices of the corresponding eigenvectors ei 
and hence y, are not unique. In addition, we have 

k k k 

J2 Varfc) = tr(Xr) = ^ A/ = Var(y;). 

(9.13) 

i=l i=l i=l 

The result of Eq. (9.13) says that 

Var (y,-) _ A.,- 

E/=i Var(r/) ^1 4-h 

PRINCIPAL COMPONENT ANALYSIS 

485 

Consequently, the proportion of total variance in r explained by the ith princi¬ 
pal component is simply the ratio between the ith eigenvalue and the sum of all 
eigenvalues of T,r. One can also compute the cumulative proportion of total vari¬ 

ance explained by the first i principal components [i.e., ^j)/(Hkj=i 'V/)]- 
In practice, one selects a small i such that the resulting cumulative proportion is 

large. 

Since tr(/Or) = k, the proportion of variance explained by the ith principal 
component becomes A; / k when the correlation matrix is used to perform the PCA. 
A by-product of the PCA is that a zero eigenvalue of Zr, or pr, indicates the 
existence of an exact linear relationship between the components of r. For instance, 
if the smallest eigenvalue A^ = 0, then by Result 9.1 Var(y^) = 0. Therefore, yk = 
J2kj=i ekjfj is a constant and there are only k - 1 random quantities in r. In this 
case, the dimension of r can be reduced. For this reason, PCA has been used in 
the literature as a tool for dimension reduction. 

9.4.2 Empirical PCA 

In application, the covariance matrix £, and the correlation matrix pr of the return 
vector r are unknown, but they can be estimated consistently by the sample covari¬ 
ance and correlation matrices under some regularity conditions. Assuming that the 
returns are weakly stationary and the data consist of {rt\t = 1, ..., T}, we have 

the following estimates: 

pr = S S 

(9.14) 

(9.15) 

where 5 = diag^oo,,-, ..., sjokkj} is the diagonal matrix of sample standard 
errors of rt. Methods to compute eigenvalues and eigenvectors of a symmetric 
matrix can then be used to perform the PCA. Most statistical packages now have 
the capability to perform principal component analysis. In R and S-Plus, the basic 
command of PCA is princomp, and in FinMetrics the command is mfactor. 

Example 9.1. Consider the monthly log stock returns of International Business 
Machines, Hewlett-Packard, Intel Corporation, J.P. Morgan Chase, and Bank of 
America from January 1990 to December 2008. The returns are in percentages and 
include dividends. The data set has 228 observations. Figure 9.4 shows the time 
plots of these five monthly return series. As expected, returns of companies in the 

same industrial sector tend to exhibit similar patterns. 

Denote the returns by r' = (IBM, HPQ, INTC, JPM, BAC). The sample mean 
vector of the returns is (0.70,0.99, 1.20, 0.82, 0.41)' and the sample covariance 

and correlation matrices are 

486 

PRINCIPAL COMPONENT ANALYSIS AND FACTOR MODELS 

1990 1995 2000 2005 

Year 

(c) 

Figure 9.4 Time plots of monthly log stock returns in percentages and including dividends for (a) 

International Business Machines, (b) Hewlett-Packard, (c) Intel, (d) J.P. Morgan Chase, and (e) Bank 
of America from January 1990 to December 2008. 

74.64 
42.28 
48.03 
30.10 
21.07 

112.22 

70.45 146.50 
42.42 
44.59 
29.24 
26.30 

1.00 
0.46 
1.00 
0.46  0.55 
1.00 
0.34  0.39  0.36 
1.00 
0.25  0.26  0.25  0.68 

106.04 
67.45  91.83 

- 

1.00 

Table 9.3 gives the results of PCA using both the covariance and correlation 
matrices. Also given are eigenvalues, eigenvectors, and proportions of variabilities 

PRINCIPAL COMPONENT ANALYSIS 

487 

TABLE 9.3 Results of Principal Component Analysis for Monthly Log Returns, 
Including Dividends of Stocks of IBM, Hewlett-Packard, Intel, J.P. Morgan Chase, 
and Bank of America from January 1990 to December 2008" 

Eigenvalue 
Proportion 
Cumulative 
Eigenvector 

Eigenvalue 
Proportion 
Cumulative 
Eigenvector 

Using Sample Covariance Matrix 

112.93 

0.213 
0.748 
0.139 
0.279 
0.478 
-0.550 
-0.610 

57.43 

0.108 
0.856 
-0.264 
-0.701 
0.652 
0.013 
0.119 

Using Sample Correlation Matrix 

1.072 
0.214 
0.736 
0.341 
0.356 
0.385 
-0.469 
-0.623 

0.569 
0.114 
0.850 
0.837 
-0.380 
-0.389 
-0.046 
0.035 

284.17 
0.535 
0.535 
0.330 
0.483 
0.581 
0.448 
0.347 

2.607 
0.522 
0.522 
0.428 
0.460 
0.451 
0.479 
0.416 

46.81 

0.088 
0.944 
0.895 
-0.430 
-0.096 
-0.064 
-0.009 

0.451 
0.090 
0.940 
-0.002 
0.704 
-0.704 
0.052 
-0.073 

29.87 

0.056 
1.000 
-0.014 
-0.116 
-0.016 
0.702 
-0.702 

0.301 
0.060 
1.000 
0.008 
0.145 
0.022 
-0.739 
0.658 

“The eigenvectors are in columns. 

explained by the principal components. Consider the correlation matrix and denote 
the sample eigenvalues and eigenvectors by A; and %. We have 

Xi = 2.608, ?i = (0.428, 0.460, 0.451, 0.479, 0.416)', 

A2 = 1.072, 72 = (0.341, 0.356, 0.385, -0.469, -0.623)' 

for the first two principal components. These two components explain about 74% 
of the total variability of the data, and they have interesting interpretations. The first 
component is a roughly equally weighted linear combination of the stock returns. 
This component might represent the general movement of the stock market and 
hence is a market component. The second component represents the difference 
between the two industrial sectors—namely, technologies versus financial services. 
It might be an industrial component. Similar interpretations of principal components 

can also be found by using the covariance matrix of r. 

An informal but useful procedure to determine the number of principal compo¬ 
nents needed in an application is to examine the scree plot, which is the time plot of 
the eigenvalues A; ordered from the largest to the smallest (i.e., a plot of A.,- versus 
i). Figure 9.5(a) shows the scree plot for the five stock returns of Example 9.1. By 
looking for an elbow in the scree plot, indicating that the remaining eigenvalues 

488 

PRINCIPAL COMPONENT ANALYSIS AND FACTOR MODELS 

LO 
c\i 

o 
c\i 

LO 

LO 
o 

co 

0 

03 
> 
C 
V CM 
D) 
Lu 

0 
_I3 

03 

> C 

0 
CT> 

Lu 

1 2 3 4 5 

2 3 4 5 

Component 

(a) 

Component 

(b) 

Figure 9.5 Scree plots for two 5-dimensional asset returns: (a) series of Example 9.1 and (b) bond 
index returns of Example 9.3. 

are relatively small and all about the same size, one can determine the appropriate 
number of components. For both plots in Figure 9.5, two components appear to be 
appropriate. Finally, except for the case in which Xj = 0 for j > i, selecting the 
first i principal components only provides an approximation to the total variance 
of the data. If a small i can provide a good approximation, then the simplification 
becomes valuable. 

Remark. The R and S-Plus commands used to perform the PCA are given 
below. The command princomp gives the square root of the eigenvalue and 
denotes it as standard deviation. 

> rtn=read.table(''m-5clog-9008.txt''),header=T) 
> pca.cov = princomp(rtn) 
> names(pea.cov) 
> summary(pea.cov) 
> pea.cov$loadings 
> screeplot(pea.cov) 
> pea.corr=princomp(rtn,cor=T) 
> summary(pac.corr) q 

STATISTICAL FACTOR ANALYSIS 

489 

9.5 STATISTICAL FACTOR ANALYSIS 

We now turn to statistical factor analysis. One of the main difficulties in multi¬ 
variate statistical analysis is the “curse of dimensionality.” For serially correlated 
data, the number of parameters of a parametric model often increases dramatically 
when the order of the model or the dimension of the time series is increased. Sim¬ 
plifying methods are often sought to overcome the curse of dimensionality. From 
an empirical viewpoint, multivariate data often exhibit similar patterns indicating 
the existence of common structure hidden in the data. Statistical factor analysis is 
one of those simplifying methods available in the literature. The aim of statistical 
factor analysis is to identify, from the observed data, a few factors that can account 
for most of the variations in the covariance or correlation matrix of the data. 

Traditional statistical factor analysis assumes that the data have no serial cor¬ 
relations. This assumption is often violated by financial data taken with frequency 
less than or equal to a week. However, the assumption appears to be reasonable 
for asset returns with lower frequencies (e.g., monthly returns of stocks or market 
indexes). If the assumption is violated, then one can use the parametric models 
discussed in this book to remove the linear dynamic dependence of the data and 
apply factor analysis to the residual series. 

In what follows, we discuss statistical factor analysis based on the orthogonal 
factor model. Consider the return rt = (ru,..., rkt)' of k assets at time period t and 
assume that the return series rt is weakly stationary with mean p and covariance 
matrix £r. The statistical factor model postulates that rt is linearly dependent on 
a few unobservable random variables ft = (fit, .. ■, fmi)' and k additional noises 
€t = (e\t, ..., e/ctf. Here m < k, f, are the common factors, and clt are the errors. 
Mathematically, the statistical factor model is also in the form of Eq. (9.1) except 
that the intercept a is replaced by the mean return p. Thus, a statistical factor 

model is in the form 

rt - p = fift + et, (9.16) 

where fi = [fiij]kxm is the matrix of factor loadings, fijj is the loading of the / th 
variable on the jth factor, and eit is the specific error of rit. A key feature of 
the statistical factor model is that the m factors fi, and the factor loadings fitj are 
unobservable. As such, Eq. (9.16) is not a multivariate linear regression model, 
even though it has a similar appearance. This special feature also distinguishes a 
statistical factor model from other factor models discussed earlier. 

The factor model in Eq. (9.16) is an orthogonal factor model if it satisfies the 

following assumptions: 

1. E(ft) = 0 and Cov(/,) = Im, the m x m identity matrix. 

2. E(et) = 0 and Cov(er) = D = diag{of,..., of} (i.e., D is a k x k diagonal 

matrix). 

3. ft and et are independent so that Cov(ft, et) = E(f te't) = 0mxk- 

490 

PRINCIPAL COMPONENT ANALYSIS AND FACTOR MODELS 

Under the previous assumptions, it is easy to see that 

T,r = Cov(r,) = E[(rt - fi)(rt - /*.)'] 

= E[(Pft + et)(l3ft+ety] 

= 00'+ D (9.17) 

and 

Cov(rt, ft) = E[(rt - p)ft] = 0E(ftf’) + E(etf't) = 0. (9.18) 

Using Eqs. (9.17) and (9.18), we see that for the orthogonal factor model in Eq. 
(9.16) 

Var(r,,) — 0f\ +-b pfm + of, 

Cov(rit, rjt) = 0i\0j\ + ■ • • T" PimPjmi 

Cov(rit, fjt) = Ay- 

The quantity 0fx-\-b , which is the portion of the variance of rit contributed 
by the m common factors, is called the communality. The remaining portion of 
of the variance of rlt is called the uniqueness or specific variance. Let cf = /3f{ + 
• • • + pfm be the communality, which is the sum of squares of the loadings of the 
/th variable on the m common factors. The variance of component r,r becomes 
Var (rit) = c] + of. 

In practice, not every covariance matrix has an orthogonal factor representation. 
In other words, there exists a random variable r, that does not have any orthogonal 
factor representation. Furthermore, the orthogonal factor representation of a random 
variable is not unique. In fact, for any m x m orthogonal matrix P satisfying P P' 
= P'P = I, let 0* = 0P and f* = P ft. Then 

rt-p = (lf,+€t = fiPP'f, +et= 0*f* + €t. 

In addition, E(f*) = 0 and Cov(/*) = P'Cov(ft)P = P'P = I. Thus, 0* and 
/* form another orthogonal factor model for r,. This nonuniqueness of orthogonal 
factor representation is a weakness as well as an advantage for factor analysis. It 
is a weakness because it makes the meaning of factor loading arbitrary. It is an 
advantage because it allows us to perform rotations to find common factors that 
have nice interpretations. Because P is an orthogonal matrix, the transformation 
f* = P' ft is a rotation in the m-dimensional space. 

9.5.1 Estimation 

The orthogonal factor model in Eq. (9.16) can be estimated by two methods. 
The first estimation method uses the principal component analysis of the previ¬ 
ous section. This method does not require the normality assumption of the data nor 

STATISTICAL FACTOR ANALYSIS 

491 

the prespecification of the number of common factors. It applies to both the covari¬ 
ance and correlation matrices. But as mentioned in PCA, the solution is often an 
approximation. The second estimation method is the maximum-likelihood method 
that uses normal density and requires a prespecification for the number of common 
factors. 

Principal Component Method 
Again let (Ai, e)), ..., (A,t.’e'*) be pairs of the eigenvalues and eigenvectors of the 
sample covariance matrix Xr, where Ai > A 2 > • • • > A*. Let m < k be the number 
of common factors. Then the matrix of factor loadings is given by 

? = [&■] - 

(9.19) 

The estimated specific variances are the diagonal elements of the matrix Z, — /?/? . 
That is, D = diag {of, ..., of), where of = oii<r — Yf'j=\ Pfj > where &u>r is the 

(z, z)th element of Xr. The communalities are estimated by 

i~i Art Art 

Ci = Pi\ 9-f Pirn- 

The error matrix caused by approximation is 

% ~ iffi + D). 

Ideally, we would like this matrix to be close to zero. It can be shown that the 
sum of squared elements of £, - (/?/? + D) is less than or equal to A^+1 H-h 

A|. Therefore, the approximation error is bounded by the sum of squares of the 

neglected eigenvalues. 

From the solution in Eq. (9.19), the estimated factor loadings based on the 
principal component method do not change as the number of common factors m 

is increased. 

Maximum-Likelihood Method 
If the common factors ft and the specific factors et are jointly normal, then rt 
is multivariate normal with mean p, and covariance matrix Yr = /?/T + D. The 
maximum-likelihood method can then be used to obtain estimates of /? and D under 
the constraint fi'D~l = A, which is a diagonal matrix. Here p, is estimated by 
the sample mean. For more details of this method, readers are referred to Johnson 

and Wichem (2007). 

In using the maximum-likelihood method, the number of common factors must 
be given a priori. In practice, one can use a modified likelihood ratio test to check 
the adequacy of a fitted m-factor model. The test statistic is 

LR(m) = - [T - 1 - |(2k + 5) - §m] (in |£r| - In \pp' + D|) , (9.20) 

492 

PRINCIPAL COMPONENT ANALYSIS AND FACTOR MODELS 

which, under the null hypothesis of m factors, is asymptotically distributed as a 
chi-squared distribution with ^[(k — m)2 — k — m] degrees of freedom. We discuss 
some methods for selecting m in Section 9.6.1. 

9.5.2 Factor Rotation 

As mentioned before, for any m x m orthogonal matrix P, 

rt-H = fift + €t = Fft + <t, 

where /?* = j3P and /* = P'ft. In addition, 

PP' + D = PPP'P' + D = + D. 

This result indicates that the communalities and the specific variances remain 
unchanged under an orthogonal transformation. It is then reasonable to find an 
orthogonal matrix P to transform the factor model so that the common factors 
have nice interpretations. Such a transformation is equivalent to rotating the com¬ 
mon factors in the m -dimensional space. In fact, there are infinite possible factor 
rotations available. Kaiser (1958) proposes a varimax criterion to select the rotation 
that works well in many applications. Denote the rotated matrix of factor loadings 
by /?* = [ft*;] and the ith communality by cj. Define ft*. = P*j/Ci to be the rotated 
coefficients scaled by the (positive) square root of communalities. The varimax 
procedure selects the orthogonal matrix P that maximizes the quantity 

This complicated expression has a simple interpretation. Maximizing V corresponds 
to spreading out the squares of the loadings on each factor as much as possible. 
Consequently, the procedure is to find groups of large and negligible coefficients 
in any column of the rotated matrix of factor loadings. In a real application, factor 
rotation is used to aid the interpretations of common factors. It may be helpful in 
some applications, but not informative in others. There are many criteria available 
for factor rotation. 

9.5.3 Applications 

Given the data {rf} of asset returns, the statistical factor analysis enables us to 
search for common factors that explain the variabilities of the returns. Since factor 
analysis assumes no serial correlations in the data, one should check the validity of 
this assumption before using factor analysis. The multivariate portmanteau statistics 
can be used for this purpose. If serial correlations are found, one can build a 
VARMA model to remove the dynamic dependence in the data and apply the factor 

STATISTICAL FACTOR ANALYSIS 

493 

analysis to the residual series. For many returns series, the correlation matrix of 
the residuals of a linear model is often very close to the correlation matrix of the 
original data. In this case, the effect of dynamic dependence on factor analysis is 
negligible. 

We consider three examples in this section. The first and third examples use the 
R or S-Plus to perform the analysis and the second example uses Minitab. Other 
packages can also be used. 

Example 9.2. Consider again the monthly log stock returns of IBM, Hewlett- 
Parkard, Intel, J.P. Morgan Chase, and Bank of America used in Example 9.1. 
To check the assumption of no serial correlations, we compute the portmanteau 
statistics and obtain <25(1) = 39.99, <2s(5) = 160.60, and <2s(10) = 293.04. Com¬ 
pared with chi-squared distributions with 25, 125, and 250 degrees of freedom, the 
p values of these test statistics are 0.029, 0.017, and 0.032, respectively. Therefore, 
there exists some minor serial dependence in the returns, but the dependence is not 
significant at the 1 % level. For simplicity, we ignore the serial dependence in factor 

analysis. 

Table 9.4 shows the results of factor analysis based on the correlation matrix 
using the maximum-likelihood method. We assume that the number of common 
factors is 2, which is reasonable according to the principal component analysis of 
Example 9.1. From the table, the factor analysis reveals several interesting findings: 

• The two factors identified by the maximum-likelihood method explain about 

60% of the variability of the stock returns. 

• Based on the rotated factor loadings, the two common factors have some 
meaningful interpretations. The technology stocks (IBM, Hewlett-Packard, 

TABLE 9.4 Factor Analysis of Monthly Log Stock Returns of IBM, 
Hewlett-Packard, Intel, J.P. Morgan Chase, and Bank of America0 

Estimates of 
Factor Loadings 

Variable 

h 

h 

Rotated 
Factor Loadings 
/•* 
J 2 

/r 

Communalities 

\-of 

Maximum-Likelihood Method 

IBM 
HPQ 
INTC 
JPM 
BAC 
Variance 
Proportion 

0.327 
0.348 
0.337 
0.734 
0.960 
1.801 
0.360 

0.530 
0.669 
0.647 
0.186 
-0.111 
1.193 
0.239 

0.593 
0.733 
0.709 
0.358 
0.124 
1.535 
0.307 

0.189 
0.177 
0.171 
0.667 
0.958 
1.459 
0.292 

0.387 
0.568 
0.531 
0.573 
0.934 
2.994 
0.599 

“The returns include dividends and are from January 1990 to December 2008. The analysis is based on 

the sample cross-correlation matrix and assumes two common factors. 

494 

PRINCIPAL COMPONENT ANALYSIS AND FACTOR MODELS 

and Intel) load heavily on the first factor, whereas the financial stocks (J.P. 
Morgan Chase and Bank of America) load highly on the second factor. These 
two rotated factors jointly differentiate the industrial sectors. 

• In this particular instance, the varimax rotation seems to alter the ordering of 

the two common factors. 

• The specific variance of IBM stock returns is relatively large, indicating that 

the stock has its own features that are worth further investigation. 

Example 9.3. In this example, we consider the monthly log returns of U.S. 
bond indexes with maturities in 30 years, 20 years, 10 years, 5 years, and 1 year. 
The data are described in Example 8.2 but have been transformed into log returns. 
There are 696 observations. As shown in Example 8.2, there is serial dependence 
in the data. However, removing serial dependence by fitting a VARMA(2,1) model 
has hardly any effects on the concurrent correlation matrix. As a matter of fact, the 
correlation matrices before and after fitting a VARMA(2,1) model are 

1.0 
0.98 
0.92 
0.85 
0.63 

1.0 
0.98 
0.92 
0.85 
0.66 

1.0 
0.91 
0.86 
0.64 

1.0 
0.92 
0.86 
0.67 

1.0 
0.90 
0.67 

1.0 
0.90 
0.71 

1.0 
0.81 

1.0 

1.0 
0.84 

1.0 

where pn is the correlation matrix of the original log returns. Therefore, we apply 
factor analysis directly to the return series. 

Table 9.5 shows the results of statistical factor analysis of the data. For both 
estimation methods, the first two common factors explain more than 90% of the 
total variability of the data. Indeed, the high communalities indicate that the specific 
variances are very small for the five bond index returns. Because the results of the 
two methods are close, we only discuss that of the principal component method. 
The unrotated factor loadings indicate that (a) all five return series load roughly 
equally on the first factor, and (b) the loadings on the second factor are positively 
correlated with the time to maturity. Therefore, the first common factor represents 
the general U.S. bond returns, and the second factor shows the “time-to-maturity” 
effect. Furthermore, the loadings of the second factor sum approximately to zero. 
Therefore, this common factor can also be interpreted as the contrast between 
long-term and short-term bonds. Here a long-term bond means one with maturity 
10 years or longer. For the rotated factors, the loadings are also interesting. The 
loadings for the first rotated factor are proportional to the time to maturity, whereas 
the loadings of the second factor are inversely proportional to the time to maturity. 

STATISTICAL FACTOR ANALYSIS 

495 

TABLE 9.5 Factor Analysis of Monthly Log Returns of U.S. Bond Indexes with 
Maturities in 30 Years, 20 Years, 10 Years, 5 Years, and 1 Year" 

Estimates of 
Factor Loadings 

Variable 

h 

fi 

Rotated 
Factor Loadings 
n 

/f 

Communalities 

1 -of 

30 years 
20 years 
10 years 
5 years 
1 year 
Variance 
Proportion 

30 years 
20 years 
10 years 
5 years 
1 year 
Variance 
Proportion 

0.952 
0.954 
0.956 
0.955 
0.800 
4.281 
0.856 

0.849 
0.857 
0.896 
1.000 
0.813 
3.918 
0.784 

Principal Component Method 

0.253 
0.240 
0.140 
-0.142 
-0.585 
0.504 
0.101 

0.927 
0.922 
0.866 
0.704 
0.325 
3.059 
0.612 

Maximum-Likelihood Method 

-0.513 
-0.486 
-0.303 
0.000 
0.123 
0.607 
0.121 

0.895 
0.876 
0.744 
0.547 
0.342 
2.538 
0.508 

0.333 
0.345 
0.429 
0.660 
0.936 
1.726 
0.345 

0.430 
0.451 
0.584 
0.837 
0.747 
1.987 
0.397 

0.970 
0.968 
0.934 
0.931 
0.982 
4.785 
0.957 

0.985 
0.970 
0.895 
1.000 
0.675 
4.525 
0.905 

aThe data are from January 1942 to December 1999. The analysis is based on the sample cross¬ 

correlation matrix and assumes two common factors. 

Example 9.4. Again, consider the monthly excess returns of the 10 stocks 
in Table 9.2. The sample span is from January 1990 to December 2003 and the 
returns are in percentages. Our goal here is to demonstrate the use of statistical 
factor models using the R or S-Plus command factanal. We started with a two- 
factor model, but it is rejected by the likelihood ratio test of Eq. (9.20). The test 
statistic is LR(2) = 72.96. Based on the asymptotic x%6 distribution, p value of 

the test statistic is close to zero. 

> rtn=read.table(''m-barra-9003.txt' ',header=T) 
> stat.fac=factanal(rtn,factors=2,method='mle') 

> stat.fac 
Sums of squares of loadings: 

Factorl Factor2 
2.696479 2.19149 

Component names: 

"loadings" "uniquenesses" "correlation" "criteria" 
"factors" "dof" "method" "center" "scale" "n.obs" 

"scores" "call" 

496 

PRINCIPAL COMPONENT ANALYSIS AND FACTOR MODELS 

We then applied a three-factor model that appears to be reasonable at the 5% 

significance level. The p value of the LR(3) statistic is 0.0892. 

> stat.fac=factanal(rtn,factor=3,method='mle') 
> stat.fac 
Test of the hypothesis that 3 factors are sufficient 
versus the alternative that more are required: 
The chi square statistic is 26.48 on 18 degrees of freedom. 
The p-value is 0.0892 

> summary(stat.fac) 
Importance of factors: 

SS loadings 
Proportion Var 
Cumulative Var 

Factorl 
2.635 
0.264 
0.264 

Factor2 
1.825 
0.183 
0.446 

Factor3 
1.326 
0.133 
0.579 

Uniquenesses: 
AGE C 

IBM 
0.479 0.341 0. .201 0.216  0.690 0.346  0.638 

MWD MER  DELL HPQ 

AA CAT PG 
0.417 0.000 0.885 

Loadings: 

0.121 
0.213 

Factorl Factor2 Factor3 
0.217 
0.259 
0.356 
0.329 
0.547 
0.771 
0.515 
0.546 
0.138 

AGE  0.678 
C 
0.739 
MWD  0.817 
MER  0.819 
DELL  0.102 
HPQ  0.230 
IBM  0.200 
AA 
0.19.4 
CAT  0.198 
0.331 
PG 

0.238 
0.497 
0.970 

The factor  loadings can  also be 

> plot(loadings(stat.fac) 

and the plots are in Figure 9.6. From the plots, factor 1 represents essentially the 
financial service sector, and factor 2 mainly consists of the excess returns from the 
high-tech stocks and the Alcoa stock. Factor 3 depends heavily on excess returns 
of CAT and AA stocks and, hence, represents the remaining industrial sector. 

Factor rotation can be obtained using the command rotate, which allows for 
many rotation methods, and factor realizations are available from the command 
predict. 

STATISTICAL FACTOR ANALYSIS 

497 

00 
o 

CD 
o 

o 
CM 
o 
q 
o 

Factorl 

Factor2 

Facto r3 

Figure 9.6 Plots of factor loadings when a 3-factor statistical factor model is fitted to 10 monthly 

excess stock returns in Table 9.2. 

> stat.fac2 = rotate(stat.fac,rotation^'quartimax') 

> loadings(stat.fac2) 

0.124 

Factorl  Factor2  Factor3 
0.171 
0.216 
0.291 
0.264 
0.536 
0.753 
0.518 
0.575 
0.219 

AGE  0.700 
0.772 
C 
MWD  0.844 
MER  0.844 
DELL  0.144 
HPQ  0.294 
IBM  0.258 
0.278 
AA 
CAT  0.293 
0.334 
PG 
> factor.real=predict(stat.fac,type='weighted. Is' ) 

0.164 
0.418 
0.931 

Finally, we obtained the correlation matrix of the 10 excess returns based on the 
fitted three-factor statistical factor model. As expected, the correlations are closer to 
their sample counterparts than those of the industrial factor model in Section 9.3.1. 

498 

PRINCIPAL COMPONENT ANALYSIS AND FACTOR MODELS 

One can also use GMVP to compare the covariance matrices of the returns and the 
statistical factor model. 

> corr.fit=fitted(stat.fac) 
> print(corr.fit,digits=l,width=2) 

AGE 

C  MWD  MER  DELL  HPQ  IBM  AA  CAT  PG 

AGE  1.0  0.6  0.6  0.6  0.19  0.3  0.3  0.3  0.3  0.2 
0.6  1.0  0.7  0.7  0.22  0.4  0.3  0.4  0.4  0.3 
C 
MWD  0.6  0.7  1.0  0.8  0.28  0.5  0.4  0.4  0.3  0.3 
MER  0.6  0.7  0.8  1.0  0.26  0.5  0.4  0.4  0.3  0.3 
DELL  0.2  0.2  0.3  0.3  1.00  0.5  0.3  0.3  0.1  0.0 
HPQ  0.3  0.4  0.5  0.4  0.45  1.0  0.5  0.5  0.2  0.1 
IBM  0.3  0.3  0.4  0.3  0.31  0.5  1.0  0.4  0.3  0.1 
AA 
0.3  0.4  0.4  0.4  0.33  0.5  0.4  1.0  0.6  0.1 
CAT  0.3  0.4  0.3  0.3  0.11  0.2  0.3  0.6  1.0  0.1 
0.2  0.3  0.3  0.3  0.03  0.1  0.1  0.1  0.1  1.0 
PG 

9.6 ASYMPTOTIC PRINCIPAL COMPONENT ANALYSIS 

So far, our discussion of PCA assumes that the number of assets is smaller than the 
number of time periods, that is, k < 7. To deal with situations of a small 7 and 
large k, Conner and Korajczyk (1986, 1988) developed the concept of asymptotic 
principal component analysis (APCA), which is similar to the traditional PCA but 
relies on the asymptotic results as the number of assets k increases to infinity. Thus, 
the APCA is based on eigenvalue-eigenvector analysis of the 7 x 7 matrix 

«T = l(R-lTr)(R-lTF')', 

k 

where lT is the 7-dimensional vector of ones and r = (ri,...,r*)' with rt = 
(1 'rRi)/T being the sample mean of the ith return series. Conner and Korajczyk 
(1988) showed that as k -> oo eigenvalue-eigenvector analysis of ft^ is equivalent 
to the traditional statistical factor analysis. In other words, the APCA estimates of 
the factors ft are the first m eigenvectors^ of ft^. Let F, be the m x 7 matrix 
consisting of the first m eigenvectors of ftr. Then ft is the tth column of Ft. 
Using an idea similar to the estimation of BARRAJactor models, Connor and 
Korajczyk (1988) propose refining the estimation of ft as follows: 

1. Use the sample covariance matrix fty to obtain an initial estimate of ft for 

^ ■_ 

t = 1.....7. 

2. For each asset, perform the OLS estimation of the model 

rit =at +Pift +eit, t = 1,... ,T, 

where fi, = ..., (iim) and compute the residual variance a2. 

ASYMPTOTIC PRINCIPAL COMPONENT ANALYSIS 

499 

3. Form the diagonal matrix D = diagfdj2,..., a£} and rescale the returns as 

4. Compute the T x T covariance matrix using /?* as 

S2* = -(/?*- 1tK)(r* - Irr'*)', 

k 

where r* is the ^-dimensional vectorof the column means of /?*, and perform 
eigenvalue-eigenvector analysis of ft* to obtain a refined estimate of f t. 

9.6.1 Selecting the Number of Factors 

Two methods are available in the literature to help select the number of factors 
in factor analysis. The first method proposed by Connor and Korajczyk (1993) 
makes use of the idea that if m is the proper number of common factors, then 
there should be no significant decrease in the cross-sectional variance of the asset 
specific error eit when the number of factors moves from m to m + 1. The second 
method proposed by Bai and Ng (2002) adopts some information criteria to select 
the number of factors. This latter method is based on the observation that the 
eigenvalue-eigenvector analysis of ft^ solves the least-squares problem 

Assume that there are m factors so that ft is m-dimensional. Let of(m) be the 
residual variance of the inner regression of the prior least-squares problem for 
asset i. This is done by using ft obtained from the APCA analysis. Define the 
cross-sectional average of the residual variances as 

The criteria proposed by Bai and Ng (2002) are 

where M is a prespecified positive integer denoting the maximum number of factors 
and PkT = min(Vk, y/f). One selects m that minimizes either Cp\{m) or Cpz(m) 
for 0 < m < M. In practice, the two criteria may select different numbers of factors. 

500 

PRINCIPAL COMPONENT ANALYSIS AND FACTOR MODELS 

9.6.2 An Example 

To demonstrate asymptotic principal component analysis, we consider monthly sim¬ 
ple returns of 40 stocks from January 2001 to December 2003 for 36 observations. 
Thus, we have k = 40 and T = 36. The tick symbols of stocks used are given in 
Table 9.6. These stocks are among those heavily traded on NASDAQ and the NYSE 
on a particular day of September 2004. The main S-Plus command used is mfactor. 
To select the number of factors, we used the two methods discussed earlier. 
The Connor-Korajczyk method selects m = 1, whereas the Bai-Ng method uses 
m = 6. For the latter method, the two criteria provide different results. 

> dim(rtn) % rtn is the return data. 
[1] 36 40 
> nf.ck=mfactor(rtn,k='ck',max.k=10,sig=0.05) 
> nf.ck 
Call: 
mfactor(x = rtn, k = "ck", max.k = 10, sig = 0.05) 

Factor Model: 

Factors Variables Periods 
1 40 36 

Factor Loadings: 

Min. 1st Qu. Median Mean 3rd Qu. Max. 
F.l 0.069 0.432 0.629 0.688 1.071 1.612 

Regression R-squared: 

Min. 1st Qu. Median Mean 3rd Qu. Max. 
0.090 0.287 0.487 0.456 0.574 0.831 
> nf,bn=mfactor(rtn,k='bn',max.k=10,sig=0.05) 
Warning messages: 

Cpl and Cp2 did not yield same result. The smaller one 

is used. 

> nf.bn$k 
[1] 6 

TABLE 9.6 Tick Symbols of Stocks Used in Asymptotic Principal Component 
Analysis for Sample Period from January 2001 to December 2003 

Market 

NASDAQ 

NYSE 

INTC 
ORCL 
YHOO 
ERTS 
LU 
GE 
F 
TYC 

Tick Symbol 

SUNW 
COCO 
QCOM 
ADCT 
NT 
XOM 
C 
NOK 

MSFT 
SIRI 
JDSU 
EBAY 
PFE 
TXN 
TWX 
HPQ 

CSCO 
CORV 
CIEN 
AAPL 
BAC 
FRX 
MOT 
WMT 

AMAT 
SUPG 
DELL 
JNPR 
BSX 
Q 
JPM 
AMD 

EXERCISES 

501 

Using m = 6, we apply APCA to the returns. The scree plot and estimated factor 

returns can also be obtained. 

> apca = mfactor(rtn,k=6) 

> apca 

Call: 

mfactor(x = rtn, k = 6) 

Factor Model: 

Factors Variables Periods 

6 40 36 

Factor Loadings: 

Min  1st  Qu.  Median 

Mean  3rd Qu.  Max. 

0 . . 048 

-1, .737 

-1, . 512 

0.  349 

0 .  084 

0 .  002 

-0 . .965 

-0 .  035 

-0 , .722 

-0 .  008 

-0  .840 

-0.  088 

0 . .561 

0 . .216 

0. . 076 

0 . . 078 

0 . . 056 

0  . 003 

0  . 643 

0  .214 

0  .102 

0  .048 

0  .066 

0  .003 

0 .  952 

2  .222 

0 .  323 

1  . 046 

0 .  255 

1  .093 

0.  202 

0  .585 

0 .  214 

0  .729 

0 .  071 

0  . 635 

Regression R-squared: 

Min. 1st Qu. Median Mean 3rd Qu. Max. 

0.219 0.480 0.695 0.651 0.801 0.999 

> screeplot.mfactor(apca) 

> fplot(factors(apca)) 

Figure 9.7 shows the scree plot of the APCA for the 40 stock returns. The 6 
common factors used explain about 89.4% of the variability. Figure 9.8 gives the 
time plots of the returns of the 6 estimated factors. 

EXERCISES 

9.1. Consider the monthly simple excess returns, in percentages and including 
dividends, of 13 stocks and the S&P 500 composite index from January 1990 
to December 2008. The monthly 3-month Treasury bill rate in the secondary 
market is used as the risk-free interest rate to compute the excess returns. The 
tick symbols for the stocks are AA, AXP, CAT, DE, F, FDX, HPQ, IBM, JNJ, 
KMB, MMM, PG, and WFC. The data are in the file m-fac-ex-9008. txt. 
Perform the market model analysis of Section 9.2.1 for the 13 stock returns 
to obtain the estimates of $, of, and R2 for each stock return series. 

9.2. Consider the monthly log stock returns, in percentages and including 
dividends, of Merck & Company, Johnson & Johnson, General Electric, 
General Motors, Ford Motor Company, and value-weighted index from 
January 1960 to December 2008; see the file m-mrk2vw.txt, of Exercise 8.1 

of Chapter 8. 
(a) Perform a principal component analysis of the data using the sample 

covariance matrix. 

502 

PRINCIPAL COMPONENT ANALYSIS AND FACTOR MODELS 

0.446 

F.1 F.2 F.3 F.4 F.5 F.6 F.7 

Figure 9.7 Scree plot of asymptotic principal component analysis applied to monthly simple returns 
of 40 stocks. Sample period is from January 2001 to December 2003. 

(b) Perform a principal component analysis of the data using the sample cor¬ 

relation matrix. 

(c) Perform a statistical factor analysis on the data. Identify the number of 
common factors. Obtain estimates of factor loadings using both the prin¬ 
cipal component and maximum-likelihood methods. 

9.3. The file m-excess-cl0sp-9003.txt contains the monthly simple excess 
returns of 10 stocks and the S&P 500 index. The 3-month Treasury bill rate 
on the secondary market is used to compute the excess returns. The sample 
period is from January 1990 to December 2003 for 168 observations. The 11 
columns in the file contain the returns for ABT, LLY, MRK, PFE, F, GM, 
BP, CVX, RD, XOM, and SP5, respectively. Analyze the 10 stock excess 
returns using the single-factor market model. Plot the beta estimate and R2 
for each stock, and use the global minimum variance portfolio to compare the 
covariance matrices of the fitted model and the data. 

9.4. Again, consider the 10 stock returns in m-excess-cl0sp-9003.txt. The 
stocks are from companies in 3 industrial sectors. ABT, LLY, MRK, and PFE 
are major drug companies, F and GM are automobile companies, and the rest 
are big oil companies. Analyze the excess returns using the BARRA industrial 
factor model. Plot the 3-factor realizations and comment on the adequacy of 
the fitted model. 

9.5. Again, consider the 10 excess stock returns in the file m-excess-clOsp- 
9003.txt. Perform a principal component analysis on the returns and obtain 

REFERENCES 

503 

Factor returns 

0 10 20 30 

-1-1-1— 
0 10 20 30 

0 10 20 30 

Figure 9.8 Time plots of factor returns derived from applying asymptotic principal component analysis 

to monthly simple returns of 40 stocks. Sample period is from January 2001 to December 2003. 

the scree plot. How many common factors are there? Why? Interpret the 

common factors. 

9.6. Again, consider the 10 excess stock returns in the file m-excess-clOsp- 
9 0 03 . txt. Perform a statistical factor analysis. How many common factors 
are there if the 5% significance level is used? Plot the estimated factor loadings 
of the fitted model. Are the common factors meaningful? 

9.7. The file m-fedip.txt contains year, month, effective federal funds rate, 
and the industrial production index from July 1954 to December 2003. The 
industrial production index is seasonally adjusted. Use the federal funds rate 
and the industrial production index as the macroeconomic variables. Fit a 
macroeconomic factor model to the 10 excess returns in m-excess-clOsp- 
9 003 . txt. You can use a VAR model to obtain the surprise series of the 
macroeconomic variables. Comment on the fitted factor model. 

REFERENCES 

Alexander, C. (2001). Market Models: A Guide to Financial Data Analysis. Wiley, Hoboken, 

NJ. 

Bai, J. and Ng, S. (2002). Determining the number of factors in approximate factor models. 

Econometrica 70: 191-221. 

504 

PRINCIPAL COMPONENT ANALYSIS AND FACTOR MODELS 

Campbell, J. Y., Lo, A. W., and MacKinlay, A. C. (1997). The Econometrics of Financial 

Markets. Princeton University Press, Princeton, NJ. 

Chen, N. F., Roll, R., and Ross, S. A. (1986). Economic forces and the stock market. Journal 

of Business 59: 383-404. 

Connor, G. (1995). The three types of factor models: A comparison of their explanatory 

power. Financial Analysts Journal 51: 42-46. 

Connor, G. and Korajczyk, R. A. (1986). Performance measurement with the arbitrage 
pricing theory: A new framework for analysis. Journal of Financial Economics 15: 
373-394. 

Connor, G. and Korajczyk, R. A. (1988). Risk and return in an equilibrium APT: Application 

of a new test methodology. Journal of Financial Economics 21: 255-289. 

Connor, G. and Korajczyk, R. A. (1993). A test for the number of factors in an approximate 

factor model. Journal of Finance 48: 1263-1292. 

Fama, E. and French, K. R. (1992). The cross-section of expected stock returns. Journal of 

Finance 47: 427-465. 

Grinold, R. C. and Kahn, R. N. (2000). Active Portfolio Management: A Quantitative 
Approach for Producing Superior Returns and Controlling Risk, 2nd ed. McGraw-Hill, 
New York. 

Johnson, R. A. and Wichem, D. W. (2007). Applied Multivariate Statistical Analysis, 6th 

ed. Prentice Hall, Upper Saddle River, NJ. 

Kaiser, H. F. (1958). The varimax criterion for analytic rotation in factor analysis. Psy- 

chometrika 23: 187-200. 

Sharpe, W. (1970). Portfolio Theory and Capital Markets. McGraw-Hill, New York. 

Zivot, E. and Wang, J. (2003). Modeling Financial Time Series with S-Plus. Springer New 

York. 

CHAPTER 10 

Multivariate Volatility Models 
and Their Applications 

In this chapter, we generalize the univariate volatility models of Chapter 3 to 
the multivariate case and discuss some simple methods for modeling the dynamic 
relationships between volatility processes of multiple asset returns. By multivariate 
volatility, we mean the conditional covariance matrix of multiple asset returns. 
Multivariate volatilities have many important financial applications. They play an 
important role in portfolio selection and asset allocation, and they can be used to 
compute the value at risk of a financial position consisting of multiple assets. 

Consider a multivariate return series {r,}. We adopt the same approach as the 

univariate case by rewriting the series as 

r t = Ft +at. 

where fit = E(rt\Ft~i) is the conditional expectation of r, given the past informa¬ 
tion Ft-1, and a, = (an, • • •, cikt)' is the shock, or innovation, of the series at time 
t. In addition, we assume that r, follows a multivariate time series model of Chapter 
8 so that fit is the 1-step-ahead prediction of the model. For most return series, it 
suffices to employ a simple vector ARMA structure with exogenous variables for 

fit—that is, 

(10.1) 

where xt denotes an m-dimensional vector of exogenous (or explanatory) variables 
with x\t = 1, T is a k x m matrix, and p and q are nonnegative integers. We refer 
to Eq. (10.1) as the mean equation of rt. 

Analysis of Financial Time Series, Third Edition, By Ruey S. Tsay 
Copyright © 2010 John Wiley & Sons, Inc. 

505 

506 

MULTIVARIATE VOLATILITY MODELS AND THEIR APPLICATIONS 

The conditional covariance matrix of at given Ft-\ is a k x k positive-definite 
matrix X, defined by X, = Cov(af |F,_i). Multivariate volatility modeling is con¬ 
cerned with the time evolution of Xr. We refer to a model for the {X,} process as 
a volatility model for the return series rt. 

There are many ways to generalize univariate volatility models to the multi¬ 
variate case, but the curse of dimensionality quickly becomes a major obstacle 
in applications because there are k(k + l)/2 quantities in X, for a k-dimensional 
return series. To illustrate, there are 15 conditional variances and covariances in 
Xf for a five-dimensional return series. The goal of this chapter is to introduce 
some relatively simple multivariate volatility models that are useful, yet remain 
manageable in real application. In particular, we discuss some models that allow 
for time-varying correlation coefficients between asset returns. Time-varying cor¬ 
relations are useful in finance. For example, they can be used to estimate the 
time-varying beta of the market model for a return series. 

We begin by using an exponentially weighted approach to estimate the covari¬ 
ance matrix in Section 10.1. This estimated covariance matrix can serve as a 
benchmark for multivariate volatility estimation. Section 10.2 discusses some gen¬ 
eralizations of univariate GARCH models that are available in the literature. We 
then introduce two methods to reparameterize Xr for volatility modeling in Section 
10.3. The reparameterization based on the Cholesky decomposition is found to be 
useful. We study some volatility models for bivariate returns in Section 10.4, using 
the GARCH model as an example. In this particular case, the volatility model can 
be bivariate or three dimensional. Section 10.5 is concerned with volatility models 
for higher dimensional returns and Section 10.6 addresses the issue of dimension 
reduction. We demonstrate some applications of multivariate volatility models in 
Section 10.7. Finally, Section 10.8 gives a multivariate Student-f distribution useful 
for volatility modeling. 

10.1 EXPONENTIALLY WEIGHTED ESTIMATE 

Given the innovations Ft-\ = {«i, ..., a,_i}, the (unconditional) covariance matrix 
of the innovation can be estimated by 

where it is understood that the mean of aj is zero. This estimate assigns equal 
weight 1 /(r — 1) to each term in the summation. To allow for a time-varying 
covariance matrix and to emphasize that recent innovations are more relevant, one 
can use the idea of exponential smoothing and estimate the covariance matrix of 
at by 

t-1 

(10.2) 

EXPONENTIALLY WEIGHTED ESTIMATE 

507 

where 0 < A < 1 and the weights (1 — A)A; 1 /(I — X* [) sum to one. For a suf¬ 
ficiently large t such that A'-1 ~ 0, the prior equation can be rewritten as 

E, = (1 - k)at-\a't_x + AE,_i. 

Therefore, the covariance estimate in Eq. (10.2) is referred to as the exponentially 
weighted moving-average (EWMA) estimate of the covariance matrix. 

Suppose that the return data are {ri, ..., ry}- For a given A and initial estimate 
£i, Er can be computed recursively. If one assumes that a, = rt — fit follows a 
multivariate normal distribution with mean zero and covariance matrix Ef, where 
fit is a function of parameter 0, then A and 0 can be estimated jointly by the 
maximum-likelihood method because the log-likelihood function of the data is 

In L(0, A) a -- |E,| - - j^(r, - - fit), 

T T 

z ,=i z r=i 

which can be evaluated recursively by substituting E? for Ef. 

Example 10.1. To illustrate, consider the daily log returns of the Hang Seng 
index of Hong Kong and the Nikkei 225 index of Japan from January 4, 2006, to 
December 30, 2008, for 713 observations. The indexes were obtained from Yahoo 
Finance. For simplicity, we only employ data when both markets were open to 
calculate the log returns, which are in percentages. Figure 10.1 shows the time 
plots of the two index returns. The effect of recent global financial crisis is clearly 
seen from the plots. Let r\t and r2t be the log returns of the Hong Kong and 
Japanese markets, respectively. If univariate GARCH models are entertained, we 

obtain the models 

f\t — 0.109 T ci\j, ci\t—o\t€\t, 

a\t = 0.038 + Q.U3a\j_x + 0.S55alt_v 

(10.3) 

r2t = 0.003 + a2t, a2t = cr2te 2t, 

ol = 0.044 + 0.127a!,,-! + 0.861 alt_v 

(10.4) 

where all of the parameter estimates are significant at the 5% level except for 
the constant term of the mean equation for the Nikkei 225 index returns. The 
Ljung-Box statistics of the standardized residuals and their squared series of the 
two univariate models fail to indicate any model inadequacy. The two volatility 
equations are close to an IGARCH(1,1) model. This is reasonable because of the 
increased volatility caused by the subprime financial crisis. Figure 10.2 shows 
the estimated volatilities of the two univariate GARCH(1,1) models. Indeed, the 
volatility series confirm that both markets were more volatile than usual in 2008. 
Turn to bivariate modeling. We apply the EWMA approach to obtain volatility 

estimates, using the command mgarch in S-Plus FinMetrics: 

508 

MULTIVARIATE VOLATILITY MODELS AND THEIR APPLICATIONS 

(a) 

(b) 

Figure 10.1 Time plots of daily log returns in percentages of stock market indexes for Hong Kong and 
Japan from January 4, 2006, to December 30, 2008: (a) Hong Kong market and (b) Japanese market. 

(a) 

(b) 

Figure 10.2 Estimated volatilities (standard error) for daily log returns in percentages of stock market 
indexes for Hong Kong and Japan from January 4, 2006, to December 30, 2008: (a) Hong Kong market 
and (b) Japanese market. Univariate models are used. 

EXPONENTIALLY WEIGHTED ESTIMATE 

509 

> m3 =mgarch (formula .mean=~arma (0 , 0) , formula. var=~ewmal, 

series=rtn,trace=F) 

> summary(m3) 

Call: 

mgarch (formula .mean =~arma(0,0), formula. var=~ewmal, 

series=rtn,trace = F) 

Mean Equation: structure(.Data = ~arma(0,0), class="formula") 

Conditional Var. Eq. : structure (.Data=~ewmal, class=" formula") 

Conditional Distribution: gaussian 

Estimated Coefficients: 

Value Std.Error 

t value Pr(>|t|) 

C(l) 0.082425 0.030900 

2.6675 0.007816 

C(2) -0.006849 0.030093 

-0.2276 0.820020 

ALPHA 0.069492 0.004945 

14.0517 0.000000 

The estimate of X is 1 — a = 1 — 0.0695 ~ 0.9305, which is in the typical range 
commonly seen in practice. Figure 10.3 shows the estimated volatility series by the 

(a) 

(b) 

Figure 10.3 Estimated volatilities (standard error) for daily log returns in percentages of stock market 
indices for Hong Kong and Japan from January 4, 2006, to December 30, 2008: (a) Hong Kong market 

and (b) Japanese market. Exponentially weighted moving-average approach is used. 

510 

MULTIVARIATE VOLATILITY MODELS AND THEIR APPLICATIONS 

EWMA approach. Compared with those in Figure 10.2, the EWMA approach pro¬ 
duces smoother volatility series, even though the two plots show similar volatility 
patterns. 

10.2 SOME MULTIVARIATE GARCH MODELS 

Many authors have generalized univariate volatility models to the multivariate case. 
In this section, we discuss some of the generalizations. For more details, readers 
are referred to the survey article of Bauwens, Laurent, and Rombouts (2004). 

10.2.1 Diagonal Vectorization (VEC) Model 

Bollerslev, Engle, and Wooldridge (1988) generalize the exponentially weighted 
moving-average approach to propose the model 

E? — Ao + ^2, O (at-ia'f^) + Bj © T,t-j, 

i=i j=l 

(10.5) 

where m and s are nonnegative integers, Aj and Bj are symmetric matrices, and O 
denotes the Hadamard product, that is, element-by-element multiplication. This is 
referred to as the diagonal VEC(m, s) model or DVEC(wj, s) model. To appreciate 
the model, consider the bivariate DVEC(1,1) case satisfying 

on,? 

All,0 

021,7 

022,7 

A2LO A 22,0 

1
-
1

-

H

>
K

A 22,1 

-611,1 

#21,1 

#22,1 

O 

O 

a 2 

1,1-1 

01,7-102,7-1 

a 2 
2,r— 1 . 

011,7-1 
021,1-1 022,7—1 

where only the lower triangular part of the model is given. Specifically, the model is 

°rn ,? — Aii.o + A\\'\a\j_ j + #11,10^7-1, 

°2\,t = A21,0 + A21,1^1,1—1^2,7—1 + ^21,1 Or21,7—1, 

022,7 = A 22,0 + A22,1^2,7—1 + ^22,l°r22,7-l) 

where each element of Z, depends only on its own past value and the corresponding 
product term in at-\a\_v That is, each element of a DVEC model follows a 
GARCH(l,l)-type model. The model is, therefore, simple. However, it may not 
produce a positive-definite covariance matrix. Furthermore, the model does not 
allow for dynamic dependence between volatility series. 

 
 
 
SOME MULTIVARIATE GARCH MODELS 

511 

Figure 10.4 Time plot of monthly simple returns, including dividends, for Pfizer and Merck stocks 
from January 1965 to December 2008: (a) Pfizer stock and (b) Merck stock. 

Example 10.2. For illustration, consider the monthly simple returns, including 
dividends, of two U.S. major drug companies from January 1965 to December 2008 
for 528 observations. Let rXt and r2t be the monthly returns of Pfizer and Merck 
stock, respectively. The bivariate return series rt = (rXt, r2t)', shown in Figure 10.4, 
has no significant serial correlations with 0(10) being 10.48(0.40) and 11.42(0.33), 
respectively, for the two series. Therefore, the mean equation of rt consists of a 
constant term only. We fit a DVEC(1,1) model to the series using the command 

mgarch in FinMetrics of S-Plus: 

> rtn=cbind(pfe,mrk) % Output edited. 

> mdvec=mgarch (rtn~l, ~dvec (1,1) ) 

> summary(mdvec) 

Call: 
mgarch(formula.mean=rtn ~ 1, formula.var= ~ dvec(l, 1)) 
Mean Equation: structure(.Data =rtn ~ 1, class="formula") 
Conditional Var. Eq.: structure(.Data=~dvec (1,1) , 

class="formula") 

Conditional Distribution: gaussian 

Estimated Coefficients: 

Value Std.Error t value Pr(>|t|) 

C(l) 1.350e-02 3.149e-03 4.285 2.174e-05 

512 

MULTIVARIATE VOLATILITY MODELS AND THEIR APPLICATIONS 

C  (2)  1.313e-02  3.043e-03 

4.314  1.921e-05 

A  (1,  1)  7.544e-04  3.939e-04 
A i (2,  1)  7.543e-05  3.468e-05 
A i (2,  2)  7.941e-05  3.871e-05 

1.916  5.597e-02 

2.175  3.010e-02 

2.051  4.072e-02 

ARCH 1 (1;  1,  1)  7.078e-02  2.757e-02 
ARCH 1 (1;  2,  1)  2.513e-02  8.492e-03 
ARCH l H;  2,  2)  4.095e-02  1.213e-02 
GARCH(1;  1,  1)  7.858e-01  9.055e-02 
GARCH1 (1;  2,  1)  9.499e-01  1.671e-02  56.831  0.000e+00 
GARCHl 11;  2 ,  2)  9.454e-01  1.469e-02 
64.358  0.000e+00 

3.375  7.93 9e-04 

8.677  0.000e+00 

2.960  3.220e-03 

2.568  1.051e-02 

Ljung-Box test for standardized residuals: 

Statistic P-value ChiA2-d.f. 

pfe 9.531 0.6570 12 

mrk 12.349 0.4181 12 

Ljung-Box test for squared standardized residuals: 

Statistic P-value ChiA2-d.f. 

pfe 22.077 0.03666 12 

mrk 6,437 0.89246 12 

> names(mdvec) 

[1]  "residuals" 
[5]  "model" 

[9] 

" cov" 

"sigma.t" 

"df.residual" 

" coef" 

"cond.dist" 

"likelihood" 

"opt.index 

"std.residuals"  "R.t" 

" S . t" 

[13]  "prediction" 

"call" 

"series" 

From the output, all parameter estimates, but A(l,l), are significant at the 5% level, 
and the fitted volatility model is 

oiU = 0.00075 + 0.071flj r_j + 0.786an,,_i, 

02i,i = 0.00008 0.025fli ;_i<22g—l ~t~ 0.950<72i,f—1» 

<722,t = 0.00008 + 0.041a\t_x + 0.945a22,,-i. 

The output also provides some model checking statistics for individual stock 
returns. For instance, the Ljung-Box statistics for the standardized residual 
series and its squared series of Pfizer stock returns give £>(12) = 9.53(0.66) and 
£>(12) = 12.35(0.42), respectively, where the number in parentheses denotes 
the p value. Thus, checking the fitted model individually, one cannot reject the 
DVEC(1,1) model. A more informative model-checking approach is to apply 
the multivarite Q statistics to the bivariate standardized residual series and its 
squared process. Details are omitted. Interested readers are referred to Li (2004). 
Figure 10.5 shows the fitted volatility and correlation series. These series are 
stored in “sigma.t” and “R.t”, respectively. The correlations range from 0.37 
to 0.83. 

SOME MULTIVARIATE GARCH MODELS 

513 

1970 1980 1990 2000 2010 

Year 

(c) 

Figure 10.5 Estimated volatilities (standard error) and time-varying correlations of DVEC(1,1) model 

for monthly simple returns of two major drug companies from January 1965 to December 2008: (a) 

Pfizer stock volatility, (b) Merck stock volatility, and (c) time-varying correlations. 

10.2.2 BEKK Model 

To guarantee the positive-definite constraint, Engle and Kroner (1995) propose the 
Baba-Engle-Kraft-Kroner (BEKK) model, 

m s 

T, = AA' + J2A‘<a'-‘a'^A‘ +X<10'6) 

i=l ;'=i 

where A is a lower triangular matrix and A, and Bj are k x k matrices. Based 
on the symmetric parameterization of the model, Z, is almost surely positive def¬ 
inite provided that AA' is positive definite. This model also allows for dynamic 
dependence between the volatility series. On the other hand, the model has several 
disadvantages. First, the parameters in A, and B j do not have direct interpretations 
concerning lagged values of volatilities or shocks. Second, the number of param¬ 
eters employed is k2(m + s) + k(k + l)/2, which increases rapidly with m and y. 

514 

MULTIVARIATE VOLATILITY MODELS AND THEIR APPLICATIONS 

Limited experience shows that many of the estimated parameters are statistically 
insignificant, introducing additional complications in modeling. 

Example 10.3. To illustrate, we consider the monthly simple returns of Pfizer 
and Merck stocks of Example 10.2 and employ a BEKK(1,1) model. Again, S-Plus 
is used to perform the estimation: 

> mbekk=mgarch(rtn~l, ~bekk(1,1)) 

> summary(mbekk) 

Call: 

mgarch(formula.mean = rtn ~ 1, formula.var = ~ bekk(l, 1)) 

Mean Equation: structure(.Data = rtn ~ 1, class = "formula") 

Conditional Var. Eq. : structure (. Data=~bekk (1,1) , 

class="formula") 

Conditional Distribution: gaussian 

Estimated Coefficients: 

Value  Std.Error 

t value i Pr (> 111) 

C (1) 

1.329e-02  0.003247  4.094e+00  4.907e-05 

C (2) 

1.269e-02  0.003095  4.100e+00  4.792e-05 

A (1, 1) 

2.505e-02  0.008382  2.988e+00  2.93 8e-03 

A(2, 1) 

1.349e-02  0.004979  2.710e+00  6.946e-03 

A(2, 2) 

3.272e-06  8.453262  3.870e-07  1.000e+00 

ARCH(1; 1, 1) 

2.129e-01  0.084340  2.524e+00  1.190e-02 

ARCH(1; 2, 1) 

9.963e-02  0.072156  1,381e+00  1.680e-01 

ARCH(1; 1, 2) 

6.33 6e-02  0.076065  8.330e-01  4.052e-01 

ARCH(1; 2, 2) 

1.824e-01  0.062133  2.93 6e+00  3.467e-03 

GARCH(1; 1, 1) 

9.090e-01  0.063239  1.437e+01  0.000e+00 

GARCH(1; 2, 1)  -5.888e-02 

0.047766 ■ -1.233e+00  2.182e-01 
GARCH(1; 1, 2)  -8.231e-03  0.031512 ■ -2.612e-01  7.940e-01 
GARCH(1; 2, 2) 

9.824e-01  0.022587  4.349e+01  0.000e+00 

Ljung-Box test  for standardized residuals: 

Statistic P -value Chi,'2 -d.f. 

pfe 9.465 0  .6628 

mrk 11.591 0  .4791 

12 

12 

Ljung-Box test  for squared  standardized residuals: 

Statistic P-value Chi/'2-d.f. 

pfe 21.55 0.04291 12 

mrk 9.19 0.68664 12 

Model-checking statistics based on the individual residual series and provided 
by S-Plus fail to suggest any model inadequacy of the fitted BEKK(1,1) model. 
Figure 10.6 shows the fitted volatilities and the time-varying correlations of the 

SOME MULTIVARIATE GARCH MODELS 

515 

v\JViVA\y\y' 

1970 1980 1990 2000 2010 

Year 

(a) 

1970 1980 1990 2000 2010 

f 

o 
O 

Year 

(b) 

l/1 

1970 

1980 

1990 

2000 

2010 

Year 

(c) 

Figure 10.6 Estimated volatilities (standard error) and time-varying correlations of BEKK(1,1) model 
for monthly simple returns of two major drug companies from January 1965 to December 2008: (a) 

Pfizer stock volatility, (b) Merck stock volatility, and (c) time-varying correlations. 

BEKK(1,1) model. Compared with Figure 10.5, there are some differences between 
the two fitted volatility models. For instance, the time-varying correlations of the 
BEKK(1,1) model appear to be more volatile. 

The volatility equation of the fitted BEKK(1,1) model is 

011,1 
021,t 

Or12 ,t 
022 ,f 

0.025 0 
0.013 3 x 10“6  _ 

' 0.025 0.013 

0 3 x 10"6 _ 

+ 

‘ 0.213  0.063 " 
0.100  0.182 _ 

' 0.213  0.100 ‘ 
0.063  0.182 _ 

01U-1 
Or21,f—1 

012,1-1 
022,1—1 

- 

“ 

alt-i 
02,r-101,t-l 

a\t-\o 

alt-1 

0.901 
-0.059 

-0.008 
0.982 

0.901 
-0.008 

-0.059 
0.982 

516 

MULTIVARIATE VOLATILITY MODELS AND THEIR APPLICATIONS 

where three estimates are insignificant at the 5% level. In general, the BEKK model 
tends to contain some insignificant parameter estimates, and one needs to perform 
matrix multiplication to decipher the fitted model. 

10.3 REPARAMETERIZATION 

A useful step in multivariate volatility modeling is to reparameterize Z, by making 
use of its symmetric property. We consider two reparameterizations. 

10.3.1 Use of Correlations 

The first reparameterization of Z( is to use the conditional correlation coefficients 
and variances of at. Specifically, we write Z; as 

^t = \Pij,t\ = DtptDt, (10.7) 

where pt is the conditional correlation matrix of a,, and D, is a k x k diagonal 
matrix consisting of the conditional standard deviations of elements of a, (i.e., 

D, = diag{v/o7I7, • • • - V°kk~t})- 

Because pr is symmetric with unit diagonal elements, the time evolution of Zf 
is governed by that of the conditional variances cr,;if and the elements of pt, 
where j < i and 1 < i < k. Therefore, to model the volatility of at, it suffices to 
consider the conditional variances and correlation coefficients of ait. Define the 
k(k + l)/2-dimensional vector 

= (o-n,..akkj, e'ty, (10.8) 

where Qt is a k{k — l)/2-dimensional vector obtained by stacking columns of the 
correlation matrix pt, but using only elements below the main diagonal. Specifi¬ 
cally, for a k-dimensional return series, 

Qt — (P21,U • • • , Pk\,t\Pyi,tt • • • » Pkl,t \ • • ■ \Pk,k-l,t)'■ 

To illustrate, for k = 2, we have Qt = p2i,t and 

S, = (<7lU, <722,1, P21./)', (10.9) 

which is a three-dimensional vector, and for k — 3, we have Qt = (p2i,f, P3i^, 
P32,f)/ and 

Sf — (crnj, <722,11 033,t> P21,t, P3l,t, P32,t)', 

(10.10) 

which is a six-dimensional random vector. 

REPARAMETERIZATION 

517 

If a, is a bivariate normal random variable, then 3f is given in Eq. (10.9) and 

the conditional density function of a, given Ft-\ is 

f(au, a2r|2f) = 

1 

2tt Jcrn,tcr22,t(X ~ P2i,/) 

exp 

Q{a\t, an, Hf) 

2(1 - pi,) 

where 

Q{a\t, a2f, Sf) 

a 

*2t 

+ 
ffn,f a2ia 

2p2l,ffllf<32f 

V/°'iiaa'22,f 

The log probability density function of at relevant to the maximum-likelihood 
estimation is 

l(a\t, a21, Sf) 

1 

2 

+ 

ln[anifo-22,f(l - P21 f)] 

a if 
o-n,f cr22,f *Jcrn,t&22,t 

2p21,f«lf^2f 

+ 

l2t 

. (10.11) 

p\u 

This reparameterization is useful because it models covariances and correlations 
directly. Yet the approach has several weaknesses. First, the likelihood function 
becomes complicated when k > 3. Second, the approach requires a constrained 
maximization in estimation to ensure the positive definiteness of Y,t. The constraint 

becomes complicated when k is large. 

10.3.2 Cholesky Decomposition 

The second reparameterization of S, is to use the Cholesky decomposition; see 
Appendix A of Chapter 8. This approach has some advantages in estimation as it 
requires no parameter constraints for the positive definiteness of X,; see Pourah- 
madi (1999). In addition, the reparameterization is an orthogonal transformation so 
that the resulting likelihood function is extremely simple. Details of the transfor¬ 

mation are given next. 

Because X, is positive definite, there exist a lower triangular matrix Lt with 
unit diagonal elements and a diagonal matrix G, with positive diagonal elements 

such that 

Z t=LtG,L't. (10.12) 

This is the well-known Cholesky decomposition of £f. A feature of the decompo¬ 
sition is that the lower off-diagonal elements of Lt and the diagonal elements of Gt 

518 

MULTIVARIATE VOLATILITY MODELS AND THEIR APPLICATIONS 

have nice interpretations. We demonstrate the decomposition by studying carefully 
the bivariate and three-dimensional cases. For the bivariate case, we have 

Y, = 

ori.f &2\,t 
021,t °22,t 

Lt 

1 0 

<?21,/ 1 

Gt = 

gu.t 0 

0 822, t 

where gn,t > 0 for i = 1 and 2. Using Eq. (10.12), we have 

o\i,t 
0\2,t 

012, t 
022,t 

gn,t 
q2i,tgn,t 

q2i,tgu,t 
g22,t + qh,tg\i,t 

Equating elements of the prior matrix equation, we obtain 

o\i,t = gn,t, °2\,t = q2i,tgn,t, 

022 ,t = 822,t + 921,^11.0 (10.13) 

Solving the prior equations, we have 

gn,/ = 

<721,r 

<L21 ,/ 

On ,t 

g22,t = <*22,t 

a 

21,/ 

Oil, t 

(10.14) 

However, consider the simple linear regression 

<22/ = P&U + b2t, 

(10.15) 

where b2t denotes the error term. From the well-known least-squares theory, we 
have 

Cov(fli,, q2/) 

Var(ait) 

071,/ 

o\i,t 

Var(b2t) = Var(a2/) - £2Var(<2U) = a22,t - 

cr 21,/ 
o\\,t 

Furthermore, the error term b2t is uncorrelated with the regressor a\t. Consequently, 
using Eq. (10.14), we obtain 

£n,/ = on,t, <721,/ = P’ 822,t = Var (b2t), b2t±.a\t, 

where ± denotes no correlation. In summary, the Cholesky decomposition of the 
2x2 matrix T, amounts to performing an orthogonal transformation from a, to 
bt = (b\t, b2ty such that 

bit — ait and b2t — a2t — q2\,ta\t, 

where q2i,t = is obtained by the linear regression (10.15) and Cov(bt) is a diag¬ 
onal matrix with diagonal elements gu,t. The transformed quantities q2u and gn,t 
can be interpreted as follows: 

REPARAMETERIZATION 

519 

1. The first diagonal element of Gt is simply the variance of a\t. 

2. The second diagonal element of Gt is the residual variance of the simple 

linear regression in Eq. (10.15). 

3. The element q2\tt of the lower triangular matrix L, is the coefficient /3 of 

the regression in Eq. (10.15). 

The prior properties continue to hold for the higher dimensional case. For example, 
consider the three-dimensional case in which 

1 

Lt = 

921,1 

0 

1 

<731,1 

<732,1 

0 " 

0 

1 

Gt = 

gu,t 

0 

0 

0 

g22,t 

0 

0 

0 

£3,1 

From the decomposition in Eq. (10.12), we have 

0-11,1 

0-21,1 (731,t 

Or21,1 

(722,1 0-32,1 

0"31,1 

0"32,1 033,1 

£ll,i 

<?2l,i£ll,i 

<?3i,i£n,i 

= 

<?21,l£ll,l  921,l£ll,I + £22,1 

<?31,l<?21,l£ll,l + <?32,l£22,l 

<?3l,i£li,i 

<731,l<?21,l£ll,l + <?32,l£22,l 

<?31,l£ll,l +<?|2,i£22,1 T £33,1 

Equating elements of the prior matrix equation, we obtain 

(7n,t = £11,i, 021,1 = qn,tgn,t, 

(722,t = <?fi,i£ll,l + g22,t, 0-31 ,f = 931,l£ll,l> 

or32,t = q3\,tqi\,tgu,t + q?,2,tg22,t, 0-33j — qlitgiu + q22^g22,t + ^33, 

or, equivalently, 

gn,t = 

<721,1 

<?3i,i = 

031,1 

&n,t 

<732,1 = 

021,1 

cru.t 

1 

g22,t 

g22,t = (722,t - <721,f£ll,f> 

Or31 ,1 
Or32 ,1-an,t 

a 21,1 

£33,1 = 0"33,1 — <?3l, f£ll,f — q22,tg22,t ■ 

These quantities look complicated, but they are simply the coefficients and residual 

variances of the orthogonal transformation 

bu = (2\t, 

b2t = a2t ~ ^2\b\t, 

b2t = (*3t - @31 bit ~ /*32&2l, 

520 

MULTIVARIATE VOLATILITY MODELS AND THEIR APPLICATIONS 

where are the coefficients of least-squares regressions 

ait = fti\b\t + bjt, 

«31 = P?>\b\t + foibit + b?,t. 

In other words, we have qijj = fy, gaj = Var{bit) and bitA.bjt for i ^ j. 

Based on the prior discussion, using Cholesky decomposition amounts to doing 
an orthogonal transformation from a, to b,, where b\t = a\t, and bit, for 1 < i < k, 
is defined recursively by the least-squares regression 

an = qi\,tb\t + qn,tb2t + • • • + qi(i-\),tbq-\)t + bu, (10.16) 

where q,jj is the (i, y)th element of the lower triangular matrix Lt for 1 < j < i- 
We can write this transformation as 

bt = L~xat, or a,—L,bt, (10.17) 

where, as mentioned before, L~x is also a lower triangular matrix with unit diagonal 
elements. The covariance matrix of b, is the diagonal matrix G, of the Cholesky 
decomposition because 

Co v{bt) = L;xT,t{L;1)' = Gt. 

The parameter vector relevant to volatility modeling under such a transformation 
becomes 

st = (gn,/, • • •, gkkj,qn,t,<?3u<q32,t, qn,t, • • •,qk(k-i),tY, (10.18) 

which is also a k(k + l)/2-dimensional vector. 

The previous orthogonal transformation also dramatically simplifies the likeli¬ 

hood function of the data. Using the fact that \L, \ = 1, we have 

|Zrl = \LtG,L't\ = |Gf | = (10.19) 

i=l 

k 

If the conditional distribution of a, given the past information is multivariate nor¬ 
mal N(0, Zf), then the conditional distribution of the transformed series b, is 
multivariate normal N(0, Gt), and the log-likelihood function of the data becomes 
extremely simple. Indeed, we have the log probability density of at as 

i(at, Z,) =l{bt, S,) = 

1 k 

b? 
Hgu,t)+ lt 
gii,t 

i=] _ 

(10.20) 

where for simplicity the constant term is omitted and giia is the variance of bit. 

GARCH MODELS FOR BIVARIATE RETURNS 

521 

Using the Cholesky decomposition to reparameterize T,t has several advantages. 
First, from Eq. (10.19), is positive definite if gnyt >0 for all i. Consequently, 
the positive-definite constraint of can easily be achieved by modeling In(gaj) 
instead of guj. Second, elements of the parameter vector at in Eq. (10.18) have 
nice interpretations. They are the coefficients and residual variances of multiple 
linear regressions that orthogonalize the shocks to the returns. Third, the correlation 
coefficient between a\t and d2t is 

P2\,t = . '—: = <?21,f x -, 

021,r VaH T 

V*7! l,fO'22,/ *J&22,t 

which is time varying if <721,t 7^ 0. In particular, if <721,t — c 7^ 0, then p2i,? = 
o^/otIT/v^MT’ which continues to be time-varying provided that the variance ratio 
oTi,f/o22,r is not a constant. This time-varying property applies to other correlation 
coefficients when the dimension of rt is greater than 2 and is a major difference 
between the two approaches for reparameterizing X,. 

Using Eq. (10.16) and the orthogonality among the transformed shocks bn, we 

obtain 

&iij — Var(a,-i|iv_i) — ^ 'qiv,tSw,t> i — ,k, 

V=1 

j 

(Tint — Cov(fltT, cijt\Ft—i) — ^ ]qiv,tqjv,tgvv,t, j < b J 2, ..., k, 

where qVVJ = 1 for v = 1,..., k. These equations show the parameterization of 
T,t under the Cholesky decomposition. 

10.4 GARCH MODELS FOR BIVARIATE RETURNS 

Since the same techniques can be used to generalize many univariate volatility mod¬ 
els to the multivariate case, we focus our discussion on the multivariate GARCH 
model. Other multivariate volatility models can also be used. 

For a Udimensional return series rt, a multivariate GARCH model uses “exact 
equations” to describe the evolution of the k(k + 1)/2-dimensional vector Er over 
time. By exact equation, we mean that the equation does not contain any stochastic 
shock. However, the exact equation may become complicated even in the simplest 
case of k — 2 for which E t is three dimensional. To keep the model simple, some 
restrictions are often imposed on the equations. 

10.4.1 Constant-Correlation Models 

To keep the number of volatility equations low, Bollerslev (1990) considers the 

special case in which the correlation coefficient P21,/ = P21 is time invariant, where 

522 

MULTIVARIATE VOLATILITY MODELS AND THEIR APPLICATIONS 

IP211 < 1. Under such an assumption, P21 is a constant parameter and the volatility 
model consists of two equations for 3*, which is defined as 3* = (an,t, o'22,r)/- 
A GARCH(1,1) model for 3* becomes 

E? = «0+ «!«?_!+^iS;_t, (10.21) 

where = (aj r_v a\t_xy, ao is a two-dimensional positive vector, and oo and 
/?, are 2 x 2 nonnegative definite matrices. More specifically, the model can be 
expressed in detail as 

Cll,t 
022,/ 

_ 

«10 

0(20 

+ 

0111 
_ <*21 

<*12 
Ot 22 

" alt-1 ‘ 

a2 

l a2,t-\ J 

+  ' Pn 
P21 

P12 

P22 

&22,t-1 

(10.22) 

where a,o > 0 for i = 1 and 2. Defining r], = aj — 3*, we can rewrite the prior 
model as 

a2t = a0 + (of! +Pi)a2_x + rjt - /Mr-u 

which is a bivariate ARMA(1,1) model for the aj process. This result is a direct 
generalization of the univariate GARCH(1,1) model of Chapter 3. Consequently, 
some properties of model (10.22) are readily available from those of the bivariate 
ARMA( 1,1) model of Chapter 8. In particular, we have the following results: 

1. If all of the eigenvalues of ofi + are positive, but less than 1, then the 
bivariate ARMA(1,1) model for aj is weakly stationary and, hence, E(aj) 
exists. This implies that the shock process a, of the returns has a positive- 
definite unconditional covariance matrix. The unconditional variances of the 
elements of at are (aj2, cr|)' — {I — a\ — Pl)~1</>0, and the unconditional 
covariance between a\t and a2t is P2\0\o2. 

2. If ai2 = /012 = 0, then the volatility of au does not depend on the past 
volatility of a2t. Similarly, if aj\ = £21 = 0, then the volatility of a2t does 
not depend on the past volatility of a\t. 

3. If both of] and /?, are diagonal, then the model reduces to two univari¬ 
ate GARCH(1,1) models. In this case, the two volatility processes are not 
dynamically related. 

4. Volatility forecasts of the model can be obtained by using forecasting methods 
similar to those of a vector ARMA(1,1) model; see the univariate case in 
Chapter 3. The 1-step-ahead volatility forecast at the forecast origin h is 

3£(l)=a0 + aia2 + j813*. 

For the f-step-ahead forecast, we have 

3*(f) = ao + (a1+^l)S*(f-l), 

£> 1. 

GARCH MODELS FOR BIVARIATE RETURNS 

523 

These forecasts are for the marginal volatilities of a,> The €-step-ahead fore¬ 
cast of the covariance between a\t and «2r is P2\[°u,h(£)a22,h(£)]°'5 ■> where 
P21 is the estimate of P21 and are the elements of E^(f). 

Example 10.4. Again, consider the daily log returns of Hong Kong and 
Japanese markets of Example 10.1. Using a bivariate GARCH model, we obtain a 
constant correlation model that fits the data reasonably well. The mean equations 
of the bivariate model are 

r\t = 0.101 + a u, 

f2t — 0.002 -f- a.2t, 

where the standard errors of the two estimates are 0.050 and 0.048, respectively. 
The volatility equations are 

ojl,r 

<722, f 

0.079 
(0.019) 
0.054 
(0.019) 

+ 

0.145 
(0.022) 

• 

0.105 
(0.014) _ 

0.833 
(0.023) 

+ 

OTl,*-l 
<722,1-1 

0.875 
(0.020) 

(10.23) 

where the numbers in parentheses are standard errors. The estimated constant cor¬ 

relation between the two returns is 0.668. 

Let at = (a\t, fat)' be the standardized residuals, where ciit = ait/ Jouj. The 
Ljung-Box statistics of at give <22(4) = 17.29(0.37) and <22(12) = 48.21(0.46), 
where the number in parentheses denotes the p value. Here the p values are 
based on chi-squared distributions with 16 and 48 degrees of freedom, respec¬ 
tively. The <2 statistics of individual series an shown in S-Plus output also fail 
to indicate any model inadequancy. Consequently, the constant correlation model 
in Eq. (10.23) fits the data reasonably well. Figure 10.7 shows the fitted volatil¬ 
ity processes of model (10.23), which can be compared with those of Example 
10.1. 

The model in Eq. (10.23) shows two uncoupled volatility equations, indicat¬ 
ing that the volatilities of the two markets are not dynamically related, but they 
are contemporaneously correlated. We refer to the model as a bivariate diagonal 
constant-correlation model. In practice, this type of models might not be suitable 
because there exists the possibility of dynamic dependence in volatility among 
markets, that is, the spillover effect in volatility. Finally, the constant-correlation 

model can easily be estimated using S-Plus: 

524 

MULTIVARIATE VOLATILITY MODELS AND THEIR APPLICATIONS 

(a) 

(b) 

Figure 10.7 Estimated volatilities for daily log returns in percentages of stock market indexes for 

Hong Kong and Japan from January 4, 2006, to December 30, 2008: (a) Hong Kong market and (b) 
Japanese market. Model used is Eq. (10.23). 

> mccc = mgarch(rtn~l, ~ccc(1,1),trace=F) 

> summary(mccc) 

Example 10.5. As a second illustration, consider the monthly log returns, in 
percentages, of IBM stock and the S&P 500 index from January 1926 to December 
1999 used in Chapter 8. Let r\t and r2t be the monthly log returns for IBM stock 
and the S&P 500 index, respectively. If a constant-correlation GARCH(1,1) model 
is entertained, we obtain the mean equations 

r\t — 1.351 + 0.072ati?_i + 0.055ri,_2 — 0.1 \9r2,t—2 + a\t, 
r2t — 0.703 + a2t, 

where standard errors of the parameters in the first equation are 0.225, 0.029, 0.034, 
and 0.044, respectively, and the standard error of the parameter in the second 
equation is 0.155. The volatility equations are 

°Tl,f 

&22,t 

" 2.98 " 
(0.59) 
2.09 
(0.47) 

+ 

" 0.079 
(0.013) 
0.042 
(0.009) 

" 

0.045 
(0.010)_ 

0.873 
(0.020) 
-0.066 
(0.015) 

-0.031 
(0.009) 
0.913 
(0.014) 

7ll,t-l 

022,1-1 

(10.24) 

GARCH MODELS FOR BIVARIATE RETURNS 

525 

where the numbers in parentheses are standard errors. The constant correlation 
coefficient is 0.614 with standard error 0.020. Using the standardized residuals, we 
obtain the Ljung-Box statistics <22(4) = 16.77(0.21) and <22(8) = 32.40(0.30), 
where the p values shown in parentheses are obtained from chi-squared distri¬ 
butions with 13 and 29 degrees of freedom, respectively. Here the degrees of 
freedom have been adjusted because the mean equations contain three lagged pre¬ 
dictors. For the squared standardized residuals, we have 4) = 18.00(0.16) and 
(7*(8) = 39.09(0.10). Therefore, at the 5% significance level, the standardized 
residuals at have no serial correlations or conditional heteroscedasticities. This 
bivariate GARCH(1,1) model shows a feedback relationship between the volatilities 
of the two monthly log returns. 

10.4.2 Time-Varying Correlation Models 

A major drawback of the constant-correlation volatility models is that the cor¬ 
relation coefficient tends to change over time in a real application. Consider the 
monthly log returns of IBM stock and the S&P 500 index used in Example 10.5. 
It is hard to justify that the S&P 500 index return, which is a weighted aver¬ 
age, can maintain a constant-correlation coefficient with IBM return over the past 
70 years. Figure 10.8 shows the sample correlation coefficient between the two 
monthly log return series using a moving window of 120 observations (i.e., 10 
years). The correlation changes over time and appears to be decreasing in recent 
years. The decreasing trend in correlation is not surprising because the ranking of 
IBM market capitalization among large U.S. industrial companies has changed in 
recent years. A Lagrange multiplier statistic was proposed recently by Tse (2000) 
to test constant-correlation coefficients in a multivariate GARCH model. 

A simple way to relax the constant-correlation constraint within the GARCH 
framework is to specify an exact equation for the conditional correlation coeffi¬ 
cient. This can be done by two methods using the two reparameterizations of 
discussed in Section 10.3. First, we use the correlation coefficient directly. Because 
the correlation coefficient between the returns of IBM stock and S&P 500 index is 
positive and must be in the interval [0, 1], we employ the equation 

Pl\,t = 

exp jqt) 

1 +exp {q,)' 

(10.25) 

where 

_ _ . _ 01,1-1^2,1-1 

qt — CO 0 + CO\P2l,t-l + OJ2 

Vcrll,I-la22,/-l 

where crutt-1 is the conditional variance of the shock aiit-\. We refer to this equation 
as a GARCH(1,1) model for the correlation coefficient because it uses the lag-1 
cross correlation and the lag-1 cross product of the two shocks. If m\ = ur2 = 0, 
then model (10.25) reduces to the case of constant correlation. 

526 

MULTIVARIATE VOLATILITY MODELS AND THEIR APPLICATIONS 

1940 

1950 

1960 

1970 

1980 

1990 

2000 

Year 

Figure 10.8 Sample correlation coefficient between monthly log returns of IBM stock and S&P 500 

index. Correlation is computed by a moving window of 120 observations. Sample period is from January 
1926 to December 1999. 

In summary, a time-varying correlation bivariate GARCH(1,1) model consists 
of two sets of equations. The first set of equations consists of a bivariate 
GARCH(1,1) model for the conditional variances, and the second set of equation 
is a GARCH(1,1) model for the correlation in Eq. (10.25). In practice, a negative 
sign can be added to Eq. (10.25) if the correlation coefficient is negative. 
In general, when the sign of correlation is unknown, we can use the Fisher 
transformation for correlation 

or 

exp(^) - 1 

exp(^) + 1 

and employ a GARCH model for q, to model the time-varying correlation between 
two returns. 

Example 10.5 (Continued). Augmenting Eq. (10.25) to the GARCH(1,1) 
model in Eq. (10.24) for the monthly log returns of IBM stock and the S&P 
500 index and performing a joint estimation, we obtain the following model for 
the two series: 

fit — 1.318 + 0.076riif_i — 0.068r2>;_2 T- a\t, 

r21 — 0.673 + a2t. 

GARCH MODELS FOR BIVARIATE RETURNS 

527 

where standard errors of the three parameters in the first equation are 0.215, 0.026, 
and 0.034, respectively, and standard error of the parameter in the second equation 
is 0.151. The volatility equations are 

Oil ,t 

022, f 

= 

2.80 
(0.58) 
1.71 
(0.40) 

+ 

0.084 
(0.013) 
0.037 
(0.009) 

0.054 
(0.010) _ 

0.864 -0.020 
(0.021) (0.009) 
-0.058 0.914 
(0.014) (0.013) 

OTU-l 

022,1-1 

(10.26) 

where, as before, standard errors are in parentheses. The conditional correlation 

equation is 

exp(fr) 
1 + exp(<3>f) ’ 

qt = -2.024 + 3.983pr-i + 0.088 

01,1-1^2,1-1 

V'°rll,f-l°r22,I-l 

(10.27) 

where standard errors of the estimates are 0.050, 0.090, and 0.019, respectively. 
The parameters of the prior correlation equation are highly significant. Apply¬ 
ing the Ljung-Box statistics to the standardized residuals at, we have £22(4) = 
20.57(0.11) and £2(8) = 36.08(0.21). For the squared standardized residuals, we 
have Ql(4) = 16.69(0.27) and Q|(8) = 36.71(0.19). Therefore, the standardized 
residuals of the model have no significant serial correlations or conditional het- 

eroscedasticities. 

It is interesting to compare this time-varying correlation GARCH(1,1) model 
with the constant-correlation GARCH(1,1) model in Eq. (10.24). First, the mean 
and volatility equations of the two models are close. Second, Figure 10.9 shows 
the fitted conditional correlation coefficient between the monthly log returns of 
IBM stock and the S&P 500 index based on model (10.27). The plot shows that 
the correlation coefficient fluctuated over time and became smaller in recent years. 
This latter characteristic is in agreement with that of Figure 10.8. Third, the aver¬ 
age of the fitted correlation coefficients is 0.612, which is essentially the estimate 
0.614 of the constant-correlation model in Eq. (10.24). Fourth, using the sample 
variances of rit as the starting values for the conditional variances and the obser¬ 
vations from t — 4 to t = 888, the maximized log-likelihood function is -3691.21 
for the constant-correlation GARCH(l.l) model and -3679.64 for the time-varying 
correlation GARCH(1,1) model. Thus, the time-varying correlation model shows 
some significant improvement over the constant-correlation model. Finally, con¬ 
sider the 1-step-ahead volatility forecasts of the two models at the forecast origin 
h = 888. For the constant-correlation model in Eq. (10.24), we have ai,g88 = 3.075, 

528 

MULTIVARIATE VOLATILITY MODELS AND THEIR APPLICATIONS 

Figure 10.9 Fitted conditional correlation coefficient between monthly log returns of IBM stock and 

S&P 500 index using time-varying correlation GARCH(1,1) model of Example 10.5. Horizontal line 
denotes average of 0.612 of correlation coefficients. 

£12,888 = 4.931, (Tn,888 = 77.91, and 1x22,888 = 21.19. Therefore, the 1-step-ahead 
forecast for the conditional covariance matrix is 

^888(1) = 

71.09 21.83 ' 

21.83 17.79 J ’ 

where the covariance is obtained by using the constant-correlation coefficient 0.614. 
For the time-varying correlation model in Eqs. (10.26) and (10.27), we have 

£ii,888 = 3.287, <22,888 = 4.950, an,888 = 83.35, a22,888 = 28.56, and ps88 = 0.546. 
The 1-step-ahead forecast for the covariance matrix is 

^888(1) = 

75.15 23.48 " 

23.48 24.70 J ’ 

where the forecast of the correlation coefficient is 0.545. 

In the second method, we use the Cholesky decomposition of Er to model 
time-varying correlations. For the bivariate case, the parameter vector is Ef = 
(gii.n 822,t, <721,t)'\ see Eq. (10.18). A simple GARCH(1,1) type model for a, is 

£11,/ — “10 + “11^1,f-i + Pngu,t-\, 

<721,/ — Yo + Kl<721,/-1 + V2^2,t-l, 

(10.28) 

§22,/ = «20+«21^,f_i +£*22^2.,—1 +/*2l£ll,r-l + ^22g22,t-\, 

GARCH MODELS FOR BIVARIATE RETURNS 

529 

where b\t = a\t and £>2r = 02, — qi\,tCi\t. Thus, b\t assumes a univariate 
GARCH(1,1) model, b2t uses a bivariate GARCH(1,1) model, and <721,/ is 
autocorrelated and uses 02,1-1 as an additional explanatory variable. The 
probability density function relevant to maximum-likelihood estimation is given 
in Eq. (10.20) with k = 2. 

Example 10.5 (Continued). Again we use the monthly log returns of IBM 
stock and the S&P 500 index to demonstrate the volatility model in Eq. (10.28). 
Using the same specification as before, we obtain the fitted mean equations as 

r\t = 1.364 + 0.075ri,,_i - 0.058r2,,_2 + au, 

r2t = 0.643 + a2t, 

where standard errors of the parameters in the first equation are 0.219, 0.027, and 
0.032, respectively, and the standard error of the parameter in the second equation 
is 0.154. These two mean equations are close to what we obtained before. The 

fitted volatility model is 

gu,t = 3-714 + 0.1136?,,+ 0.804#!!.,-!, 

q2l t = 0.0029 + 0.9915*721,f-i - 0.0041a2,f-i, (10.29) 

#22,t = 1-023 + 0.0216?.,_! + 0.0526?.,_! - 0.040#n,,_i + 0.937#22,r-i, 

where bu = au, and b2t = a2t - <721,* 61,. Standard errors of the parameters in the 
equation of #n., are 1.033, 0.022, and 0.037, respectively; those of the parameters 
in the equation of <721,1 are 0.001, 0.002, and 0.0004; and those of the parameters in 
the equation of #22,? are 0.344, 0.007, 0.013, and 0.015, respectively. All estimates 
are statistically significant at the 1 % level. 

The conditional covariance matrix X, can be obtained from model (10.29) 
by using the Cholesky decomposition in Eq. (10.12). For the bivariate case, the 
relationship is given specifically in Eq. (10.13). Consequently, we obtain the time- 

varying correlation coefficient as 

r*2l,i <721,i/ia oa\ 
pt = ■■■ - ==. (1U.3U) 

V0'n,f^22,i y#22,+4,fS!U 

Using the fitted values of an,t and cr22,t, we can compute the standardized residuals 
to perform model checking. The Ljung-Box statistics for the standardized resid¬ 
uals of model (10.29) give (22(4) = 19.77(0.14) and ^2(8) = 34.22(0.27). For 
the squared standardized residuals, we have Ql(4) = 15.34(0.36) and Q£(8) = 
31.87(0.37). Thus, the fitted model is adequate in describing the conditional mean 
and volatility. The model shows a strong dynamic dependence in the correlation; 

see the coefficient 0.9915 in Eq. (10.29). 

530 

MULTIVARIATE VOLATILITY MODELS AND THEIR APPLICATIONS 

Figure 10.10 Fitted conditional correlation coefficient between monthly log returns of IBM stock and 

S&P 500 index using time-varying correlation GARCH(1,1) model of Example 10.5 with Cholesky 
decomposition. Horizontal line denotes average of 0.612 of the estimated coefficients. 

Figure 10.10 shows the fitted time-varying correlation coefficient in Eq. (10.30). 
It shows a smoother correlation pattern than that of Figure 10.9 and confirms 
the decreasing trend of the correlation coefficient. In particular, the fitted correla¬ 
tion coefficients in recent years are smaller than those of the other models. The 
two time-varying correlation models for the monthly log returns of IBM stock 
and the S&P 500 index have comparable maximized-likelihood functions of about 
-3672, indicating the fits are similar. However, the approach based on the Cholesky 
decomposition may have some advantages. First, it does not require any parameter 
constraint in estimation to ensure the positive definiteness of If one also uses 
log transformation for giitt, then no constraints are needed for the entire volatility 
model. Second, the log-likelihood function becomes simple under the transforma¬ 
tion. Third, the time-varying parameters qij<t and giUt have nice interpretations. 
However, the transformation makes inference a bit more complicated because the 
fitted model may depend on the ordering of elements in at\ recall that au is not 
transformed. In theory, the ordering of elements in a, should have no impact on 
volatility. 

Finally, the 1-step-ahead forecast of the conditional covariance matrix at the 

forecast origin t = 888 for the new time-varying correlation model is 

^888(1) = 

73.45 7.34 
7.34 17.87 

GARCH MODELS FOR BIVARIATE RETURNS 

531 

The correlation coefficient of the prior forecast is 0.203, which is substantially 
smaller than those of the previous two models. However, forecasts of the condi¬ 
tional variances are similar as before. 

10.4.3 Dynamic Correlation Models 

Using the parameterization in Eq. (10.7), several authors have proposed parsimo¬ 
nious models for pt to describe the time-varying correlations. We refer to those 
models as the dynamic conditional correlation (DCC) models. 

For /^-dimensional returns, Tse and Tsui (2002) assume that the conditional 

correlation matrix pt follows the model 

Pt — (1 ~ #1 — #2)P + 6\Pt-\ + Ql^t-1’ 

where 0\ and 62 are scalar parameters, p is a k x k positive-definite matrix with unit 
diagonal elements, and is the k x k sample correlation matrix using shocks 
from t — m, ..., t — 1 for a prespecified m. Typically, one assumes that 0 < 0; < 1 
and 0i + 02 < 1 so that the resulting correlation matrix p, is positive definite for 
all t. For a given p, the model is parsimonious. In applications, the choice of p 
and m deserves a careful investigation. One possibility is to let p be the sample 
correlation matrix of the returns. The correlation equation then only employs two 

parameters. 

Engle (2002) proposes the model 

Pt — JtQtJti 

where Qt = (qijj)kxk is a positive-definite matrix, J, = diag{g11>( , ..., qkkt }, 

— 1/2 —1/2 

and Qt satisfies 

Qt = (1 — 0i — #2) Q + 0io-i^-i + $2 Qt-i> 

where et is the standardized innovation vector with elements eit = ait/y'a,Q 
is the unconditional covariance matrix of €t, and 0i and 02 are nonnegative scalar 
parameters satisfying 0 < 0i + 02 < 1. The J, matrix is a normalization matrix to 

guarantee that Rt is a correlation matrix. 

An obvious drawback of the prior two models is that 0i and 02 are scalar so 
that all the conditional correlations have the same dynamics. This might be hard 
to justify in real applications, especially when the dimension k is large. 

Tsay (2006) extends the previous DCC models in two ways. First, the standard¬ 
ized innovations are assumed to follow a multivariate Student-t distribution of Eq. 
(10.42). Second, the marginal volatility models have leverage effects. Specifically, 

the volatility equation for rt is 

D2t = Aq + Ai D2_\ + A2A2^ + A3Lf_1( 

(10.31) 

532 

MULTIVARIATE VOLATILITY MODELS AND THEIR APPLICATIONS 

where Dt is the diagonal matrix of volatilities as defined in Eq. (10.7), Aj = 
diag{fli7-,..., cikj}, Ai = diagffi,-,..., 1^} are k x k diagonal matrices of param¬ 
eters and L,_i = diag{Z>i,r—i, • • •, Lk,t-\} is also a k x k diagonal matrix with 
diagonal elements 

Li,t-i — 

@i,t- 
0 

if i < 0, 

otherwise. 

In Eq. (10.31), the parameters £ij 
satisfy 0 < Yl3j=i Uj < 1, Uq > 0 for 
i = 1,,k, and iji > 0 for all positive i and j. The constraint ensures that the 
volatilities exist. Of course, if A3 = 0, then there is no leverage effect. 

The correlation equation is 

Pt = (l-0l-02)p + d1ft_l+e2pt_l, (10.32) 

where p is the sample correlation matrix of the returns and 0 < 9\ + 02 < 1 with 
6[ > 0 for i = l,2. 

Example 10.6. To illustrate the DCC model, we consider the daily exchange 
rates between U.S. dollar versus European euro and Japanese yen and the stock 
prices of IBM and Dell from January 1999 to December 2004. The exchange rates 
are the noon spot rate obtained from the Federal Reserve Bank of St. Louis and 
the stock returns are from the Center for Research in Security Prices (CRSP). We 
compute the simple returns of the exchange rates and remove returns for those 
days when one of the markets was not open. This results in a four-dimensional 
return series with 1496 observations. The return vector is rt — (rit, r2t, r2t, r4t)' 
with r\t and r2t being the returns of euro and yen exchange rate, respectively, and 
r2t and r4t are the returns of IBM and Dell stock, respectively. All returns are 
in percentages. Figure 10.11 shows the time plot of the return series. From the 
plot, equity returns have higher variability than the exchange rate returns, and the 
variability of equity returns appears to be decreasing in latter years. Table 10.1 
provides some descriptive statistics of the return series. As expected, the means of 
the returns are essentially zero and all four series have heavy tails with positive 
excess kurtosis. 

The equity returns have some serial correlations, but the magnitude is small. If 
multivariate Ljung-Box statistics are used, we have <2(3) = 59.12 with a p value 
of 0.13 and £2(5) = 106.44 with a p value of 0.03. For simplicity, we use the 
sample mean as the mean equation and apply the proposed multivariate volatility 
model to the mean-corrected data. In estimation, we start with a general model, but 
add some equality constraints as some estimates appear to be close to each other. 
The results are given in Table 10.2 along with the value of likelihood function 
evaluated at the estimates. 

For each estimated multivariate volatility model in Table 10.2, we compute the 

standardized residuals as 

GARCH MODELS FOR BIVARIATE RETURNS 

533 

1999 2000 2001 2002 2003 2004 2005 
(a) 

1999 2000 2001 2002 2003 2004 2005 
(b) 

1999 2000 2001 2002 2003 2004 2005 

(d) 

Figure 10.11 Time plots of daily simple return series from January 1999 to December 2004: 

(a) dollar-euro exchange rate, (b) dollar-yen exchange rate, (c) IBM stock, and (d) Dell stock. 

TABLE 10.1 Descriptive Statistics of Daily Returns of Example 10.6.° 

Asset 

USEU 

JPUS 

IBM 

DELL 

Mean 
Standard error 
Skewness 
Excess kurtosis 
Box-Ljung 0(12) 

0.0091 
0.6469 
0.0342 
2.7090 
12.5 

-0.0059 
0.6626 
-0.1674 
2.0332 
6.4 

0.0066 
5.4280 
-0.0530 
6.2164 

24.1 

0.0028 
10.1954 
-0.0383 
3.3054 

24.1 

flThe returns are in percentages, and the sample period is from January 1999 to December 2004 for 

1496 observations. 

where z'/2 is the symmetric square root matrix of the estimated volatility matrix 
%. We apply the multivariate Ljung-Box statistics to the standardized residuals it 
and its squared process if of a fitted model to check model adequacy. For the full 
model in Table 10.2(a), we have 0(10) = 167.79(0.32) and 0(10) = 110.19(1.00) 
for it and if, respectively, where the number in parentheses denotes p value. 
Clearly, the model adequately describes the first two moments of the return series. 
For the model in Table 10.2(b), we have 0(10) = 168.59(0.31) and 0(10) = 
109.93(1.00). For the final restricted model in Table 10.2(c), we obtain 0(10) = 

534 

MULTIVARIATE VOLATILITY MODELS AND THEIR APPLICATIONS 

TABLE 10.2 Estimation Results of Multivariate Volatility Models for Example 10.6" 

Ao 

0.0041(0.0033) 
0.0088(0.0038) 
0.0071(0.0053) 
0.0150(0.0136) 

Ao 

0.0066(0.0028) 
0.0066(0.0023) 
0.0080(0.0052) 
0.0108(0.0086) 

(a) Full Model Estimation with Lmax = —9175.80 

Ai 

A2 

(v,ouo2y 

0.9701(0.0114) 
0.9515(0.0126) 
0.9636(0.0092) 
0.9531(0.0155) 

0.0214(0.0075) 
0.0281(0.0084) 
0.0326(0.0087) 
0.0461(0.0164) 

7.8729(0.4693) 
0.9808(0.0029) 
0.0137(0.0025) 

(b) Restricted Model with Lmax = —9176.62 

A] = A x I 

0.9606(0.0068) 

A2 

(v,eu92y 

0.0255(0.0068) 
0.0240(0.0059) 
0.0355(0.0068) 
0.0385(0.0073) 

7.8772(0.7144) 
0.9809(0.0042) 
0.0137(0.0025) 

(c) Final Restricted Model with Lmax = —9177.44 

Ao(Ai, Aj, A3, A4) 

A] = A x / 

A2(bi,bi,b2, b2) 

(v,ol,e2y 

0.0067(0.0021) 
0.0067(0.0021) 
0.0061(0.0044) 
0.0148(0.0084) 

0.9603(0.0063) 

0.0248(0.0048) 
0.0248(0.0048) 
0.0372(0.0061) 
0.0372(0.0061) 

7.9180(0.6952) 
0.9809(0.0042) 
0.0137(0.0028) 

(d) Model with Leverage Effects, Lmax = —9169.04 

Ao(Ai, A2, A3, A4) 

Aj = A x / 

A2(bi,b2, £3, b4) 

(v, 01,02)' 

0.0064(0.0027) 
0.0066(0.0023) 
0.0128(0.0055) 
0.0210(0.0099) 

0.9600(0.0065) 

0.0254(0.0063) 
0.0236(0.0054) 
0.0241(0.0056) 
0.0286(0.0062) 

8.4527(0.7556) 
0.9810(0.0044) 
0.0132(0.0027) 

“Lmax denotes the value of likelihood function evaluated at the estimates, v is the degrees of freedom of 

the multivariate Student-f distribution, and the numbers in parentheses are asymptotic standard errors. 

168.50(0.31) and <2(10) — 111.75(1.00). Again, the restricted models are capable 
of describing the mean and volatility of the return series. 

From Table 10.2, we make the following observations. First, using the likelihood 
ratio test, we cannot reject the final restricted model compared with the full model. 
This results in a very parsimonious model consisting of only 9 parameters for the 
time-varying correlations of the four-dimensional return series. Second, for the two 
stock return series, the constant terms in A0 are not significantly different from zero, 
and the sum of GARCH parameters is 0.0372 + 0.9603 = 0.9975, which is very 
close to unity. Consequently, the volatility series of the two equity returns exhibit 
IGARCH behavior. On the other hand, the volatility series of the two exchange rate 
returns appear to have a nonzero constant term and high persistence in GARCH 

GARCH MODELS FOR BIVARIATE RETURNS 

535 

1999 2000 2001 2002 2003 2004 2005 

(a) 

1999 2000 2001 2002 2003 2004 2005 

(b) 

(c) 

Figure 10.12 Time plots of estimated volatility series of four asset returns. Solid line is from proposed 

model and dashed line is from a rolling estimation with window size 69: (a) dollar-euro exchange rate, 

(b) dollar-yen exchange rate, (c) IBM stock, and (d) Dell stock. 

parameters. Third, to better understand the efficacy of the proposed model, we 
compare the results of the final restricted model with those of rolling estimates. 
The rolling estimates of covariance matrix are obtained using a moving window of 
size 69, which is the approximate number of trading days in a quarter. Figure 10.12 
shows the time plot of estimated volatility. The solid line is the volatility obtained 
by the proposed model and the dashed line is for volatility of the rolling estimation. 
The overall pattern seems similar, but, as expected, the rolling estimates respond 
more slowly than the proposed model to large innovations. This is shown by the 
faster rise and decay of the volatility obtained by the proposed model. Figure 10.13 
shows the time-varying correlations of the four asset returns. The solid line denotes 
correlations obtained by the final restricted model of Table 10.2, whereas the dashed 
line is for rolling estimation. The correlations of the proposed model seem to be 

smoother. 

Table 10.2(d) gives the results of a fitted integrated GARCH-type model with 
leverage effects. The leverage effects are statistically significant for equity returns 
only and are in the form of an IGARCH model. Specifically, the A3 matrix of the 

536 

MULTIVARIATE VOLATILITY MODELS AND THEIR APPLICATIONS 

Figure 10.13 Time plots of time-varying correlations between percentage simple returns of four assets 

from January 1999 to December 2004. Solid line is from the proposed model, whereas dashed line is 
from a rolling estimation with window size 69. 

(f) Dell vs IBM 

HIGHER DIMENSIONAL VOLATILITY MODELS 

537 

correlation equation in Eq. (10.31) is 

A3 = diag {0, 0, (1 - 0.96 - 0.0241), (1 - 0.96 - 0.0286)} 

= diag{0, 0, 0.0159,0.0114}. 

Although the magnitudes of the leverage parameters are small, they are statistically 
significant. This is shown by the likelihood ratio test. Specifically, comparing the 
fitted models in Table 10.2(b) and (d), the likelihood ratio statistic is 15.16, which 
has a p value of 0.0005 based on the chi-squared distribution with 2 degrees of 

freedom. 

10.5 HIGHER DIMENSIONAL VOLATILITY MODELS 

In this section, we make use of the sequential nature of Cholesky decomposition to 
suggest a strategy for building a high-dimensional volatility model. Again write the 
vector return series as rt = ft, + at. The mean equations for rt can be specified 
by using the methods of Chapter 8. A simple vector AR model is often sufficient. 
Here we focus on building a volatility model using the shock process at. 

Based on the discussion of Cholesky decomposition in Section 10.3, the orthog¬ 
onal transformation from alt to bn only involves bjt for j < i. In addition, the 
time-varying volatility models built in Section 10.4 appear to be nested in the sense 
that the model for gnit depends only on quantities related to bjt for j < i. Con¬ 
sequently, we consider the following sequential procedure to build a multivariate 

volatility model: 

1. Select a market index or a stock return that is of major interest. Build a 

univariate volatility model for the selected return series. 

2. Augment a second return series to the system, perform the orthogonal trans¬ 
formation on the shock process of this new return series, and build a bivariate 
volatility model for the system. The parameter estimates of the univariate 
model in step 1 can be used as the starting values in bivariate estimation. 

3. Augment a third return series to the system, perform the orthogonal trans¬ 
formation on this newly added shock process, and build a three-dimensional 
volatility model. Again parameter estimates of the bivariate model can be 
used as the starting values in the three-dimensional estimation. 

4. Continue the augmentation until a joint volatility model is built for all the 

return series of interest. 

Finally, model checking should be performed in each step to ensure the adequacy 
of the fitted model. Experience shows that this sequential procedure can sim¬ 
plify substantially the complexity involved in building a high-dimensional volatility 
model. In particular, it can markedly reduce the computing time in estimation. 

538 

MULTIVARIATE VOLATILITY MODELS AND THEIR APPLICATIONS 

1992 

1994 

1996 

1998 

2000 

Year 

(a) 

o 

1992 1994 1996 1998 2000 

Year 

(c) 

Figure 10.14 Time plots of daily log returns in percentages of (a) S&P 500 index and stocks of (b) 
Cisco Systems and (c) Intel Corporation from January 2, 1991, to December 31, 1999. 

Example 10.7. We demonstrate the proposed sequential procedure by building 
a volatility model for the daily log returns of the S&P 500 index and the stocks 
of Cisco Systems and Intel Corporation. The data span is from January 2, 1991, 
to December 31, 1999, with 2275 observations. The log returns are in percentages 
and shown in Figure 10.14. Components of the return series are ordered as r, = 
(SP5?, CSCO,, INTCry. The sample means, standard errors, and correlation matrix 
of the data are 

V = 

0.066 
0.257 
0.156 

& i 
02 
_ <73 

_ 

" 0.875 " 
2.853 
2.464 

. P = 

~ 1.00 
0.52 
0.52 
1.00 
0.50  0.47 

0.50 “ 
0.47 
1.00 

Using the Ljung—Box statistics to detect any serial dependence in the return 
series, we obtain 03(1) = 26.20, 03(4) = 79.73, and g3(8) = 123.68. These test 
statistics are highly significant with p values close to zero as compared with 

HIGHER DIMENSIONAL VOLATILITY MODELS 

539 

TABLE 10.3 Sample Cross-Correlation Matrices of Daily Log Returns of S&P 500 
Index and Stocks of Cisco Systems and Intel Corporation from January 2, 1991, to 

December 31, 1999 

1 

2 

. _ . 

Lag 

3 

— 

4 

5 

6 

- • • 

. _ . 

chi-squared distributions with degrees of freedom 9, 36, and 72, respectively. 
There is indeed some serial dependence in the data. Table 10.3 gives the first 
five lags of sample cross-correlation matrices shown in the simplified notation of 
Chapter 8. An examination of the table shows that (a) the daily log returns of 
the S&P 500 index does not depend on the past returns of Cisco or Intel, (b) the 
log return of Cisco stock has some serial correlations and depends on the past 
returns of the S&P 500 index (see lags 2 and 5), and (c) the log return of Intel 
stock depends on the past returns of the S&P 500 index (see lags 1 and 5). These 
observations are similar to those between the returns of IBM stock and the S&P 
500 index analyzed in Chapter 8. They suggest that returns of individual large- 
cap companies tend to be affected by the past behavior of the market. However, 
the market return is not significantly affected by the past returns of individual 

companies. 

Turning to volatility modeling and following the suggested procedure, we start 

with the log returns of the S&P 500 index and obtain the model 

ru = 0.078 + 0.042ru_i - 0.062ru_3 - 0.048ru_4 - 0.052ru_5 + au, 

ffUtt = 0.013 + 0.092a^_1 + 0.894ofiU_i, (10.33) 

where standard errors of the parameters in the mean equation are 0.016, 0.023, 
0.020, 0.022, and 0.020, respectively, and those of the parameters in the volatility 
equation are 0.002, 0.006, and 0.007, respectively. Univariate Ljung-Box statistics 
of the standardized residuals and their squared series fail to detect any remaining 
serial correlation or conditional heteroscedasticity in the data. Indeed, we have 
<2(10) = 7.38(0.69) for the standardized residuals and 0(10) = 3.14(0.98) for the 

squared series. 

Augmenting the daily log returns of Cisco stock to the system, we build a 

bivariate model with mean equations given by 

r\t = 0.065 — 0.046ri)f_3 + a\t, 

r2t = 0.325 + 0.195n,t-2 - 0.091r2,f_2 + a2t, (10.34) 

540 

MULTIVARIATE VOLATILITY MODELS AND THEIR APPLICATIONS 

where all of the estimates are statistically significant at the 1% level. Using the 
notation of Cholesky decomposition, we obtain the volatility equations as 

gn,t = 0.006 + 0.0516?>,_1 +0.943glu_1, 

qiu = 0.331 + 0.790^2i,r-i - 0.041a2>f_i, (10.35) 

822,t = 0.177 + 0.0826f>,_1 + 0.890g22,r-i, 

where b\t = a\t, b2, = ait ~ #21,standard errors of the parameters in the 
equation of gnjf are 0.001, 0.005, and 0.006, those of the parameters in the 
equation of q2\j are 0.156, 0.099, and 0.011, and those of the parameters in 
the equation of g22jt are 0.029, 0.008, and 0.011, respectively. The bivariate 
Ljung-Box statistics of the standardized residuals fail to detect any remaining 
serial dependence or conditional heteroscedasticity. The bivariate model is 
adequate. Comparing with Eq. (10.33), we see that the difference between the 
marginal and univariate models of r\t is small. 

The next and final step is to augment the daily log returns of Intel stock to the 

system. The mean equations become 

rXt = 0.065 — 0.043ri;,_3 + a\t, 

r2t — 0.326 + 0.201ri;f_2 — 0.089r2?_i + a2t, (10.36) 

r3, = 0.192 - 0.264ru_! + 0.059r3jf_i + a3t, 

where standard errors of the parameters in the first equation are 0.016 and 0.017, 
those of the parameters in the second equation are 0.052, 0.059, and 0.021, and 
those of the parameters in the third equation are 0.050, 0.057, and 0.022, respec¬ 
tively. All estimates are statistically significant at about the 1 % level. As expected, 
the mean equations for r\t and r2t are essentially the same as those in the bivariate 
case. 

The three-dimensional time-varying volatility model becomes a bit more com¬ 

plicated, but it remains manageable as 

8u,t = 0.006 + 0.0506?>I_1 + 0.943gn,f_1, 

#21,t = 0.277 + 0.824<72i ?_i — 0.035a2j_i, 

822,t = o. 178 + 0.082b\t_y + 0.889"22,,_i , 

#31,r = 0.039 + 0.973^3ii,_1 + 0.010a3)i_i, (10.37) 

#32,; — 0.006 + 0.981g32i,_i + 0.004<32 r_i, 

£33,/ = 1.188 + 0.05 36?+0.687ft3i/_! - 0.0l9g22,t-i, 

where bu = a\t, b2t = a2t — q2\jb\t, b2t = a2t — q2\ttb\t — q22^b2t, and standard 
errors of the parameters are given in Table 10.4. Except for the constant 
term of the q22j equation, all estimates are significant at the 5% level. Let 

HIGHER DIMENSIONAL VOLATILITY MODELS 

541 

TABLE 10.4 Standard Errors of Parameter Estimates of Three-Dimensional 
Volatility Model for Daily Log Returns in Percentages of S&P 500 Index and Stocks 
of Cisco Systems and Intel Corporation from January 2, 1991, to December 31, 1999° 

Equation 

Standard Error 

Equation 

Standard Error 

gll.r 

§22, t 

§33,t 

0.001 

0.029 

0.407 

0.005 

0.009 

0.015 

0.006 

0.011 

0.100 0.008 

<7217 

<731,r 

<732,f 

0.135 

0.017 

0.004 

0.086 0.010 

0.012 0.004 

0.013 0.001 

“The ordering of the parameter is the same as appears in Eq. (10.37). 

at = {a\,/a\t, ci3t/^3t)' be the standardized residual series, where 
btr = s/cfuj is the fitted conditional standard error of the i th return. The 
Ljung-Box statistics of at give <23(4) = 34.48(0.31) and £>3(8) = 60.42(0.70), 
where the degrees of freedom of the chi-squared distributions are 31 and 
67, respectively, after adjusting for the number of parameters used in the 
mean equations. For the squared standardized residual series at, we have 
Q*(4) = 28.71(0.58) and <2*(8) = 52.00(0.91). Therefore, the fitted model 
appears to be adequate in modeling the conditional means and volatilities. 

The three-dimensional volatility model in Eq. (10.37) shows some interest¬ 
ing features. First, it is essentially a time-varying correlation GARCH(1,1) model 
because only lag-1 variables are used in the equations. Second, the volatility of 
the daily log returns of the S&P 500 index does not depend on the past volatil¬ 
ities of Cisco or Intel stock returns. Third, by taking the inverse transformation 
of the Cholesky decomposition, the volatilities of daily log returns of Cisco and 
Intel stocks depend on the past volatility of the market return; see the relationships 
between elements of £f, Lt, and Gt given in Section 10.3. Fourth, the correlation 
quantities qijj have high persistence with large AR(1) coefficients. 

Figure 10.15 shows the fitted volatility processes of the model (i.e., &aj) for 
the data. The volatility of the index return is much smaller than those of the two 
individual stock returns. The plots also show that the volatility of the index return 
has increased in recent years, but this is not the case for the return of Cisco Systems. 
Figure 10.16 shows the time-varying correlation coefficients between the three 
return series. Of particular interest is to compare Figures 10.15 and 10.16. They 
show that the correlation coefficient between two return series increases when the 
returns are volatile. This is in agreement with the empirical study of relationships 
between international stock market indexes for which the correlation between two 
markets tends to increase during a financial crisis. 

The volatility model in Eq. (10.37) consists of two sets of equations. The first 
set of equations describes the time evolution of conditional variances (i.e., gaj), 
and the second set of equations deals with correlation coefficients (i.e., qijj with 
i > j). For this particular data set, an AR(1) model might be sufficient for the 
correlation equations. Similarly, a simple AR model might also be sufficient for 
the conditional variances. Define vt = (uh,d V22,t. ^33,?)', where vuj = ln(g/;)f), 
and q, = (q2\,t, q3\,t, <732,tY■ The previous discussion suggests that we can use the 

542 

MULTIVARIATE VOLATILITY MODELS AND THEIR APPLICATIONS 

(a) 

1992 1994 1996 1998 2000 

Year 

(c) 

Figure 10.15 Time plots of fitted volatilities for daily log returns, in percentages, of (a) S&P 500 
index and stocks of (b) Cisco Systems and (c) Intel Corporation from January 2, 1991, to December 
31, 1999. 

simple lag-1 models 

vt — c 9t~c2 + ^2^r~l 

as exact functions to model the volatility of asset returns, where ct are constant 
vectors and /3( are 3 x 3 real-valued matrices. If a noise term is also included in 
the above equations, then the models become 

v, —c l + + elt, qt=c2 + + eit, 

where eit are random shocks with mean zero and a positive-definite covariance 
matrix, and we have a simple multivariate stochastic volatility model. In a recent 
manuscript, Chib, Nardari, and Shephard (1999) use Markov chain Monte Carlo 
(MCMC) methods to study high-dimensional stochastic volatility models. The 
model considered there allows for time-varying correlations, but in a relatively 
restrictive manner. Additional references of multivariate volatility model include 

FACTOR-VOLATILITY MODELS 

543 

Figure 10.16 Time plots of fitted time-varying correlation coefficients between daily log returns of 

S&P 500 index and stocks of Cisco Systems and Intel Corporation from January 2, 1991, to December 

31, 1999. 

Harvey, Ruiz, and Shephard (1994). We discuss MCMC methods in volatility mod¬ 

eling in Chapter 12. 

10.6 FACTOR-VOLATILITY MODELS 

Another approach to simplifying the dynamic structure of a multivariate volatility 
process is to use factor models. In practice, the “common factors” can be determined 
a priori by substantive matter or empirical methods. As an illustration, we use the 
factor analysis of Chapter 8 to discuss factor-volatility models. Because volatility 
models are concerned with the evolution over time of the conditional covariance 
matrix of at, where a, — rt - fit, a simple way to identify the “common factors” 
in volatility is to perform a principal component analysis (PCA) on at; see the 
PC A of Chapter 8. Building a factor-volatility model thus involves a three-step 

procedure: 

• Select the first few principal components that explain a high percentage of 

variability in at. 

544 

MULTIVARIATE VOLATILITY MODELS AND THEIR APPLICATIONS 

• Build a volatility model for the selected principal components. 

• Relate the volatility of each alt series to the volatilities of the selected principal 

components. 

The objective of such a procedure is to reduce the dimension but maintain an 
accurate approximation of the multivariate volatility. 

Example 10.8. Consider again the monthly log returns, in percentages, of IBM 
stock and the S&P 500 index of Example 10.5. Using the bivariate AR(3) model 
of Example 8.4, we obtain an innovational series a,. Performing a PCA on a, 
based on its covariance matrix, we obtained eigenvalues 63.373 and 13.489. The 
first eigenvalue explains 82.2% of the generalized variance of at. Therefore, we 
may choose the first principal component xt = 0.191a\t + 0.604a2r as the common 
factor. Alternatively, as shown by the model in Example 8.4, the serial dependence 
in rt is weak and, hence, one can perform the PCA on r, directly. For this particular 
instance, the two eigenvalues of the sample covariance matrix of rt are 63.625 and 
13.513, which are essentially the same as those based on a,. The first principal 
component explains approximately 82.5% of the generalized variance of rt, and 
the corresponding common factor is xt = 0.796ri, +0.605r2,. Consequently, for 
the two monthly log return series considered, the effect of the conditional mean 
equations on PCA is negligible. 

Based on the prior discussion and for simplicity, we use xt = 0.796n, + 
0.605r2f as a common factor for the two monthly return series. Figure 10.17(a) 
shows the time plot of this common factor. If univariate Gaussian GARCH models 
are entertained, we obtain the following model for xt: 

xt — 1.317 + 0.096x/_i 4- at, at = 07^, 

crf2 = 3.834 + 0.110a2_, + 0.825af2_1. (10.38) 

All parameter estimates of the previous model are highly significant at the 1% level, 
and the Ljung-Box statistics of the standardized residuals and their squared series 
fail to detect any model inadequacy. Figure 10.17(b) shows the fitted volatility of 
x, [i.e., the sample af series in Eq. (10.38)]. 

Using o} of model (10.38) as a common volatility factor, we obtain the following 

model for the original monthly log returns. The mean equations are 

r\t — 1.140 4- 0.079r ii?_i + 0.067rif_2 — 0.122 r2i_2 + a\t, 

r21 = 0.537 4- a2(, 

where standard errors of the parameters in the first equation are 0.211, 0.030, 0.031, 
and 0.043, respectively, and standard error of the parameter in the second equation 

FACTOR-VOLATILITY MODELS 

545 

o 

o 
C\J 

o 

o 
CM 
I 
o 

< 
o 
a. 

1940 1960 1980 2000 

Year 

(a) 

(b) 

Figure 10.17 (a) Time plot of first principal component of monthly log returns of IBM stock and S&P 

500 index, (b) Fitted volatility process based on a GARCH(1,1) model. 

is 0.165. The conditional variance equation is 

<^11,/ 

022,/ 

0.098 
(0.044) 

+ 

19.08 
(3.70) 
-5.62 
(2.36) 

’ alt-i '  + 

a2 

0.333 
(0.076) 
0.596 
(0.050) 

(10.39) 

where, as before, standard errors are in parentheses, and of is obtained from model 

(10.38). The conditional correlation equation is 

_ exp (qt) 

Pt 1+expO?,)’ 

qt = -2.098 + 4.120a-! + 0.078 ai-f~lfl2’r-1 , (10.40) 

V/04l,f-10'22,/-l 

where standard errors of the three parameters are 0.025, 0.038, and 0.015, 
respectively. Defining the standardized residuals as before, we obtain 
g2(4) = 15.37(0.29) and £>2(8) = 34.24(0.23), where the number in parentheses 
denotes the p value. Therefore, the standardized residuals have no serial 

546 

MULTIVARIATE VOLATILITY MODELS AND THEIR APPLICATIONS 

correlations. Yet we have Q\{4) = 20.25(0.09) and g*(8) = 61.95(0.0004) for 
the squared standardized residuals. The volatility model in Eq. (10.39) does not 
adequately handle the conditional heteroscedasticity of the data especially at 
higher lags. This is not surprising as the single common factor only explains about 
82.5% of the generalized variance of the data. 

Comparing the factor model in Eqs. (10.39) and (10.40) with the time-varying 
correlation model in Eqs. (10.26) and (10.27), we see that (a) the correlation 
equations of the two models are essentially the same, (b) as expected the factor 
model uses fewer parameters in the volatility equation, and (c) the common- 
factor model provides a reasonable approximation to the volatility process of 
the data. 

Remark. In Example 10.8, we used a two-step estimation procedure. In the 
first step, a volatility model is built for the common factor. The estimated volatility 
is treated as given in the second step to estimate the multivariate volatility model. 
Such an estimation procedure is simple but may not be efficient. A more efficient 
estimation procedure is to perform a joint estimation. This can be done relatively 
easily provided that the common factors are known. For example, for the monthly 
log returns of Example 10.8, a joint estimation of Eqs. (10.38)—(10.40) can be per¬ 
formed if the common factor xt = 0.769/t; + 0.605r2r is treated as given. □ 

10.7 APPLICATION 

We illustrate the application of multivariate volatility models by considering the 
value at risk (VaR) of a financial position with multiple assets. Suppose that an 
investor holds a long position in the stocks of Cisco Systems and Intel Corpora¬ 
tion each worth $1 million. We use the daily log returns for the two stocks from 
January 2, 1991, to December 31, 1999, to build volatility models. The VaR is 
computed using the 1-step-ahead forecasts at the end of data span and 5% critical 
values. 

Let VaRj be the value at risk for holding the position on Cisco Systems stock 
and VaR2 for holding Intel stock. Results of Chapter 7 show that the overall daily 
VaR for the investor is 

VaR = .y/vaRj + VaR^ + 2pVaR! VaR2. 

In this illustration, we consider three approaches to volatility modeling for cal¬ 
culating VaR. For simplicity, we do not report standard errors for the parameters 
involved or model checking statistics. Yet all of the estimates are statistically signifi¬ 
cant at the 5% level, and the models are adequate based on the Ljung-Box statistics 
of the standardized residual series and their squared series. The log returns are in 
percentages so that the quantiles are divided by 100 in VaR calculations. Let rit 
be the return of Cisco stock and r2t the return of Intel stock. 

APPLICATION 

547 

Univariate Models 
This approach uses a univariate volatility model for each stock return and uses the 
sample correlation coefficient of the stock returns to estimate p. The univariate 
volatility models for the two stock returns are 

r\t = 0.380 + 0.034ri!?_i — 0.061ri,;_2 — 0.055ri,f_3 + a\t, 

erg = 0.599 + 0.117a? f_L + O.BWof^ 

and 

C2r —0.187 + a2t, 

af = 0.310 + 0.032^^! + 0.918 a22,_j. 

The sample correlation coefficient is 0.473. The 1-step-ahead forecasts needed in 
VaR calculation at the forecast origin t = 2275 are 

0.626, of — 4.152, r2 = 0.187, a22 = 6.087, p = 0.473. 

The 5% quantiles for both daily returns are 

qx = 0.626 - 1.65V4.152 = -2.736, q2 = 0.187 - 1.65V6.087 = -3.884, 

where the negative sign denotes loss. For the individual stocks, VaRi = 
SlOOOOOOgi/100 = $27,360rmrfVaR2 = $1000000^2/100 = $38,840. Conse¬ 
quently, the overall VaR for the investor is VaR = $57,117. 

Constant-Correlation Bivariate Model 
This approach employs a bivariate GARCH(1,1) model for the stock returns. The 
correlation coefficient is assumed to be constant over time, but it is estimated jointly 

with other parameters. The model is 

rxt = 0.385 + 0.038ru_! - 0.060ru_2 - 0.047ru_3 + au, 

r2t — 0.222 + a2t, 

an,r =0.624+ 0.110a^_1 +0.816an,f_i, 

a22,r = 0.664 + 0.038a2,;_i + 0.853a22,;_i, 

and p = 0.475. This is a diagonal bivariate GARCH(1,1) model. The 1-step-ahead 
forecasts for VaR calculation at the forecast origin t = 2275 are 

r\ = 0.373, of = 4.287, r2 = 0.222, of = 5.706, p = 0.475. 

Consequently, we have VaRi = $30,432 and VaR2 = $37,195. The overall 5% 

VaR for the investor is VaR = $58,180. 

548 

MULTIVARIATE VOLATILITY MODELS AND THEIR APPLICATIONS 

Time-Varying Correlation Model 
Finally, we allow the correlation coefficient to evolve over time by using the 
Cholesky decomposition. The fitted model is 

rh = 0.355 + 0.039ru_i - 0.057ru_2 - 0.038ru_3 + au, 

r2t = 0.206 + a2l, 

Su,t — 0.420 + 0.091 b\ t_{ + 0.858gn,?_i, 

— 0.123 + 0.689*21,r-i — 0.014a2l/-i, 

822,, = 0.080 + 0.013b\t_x + 0.971 g22,t-i, 

where b\t = a\t and b2t = a2t — q2\,ta\t- The 1-step-ahead forecasts for VaR cal¬ 
culation at the forecast origin t = 2275 are 

r\= 0.352, r2 = 0.206, gu =4.252, q2l =0.421, g22 = 5.594. 

Therefore, we have of = 4.252, o2\ = 1.791, and <x22 = 6.348. The correlation 
coefficient is p — 0.345. Using these forecasts, we have VaRj = $30,504, VaR2 = 
$39,512, and the overall VaR = $57,648. 

The estimated VaR values of the three approaches are similar. The univariate 
models give the lowest VaR, whereas the constant-correlation model produces the 
highest VaR. The range of the difference is about $1100. The time-varying volatility 
model seems to produce a compromise between the two extreme models. 

10.8 MULTIVARIATE t DISTRIBUTION 

Empirical analysis indicates that the multivariate Gaussian innovations used in the 
previous sections may fail to capture the kurtosis of asset returns. In this situation, 
a multivariate Student-t distribution might be useful. There are many versions of 
the multivariate Student-t distribution. We give a simple version here for volatility 
modeling. 

A k-dimensional random vector x = (jci, ..., xk)' has a multivariate Student- 
t distribution with v degrees of freedom and parameters ft = 0 and X = / (the 
identity matrix) if its probability density function (pdf) is 

mV) = 3^/2) (‘ + «-1*'*r<1'+w2, (10.41) 

where T(y) is the gamma function; see Mardia, Kent, and Bibby (1979, p. 57). The 
variance of each component xt in Eq. (10.41) is v/(v - 2), and hence we define 
€t = V(v ~ 2)/vx as the standardized multivariate Student-t distribution with v 
degrees of freedom. By transformation, the pdf of et is 

/(«r |v) = 

r[(v + k)/2] 

[n{v -2)fl2T{v/2) 

[1 +(u-2 )~x('tet]^v+k^2. 

(10.42) 

appendix: some remarks on estimation 

549 

For volatility modeling, we write a, = Y^2 et and assume that et follows the 
multivariate Student-t distribution in Eq. (10.42). By transformation, the pdf of 
at is 

f(at\v, Yt) 

r[(v + k)/2) 

[7T(U 

-2)]k/2r(v/2)\Yt\'/2 

[1 + (v - 2) la'tYt ]at] 

(v+k)/2 

Furthermore, if we use the Cholesky decomposition of Yt, then the pdf of the 
transformed shock b, becomes 

f(bt\v, Lt, Gt) = 

V[(v + k)/2] 

1/2 
[7t(v - 2)]*/2r(u/2) n)=i gjjj 
jj< 

x 

* bJ< 
1 + (v - 2)-1 — 

-i {v+k)/2 

where at = L,bt and gjjtt is the conditional variance of bjt. Because this pdf does 
not involve any matrix inversion, the conditional-likelihood function of the data is 

easy to evaluate. 

APPENDIX: SOME REMARKS ON ESTIMATION 

The estimation of multivariate ARMA models in this chapter is done by using 
the time series program SCA of Scientific Computing Associates. The estimation 
of multivariate volatility models is done by using either the S-Plus package with 
FinMetrics or the Regression Analysis for Time Series (RATS) program or Matlab. 
Below are some run streams for estimating multivariate volatility models using the 
RATS program. A line starting with * means “comment” only. 

Estimation of the Diagonal Constant-Correlation AR(2)-GARCH(1,1) Model 

for Example 10.5 
The program includes some Ljung-Box statistics for each component and some 
fitted values for the last few observations. The data file is m-ibmspln.txt, which 

has two columns, and there are 888 observations. 

all 0 888:1 
open data m-ibmspln.txt 

data(org=obs) / rl r2 

set hi = 0.0 
set h2 = 0.0 
nonlin aO al bl aOO all bll rho cl c2 pi 
frml alt = rl(t)-cl-pl*r2(t-1) 

frml a2t = r2(t)-c2 
frml gvarl = a0+al*alt(t-1)**2+bl*hl(t-1) 

550 

MULTIVARIATE VOLATILITY MODELS AND THEIR APPLICATIONS 

frml gvar2 = a00+all*a2t(t-1)**2+bll*h2(t-1) 

frml gdet = -0.5*(log(hi(t)=gvarl(t))+log(h2(t)=gvar2(t)) $ 

+ log(1.0-rho* *2)) 

frml gin = gdet(t)-0.5/(1.0-rho**2)*((alt(t)**2/hl(t)) $ 

+(a2t(t)**2/h2(t))-2*rho*alt(t)*a2t(t)/sgrt(hi(t)*h2(t))) 

smpl 3 888 

compute cl = 1.22, c2 = 0.57, pi = 0.1, rho = 0.1 

compute aO = 3.27, al = 0.1, bl = 0.6 

compute aOO = 1.17, all = 0.13, bll = 0.8 

maximize(method=bhhh,recursive,iterations-150) gin 

set fvl = gvarl(t) 

set resil = alt(t)/sqrt(fvl(t)) 

set residsq = resil(t)*resil(t) 

* Checking standardized residuals * 

cor(qstats,number=12,span=4) resil 

* Checking squared standardized residuals * 

cor(qstats,number=12,span=4) residsq 

set fv2 = gvar2(t) 

set resi2 = a2t(t)/sqrt(fv2(t)) 

set residsq = resi2(t)*resi2(t) 

* Checking standardized residuals * 

cor(qstats,number=12,span=4) resi2 

* Checking squared standardized residuals * 

cor(qstats,number=12,span=4) residsq 

* Last few observations needed for computing forecasts * 

set shockl = alt(t) 

set shock2 = a2t(t) 

print 885 888 shockl shock2 fvl fv2 

Estimation of the Time-Varying Coefficient Model in Example 10.5 

all 0 888:1 

open data m-ibmspln.txt 

data(org=obs) / rl r2 

set hi = 45.0 

set h2 = 31.0 

set rho = 0.8 

nonlin aO al bl fl aOO all bll dll fll cl c2 pi p3 qO ql q2 

frml alt = rl(t)-cl-pl*rl(t-1)-p3*r2(t-2) 

frml a2t = r2(t)-c2 

frml gvarl = a0+al*alt(t-1)**2+bl*hl(t-1)+fl*h2(t-1) 

frml gvar2 = a00+all*a2t(t-1)**2+bll*h2(t-1)+fll*hl(t-1) $ 

frml rhl = qO + ql*rho(t-l) $ 

+dll*alt(t-1)* *2 

+ q2*alt(t-1)*a2t(t-1)/sqrt(hi(t-1)*h2(t-1)) 

frml rh = exp(rhl(t))/(1+exp(rhl(t))) 

frml gdet = -0.5*(log(hi(t)=gvarl(t))+log(h2(t)=gvar2(t)) $ 

+log(l.0-(rho(t)=rh(t))**2)) 

frml gin = gdet(t)-0.5/(1.0-rho(t)**2)*((alt(t)**2/hl(t)) $ 

appendix: some remarks on estimation 

551 

+(a2t(t)**2/h2(t))-2*rho(t)*alt(t)*a2t(t)/sqrt(hi(t)*h2(t))) 

smpl 4 888 

compute cl = 1.4, c2 = 0.7, pi = 0.1, p3 = -0.1 

compute aO = 2.95, al = 0.08, bl = 0.87, fl = -.03 

compute aOO = 2.05, all = 0.05 

compute bll = 0.92, fll=-.06, dll=.04, qO = -2.0 

compute ql = 3.0, q2 = 0.1 

nlpar(criterion=value,cvcrit=0.00001) 

maximize(method=bhhh,recursive,iterations=150) gin 

set fvl = gvarl(t) 

set resil = alt(t)/sqrt(fvl(t)) 

set residsq = resil(t)*resil(t) 

* Checking standardized residuals * 

cor(qstats,number=16,span=4) resil 

* Checking squared standardized residuals * 

cor(qstats,number=16,span=4) residsq 

set fv2 = gvar2(t) 

set resi2 = a2t(t)/sqrt(fv2(t)) 

set residsq = resi2(t)*resi2(t) 

* Checking standardized residuals * 

cor(qstats,number=16,span=4) resi2 

* Checking squared standardized residuals * 

cor(qstats,number=16,span=4) residsq 

* Last few observations needed for computing forecasts * 

set rhohat = rho(t) 

set shockl = alt(t) 

set shock2 = a2t(t) 

print 885 888 shockl shock2 fvl fv2 rhohat 

Estimation of the Time-Varying Coefficient Model in Example 10.5 Using 

Cholesky Decomposition 

all 0 888:1 

open data m-ibmspln.txt 

data(org=obs) / rl r2 

set hi = 45.0 

set h.2 = 2 0.0 

set q = 0.8 

nonlin aO al bl aOO all bll dll fll cl c2 pi p3 tO tl t2 

frml alt = rl(t)-cl-pl*rl(t-1)-p3*r2(t-2) 

frml a2t = r2(t)-c2 

frml vl = a0+al*alt(t-1)**2+bl*hl(t-1) 

frml qt = tO + tl*q(t-l) + t2*a2t(t-l) 

frml bt = a2t(t) - (q(t)=qt(t))*alt(t) 

frml v2 = a00+all*bt(t-1)**2+bll*h2(t-1)+fll*hl(t-1) $ 

+dll*alt(t-1)* *2 

frml gdet = -0.5*(log(hi(t) = vl(t))+ log(h2(t)=v2(t))) 

frml garchln = gdet-0.5*(alt(t)**2/hl(t)+bt(t)**2/h2(t)) 

smpl 5 888 

552 

MULTIVARIATE VOLATILITY MODELS AND THEIR APPLICATIONS 

compute cl = 1.4, c2 = 0.7, pi = 0.1, p3 = -0.1 

compute aO = 1.0, al = 0.08, bl = 0.87 

compute aOO = 2.0, all = 0.05, bll = 0.8 

compute dll=.04, fll=-.06, tO =0.2, tl = 0.1, t2 = 0.1 

nlpar(criterion=value,cvcrit=0.00001) 

maximize(method=bhhh,recursive,iterations=150) garchln 

set fvl = vl(t) 

set resil = alt(t)/sqrt(fvl(t)) 

set residsq = resil(t)*resil(t) 

* Checking standardized residuals * 

cor(qstats,number=16,span=4) resil 

* Checking squared standardized residuals * 

cor(qstats,number=16,span=4) residsq 

set fv2 = v2(t)+qt(t)**2*vl(t) 

set resi2 = a2t(t)/sqrt(fv2(t)) 

set residsq = resi2(t)*resi2(t) 

* Checking standardized residuals * 

cor(qstats,number=16,span=4) resi2 

* Checking squared standardized residuals * 

cor(qstats,number=l6,span=4) residsq 

* Last few observations needed for forecasts * 

set rhohat = qt(t)*sqrt(vl(t)/fv2(t)) 

set shockl = alt(t) 

set shock2 = a2t(t) 

set g22 = v2(t) 

set q21 = qt(t) 

set b2t = bt(t) 

print 885 888 shockl shock2 fvl fv2 rhohat g22 q21 b2t 

Estimation of Three-Dimensional Time-Varying Correlation Volatility Model in 
Example 10.7 Using Cholesky Decomposition 
Initial estimates are obtained by a sequential modeling procedure. 

all 0 2275:1 

open data d-cscointc.txt 

data(org=obs) / rl r2 r3 

set hi = 1.0 

set h2 = 4.0 

set h3 = 3.0 

set q21 = 0.8 

set q31 = 0.3 

set q32 = 0.3 

nonlin cl c2 c3 p3 p21 p22 p31 p33 aO al a2 tO tl t2 bO bl $ 

b2 uO ul u2 wO wl w2 dO dl d2 d5 

frml alt = rl(t)-cl-p3*rl(t-3) 
frml a2t = r2(t)-c2-p21*rl(t-2)-p22*r2(t-2) 
frml a3t = r3(t)-c3-p31*rl(t-1)-p33*r3(t-1) 

frml vl = a0+al*alt(t-1)**2+a2*hl(t-1) 

appendix: some remarks on estimation 

553 

frml qlt = tO + tl*q21(t-l) + t2*a2t(t-l) 

frml bt = a2t(t) - (q21(t)=qlt(t))*alt(t) 

frml v2 = bO+bl*bt(t—1)**2+b2*h2(t-1) 

frml q2t = uO + ul*q31(t-l) + u2*a3t(t-l) 

frml q3t = wO + wl*q32(t-1) + w2*a2t(t-l) 

frml bit = a3t (t) - (q31 (t) =q2t (t) ) *alt (t) - (q32 (t) =q3t (t) ) *bt (t) 

frml v3 = dO+dl*blt(t-1)**2+d2*h3(t-1)+d5*h2(t-1) 

frml gdet = -0.5*(log(hi(t) = vl(t))+ log(h2(t)=v2(t)) $ 

+log(h3(t)=v3(t))) 

frml garchln = gdet-0.5*(alt(t)**2/hl(t)+bt(t)**2/h2(t) $ 

+blt(t)**2/h3(t)) 

smpl 8 2275 

compute  cl =  0.07, c2  = 0  • 33, , c3 = = 0  h

>
-

O
k

  o

i
i

. 1, p3 

p21  =0.2, p22 

0.1. , p31  =  -0.26, p33 = = 0.06 

aO = 

.01, al =  0 .  05,  a2 =  0 .  94 

to =  0.28, tl  = 0 .  82, 

t2 =  -0  . 035 

bO = 

.17, bl =  0 .  08,  b2 =  0 .  89 

u0 = 

o

o

3

t

—

1

I
I

0 .  97,  u2 =  0.  01 

compute 

wO  =0.006, wl = 0 .  98,  w2 = 0  .004 

dO = 1.38, dl =  0 .  06,  d2 =  0 .  64, d5 = -0, . 027 

nlpar(criterion=value,cvcrit=0.00001) 

maximize(method=bhhh,recursive,iterations=250) garchln 

set fvl = vl(t) 

set resil = alt(t)/sqrt(fvl(t)) 

set residsq = resil(t)*resil(t) 

* Checking standardized residuals * 

cor(qstats,number=12,span=4) resil 

* Checking squared standardized residuals * 

cor(qstats,number=12,span=4) residsq 

set fv2 = v2(t)+qlt(t)**2*vl(t) 

set resi2 = a2t(t)/sqrt(fv2(t)) 

set residsq = resi2(t)*resi2(t) 

* Checking standardized residuals * 

cor(qstats,number=12,span=4) resi2 

* Checking squared standardized residuals * 

cor(qstats,number=12,span=4) residsq 

set fv3 = v3(t)+q2t(t)**2*vl(t)+q3t(t)**2*v2(t) 

set resi3 = a3t(t)/sqrt(fv3(t)) 

set residsq = resi3(t)*resi3(t) 

* Checking standardized residuals * 

cor(qstats,number=12,span=4) resi3 

* Checking squared standardized residuals * 

cor(qstats,number=12,span=4) residsq 

* print standardized residuals and correlation-coefficients 

set rho21 = qlt(t)*sqrt(vl(t)/fv2(t)) 

set rho31 

= q2t(t)*sqrt(vl(t)/fv3(t)) 

set rho32 

= (q2t(t)*qlt(t)*vl(t) $ 

+q31(t)*v2(t))/sqrt(fv2(t)* fv3(t)) 

print 10 

2275 resil resi2 resi3 

 
 
 
 
 
 
 
 
554 

MULTIVARIATE VOLATILITY MODELS AND THEIR APPLICATIONS 

print 10 2275 rho21 rho31 rho32 

print 10 2275 fvl fv2 fv3 

EXERCISES 

10.1. Consider the monthly simple returns, including dividends, of IBM stock, 
Hewlett-Packard (HPQ) stock, and the S&P composite index from January 
1962 to December 2008 for 564 observations. The returns are in the file 
m-ibmhpqsp62 08 . txt. Transform into log returns in percentages. Use the 
exponentially weighted moving-average method to obtain a multivariate 
volatility series for the three return series. What is the estimated A.? Plot the 
three volatility series. 

10.2. Focus on the monthly log returns of IBM and HPQ stocks from January 
1962 to December 2008. Fit a DVEC(1,1) model to the bivariate return 
series. Is the model adequate? Plot the fitted volatility series and the time- 
varying correlations. 

10.3. Focus on the monthly log returns of the S&P composite index and HPQ 
stock. Build a BEKK model for the bivariate series. What is the fitted 
model? Plot the fitted volatility series and the time-varying correlations. 

10.4. Build a constant-correlation volatility model for the three monthly log 
returns of IBM stock, HPQ stock, and S&P composite index. Write down 
the fitted model. Is the model adequate? Why? 

10.5. The file m-geibmsp2 608 . txt contains the monthly simple returns of Gen¬ 
eral Electric stock, IBM stock, and the S&P composite index from January 
1926 to December 2008. The returns include dividends. Transform into log 
returns in percentages. Focus on the monthly log returns in percentages of 
GE stock and the S&P 500 index. Build a constant-correlation GARCH 
model for the bivariate series. Check the adequacy of the fitted model, and 
obtain the 1-step-ahead forecast of the covariance matrix at the forecast 
origin December 2008. 

10.6. Again, consider the monthly log returns of GE, IBM, and S&P composite 
index from January 1926 to December 2008. Build a dynamic correla¬ 
tion model for the three-dimensional series. For simplicity, use the sample 
correlation matrix for p in Eq. (10.32). 

10.7. The file m-spibmge. txt contains the monthly log returns in percentages 
of the S&P composite index, IBM stock, and GE stock from January 1926 
to December 1999. Focus on GE stock and the S&P 500 index. Build a 
time-varying correlation GARCH model for the bivariate series using a 
logistic function for the correlation coefficient. Check the adequacy of the 
fitted model, and obtain the 1-step-ahead forecast of the covariance matrix 
at the forecast origin December 1999. 

10.8. Focus on the monthly log returns in percentages of GE stock and the S&P 
500 index from January 1926 to December 1999. Build a time-varying 

REFERENCES 

555 

correlation GARCH model for the bivariate series using the Cholesky 
decomposition. Check the adequacy of the fitted model, and obtain the 
1-step-ahead forecast of the covariance matrix at the forecast origin 
December 1999. Compare the model with the other model built in the 
previous exercise. 

10.9. Consider the three-dimensional return series of the previous exercise jointly. 
Build a multivariate time-varying volatility model for the data, using the 
Cholesky decomposition. Discuss the implications of the model and com¬ 
pute the 1-step-ahead volatility forecast at the forecast origin t = 888. 

10.10. An investor is interested in daily value at risk of his position on holding 
long $0.5 million of Dell stock and $1 million of Cisco Systems stock. 
Use 5% critical values and the daily log returns from February 20, 1990, 
to December 31, 1999, to do the calculation. The data are in the file d- 
dellcsco9099 . txt. Apply the three approaches to volatility modeling in 
Section 10.7 and compare the results. 

REFERENCES 

Bauwens, L., Laurent, S., and Rombouts, J. V. K. (2004). Multivariate GARCH models: A 

survey. Journal of Applied Econometrics 21:79-109. 

Bollerslev, T. (1990). Modeling the coherence in short-term nominal exchange rates: A mul¬ 
tivariate generalized ARCH approach. Review of Economics and Statistics 72:498-505. 

Bollerslev, T., Engle, R. F., and Wooldridge, J. M. (1988). A capital-asset pricing model 

with time-varying covariances. Journal of Political Economy 96:116-131. 

Chib, S., Nardari, F. and Shephard, N. (1999). Analysis of high dimensional multivariate 

stochastic volatility models. Working paper, Washington University, St. Louis. 

Engle, R. F. (2002). Dynamic conditional correlation: A simple class of multivariate GARCH 

models. Journal of Business and Economic Statistics 20:339—350. 

Engle, R. F. and Kroner, K. F. (1995). Multivariate simultaneous generalized ARCH. Econo¬ 

metric Theory 11:122-150. 

Harvey, A, Ruiz, E., and Shephard, N. (1994). Multivariate stochastic variance models. 

Review of Economic Studies 62:247—264. 

Li, W. K. (2004). Diagnostic Checks in Time Series. Chapman and Hall, London. 

Mardia, K. V., Kent, J. T., and Bibby, J. M. (1979). Multivariate Analysis. Academic, New 

York. 

Pourahmadi, M. (1999). Joint mean-covariance models with applications to longitudinal data: 

Unconstrained parameterization. Biometrika 86:677-690. 

Tsay, R. S. (2006). Multivariate volatility models in H. C. Ho, C. K. Ing, and T. L. Lai (eds.). 
Time Series and Related Topics in memory of C.Z. Wei, Lecture Notes Monograph 
Series, Institute of Mathematical Statistics. Beachwood, Ohio. 

Tse, Y. K. (2000). A test for constant correlations in a multivariate GARCH model. Journal 

of Econometrics 98:107-127. 

Tse, Y. K. and Tsui, A. K. C. (2002). A multivariate GARCH model with time-varying 

correlations. Journal of Business & Economic Statistics 20:351—362. 

. 

CHAPTER 11 

State-Space Models 
and Kalman Filter 

The state-space model provides a flexible approach to time series analysis, espe¬ 
cially for simplifying maximum-likelihood estimation and handling missing values. 
In this chapter, we discuss the relationship between the state-space model and the 
ARIMA model, the Kalman filter algorithm, various smoothing methods, and some 
applications. We begin with a simple model that shows the basic ideas of the state- 
space approach to time series analysis before introducing the general state-space 
model. For demonstrations, we use the model to analyze realized volatility series of 
asset returns, the time-varying coefficient market models, and the quarterly earnings 

per share of a company. 

There are many books on statistical analysis using the state-space model. Durbin 
and Koopman (2001) provide a recent treatment of the approach, Kim and Nelson 
(1999) focus on economic applications and regime switching, and Anderson and 
Moore (1979) give a nice summary of theory and applications of the approach for 
engineering and optimal control. Many time series textbooks include the Kalman 
filter and state-space model. For example, Chan (2002), Shumway and Stoffer 
(2000), Hamilton (1994), and Harvey (1993) all have chapters on the topic. West 
and Harrison (1997) provide a Bayesian treatment with emphasis on forecasting, 
and Kitagawa and Gersch (1996) use a smoothing prior approach. 

The derivation of Kalman filter and smoothing algorithms necessarily involves 
heavy notation. Therefore, Section 11.4 could be dry for readers who are interested 
mainly in the concept and applications of state-space models and can be skipped 

on the first read. 

Analysis of Financial Time Series, Third Edition, By Ruey S. Tsay 
Copyright © 2010 John Wiley & Sons, Inc. 

557 

558 

STATE-SPACE MODELS AND KALMAN FILTER 

11.1 LOCAL TREND MODEL 

Consider the univariate time series yt satisfying 

yt = l*t + et, et ~ N(0, cr2), 

dt+\ = l*t + *h, Vt ~ A(0, a2), 

(11.1) 

(11.2) 

where {et} and {/?,} are two independent Gaussian white noise series and t — 
1,,T. The initial value yti is either given or follows a known distribution, and 
it is independent of {et} and {iy} for t > 0. Here p,t is a pure random walk of 
Chapter 2 with initial value /xi, and yt is an observed version of ytt with added 
noise et. In the literature, p,t is referred to as the trend of the series, which is not 
directly observable, and yt is the observed data with observational noise et. The 
dynamic dependence of yt is governed by that of /x, because {et} is not serially 
correlated. 

The model in Eqs. (11.1) and (11.2) can readily be used to analyze realized 
volatility of an asset price; see Example 11.1. Here /x, represents the underlying 
log volatility of the asset price and yt is the logarithm of realized volatility. The 
true log volatility is not directly observed but evolves over time according to a 
random-walk model. On the other hand, yt is constructed from high-frequency 
transactions data and subjected to the influence of market microstructure noises. 
The standard deviation of e, denotes the scale used to measure the impact of market 
microstructure noises. 

The model in Eqs. (11.1) and (11.2) is a special linear Gaussian state-space 
model. The variable p.t is called the state of the system at time t and is 
not directly observed. Equation (11.1) provides the link between the data yt 
and the state p.t and is called the observation equation with measurement 
error et. Equation (11.2) governs the time evolution of the state variable 
and is the state equation (or state transition equation) with innovation qt. 
The model is also called a local-level model in Durbin and Koopman (2001, 
Chapter 2), which is a simple case of the structural time series model of 
Harvey (1993). 

Relationship to ARIMA Model 

If there is no measurement error in Eq. (11.1), that is, oe = 0, then yt = pt, which 
is an ARIMA(0,1,0) model. If oe > 0, that is, there exist measurement errors, then 
yt is an ARIMA(0,1,1) model satisfying 

(1 - B)y, = (1 -dB)a„ 

(1L3) 

where {at} is a Gaussian white noise with mean zero and variance cr2. The values 
of 6 and oa are determined by oe and an. This result can be derived as follows. 

From Eq. (11.2), we have 

(1 - B)pt+i = Vt or 

1 

dt+i = ~— 

LOCAL TREND MODEL 

559 

Using this result, Eq. (11.1) can be written as 

Multiplying by (1 — B), we have 

yt = 

1 - B 

Vt-i + et. 

(1 - B)yt - rit-i + et - et-\. 

Let (1 — B)yt = wt. We have wt = r]t-\ + et — et-\. Under the model assump¬ 
tions, it is easy to see that (a) wt is Gaussian, (b) Var(u;?) = 2cre2 + cr2, (c) 

Cov(wt, = —er2, and (d) Cov(wt, wt-j) = 0 for j > 1. Consequently, wt 
follows an MA(1) model and can be written as wt = (1 —9B)at. By equating 
the variance and lag-1 autocovariance of wt — (1 — 6B)at = rjt-\ + et — et-\, we 
have 

(11.4) 

(11.5) 

For given cr2 and cr2, one considers the ratio of the prior two equations to form a 
quadratic function of 9. This quadratic form has two solutions so one should select 
the one that satisfies \9\ < 1. The value of cr2 can then be easily obtained. Thus, 
the state-space model in Eqs. (11.1) and (11.2) is also an ARIMA(0,1,1) model, 
which is the simple exponential smoothing model of Chapter 2. 

On the other hand, for an ARIMA(0,1,1) model with positive 9, one can use 
the prior two identities to solve for a2 and a2, and obtain a local trend model. 
If 9 is negative, then the model can still be put in a state-space form without 
the observational error, that is, oe = 0. In fact, as will be seen later, an ARIMA 
model can be transformed into state-space models in many ways. Thus, the linear 
state-space model is closely related to the ARIMA model. 

In practice, what one observes is the yt series. Thus, based on the data alone, 
the decision of using ARIMA models or linear state-space models is not critical. 
Both model representations have pros and cons. The objective of data analy¬ 
sis, substantive issues, and experience all play a role in choosing a statistical 
model. 

Example 11.1. To illustrate the ideas of the state-space model and Kalman 
filter, we consider the intradaily realized volatility of Alcoa stock from January 2, 
2003, to May 7, 2004, for 340 observations. The daily realized volatility used is 
the sum of squares of intraday 10-minute log returns measured in percentage. No 
overnight returns or the first 10-minute intraday returns are used. See Chapter 3 for 
more information about realized volatility. The series used in the demonstration is 
the logarithm of the daily realized volatility. 

Figure 11.1 shows the time plot of the logarithms of the realized volatility of 
Alcoa stock from January 2, 2003, to May 7, 2004. The transactions data are 
obtained from the TAQ database of the NYSE. If ARIMA models are entertained, 

560 

STATE-SPACE MODELS AND KALMAN FILTER 

Figure 11.1 Time plot of logarithms of intradaily realized volatility of Alcoa stock from January 2, 

2003, to May 7, 2004. Realized volatility is computed from intraday 10-minute log returns measured 
in percentage. 

we obtain an ARIMA(0,1,1) model 

(1 - B)yt = (1 -0.858fl)ar, <rfl = 0.5184, (11.6) 

where yt is the log realized volatility, and the standard error of 6 is 0.028. The 
residuals show Q(12) = 12.4 with a p value of 0.33, indicating that there is 
no significant serial correlation in the residuals. Similarly, the squared residuals 
give Q(12) = 8.2 with a p value of 0.77, suggesting no ARCH effects in the 
series. 

Since § is positive, we can transform the ARIMA(0,1,1) model into a local 
trend model in Eqs. (11.1) and (11.2). The maximum-likelihood estimates (MLE) 
of the two parameters are = 0.0735 and oe = 0.4803. The measurement errors 
have a larger variance than the state innovations, confirming that intraday high- 
frequency returns are subject to measurement errors. Details of estimation will be 
discussed in Section 11.1.7. Here we treat the two estimates as given and use the 
model to demonstrate application of the Kalman filter. Note that using the model 
in Eq. (11.6) and the relation in Eqs. (11.4) and (11.5), we obtain oe = 0.480 and 
av = 0.0736. These values are close to the MLE shown above. 

LOCAL TREND MODEL 

11.1.1 Statistical Inference 

561 

Return to the state-space model in Eqs. (11.1) and (11.2). The aim of the analysis 
is to infer properties of the state fit from the data [yt\t = \, ... ,T] and the model. 
Three types of inference are commonly discussed in the literature. They are filter¬ 
ing, prediction, and smoothing. Let F, = {yi, ..., yt) be the information available 
at time t (inclusive) and assume that the model is known, including all parameters. 
The three types of inference can briefly be described as follows: 

• Filtering. Filtering means to recover the state variable pt given Ft, that is, 

to remove the measurement errors from the data. 

• Prediction. Prediction means to forecast p,+h or yt+h for h > 0 given Ft, 

where t is the forecast origin. 

• Smoothing. Smoothing is to estimate p.t given Ft, where T > t. 

A simple analogy of the three types of inference is reading a handwritten note. 
Filtering is figuring out the word you are reading based on knowledge accumulated 
from the beginning of the note, predicting is to guess the next word, and smoothing 
is deciphering a particular word once you have read through the note. 

To describe the inference more precisely, we introduce some notation. Let 
pt\j = E(pt\Fj) and T,t\j = Var(/xf|Fv) be, respectively, the conditional mean and 
variance of p,t given Fj. Similarly, yt\j denotes the conditional mean of yt given 
Fj. Furthermore, let vt — yt — yt\t-\ and Vt = Var(u,|Ft-\) be the 1-step-ahead 
forecast error and its variance of yt given Ft-\. Note that the forecast error vt is 
independent of Ft-\ so that the conditional variance is the same as the unconditional 
variance; that is, Var(uf |Ff_i) = Var(iy). From Eq. (11.1), 

yt\t-1 = E(yt\Ft-i) = E(ji, + et\Ft-\) = E(jit\Ft-\) = nt\t-l- 

Consequently, 

and 

vt = yt - yt\t-i =yt- Pt\t-1 

(11.7) 

Vt = Var(y, - pt\t-\|F,_i) = Var(pt + et - nt\t-\\Ft-i) 

= Var(fit - iit\t-i\Ft-i) + Var(ef|Ff_i) = + o]. (11.8) 

It is also easy to see that 

E{vt) = E[E(vt\Ft-i)] = E[E(y, - yqf_i|Ff_i)] = E[yt\t-\ - yq»-i] = 0, 

Cov(u?, yj) = E(vtyj) = E[E(vtyj\Ft-i)] = E[yjE(vt\Ft-\)] = 0, j < t. 

562 

STATE-SPACE MODELS AND KALMAN FILTER 

Thus, as expected, the 1-step-ahead forecast error is uncorrelated (hence, indepen¬ 
dent) with yj for j < t. Furthermore, for the linear model in Eqs. (11.1) and (11.2), 
F>t\t = E(jj,t\Ft) = E(nt\Ft-i, v,) and E,|, = Var(/x,|F,) = Var(/xf |F,_i, vt). In 
other words, the information set Ft can be written as Ft = {F,_i, yt} = {Ff_i, vt}. 
The following properties of multivariate normal distribution are useful in study¬ 
ing the Kalman filter under normality. They can be shown via the multivariate linear 
regression method or factorization of the joint density. See, also, Appendix B of 
Chapter 8. For random vectors w and m, denote the mean vectors and covariance 
matrix as E{w) = fiw, E{m) = fim, and Cov(m, w) = Emu), respectively. 

Theorem 11.1. Suppose that x, y, and z are three random vectors such that 
their joint distribution is multivariate normal. In addition, assume that the diag¬ 
onal block covariance matrix Tww is nonsingular for w = x, y, z, and Eyz = 0. 
Then, 

1. F(x|y) = (ix + 'ZXy'Z~j(y - fiy). 

2. Var(x|y) = Yxx- E^Ej^E^. 

3. E(x\y, z) = E(x\y) + ExzE“!(z - fiz). 

4. Var(x|y, z) = Var(x|y) - E„E^E„. 

11.1.2 Kalman Filter 

The goal of the Kalman filter is to update knowledge of the state variable recur¬ 
sively when a new data point becomes available. That is, knowing the conditional 
distribution of /it given F,_i and the new data yt, we would like to obtain the con¬ 
ditional distribution of /xt given Fr, where, as before, Fj = {yi,..., yj}. Since Ft 
= [Ft-\, vt), giving y, and Fr_j is equivalent to giving v, and F,_j. Consequently, 
to derive the Kalman filter, it suffices to consider the joint conditional distribution 
of (fxt, vtY given Ff_i before applying Theorem 11.1. 

The conditional distribution of v, given F,_! is normal with mean zero and 
variance given in Eq. (11.8), and that of /r, given Ff_i is also normal with mean 
F-t\t-\ and variance E,|,_i. Furthermore, the joint distribution of (/zf, vt)' given 
Ff—i is also normal. Thus, what remains to be solved is the conditional covariance 
between iit and vt given Ft-\. From the definition, 

Cov(nt, vt\Ft-i) = E(iitvt\Ft_i) = E[/xt(yt - /zf|f_!)|Ff_i] [by Eq. (11.7)] 

= +et - fit\t-i)\Ft-i] 

= — [Mt\t-\)\Ft-i] + E(iitet\Ft-i) 

= - /zf|f_i)2|F,_i] = Var(/z?|Ff_i) = (11.9) 

where we have used the fact that F^^O-i, — /xfU_,)|Fv_i] = 0. Putting the 
results together, we have 

LOCAL TREND MODEL 

563 

Ft 

vt 

~ N ( 
Ft-, V 

Ft\t-\ 
0 

> 

£f|f-l Vt 

By Theorem 11.1, the conditional distribution of \xt given Ft is normal with mean 
and variance 

Ft\t = Ft\t-i H ~ = Ft\t-i + Ktvt, 
Vt 

i V[ 

(11.10) 

S2 

Sf|r = = Sf|f_!(l - Kt), 

(li.ii) 

Vt 

where Kt = Ef|f_i/ Vt is commonly referred to as the Kalman gain, which is the 
regression coefficient of fit on vt. From Eq. (11.10), Kalman gain is the factor that 
governs the contribution of the new shock vt to the state variable /xt. 

Next, one can make use of the knowledge of /xt given Ft to predict /xf+i via 

Eq. (11.2). Specifically, we have 

Ft+i\t = E(nt + rit\Ft) = E(fit\F,) = (11.12) 

S/+i|r = Var(/xf+1 \Ft) = Var(/x,|F;) + Var(?]f) = T,t\, + a2. (11.13) 

Once the new data yt+\ is observed, one can repeat the above procedure to update 
knowledge of /at+i. This is the famous Kalman filter algorithm proposed by Kalman 
(1960). 

In summary, putting Eqs. (11.7) and (11.13) together and conditioning on the 
initial assumption that /i\ is distributed as Af(£ti|o> Eqo), the Kalman filter for the 
local trend model is as follows: 

vt = yt~ A6|r-l, 

Vt = + er2, 

Kt = J:t\t-i/Vt, (11.14) 

Ft+i\t = l^t\t-l + Ktvt, 

Sf+l|r = Sf|f_i(l — Kt) + cr2, t = 1, ..., T. 

There are many ways to derive the Kalman filter. We use Theorem 11.1, which 
describes some properties of multivariate normal distribution, for its simplicity. In 
practice, the choice of initial values Eqo and fi\\Q requires some attention and we 
shall discuss it later in Section 11.1.6. For the local trend model in Eqs. (11.1) and 
(11.2), the two parameters ae and at] can be estimated via the maximum-likelihood 
method. Again, the Kalman filter is useful in evaluating the likelihood function of 
the data in estimation. We shall discuss estimation in Section 11.1.7. 

564 

STATE-SPACE MODELS AND KALMAN FILTER 

Day 

(a) 

Day 

(b) 

Figure 11.2 Time plots of output of Kalman filter applied to daily realized log volatility of Alcoa 

stock based on local trend state-space model: (a) filtered state [l,|, and (b) 1-step-ahead forecast error vt. 

Example 11.1 (Continued). To illustrate application of the Kalman filter, we 
use the fitted state-space model for daily realized volatility of Alcoa stock returns 
and apply the Kalman filter algorithm to the data with Ep0 = oo and /G|0 = 0. The 
choice of these initial values will be discussed in Section 11.1.6. Figure 11.2(a) 
shows the time plot of the filtered state variable ixt\t, and Figure 11.2(b) is the time 
plot of the 1-step-ahead forecast error vt. Compared with Figure 11.1, the filtered 
states are smoother. The forecast errors appear to be stable and center around zero. 
These forecast errors are out-of-sample 1-step-ahead prediction errors. 

11.1.3 Properties of Forecast Error 

The 1-step-ahead forecast errors {ut} are useful in many applications, hence it pays 
to study carefully their properties. Given the initial values Ep0 and /G|o, which 
are independent of yt, the Kalman filter enables us to compute v, recursively as a 
linear function of {yi,..., yf}. Specifically, by repeated substitutions, 

ui = yi ~ Hi|o. 

v2 — yi - M211 =yi~ agio - -/GOd - agio), 

v3 — y3 - AG|2 = y3- AGIO - K2(j2 - AG|o) — ^i(l - K2)(yi — AG|o), 

LOCAL TREND MODEL 

565 

and so on. This transformation can be written in matrix form as 

v = K(y-m |0lr), (11.15) 

where v = (iq,..vj)', y = (yi,..., yrY, 1 t is the T-dimensional vector of 
ones, and K is a lower triangular matrix defined as 

' 1 

k-2\ 

£31 

0 
1 

&32 

0 •  •• 0 
0 • 
0 
1 
0 

K - 

kT\  kr 2 

kj3 ■ •• 1 

where ktj-i = -Kt-1 and ktj = -(1 - ^_i)(l - Kt_2) •••(!- Kj+i)Kj for i — 
2,,T and j = 1,— 2. It should be noted that, from the definition, the 
Kalman gain Kt does not depend on p\\Q or the data {yi,..., yt}\ it depends on 
E]|0 and cr2 and cr2. 

The transformation in Eq. (11.5) has several important implications. First, {u,} 
are mutually independent under the normality assumption. To show this, consider 
the joint probability density function of the data 

T 

p(yi, • • •, yr) = p(yi) ]~[ p(yj\Fj-1). 

7=2 

Equation (11.15) indicates that the transformation from yt to vt has a unit Jacobian 
so that p(v) = p(y). Furthermore, since /xi|o is given, p{v\) = p(y\). Conse¬ 
quently, the joint probability density function of v is 

T T T 

p(v) = p(y) = p(yi)Y\p(yj\Fj-i) = P(v\)Y\p(vj) = n^)- 

7=2 7 7=1 

This shows that {vt} are mutually independent. 

Second, the Kalman filter provides a Cholesky decomposition of the covariance 
matrix of y. To see this, let S2 = Cov(y). Equation (11.15) shows that Cov(t;) = 
KKK'. On the other hand, {vt} are mutually independent with Var(uf) = Vt. There¬ 
fore, KSIK' = diag{Ti,..., Vt), which is precisely a Cholesky decomposition of 
£2. The elements kij of the matrix K thus have some nice interpretations; see 

Chapter 10. 

State Error Recursion 
Turn to the estimation error of the state variable /it. Define 

A; — p^t Pt\t—\ 

566 

STATE-SPACE MODELS AND KALMAN FILTER 

as the forecast error of the state variable /xt given data Ft-\. From Section 11.1.1, 
Var(x?|Fr_i) = T,t\t~i. From the Kalman filter in Eq. (11.14), 

Vt — yt ~ = Ft + et ~ — xt + et. 

and 

Xl+l = /Xt+1 - = (I, + T], - (jJLt\t-l + KtVt) 

= xt + rit - Ktv, = xt + rjt - Kt(xt + et) = Ltxt + iy - Ktet, 

where Lt = 1 - Kt — 1 - = (Vt - 'Zt\t-i)/Vt = aj/Vt. Consequently, 
for the state errors, we have 

vt—xt+et, xt+\ = Ltx, + rjt - Ktet, t — (11.16) 

where x\ =■ ii\ — /ri|o. Equation (11.16) is in the form of a time-varying state-space 
model with xt being the state variable and vt the observation. 

11.1.4 State Smoothing 

Next we consider the estimation of the state variables {/x\,..., fir) given the data 
Ft and the model. That is, given the state-space model in Eqs. (11.1) and (11.2), 
we wish to obtain the conditional distribution iit\Fj for all t. To this end, we first 
recall some facts available about the model: 

• All distributions involved are normal so that we can write the conditional 
distribution of /x, given FT as N((xt\T, Eqr), where t <T. We refer to fxt\T 
as the smoothed state at time t and as the smoothed state variance. 

• Based on the properties of {vt} shown in Section 11.1.3, {iq,..., vt] are 

mutually independent and are linear functions of {yi, ..., yT). 

• If yi,..., yr are fixed, then Ft_\ and {ur,..., vj) are fixed, and vice versa. 

• {vt,..., vt) are independent of Ft-\ with mean zero and variance Var(u;) = 

Vj for j > t. 

Applying Theorem 11.1(3) to the conditional joint distribution of (/it, 

vt,..., vt) given i, we have 

FtiT — E(fxt\FT) = E(fj,t\Ft-i, vt,vT) 

= E(nt\Ft-i) + Co\[fit, (vt,vT)']Cov[(vt,Vt)']-1^, ■■■, vT)' 

Cov(nt, vt) 
Cov(jit, vt+i) 

/ 

o

o

+

•

•

_ CovOiLt, vT) _ 

•

•

o

_
1

•

•

o

o

  -1 
1
_

" vt ' 
Vt+1 

o

•

•
•

£

1

vt 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
LOCAL TREND MODEL 

7 

567 

= jt%_i + ^Cov(^,, Vj)V~rlVj. (11.17) 

j=t 

From the definition and independence of {i;,}, Cov(/z,, uy) = Cov(xt, Vj) for j = 
t,..., T, and 

CovO,, v,) = E[xt(x, + et)] = Var(jq) = 

Cov(a;, uf+i) = E[x,(xt+i + et+i)] = E[xt(Ltxt + rjt - Ktet)] = E,|f_iLt. 

Similarly, we have 

CovO;, vt+2) = £[Af(Af+2 + ef+2)] = ■ ■ • = Sr|f-iLrLf+1, 

Cov(xf, vT) = E[xt(xT +eT)] = • ■ ■ = £,|f_i J~[ 

;=f 

r-i 

Consequently, Eq. (11.17) becomes 

f^t\T — f^t\t-1 + —-r ^t\t-l^t^t+\ 77 r 

1 V' I X' r I )r )T ^f + 2 ! 

Vf Vfi+1 \fi+2 

where 

ff+i 

Uf+2 

r-i 

1)7’ 

VV 

is a weighted linear combination of the innovations {vt,..., Vj}- This weighted 
sum satisfies 

7

  +

I
I

  +
"
T
J

a
*

 _

^

1

+

l J Vt+2 | 
+ i,+' V,+2 + ' 

/ 7-1 \ 

vt 

vv 

•+uH 

— 77 + Etq,. 

vt 

Vt 

Therefore, using the initial value qj — 0, we have the backward recursion 

qt-1 = — + Etqt, t — T, T — 1,..., 1. 

(11.19) 

Vt 

Putting Eqs. (11.17) and (11.19) together, we have a backward recursive algorithm 
to compute the smoothed state variables: 

 
 
 
 
 
 
568 

STATE-SPACE MODELS AND KALMAN FILTER 

<?r-i = Vt lvt + Ltqt, nt\T = + 'Lt\t-\qt-\, t = T,..., 1, 

(11.20) 

where qr = 0, and £qf-i and L, are available from the Kalman filter in 
Eq. (11.14). 

Smoothed State Variance 
The variance of the smoothed state variable fit\r can be derived in a similar manner 
via Theorem 11.1(4). Specifically, letting vf = (vt,, VjY, we have 

E;|r = Var(/x,|Fr) = Var(/^|Ff_i, vt,..., vT) 

= Var(/i,|F,_i) - Cov[jut, (vfy]Cov[(vJ’)]~1Cov[/u,t, (vj)] 

T 

= St|,_i - JjCov(/l;, Vj)fV7l, (11.21) 

j=t 

where Cov(/u.t, vj) = Cov(xt, Vj) are given earlier after Eq. (11.17). Thus, 

^t\T — S/|r-i £/jf_i 

= Sf|f_i — 1, 

1 

~Vj 

(11.22) 

where 

Mt.i 

1 

1 

+ Lf 
V, ‘ Vt+1 

+ l2l2 
n- L>t+1 

+ 

Vt+2 

is a weighted linear combination of the inverses of variances of the 1-step-ahead 
forecast errors after time t - 1. Let MT - 0 because no 1-step-ahead forecast error 
is available after time index T. The statistic A/,_i can be written as 

Mt~x ~V,+L^ 

1 T 2 1 
Vh-!+ f+1V,+2 

T-l 

+•••+ n l) 

<j=t+1 

vv 

= - + L2Mt, t = T,T- 1-- 1. 

1 

vt 

Note that from the independence of {u,} and Eq. (11.18), we have 

LOCAL TREND MODEL 

569 

Figure 11.3 Filtered state variable and its 95% pointwise confidence interval for daily log realized 

volatility of Alcoa stock returns based on fitted local-trend state-space model. 

Combining the results, variances of the smoothed state variables can be computed 
efficiently via the backward recursion 

i — Vt 1 + L2Mt, Xf|r — 

t = T,.  .,1, 

(11.23) 

where Mr — 0. 

Example 11.1 (Continued). Applying the Kalman filter and state-smoothing 
algorithms in Eqs. (11.20) and (11.23) to the daily realized volatility of Alcoa stock 
using the fitted state-space model, we can easily compute the filtered state fxt\t and 
the smoothed state /xt\r and their variances. Figure 11.3 shows the filtered state 
variable and its 95% pointwise confidence interval, whereas Figure 11.4 provides 
the time plot of smoothed state variable and its 95% pointwise confidence interval. 
As expected, the smoothed state variables are smoother than the filtered state vari¬ 
ables. The confidence intervals for the smoothed state variables are also narrower 
than those of the filtered state variables. Note that the width of the 95% confidence 

interval of ii\\\ depends on the initial value Spo. 

570 

STATE-SPACE MODELS AND KALMAN FILTER 

Figure 11.4 Smoothed state variable and its 95% pointwise confidence interval for daily log 

realized volatility of Alcoa stock returns based on fitted local-trend state-space model. 

11.1.5 Missing Values 

An advantage of the state-space model is in handling missing values. Suppose 
that the observations {yt}^+l are missing, where h > 1 and 1 < l < T. There 
are several ways to handle missing values in state-space formulation. Here we 
discuss a method that keeps the original time scale and model form. For t e {l + 
1,..., l + h}, we can use Eq. (11.2) to express n, as a linear combination of iii+\ 
and {r]j}'j^l+l. Specifically, 

lit = iit-1 + fir-1 = • • • = lie+1 + ^ r]j, 

r-l 

j=i+1 

where it is understood that the summation term is zero if its lower limit is greater 
than its upper limit. Therefore, for t e {l + 1, ..., t + h}, 

E(ii,\Ft-i) = E(/it\Fe) = iie+m, 

Var(^|Ft_i) = Vax(ii,\Fe) = £*+n* + (t - i - 1 )a*. 

Consequently, we have 

Mr|/—1 = AC-l|f-2! 11;_2 + cr“, (11.24) 

LOCAL TREND MODEL 

571 

for t = i + 2, ..., t + h. These results show that we can continue to apply the 
Kalman filter algorithm in Eq. (11.14) by taking vt = 0 and Kt = 0 for t = i + 
1 + h. This is rather natural because when yt is missing, there is no new 
innovation or new Kalman gain so that vt = 0 and Kt = 0. 

11.1.6 Effect of Initialization 

In this section, we consider the effects of initial condition /xj ~ N(/iqo, Sqo) on 
the Kalman filter and state smoothing. From the Kalman filter in Eq. (11.14), 

vi — yi - M-iio, Vi = Eqo + cr2, 

and, by Eqs. (11.10)—(11.13), 

M2|l 

Eqo 

O|0 

Ml|0 + _77_l’l = Ml|0 + 

Vi 

Eqo + cre 

:(yi - Ml|o), 

^2|l = Sl|0 

1 - 

-llO 

Eqo + °e 

i JL _ ^110 _2 , „2 
+ an - V-TT21°e + av ■ 
Eqo + CTZ 

Therefore, letting Eqo increase to infinity, we have fj,2\\ = yi and S211 = cr2 + cr2. 
This is equivalent to treating yi as fixed and assuming ~ N(y\, cr2). In the lit¬ 
erature, this approach to initializing the Kalman filter is called diffuse initialization 
because a very large Eqo means one is uncertain about the initial condition. 

Next, turn to the effect of diffuse initialization on state smoothing. It is obvious 
that based on the results of Kalman filtering, state smoothing is not affected by the 
diffuse initialization for t = T,..., 2. Thus, we focus on /zj given Fj. From Eq. 
(11.20) and the definition of L\ — 1 — K\ = Vf1^, 

f^ljT = Ml|0 + Si ,0^0 

Mqo + Si|o 

■vi + 1- 

. Eqo + cre 

1 

nio 

Eqo + cr, 

qx 

Mi|0 + 

D|0 

Eqo + cr, 

(«i + Og<?i). 

Letting Eq0 00, we have flip = fJtqo + tq + crjqx = y\ + vr^qx- Furthermore, 
from Eq. (11.23) and using V\ = Eqo + cr2, we have 

Eqr — Eqo — E 1|0 

Eqo + cr( 

+ 1 

1 

O|0 

Ei|o + cr( 

Mi 

O|0 

1 - 

S110 \ 

Spo + cr, 

1 - 

Eqo 

2 
Eqo + cr, 
e 

E20Mi 

i|o_\ 2 
2 ) 

. Eqo + cr, 

"110 

Eqo + o', 

I o\f M\. 

572 

STATE-SPACE MODELS AND KALMAN FILTER 

Thus, letting Spo —> oo, we obtain Xqr = cr2 — a^M\. 

Based on the prior discussion, we suggest using diffuse initialization when little 
is known about the initial value ii\. However, it might be hard to justify the use 
of a random variable with infinite variance in real applications. If necessary, one 
can treat ii\ as an additional parameter of the state-space model and estimate it 
jointly with other parameters. This latter approach is closely related to the exact 
maximum-likelihood estimation of Chapters 2 and 8. 

11.1.7 Estimation 

In this section, we consider the estimation of oe and an of the local trend model 
in Eqs. (11.1) and (11.2). Based on properties of forecast errors discussed in 
Section 11.1.3, the Kalman filter provides an efficient way to evaluate the like¬ 
lihood function of the data for estimation. Specifically, the likelihood function 
under normality is 

p(yi, • • •, yT\oe, cfn) = p(yx\oe, o^YYyAFt-x, oe, 0^) 

T 

t=2 

T 

= p(yi\cre, atl)Y[(vt\Ft-i, ae, a^), 

t=2 

where y\ ~ Ui) and vt — (yt — p,t\t-i) ~ N(0, Vt). Consequently, assum¬ 
ing /xi|0 and Epo are known, and taking the logarithms, we have 

ln[L(og, (Tr,)] = ~ ln(27t) - )- ^ 

ln(V)) + 

(11.25) 

T 

,2i 

t=l *- 

which involves v, and Vt. Therefore, the log-likelihood function, including cases 
with missing values, can be evaluated recursively via the Kalman filter. Many soft¬ 
ware packages perform state-space model estimation via a Kalman filter algorithm 
such as Matlab, RATS, and S-Plus. In this chapter, we use the SsfPack program 
developed by Koopman, Shephard, and Doornik (1999) and available in S-Plus and 
OX. Both Ssfpack and OX are free and can be downloaded from their websites. 

11.1.8 S-Plus Commands Used 

We provide here the SsfPack commands used to perform analysis of the daily 
realized volatility of Aloca stock returns. Only brief explanations are given. For 
further details of the commands used, see Durbin and Koopman (2001, Section 
6.6). S-Plus uses specific notation to specify a state-space model; see Table 11.1. 
The notation must be followed closely. In Table 11.2, we give some commands 
and their functions. 

LOCAL TREND MODEL 

573 

TABLE 11.1 State-Space Form and Notation in S-Plus 

State-Space Parameter 

S-Plus Name 

8 

ft 
E 

mDelta 
mPhi 
mOmega 
mSigma 

TABLE 11.2 Some Commands of SsfPack Package 

Command 

Function 

SsfFit 
CheckSsf 
KalmanFil 
KalmanSmo 
SsfMomentEst with task “STFIL” 
SsfMomentEst with task “STSMO” 
SsfCondDens with task “STSMO” 

Maximum-likelihood estimation 
Create “Ssf’ object in S-Plus 
Perform Kalman filtering 
Perform state smoothing 
Compute filtered state and variance 
Compute smoothed state and variance 
Compute smoothed state without variance 

In our analysis, we first perform maximum-likelihood estimation of the state- 
space model in Eqs. (11.1) and (11.2) to obtain estimates of oe and oT]. The initial 
values used are Epo = —1 and /xpo = 0, where —1 signifies diffuse initialization, 
that is, Ei|o is very large. We then treat the fitted model as given to perform Kalman 
filtering and state smoothing. 

SsfPack and S-Plus Commands for State-Space Model 

> da = read.table(file='aa-rv-0304.txtheader=F) % load data 

> y = log(da[,l]) % log(RV) 

> ltm.start=c(3,1) % Initial parameter values 

> Pi = -1 % Initialization of Kalman filter 

> al = 0 

> ltm.m=function(parm){ % Specify a function for the 

+ sigma.eta=parm[l] % local trend model. 

+ sigma.e=parm[2] 

+ ssf.m=list(mPhi=as.matrix(c(1,1)), 

+ mOmega=diag (c (sigma . eta-'2 , sigma. eA2) ) , 

+ mSigma=as.matrix(c(Pi, al))) 

+ CheckSsf(ssf.m) 

+ 1 
% perform estimation 

> ltm.mle=SsfFit(ltm.start,y,"ltm.m",lower=c(0,0) , 

+ upper=c (100,100)) 

> ltm.mle$parameters 

574 

STATE-SPACE MODELS AND KALMAN FILTER 

[1] 0.07350827 0.48026284 

> sigma.eta=ltm.mle$parameters[1] 

> sigma.eta 
[1] 0.07350827 

> sigma.e=ltm.mle$parameters[2] 

> sigma.e 
[1] 0.4802628 

% Specify a state-space model in S-Plus. 

> ssf.ltm.list=list(mPhi=as.matrix(c(1,1)), 

+ mOmega=diag(c(sigma.etaA2,sigma.eA2)), 

+ mSigma=as.matrix(c(PI,al))) 

% check validity of the specified model. 

> ssf.ltm=CheckSsf(ssf.ltm.list) 

> ssf.ltm 

$mPhi: 

[,1] 

[1, ] 1 

[2, ] 1 

$mOmega: 

[, 1] [,2] 

t1,] 0.0054035 0.0000000 
[2, ] 0.0000000 0.2306524 

$mSigma: 

[,1] 
[1,] -1 
[2, ] 0 
$mDelta: 

[,1] 

[1, ] 0 
[2, ] 0 
$mJPhi: 

[1] 0 

$mJOmega: 

[1] 0 

$mJDelta: 

[1] 0 
$mX: 

[1] 0 

$cT: 

[1] 0 
$cX: 

[1] 0 
$cY: 

[1] 1 
$cSt: 

(1] 1 

attr(, "class"): 

[1] "ssf" 

% Apply Kalman filter 

LOCAL TREND MODEL 

575 

> KalmanFil.ltm=KalmanFil(y,ssf.ltm,task="STFIL") 

> names(KalmanFil.ltm) 

[1] 

[6] 

"mOut" 

"innov" 

"std.innov" 

"mGain" 

"loglike" 

"loglike.cone" 

"dVar" 

"mEst" 

"mOffP" 

"task" 

[11] 

" err" 

"call" 

> par(mfcol=c(2,1)) % Obtain plot 

> plot(KalmanFil.ltm$ mEst[,1],xlab='day', 

+ ylab='filtered state',type='1') 

> title(main='(a) Filtered state variable') 

> plot(KalmanFil.ltm$ mOut[,1],xlab='day', 

+ ylab='v(t)',type='1') 

> .title(main='(b) Prediction error') 

% Obtain residuals and their variances 

> KalmanSmo.ltm=KalmanSmo(KalmanFil.ltm,ssf.ltm) 

> names(KalmanSmo.ltm) 

[1] "state.residuals" "response.residuals" "state.variance 

[4] "response.variance" "aux.residuals" "scores" 

[7] "call" 

% Filtered states 

> FiledEst.1tm=SsfMomentEst(y,ssf.ltm,task="STFIL") 

> names(FiledEst.ltm) 

[1] "state.moment" "state.variance" "response.moment" 

[4] "response.variance" "task" 

% Smoothed states 

> SmoedEst.ltm=SsfMomentEst(y,ssf.ltm,task="STSMO") 

> names(SmoedEst.ltm) 

[1] "state.moment" "state.variance" "response.moment" 

[4] "response.variance" "task" 

% Obtain plots of filtered and smoothed states with 95% C.I. 

> up=FiledEst.ltm$ state,moment+ 

+ 2*sqrt(FiledEst.ltm$ state.variance) 

> lw=FiledEst.ltm$ state.moment- 

+ 2*sqrt(FiledEst.ltm$ state.variance) 

> par(mfcol=c (1,1)) 

> plot(FiledEst.ltm$ state.moment,type='1',xlab='day', 

+ ylab='value',ylim=c(-0.1,2.5)) 

> lines(1:340,up,lty=2) 

> lines(1:340,lw,lty=2) 

> title(main='Filed state variable') 

> up=SmoedEst.ltm$ state.moment+ 

+ 2*sqrt(SmoedEst.ltm$ state.variance) 

> lw=SmoedEst.ltm$ state.moment- 

+ 2*sqrt(SmoedEst.ltm$ state.variance) 

> plot(SmoedEst.ltm$ state.moment,type='1',xlab='day', 

+ ylab='value',ylim=c(-0.1,2.5)) 

> lines(1:340,up,lty=2) 

> lines(1:340,lw,lty=2) 

> title(main='Smoothed state variable') 

% Model checking via standardized residuals 

576 

STATE-SPACE MODELS AND KALMAN FILTER 

> resi=KalmanFil.ltm$ mOut[,1]*sqrt(KalmanFil.ltm$ mOut[,3]) 

> archTest(resi) 

> autocorTest(resi) 

For the daily realized volatility of Alcoa stock returns, the fitted local trend 
model is adequate based on residual analysis. Specifically, given the parameter 
estimates, we use the Kalman filter to obtain the 1-step-ahead forecast error vt and 
its variance Vt. We then compute the standardized forecast error vt = vt/«JVt and 
check the serial correlations and ARCH effects of {£,}. We found that <2(25) = 
23.37(0.56) for the standardized forecast errors, and the LM test statistic for ARCH 
effect is 18.48(0.82) for 25 lags, where the number in parentheses denotes p 
value. 

11.2 LINEAR STATE-SPACE MODELS 

We now consider the general state-space model. Many dynamic time series models 
in economics and finance can be represented in state-space form. Examples include 
the ARIMA models, dynamic linear models with unobserved components, time- 
varying regression models, and stochastic volatility models. A general Gaussian 
linear state-space model assumes the form 

s?+i — dt + Ttst + Rtrjt 

yt — ct + Ztst + et, 

(11.26) 

(11.27) 

where st = (sit,..., smt)' is an m-dimensional state vector, yt = (yu,..., ykt)' is a 
^-dimensional observation vector, dt and c, are m- and £-dimensional deterministic 
vectors, Tt and Zt are m x m and k x m coefficient matrices, R, is an m x n matrix 
often consisting of a subset of columns of the m x m identity matrix, and {j/,} and 
{et} are n- and k-dimensional Gaussian white noise series such that 

Vt ~ N(0, Qt), et ~ N(0, Ht) 

where Qt and Ht are positive-definite matrices. We assume that {e,} and {vt} are 
independent, but this condition can be relaxed if necessary. The initial state $i is 
WOhio, 51i|o), where and Zpo are given, and is independent of et and vt for 
t > 0. 

Equation (11.27) is the measurement or obsen’ation equation that relates the 
vector of observations yt to the state vector st, the explanatory variable ct, and 
the measurement error et. Equation (11.26) is the state or transition equation that 
describes a first-order Markov Chain to govern the state transition with innovation 
Vr The matrices Tt, R,, Qt, Zt, and H, are known and referred to as system 
matrices. These matrices are often sparse, and they can be functions of some 
parameters 9, which can be estimated by the maximum-likelihood method. 

The state-space model in Eqs. (11.26) and (11.27) can be rewritten in a compact 

form as 

MODEL TRANSFORMATION 

where 

sr+l 

yt 

— + *&tst + 

577 

(11.28) 

S,= 

dt 

c, 

<*>, = 

Tt 
Zt 

ut 

RtVt 
et 

and {«f} is a sequence of Gaussian white nosies with mean zero and covariance 
matrix 

S2, = Cov(ut) = 

RtQ,R't 0 
0 H, 

The case of diffuse initialization is achieved by using 

21|0 — E* + ^oo, 

where X* and I^ are m x m symmetric positive-definite matrices and X is a large 
real number, which can approach infinity. In S-Plus and SsfPack, the notation 

Y = 

£l|0 

A\o  (m+l)xm 

is used; see the notation in Table 11.1. 

In many applications, the system matrices are time invariant. However, these 

matrices can be time varying, making the state-space model flexible. 

11.3 MODEL TRANSFORMATION 

To appreciate the flexibility of the state-space model, we rewrite some well-known 
econometric and financial models in state-space form. 

11.3.1 CAPM with Time-Varying Coefficients 

First, consider the capital asset pricing model (CAPM) with time-varying intercept 
and slope. The model is 

rt = at + P,rM,t +et, et ~ N(0, <rf2), (11.29) 

at+i = at + rjt, r)t ~ N(0, cr%), 

fit+l = A + 9, G ~ N(0, crf2), 

where r, is the excess return of an asset, rM t is the excess return of the market, 
and the innovations {et, rjt,et} are mutually independent. This CAPM allows for 

578 

STATE-SPACE MODELS AND KALMAN FILTER 

time-varying a and f that evolve as a random walk over time. We can easily 
rewrite the model as 

1
_
1

1

+

+

1

=  '1 o' 

0 1_ 

n = [1, rMj] 

r), 
Jt_ 

Ol, 

A  + 
i
_
i

i

■

+ et. 

Thus, the time-varying CAPM is a special case of the state-space model with st = 
(at, ft,)', Tt = R, = I2, the 2 x 2 identity matrix, d, = 0, c, — 0, Zt = (1, rMj), 
Ht =CTg, and Qt = diag{cr2, cr2}. Furthermore, in the form of Eq. (11.28), we 
have S, = 0, u, = (rjt, et, et)'. 

" 1 
0 
1 

0 
1 

T/w,r 

, — 

0 

0 

0 

0 

<*t _ 

0 
0 

If diffuse initialization is used, then 

E = 

-1 0 
0 -1 
0 0 

SsfPack/S-Plus Specification of Time-Varying Models 
For the CAPM in Eq. (11.29), <I>, contains r^,t, which is time varying. Some 
special input is required to specify such a model in SsfPack. Basically, it requires 
two additional variables: (a) a data matrix X that stores Zt and (b) an index matrix 
for <J>, that identifies Zt from the data matrix. The notation for index matrices of 
the state-space model in Eq. (11.28) is given in Table 11.3. Note that the matrix 
Jct must have the same dimension as 4>r. The elements of are all set to -1 
except the elements for which the corresponding elements of are time varying. 
The nonnegative index value of J $ indicates the column of the data matrix X, 
which contains the time-varying values. 

To illustrate, consider the monthly simple excess returns of General Motors 
stock from January 1990 to December 2003 used in Chapter 9. The monthly simple 

TABLE 11.3 Notation and Name Used in SsfPack/S-Plus for Time-Varying 
State-Space Model 

Index Matrix 

Name Used in SsfPack/S-Plus 

Js 
J 
Jn 

mJDelta 
mJPhi 
mJOmega 

Time-Varying Data Matrix 
X 

Name Used in SsfPack/S-Plus 
mX 

 
 
 
 
 
 
 
 
MODEL TRANSFORMATION 

579 

excess return of the S&P 500 composite index is used as the market return. The 
specification of a time-varying CAPM requires values of the variances er2, er2, and 

a1-. Suppose that (ov, cre, ae) = (0.02, 0.04, 0.1). The state-space specification for 
the CAPM under SsfPack/S-Plus is given below: 

> X,mtx=cbind(1,sp) % Here ''sp'' is market excess returns. 

> Phi.t = rbind(diag(2),rep(0,2)) 

> Sigma=-Phi.t 

> sigma.eta=.02 

> sigma.ep=.04 

> sigma.e=.l 

> Omega=diag(c(sigma.etaA2,sigma.epA2,sigma.eA2) ) 

> JPhi = matrix(-1,3,2) % Create a 3-by-2 matrix of -1. 

> JPhi[3,11=1 

> JPhi[3,2]=2 

> ssf.tv.capm=list(mPhi=Phi.t, 

+ mOmega=Omega, 

+ mJPhi=JPhi, 

+ mSigma=Sigma, 

+ mX=X.mtx) 

> ssf.tv.capm 

$mPhi: 

[,1] 
1 

[ ,23 
0 

0 

0 

1 

0 

[1, ] 

[2, ] 

[3, ] 

$mOmega: 

[, 2 ] 

[ / 3 ] 
[ 

C, 13 
O

1

)

 Q

0.0000  0 
[1, 1 
[2, ]  0e+00  0.0016  0 
[3,]  0e+00  0.0000  0 

$mJPhi: 

[,1] 

[,2] 

-1 

-1 

1 

-1 

-1 

2 

[1,1 

[2, ] 

[3, ] 

$mSigma: 

[, 1] 

[ / 2 ] 

-1 

0 

0 

0 

-1 

0 

[1,1 

[2, ] 

[3, ] 

$mX: 

numeric matrix: 168 

2 columns. 

[1, 1 

\

—
1

1

O

.075187 

sp 

[168,  ] i i 0.05002 

 
 
 
 
 
580 

STATE-SPACE MODELS AND KALMAN FILTER 

11.3.2 ARMA Models 

Consider a zero-mean ARMA(/?, q) process yt of Chapter 2: 

^b(B)yt=9(B)at, at ~ N(0, oj), (11.30) 

where 0(5) = 1 — ]f+=1 0/#! and 0(B) = 1 — Y^j=i 0jBj, and p and q are non¬ 
negative integers. There are many ways to transform such an ARMA model into 
a state-space form. We discuss three methods available in the literature. Let m = 
max(/?, q -f- 1) and rewrite the ARMA model in Eq. (11.30) as 

m m— 1 

yt = X/fiiJt-i +at - ^Ojdt-j, (11.31) 

;=i r=i 

where 0/ = 0 for i > p and 0j = 0 for j > q. In particular, 6m = 0 because m> q. 

Akaike’s Approach 
Akaike (1975) defines the state vector st as the minimum collection of variables 
that contains all the information needed to produce forecasts at the forecast origin 
t. It turns out that, for the ARMA process in Eq. (11.30) with m = max(p, q + 
1), st = (yt\t, yt+\\t, ..., yt+m-\\t)', where yt+j\t = E(yt+j\Ft) is the conditional 
expectation of yt+j given Ft = {yi,..., yt). Since yt\t = yt, the first element of s, 
is yt. Thus, the observation equation is 

yt = Zst, (11.32) 

where Z = (1, 0,..., 0)iXm- We derive the transition equation in several steps. 
First, from the definition, 

^l.f+l = 37+1 — yt+\\t + (yr+1 - yf+l|t) = S2t + £fi+1, (11.33) 

where sit is the ith element of st. Next, consider the MA representation of ARMA 
models given in Chapter 2. That is, 

yt — at + + 02««-2 H-= ^ x/fiat-i, 

;=0 

where 00 = 1 and other 0 weights can be obtained by equating coefficients of B‘ 
in 1 + YlhLi 07#' = 0(B)/4>(B). In particular, we have 

01 = 01 - 0i, 

02 = 0101 +02 -02, 

MODEL TRANSFORMATION 

581 

0m-l — 01 0m—2 + 020m—3 + • • ’ + 0m-201 + 0m-1 — #m-l 

m —1 

= y>0m-l-< ~ 0m-1- (11.34) 

1=1 

Using the MA representation, we have, for j > 0, 

tt-h/l* = ^OTh/l^r) = E [Yitiat+j-i\Ft 

w =0 

0y-af + 0y+iaf_i + 07+2ar-2 + 

and 

tt+jlr+i = £(yt+;|Fr+i) = fj-iat+i + fjdt + 0y+iaf_i H- 

= 0y-ifl*+i +yt+j\t- 

Thus, for j > 0, we have 

yt+j\t+i = yt+j\t + 02-101+1- (11.35) 

This result is referred to as the forecast updating formula of ARM A models. It 
provides a simple way to update the forecast from origin t to origin t + 1 when 
yt_|_i becomes available. The new information of yf+i is contained in the innovation 
at+1, and the time-/ forecast is revised based on this new information with weight 
0y-_ i to compute the time-(/ + 1) forecast. 

Finally, from Eq. (11.31) and using E(at+j\Ft+i) = 0 for j > 1, we have 

m 

yt+m\t+1 = ^ '/(Piyt+m—i\t+1 dm — lat + l- 

i=1 

Taking Eq. (11.35), the prior equation becomes 

m —1 

yt+m\t+\ — ^ ' 0i (yt+m—i\t T 0m—i — \at + \) T 0m.Vf|l — $m — lat+l 

i=l 
m /m —1 \ 

= ^ ' (piyt+m—i\t T I ^ ^ 0; 0m —l—i dm — 1 J <Zf+l 

1 = 1 V 1=1 / 
m 

= ^ '0;};f+m—i|f T 0m —1^+1 > 

i=l 

(11.36) 

582 STATE-SPACE MODELS AND KALMAN FILTER 

where the last equality uses Eq. (11.34). Combining Eqs. (11.33) and (11.35) for 
j = 2,..., m — 1, and (11.36) together, we have 

yt+1 
Jf+2|r+l 

"01 0 
0 0 1 

o

o

1
_

yt 
yt+i\t 

37+m-llf+l 

yt+m\t+\ 

0 0 0 

■ 1 

0m 0m-1 0m—2 ‘ 

• 01 

yt+m-2\t 
yt+m~l\t 

1 

0i 

+ 

at+i. 

(11.37) 

fm-2 
0m-1 

Thus, the transition equation of Akaike’s approach is 

st+i = Tst + Rrjt, r}t ~ TV(0, cra2), (11.38) 

where rjt = at+\, and T and R are the coefficient matrices in Eq. (11.37). 

Harvey’s Approach 

Harvey (1993, Section 4.4) provides a state-space form with an m-dimensional 
state vector st, the first element of which is yt, that is, su = yt. The other elements 
of st are obtained recursively. From the ARMA(m, m - 1) model, we have 

m m — 1 
37-t-t — 0iy? + y ](t>iyt+i-i — }]0jat+1_j + at+\ 
1=2 7=1 

= 01^12 + S21 + T]t, 

where s2t = Ya=2 0/Tr+i-; ~ 12%! djat+l-j> Vt =at+1, and as defined earlier 
sit = yt- Focusing on s2,t+i, we have 

m m — 1 

J2,2+l = ^ 0/>2+2-1 - T, 6jat+2- j 

'•=2 7=1 

m m —1 

= 02yt + y 0/yt+2—i - y 9jat+2-j - 9\at+i 

2=3 7=2 

= 4>2S\t + s^t + 

 
 
 
MODEL TRANSFORMATION 

583 

where s3t = J2T=3 0; >7+2-1 ~ T!J=2 0jat+2-j- Next, considering s3,r+l, we have 

m m — 1 

■*3,1+1 = X! 0i >7+3-1 ~ X! @jat+3-j 

i=3 j=2 

m m — 1 

= 03yt + X foyt+l-i - X dja‘+l-j + (~^2)a/+l 

i'=4 j=3 

= 03*11 + *4l + (—#2 )Vt, 

where s4t = Ef=4 0i>7+3-« ~ ET= 3 OjOt+s-j- Repeating the procedure, we have 
*ml — Ei=m 0i>7+m—1-i — Ey=m-1 @jat+m-\-j = 0m>7-1 — 9m-\at- Finally, 

*m,l+l = 0m>7 l^i+l 

= 0m*!/ "P ( $m —l)fil- 

Putting the prior equations together, we have a state-space form 

Sf+i = T*i + Rfir, rjt~N(0, a^), (11.39) 

y, = Zs„ (11-40) 

where the system matrices are time invariant defined as Z = (1, 0, ..., 0)ixm, 

01 

02 

T = 

_

1

i

-
<
1

1 
0 

0 
0 

0 • 
1 

0 
0 • 

j
_
:
_

0
0

•

•

•

1 

• 0 . 

R = 

1 
-di 

~9m-1 

and dt, ct, and Ht are all zero. The model in Eqs. (11.39) and (11.40) has no 
measurement errors. It has an advantage that the AR and MA coefficients are 

directly used in the system matrices. 

Aoki’s Approach 
Aoki (1987, Chapter 4) discusses several ways to convert an ARMA model into a 
state-space form. First, consider the MA model, that is, yt — 9(B)at. In this case, 

we can simply define st == (at-q, at-q+2,..., at-\)' and obtain the state-space form 

 
 
 
 
 
 
 
 
584 

STATE-SPACE MODELS AND KALMAN FILTER 

at—q+1 

at—q+2 

" 0 

1 
0  0 

0 • 
1 

• 0 “ 
0 

at—q 
at—q+1 

0 " 
0 

+ 

at, 

(11.41) 

at-1 

at 

0  0  0 
_ 0  0  0 • 

1 

' 0 . 

at-2 

at-i 

0 
1 

yt — (~6q, —9q-1, • • • , —6\)st + at. 

Note that, in this particular case, at appears in both state and measurement 
equations. 

Next, consider the AR model, that is, (p(B)zt = at. Aoki (1987) introduces two 
methods. The first method is a straightfoward one by defining st = (zt-P+1, ..., ztY 
to obtain 

1
—

1

  _
o
t

+

T

Zt-p+3 

Zt 
Zt+1 

0 
0 

0 

0 p 

1 
0 

0 

0 
1 

0 

0p—l 

0p—2 

0 

0 

1 

01 

Zt—p+1 

Zt-p+2 

' 0 ' 
0 

+ 

0 
1 

Zt+l 

Zt 

(11.42) 

zt = (0, 0, • • ■, 0, l)sf. 

The second method defines the state vector in the same way as the first method 
except that a, is removed from the last element; that is, st = zt - at if p = 1 and 
st = (zt-p+1,..., Zt-1, Zt — atY if p > 1. Simple algebra shows that 

Zt-p+2 

Zt-p+3 

Zt 

0 
0 

0 

1 
0 

0 

Zt+i — at+1 

0p 

0/J—1 

0 
1 

•• 0 
0 

0 
0p—2 

1 
• 01 

Zt—p+1 

Zt-p+2 

Zt-l 

Zt — at 

0 
0 

at, 

(11.43) 

Zt — (0, 0,..., 0, l)sf -T at. 

Again, at appears in both transition and measurement equations. 

 
 
 
 
 
MODEL TRANSFORMATION 

585 

Turn to the ARMA(p, q) model <p(B)yt = 6(B)at. For simplicity, we assume 

q < p and introduce an auxiliary variable Zt — [1 /4>(B)]at. Then, we have 

<P(B)zt=a,, yt=d(B)zt- 

Since zt is an AR(p) model, we can use the transition equation in Eq. (11.42) or 
(11.43). If Eq. (11.42) is used, we can use yt = 0(B) Zt to construct the measurement 
equation as 

yt = (.—Op-1, —Op-2, ■ ■ ■, ~0\, l)sf, (11.44) 

where it is understood that p > q and Oj = 0 for j > q. On the other hand, if Eq. 
(11.43) is used as the transition equation, we construct the measurement equation 

as 

yt = (—Op-1, —Op-2, ..., —0\, 1 )st + a,. (11.45) 

In summary, there are many state-space representations for an ARMA model. 
Each representation has its pros and cons. For estimation and forecasting purposes, 
one can choose any one of those representations. On the other hand, for a time- 
invariant coefficient state-space model in Eqs. (11.26) and (11.27), one can use the 
Cayley-Hamilton theorem to show that the observation yt follows an ARMA(m, m) 
model, where m is the dimension of the state vector. 

SsfPack Command 
In SsfPack/S-Plus, a command GetSsfArma can be used to transform an ARMA 
model into a state-space form. Harvey’s approach is used. To illustrate, consider 

the AR(1) model 

yt = 0.6yt-i + at, at ~ N(0, 0.42). 

The state-space form of the model is 

> ssf.arl = GetSsfArma(ar=0.6,sigma=0.4) 

> ssf.arl 

$mPhi: 

[,1] 
[1,1 0.6 
[2,] 1.0 

$mOmega: 

[, 1] [,2] 
[1,] 0.16 0 
[2,] 0.00 0 
$mSigma: 

[,11 
[1,] 0.25 
[2,] 0.00 

586 

STATE-SPACE MODELS AND KALMAN FILTER 

Since the AR(1) model is stationary, the program uses Zi,o = Var(yf) = (0.4)2/ 
(1 — 0.62) = 0.25 and fi^0 = 0. These values appear in the matrix mSigma. 

As a second example, consider the ARMA(2,1) model 

yt = l.2yt-i-0.35yt-2 + at-0.25at-i, at ~ N(0, l.l2). 

The state-space form of the model is 

> arma21.m = list(ar=c(1.2,-0.35),ma=c(-0.25),sigma=l.1) 
> ssf.arma21= GetSsfArma(model=arma21.m) 
> ssf.arma21 
$mPhi: 

LI] 1 / 2 ] 
[1,] 1.20 1 
[2,] -0.35 0 
[3,] 1.00 0 
$mOmega: 

[,1] [,2] 
[1,] 1.2100 -0.302500 
[2,] -0.3025 0.075625 
[3,] 0.0000 0.000000 
$mSigma: 

L 3 ] 
0 
0 
0 

[,1] [ / 2 ] 
[1,1 4.060709 -1.4874057 
[2,] -1.487406 0.5730618 
[3,] 0.000000 0.0000000 

As expected, the output shows that 

T = 

1.2 
-0.35 

Z = ( 1,0), 

and mPhi and mOmega follow the format of Eq. (11.28), and the covariance matrix 
of (Sh, s2t)' is used in mSigma, where s\, = yt and s2t = -0.35y,_! — 0.25y,_2. 
Note that in SsfPack, the MA polynomial of an ARMA model assumes the form 
9(B) = 1 + 9\B + ... + 9qBq, not the form 9(B) = 1 - 9\B - ... — 9qBq com¬ 
monly used in the literature. 

11.3.3 Linear Regression Model 

Multiple linear regression models can also be represented in state-space form. 
Consider the model 

yt =x[p + et, 

et ~ N(0, a2) 

MODEL TRANSFORMATION 

587 

where xt is a ^-dimensional explanatory variable and (5 is a /^-dimensional param¬ 
eter vector. Let st = fi for all t. Then the model can be written as 

St+l 
yt 

(11.46) 

Thus, the system matrices are Tt = Ip, Zt = x't, d, =0, ct — 0, Qt = 0, and 
Ht = <jp. Since the state vector is fixed, a diffuse initialization should be used. 

One can extend the regression model so that fit is random, say. 

Pt+i = Pt +RtVt> rjt ~ N(0, 1), 

and Rt = (or,..., op)' with cr,- > 0. If a, = 0, then is time invariant. 

SsfPack Command 
In SsfPack, the command GetSsfReg creates a state-space form for the multiple 
linear regression model. The command has an input argument that contains the data 
matrix of explanatory variables. To illustrate, consider the simple market model 

rt = A) + + et> t = 1,168, 

where rt is the return of an asset and rM,t is the market return, for example, the 
S&P 500 composite index return. The state-space form can be obtained as 

> ssf.reg=GetSsfReg(cbind(l,sp)) % 'sp' is market return. 

> ssf.reg 

$mPhi: 

[, 1] 

[ ,2] 

1 

0 

0 

0 

1 

0 

[1,1 

[2,] 

[3,] 

$mOmega: 

[ , 11 

[, 2 ] 

[1, 1 

[2,] 

[3,] 

0 

0 

0 

0 

0 

0 

$mSigma: 

[,1] 

[, 2 ] 

-1 

0 

0 

0 

-1 

0 

[1, 1 

[2,] 

[3,1 

$mJPhi: 

[,1] 

[ , 2 ] 

-1 

-1 

1 

-1 

-1 

2 

[1,1 
[2,] 

[3,] 

$mX: 

588 

STATE-SPACE MODELS AND KALMAN FILTER 

numeric matrix: 168 rows, 2 columns, 

[1,] 1 -0.075187 

sp 

[168,] 1 0.05002 

11.3.4 Linear Regression Models with ARMA Errors 

Consider the regression model with ARMA(p, q) errors: 

yt =x',p + zt, (p(B)zt = 6(B)at, (11.47) 

where a, ~ N(0, o%) and xt is a ^-dimensional vector of explanatory variables. A 
special case of this model is the nonzero mean ARMA(p, q) model in which xt 
= 1 for all t and /? becomes a scalar parameter. Let st be a state vector for the zt 
series, for example, that defined in Eq. (11.39). We can define a state vector s* for 
y, as 

(11.48) 

where j$t = (i for all t. Then, a state-space form for yt is 

4 = n; + n, 01.49) 

3h = Z*ts*t, (11.50) 

where Z* = (1, 0,..., 0, *J)iX(m-Hfc), m = max(p, q + 1), and 

T 0 

0 h 

R* = 

R 
0 ’ 

where T and R are defined in Eq. (11.39). In a compact form, we have the state- 
space model 

’f+i 
yt 

y** 

< + 

R*vt 
o 

SsfPack Command 

SsdPack uses the command GetSsfRegArma to construct a state-space form for 
linear regression models with arma errors. The arguments of the command can be 
found using the command args (GetSsfRegArma). They consist of a data matrix 
for the explanatory variables and ARMA model specification. To illustrate, consider 
the model 

MODEL TRANSFORMATION 

589 

yt = Po + Pixt+zt, t — 1, • - -, 168, 

zt — l.2zt-i - 0.35zr_2 + at - 0.25a,_i, at ~ iV(0, cr2). 

We use the notation X to denote the T x 2 matrix of regressors (1, xt). A state-space 
form for the prior model can be obtained as 

> ssf.reg.arma2l=GetSsfRegArma(X,ar=c(1.2,-0.35) , 

+ ma=c(-0.25)) 

> ssf.reg.arma21 

$mPhi: 

[1,] 1.20 
[2,] -0.35 

[, 1] [,2] [, 3] [, 4] 
10 0 
0 0 0 

[3,] 0.00 

0 10 

[4,] 0.00 

[5,] 1.00 

$mOmega: 

0 0 1 
0 0 0 

[, 1] 

[1,] 1.00-0 
[2,] -0.25 0 

[3,] 0.00 0 

[4,] 0.00 0 

[5,] 0.00 0 

[ / 2] [,3] [,4] 
2500 0 0 

0625 0 0 
0000 0 0 
0000 0 0 
0000 0 0 

$mSigma: 

[, 1] 
[1,] 3.35595 

[/2] [,3] 

-1.229260 0 

[2,] -1.22926 

0.473604 0 

[3,] 0.00000 

0.000000 -1 

[4,] 0.00000 

[5,] 0.00000 

0.000000 0 
0.000000 0 

[, 5] 
0 
0 
0 
0 
0 

[ / 4] 
0 
0 
0 
-1 
0 

$mJPhi: 

[,1] 
-1 

-1 

-1 

-1 

-1 

[, 2] 

[ / 3 ] 

[ / 4 ] 

-1 
-1 

-1 

-1 

-1 

-1 

-1 
-1 

-1 

1 

-1 

-1 

-1 

-1 

2 

[1,1 
[2,] 

[3, ] 

[4, ] 

[5, ] 

$mX: 

numeric matrix: 168 rows, 2 columns. 

xt 

[1,] 1 0.4993 

[168,] 1 0.7561 

590 

STATE-SPACE MODELS AND KALMAN FILTER 

11.3.5 Scalar Unobserved Component Model 

The basic univariate unobserved component model, or the structural time series 
model (STSM), assumes the form 

yt = dt + Yt + &>f + et, (11.51) 

where fit, yt, and a), represent the unobserved trend, seasonal, and cycle compo¬ 
nents, respectively, and et is the unobserved irregular component. In the literature, 
a nonstationary (possibly double-unit-root) model is commonly used for the trend 
component: 

dt+i= dt + Pt + rjt, V, ~ N(0,cr*), (11.52) 

Pt=Pt-i+St, SV ~ N(0, cr2), 

where //.i ~ N(0, i-) and fi\ ~ 7V(0, £) with £ a large real number, for example, £ = 
108. See, for instance, Kitagawa and Gersch (1996). If crs = 0, then pt follows a 
random walk with drift If ag = = 0, then /x, represents a linear deterministic 
trend. 

The seasonal component yt assumes the form 

(1+5H-b Bs~x)yt = cot, o)t ~ iV(0, ct2), (11.53) 

where 5 is the number of seasons in a year, that is, the period of the seasonal¬ 
ity. If ow = 0, then the seasonal pattern is deterministic. The cycle component is 
postulated as 

<*>t+1 
L Mt+i J 

= s 

cos(Ac) 
- sin(Ac) 

sin(Ac) 
cos(Ac) 

cot 
L <°t J 

+ 

St 
L£* J 

(11.54) 

where 

N 

0 
0 

-ni2 

~ N(0, a2), co* ~ N(0, cr2), and Cov(w0, (o*Q) = 0, 5 e (0, 1] is called a damp¬ 

ing factor, and the frequency of the cycle is Ac = 2n/q with q being the period. 
If ^ 1» then the cycle becomes a deterministic sine—cosine wave. 

SsfPack/S-Plus Command 

The command GetSsfstsm constructs a state-space form for the structural time 
series model. It allows for 10 cycle components; see the output of the command 
args (GetSsfstsm). Table 11.4 provides a summary of the arguments and their 
corresponding symbols of the model. To illustrate, consider the local trend model 
in Eqs. (11.1) and (11.2) with a, = 0.4 and an = 0.2. This is a special case of the 
scalar unobserved component model. One can obtain a state-space form as 

KALMAN FILTER AND SMOOTHING 

591 

TABLE 11.4 Arguments of Command GetSsfStsm in SsfPack/S-Plus 

Argument 

irregular 
level 
slope 
seasonalDummy 
seasonalTrig 
SeasonalHS 
CycleO 

Cycle9 

STSM parameter 

oe 

@0)1 $ 

0(0, s 

0(0, s 

&£■> hc, 8 

&£■> 8 

> ssf.stsm=GetSsfStsm(irregular=0.4,level=0.2) 

> ssf.stsm 

$mPhi: 

[,1] 
[1, ] 1 
[2, ] 1 
$mOmega: 

[,1] [,2] 

[1,] 0.04 0.00 

[2, ] 0.00 0.16 

$mSigma: 

[, 1] 

[1,] -1 
[2,] 0 

11.4 KALMAN FILTER AND SMOOTHING 

In this section, we study the Kalman filter and various smoothing methods for 
the general state-space model in Eqs. (11.26) and (11.27). The derivation follows 
closely the steps taken in Section 11.1. For readers interested in applications, this 
section can be skipped at the first read. A good reference for this section is Durbin 

and Koopman (2001, Chapter 4). 

11.4.1 Kalman Filter 

Recall that the aim of the Kalman filter is to obtain recursively the conditional 
distribution of st+i given the data Ft = {yl,..., y,} and the model. Since the 
conditional distribution involved is normal, it suffices to study the conditional mean 
and covariance matrix. Let sj\i and be the conditional mean and covariance 

matrix of sj given Fit that is, sj\Fi ~ N(jHj\it £;|,-). From Eq. (11.26), 

592 

STATE-SPACE MODELS AND KALMAN FILTER 

*r+H/ = E(dt + Ttst + Rttit\Ft) = d, + Ttst]t, (11.55) 

Xt+i\t =Var(TlSt + Rtr)t\Ft) = TtTtltT't + RtQtR't. ' (11.56) 

Similarly to that of Section 11.1, let ytlt_, be the conditional mean of y, given 
Ft-\. From Eq. (11.27), 

yt\t-1 — °t + ZtSt\t-i- 

Let 

vt = yt- y,\,-1 =yt- (c, + Zts,\t-i) = Zt(s, - st„_i) + e„ (11.57) 

be the 1-step-ahead forecast error of yt given Ft~\. It is easy to see that (a) 
E(i>t\Ft-i) = 0; (b) v, is independent of Ff_ls that is, Cov(v,, yj) = 0 for 1 < 
j < t', and (c) {p;} is a sequence of independent normal random vectors. Also, let 
Vt = Var(pf|Ff_i) = Var(v,) be the covariance matrix of the 1-step-ahead forecast 
error. From Eq. (11.57), we have 

Vt = Var[Zt(st - st]t_x) + et] = ZtEt\t_xZ't + Ht. (11.58) 

Since Ft = {Ft-X, yr} = {Ff_i, v,}, we can apply Theorem 11.1 to obtain 

sf|f = E(st\Ft) = E(st\Ft-\, vt) 

= £'(^r|F?_i) + Cov(s;, p,)[Var(p,)]“1p, 

= Jt|r-i +CtV~1vt, (11.59) 

where C, = Cov(sf, vt\Ft-X) given by 

C, = Cov(sf, v,\F,_0 = Cov[sf, Z,(st - st\,-i) + e,\Ff_j] 

= Cov[5f, Zt(s, - s/(f_1)|F/_1] = 

Here we assume that V, is invertible because H, is. Using Eqs. (11.55) and (11.59), 
we obtain 

S'+U -dt + Ttst\t-i + TtCtVt lv, =dt + Ttst|,_j + Ktvt, (11.60) 

where 

K, = T,C,V~' = 7',Z,l,_iZ;v,-1. 

(11.61) 

KALMAN FILTER AND SMOOTHING 

593 

which is the Kalman gain at time t. Applying Theorem 11.1(2), we have 

Ef|f = Var(s;|F;_i, vt) 

= Var(Sf|Ff_i) - Cov(s,, uf)[Var(i;r)]_1Cov(sr, v,)' 

= Ttl t^-CtV-xC\ 

= - S/|f_1z;Vf-1ZrXt|f-i. (11-62) 

Plugging Eq. (11.62) into Eq. (11.56) and using Eq. (11.61), we obtain 

?t+i\t = TtYt{t_lLft + RtQtR', (11.63) 

where 

Lt = T t — KtZt. 

Putting the prior equations together, we obtain the celebrated Kalman filter for the 
state-space model in Eqs. (11.26) and (11.27). Given the starting values S\\o and 
Eiio, the Kalman filter algorithm is 

vt = yt - ct - Ztst\t-\, 

V t — Zt'Lt\t-\Z't + Ht, 

Kt = TtYt\t-XZ'tV;\ (11.64) 

Lt = T t — KtZt, 

S/+i|f = dt + TtSt\t-1 + Ktvt, 

Zt+i\, = TtYtV^L't + R,QtRt, t = 

If the filtered quantities st\t and are also of interest, then we modify the filter 
to include the contemporaneous filtering equations in Eqs. (11.59) and (11.62). The 

resulting algorithm is 

vt = yt - ct - zts,\t-1, 

Ct = ^t\t-iZ't, 

Vt = ZtY.tV-\Z't + Ht = Z,Ct + Ht, 

st\t — st\t-\ +CtVt lvt, 

Tt\t = 'Et\t-i-CtV-lC,t, 

sr+l|r = dt + Ttst\t, 

’Zt+xit — TtYjt\tT't + Rt QtRf 

594 

STATE-SPACE MODELS AND KALMAN FILTER 

Steady State 
If the state-space model is time invariant, that is, all system matrices are time 
invariant, then the matrices Y,t\t-\ converge to a constant matrix .E*, which is a 
solution of the matrix equation 

Z* = rz*r - T'Z*ZV-lZ'Z*T +RQR', 

where V = ZZ^Z' + H. The solution that is reached after convergence to Z* is 
referred to as the steady-state solution of the Kalman filter. Once the steady state is 
reached, Vt, Kt, and Zf+ \\t are all constant. This can lead to considerable saving 
in computing time. 

11,4.2 State Estimation Error and Forecast Error 

Define the state prediction error as 

Xf — Sj ^t\t—l ■ 

From the definition, the covariance matrix of jc, is VarQc,|Fr_i) = Var(s?|F,_i) = 
E,|f_i. Following Section 11.1, we investigate properties of xt. First, from Eq. 
(11.57), 

vt — Zt(st ~ s«|f-t) + Ct — Ztx, -(- er. 

Second, from Eqs. (11.64) and (11.26), and the prior equation, we have 

xt+1 = Sf+l ~ sr+l|/ 

— Tt{st — S;|/_i) + Rti}t — K,vt 

= Ttxt + Rtr]t - Kt(Ztxt + et) 

= Ltx, + Rtr]t - Ktet, 

where, as before, L, — Tt — K,Zt. Consequently, we obtain a state-space form for 
vt as 

Vt — %txt T eti ^f+i — Ltxt + RtiJt — Ktet, (11.65) 

with jcj = si - si|0 for t = 1, ..., T. 

Finally, similar to the local-trend model in Section 11.1, we can show that the 
1-step-ahead forecast errors {t>,} are independent of each other and {t>?, ..., vT) is 
independent of Ft_\. 

KALMAN FILTER AND SMOOTHING 

595 

11.4.3 State Smoothing 

State smoothing focuses on the conditional distribution of st given Fj. Notice that 
(a) Ft-1 and {vt,.. .,Vt) are independent and (b) v, are serially independent. We 
can apply Theorem 11.1 to the joint distribution of st and {vt, ..., vj] given Ft-\ 
and obtain 

s,\T - E(st\FT) = E(st\Ft-i, vt, ...,vT) 

T 

= E(st\Ft-i) + J^Cov(sf, v7)[Var(vy)]_1 vy 

j=t 

T 

= Sf|f-1 + ^Cov(st,Vj)VjlVj, (11.66) 

j=t 

where the covariance matrices are conditional on Ft-\. The covariance matrices 
Cov(s,, Vj) for j = t, ... ,T can be derived as follows. By Eq. (11.65), 

Cov(s,, vj) = E(stv'j) 

= E\st(ZjXj + ej)'] = E{stx'j)Z'j, j = t,...,T. (11.67) 

Furthermore, 

E(stx't) = E[st(st - %_i)'] = Var(sf) = 

E(stx't+l) = E[st(L,x, + R,rit - Ktet)'] = Z,|,_iL't, 

E(stx ^2) —’^‘t\t—\EtEJjr-y, (11.68) 

E(SfXT) — Yit\t—\Lt ■ ■ ■ Lj_j. 

Plugging the prior two equations into Eq. (11.66), we have 

sr|r — st\t-\ + ^T|r-i2/rF7-lT7’, 

sr-iir = st-i\t-2 + r-\vt-\ + 'Et-\\t-2L'T-\Z'TV t1vt, 

St\T — £f|f-i + 'Et\t-iZ'tV1 1 vt + Yt\t-\L'tZ't+lV ^Vt-yi 

+ • • • + 'Et\t-lL'tL't+{ • ‘' Et_\Zt Vt 1 Vf, 

for t = T — 2, T — 3,..., 1, where it is understood that L\ • • • L'r_j = Im when 
t = T. These smoothed state vectors can be expressed as 

$t\T = 

(11.69) 

596 

STATE-SPACE MODELS AND KALMAN FILTER 

where qT_x — Z'TV Txvj, qr-2 — ^t-i^+ L'T_XZ'TV Txvj, and 

9,-1 = z;v,-1r, + i;z;+1v,-+11»,+1 + --- + z.;ti+r--t'r_1z'rv^rr. 

for t = T — 2, T — 3,..., 1. The quantity is a weighted sum of the 1-step- 
ahead forecast errors vj occurring after time t — 1. From the definition in the prior 
equation, qt can be computed recursively backward as 

qt_l=Z,tV;1vt + L'qt, t = T,...,l, (11.70) 

with qT = 0. Putting the equations together, we have a backward recursion for the 
smoothed state vectors as 

qt_l = Z,,Vtlvt + L'tqt, s,\T = t = T,..., 1, 

(11.71) 

starting with qT = 0, where Lt, and Vt are available from the Kalman 
filter. This algorithm is referred to as the fixed interval smoother in the literature; 
see de Jong (1989) and the references therein. 

Covariance Matrix of Smoothed State Vector 
Next, we derive the covariance matrices of the smoothed state vectors. Applying 
Theorem 11.1(4) to the conditional joint distribution of st and {, ..., vj} given 
Ft-1, we have 

Zqr = E/|r-i -^2Cov(st,Vj)[\ar(vj)] ^Covfo, vj)]'. 

j=t 

Using the covariance matrices in Eqs. (11.67) and (11.68), we further obtain 

E/|r = £f|f-i — 'Et\t-iZ'tVt — X,|f_iL'Zj+1F;+11Z,+iL/Ii,|f_i 

— ■ • • — 'Lt\t-\Lt ■ ■ • Lt_xZ'tVTlZTLT-\ • • • LrXf|?_i 

= — 2f|r-i Mt-\, 

where 

Mt-i  = z'tv;1zt + L'tz't+l 

V t+1 ^t+i Lt 

+ • • • + Lt • • • LT_X 

Z'TVT1 ZjLt-i ■ • • Lt. 

Again, L\ • • • LT-X = Im when t = T. From its definition, the Mt-X matrix satisfies 

M(_! = Z'tV;lZ, + L'tMtLt, 

t = T, 

1, 

(11.72) 

KALMAN FILTER AND SMOOTHING 

597 

with the starting value Mj = 0. Collecting the results, we obtain a backward 
recursion to compute Z^r as 

Mt-l = Z'tVT'Zt,+L'tMtLt, Zt\T = ZMf_i (11.73) 

for t = T,, 1 with Mr = 0. Note that, like that of the local trend model in 
Section 11.1, Mt = Var(<7r). 

Combining the two backward recursions of smoothed state vectors, we have 

<lt-\ = Z'tV-lvt + L'tqt, 

St\T = l + Ztp-iqt-i, (11-74) 

M,_! = Z'tV-lZt + L’tMtLt, 

£t\T = £f|r-l — t — T, ..., 1, 

with qT — 0 and Mj = 0. 

Suppose that the state-space model in Eqs. (11.26) and (11.27) is known. Appli¬ 
cation of the Kalman filter and state smoothing can proceed in two steps. First, the 
Kalman filter in Eq. (11.64) is used for t = 1,..., T and the quantities vt,Vt, Kt, 
st\t~i, and Xf|f_i are stored. Second, the state smoothing algorithm in Eq. (11.74) 
is applied for t = T, T — 1,..., 1 to obtain st\j and Xqj. 

11.4.4 Disturbance Smoothing 

Let et\r = E(et\Fr) and r]^T = E(n]t\Fr) be the smoothed disturbances of the 
observation and transition equation, respectively. These smoothed disturbances are 
useful in many applications, for example, in model checking. In this section, we 
study recursive algorithms to compute smoothed disturbances and their covariance 
matrices. Again, applying Theorem 11.1 to the conditional joint distribution of et 
and 117-} given Ft~\, we obtain 

et\T = E(et\Ft-i,vt, ...,vT) = ^ E(etv'j) V~lvj, (11.75) 

T 

j=t 

where E(et\Ft-\) = 0 is used. Using Eq. (11.65), 

E(etv'j) = E(etx'j)Z'j + E(ete'j). 

Since E{etx\) = 0, we have 

E(etv'j) = 

Ht, if j=t, 

E{etx'j^Z'j, for j — t + 1,..., T. 

(11.76) 

598 

STATE-SPACE MODELS AND KALMAN FILTER 

Using Eq. (11.65) repeatedly and the independence between {et} and {qt}, we 
obtain 

E(etx't+l) = —HtK\, 

E(etx't+2) = -HtK'tL't+l, 

E(etx'T) — —H,K'tL't+l • • • L'T_X, 

(11.77) 

where it is understood that L'+1 • • • VT_X = Im if t = T — 1. Based on Eqs. (11.76) 
and (11.77), 

e,\T = Ht{V;xvt - K,tZ,t+1V^1vt+1-K,tL't+l---L,T_lZ'TV-1vT) 

= Ht(V;lvt-K'tqt) 

= Htot, t = T, •••,!, (11.78) 

where qt is defined in Eq. (11.69) and ot — V~xvt — K\qt. We refer to ot as the 
smoothing measurement error. 

The smoothed disturbance r)t\T can be derived analogously, and we have 

T 

Vt\r = YlE^tv'jS>VjlvJ- (11.79) 

j=t 

The state-space form in Eq. (11.69) gives 

E(vtv'j) 

QtR'tZ't+l, if j — t + 1, 

E(qtx'j)Z'j, if j = t + 2, •••, T, 

where 

E(ritx't+2)=QtR'tL'+1, 

rltxt+’3) = Qt^t^t+l^t+2 

E(Vtx,T)=QtR'tLft+l...L'T_ i, 

for t = 1, ..., T. Consequently, Eq. (11.79) implies 

Vtir — QtEt(Zt+x Vt+xvt+\ + Lt+xZt+2V t_^2Vt+2 

H-1- L't+l ■ ■ ■ L't_xZtVjXvt) 

= QtX'tVt, t = T,..., 1, 

(11.80) 

KALMAN FILTER AND SMOOTHING 

599 

where qt is defined earlier in Eq. (11.70). 

Koopman (1993) uses the smoothed disturbance rj^T to derive a new recursion 

for computing st\r. From the transition equation in Eq. (11.26), 

st+l\T = dt + Ttst\r + Rtrjt\T. 

Using Eq. (11.80), we have 

St+1\T =dt + Ttst\T + RtQtRtqt, t = l,...,T, (11.81) 

where the initial value is sqr = si|o + Si|o9o with q0 obtained from the recursion 
in Eq. (11.70). 

Covariance Matrices of Smoothed Disturbances 
The covariance matrix of the smoothed disturbance can also be obtained using 
Theorem 11.1. Specifically, 

Var(<?r|Fr) = Var(e,|F,_i, vt,...,vT) 

= Varfe|Ff_j) - J]Cov(e„ vjWJ1 [Cow(e„ x>;)]'. 

T 

j=t 

Note that Cov(er, vj) = E(etv'j), which is given in Eq. (11.76). Thus, we have 

Var(e,|Fr) = Ht - Ht(Vfl + K'tZ't+lV;xxZt+lKt 

+ K'L't+lZ't+2V;_l2Zt+2Lt+1Kt 

+ • • • + KtLt_j_j • • • LT_XZTVT ZtLt~\ • ■ • Lt+\Kt)Ht 

— Ht — Ht(Vfl + K'tMtKt)Ht 

= Ht-HtNtHt, 

where Nt = Vf] + K'tMtKt, where Mt is given in Eq. (11.72). Similarly, 

\ar(qt\FT) = Var(?/r) - J]Cov(?/,, vt)Vf'[Cov(rjt, v,)]~x, 

T 

i=t 

where Co\{qt, Vj) = E(qtv'-), which is given before when we derived the formula 
for qt\T. Consequently, 

Var(qt\Fr) = Qt - QtR't(Z't+lVtxxZt+i + L't+lZ't+2V t+2Zt+2Lt+l 

H-+ Lt+1 • • • LT_xZTVr ZjEj-\ ■ • • Lt+\)Rt Qt 

= Qt — QtR,MtRt Qt. 

600 

STATE-SPACE MODELS AND KALMAN FILTER 

In summary, the disturbance smoothing algorithm is as follows: 

etl T=Ht(V-1vt-K'tqt), 

Vtir = QtKVf 

qt-\ — Z\V^xvt + L\qt, (11.82) 

Var(e,|Fr) = H, - Ht(V~l + KtMtKt)Ht, 

Var(*,| FT)=Qt-QtR'tMtRtQt, 

Mt-i = Z'tV;lZt + L\MtLt, t = T,...,l, 

where qT — 0 and Mj = 0. 

11.5 MISSING VALUES 

For the general state-space model in Eqs. (11.26) and (11.27), we consider two 
cases of missing values. First, suppose that similar to the local trend model in 
Section 11.1 the observations yt at t = l + ..., £ + h are missing. In this case, 
there is no new information available at these time points and we set 

vt = 0, Kt = 0, for t = i + 1, ..., i + h. 

The Kalman filter in Eq. (11.64) can then proceed as usual. That is, 

S/TiU = dt + Ttst\t-1, = Tt'E‘t\t-iT't + Rt QtRt, 

for t = l + 1, ...,£ + h. Similarly, the smoothed state vectors can be computed 
as usual via Eq. (11.74) with 

q,_x = rtqt, M,_! = T’tMtTt, 

for t = l + 1, ...,£ + h. 

In the second case, some components of yt are missing. Let y* = Jyt be the 
vector of observed data at time t, where J is an indicator matrix identifying the 
observed data. More specifically, rows of J are a subset of the rows of the k x k 
identity matrix. In this case, the observation equation (11.27) of the model can be 
transformed as 

y* =c* + Z*st + e% 

where c* = Jct, Z* = JZt, and e* = Je, with covariance matrix Var(e*) = H* = 
JHtJ . The Kalman filter and state-smoothing recursion continue to apply except 
that the modified observation equation is used at time t. Consequently, the ease in 
handling missing values is a nice feature of the state-space model. 

FORECASTING 

11.6 FORECASTING 

601 

Suppose that the forecast origin is t and we are interested in predicting yt+j 
for j = 1, ,h, where h > 0. Also, we adopt the minimum mean-squared error 
forecasts. Similar to the ARMA models, the j-step-ahead forecast yt(j) turns out to 
be the expected value of yt+j given Ft and the model. That is, yt(j) = E(yt+j\Ft). 
In what follows, we show that these forecasts and the covariance matrices of the 
associated forecast errors can be obtained via the Kalman filter in Eq. (11.64) by 
treating {yf+1, ..., yt+h) as missing values, that is, the first case in Section 11.5. 

Consider the 1-step-ahead forecast. From Eq. (11.27), 

37(0 = E(yt+l\Ft) = ct+1 + Zt+lst+fy, 

where sy+i|f is available via the Kalman filter at the forecast origin t. The associated 
forecast error is 

*t{ 1) = 3fi+i - 37(1) = Zt+i(st+i - st+i\t) + et+1. 

Therefore, the covariance matrix of the 1-step-ahead forecast error is 

Var[ef(l)] = Zt+iEt+i\tZ't+1 + Ht+1. 

This is precisely the covariance matrix Vt+\ of the Kalman filter in Eq. (11.64). 
Thus, we have showed the case for h = 1. 

Now, for h > 1, we consider 1-step- to /z-step-ahead forecasts sequentially. From 

Eq. (11.27), the j-step-ahead forecast is 

370') — ct+j "b Zt+jSf+j\t, 

(11.83) 

and the associated forecast error is 

^t(j) — Zf+j (S(+j — St+j\t) + &t+j- 

Recall that st+j\t and 'Et+j\t are, respectively, the conditional mean and covariance 
matrix of st+j given Ft. The prior equation says that 

VarfoO')] = Zt+j'Et+j\tZ't+j + Ht+j. (11.84) 

Furthermore, from Eq. (11.26), 

®f+y'+l|f — ^t+j + Tt+j.St+j\t, 

which in turn implies that 

st+j+1 st+j+l\t — T t-\~j(St+j ■S’r+-y |r) + Rt+jVt+j • 

602 

Consequently, 

STATE-SPACE MODELS AND KALMAN FILTER 

^t+j+i\t = Tt+j^t+j\tTt+jRt+j Qt+jRt+j- (11.85) 

Note that Var[er(j)\ = Vt+j and Eqs. (11.83) and (11.85) are the recursion of 
the Kalman filter in Eq. (11.64) for t + j with j = 1,,h when vt+j = 0 and 
Kt+j — 0. Thus, the forecast yt(j) and the covariance matrix of its forecast error 
et(j) can be obtained via the Kalman filter with missing values. 

Finally, the prediction error series {v?} can be used to evaluate the likelihood 
function for estimation and the standardized prediction errors D, v, can be used 
for model checking, where D, = diag{Vf(l, 1),..., Vt(k, k)} with Vt(i, i) being 
the (i, i)th element of Vt. 

11.7 APPLICATION 

In this section, we consider some applications of the state-space model in finance 
and business. Our objectives are to highlight the applicability of the model and to 
demonstrate the practical implementation of the analysis in S-Plus with SsfPack. 

Example 11.2. Consider the CAPM for the monthly simple excess returns of 
General Motors (GM) stock from January 1990 to December 2003; see Chapter 9. 
We use the simple excess returns of the S&P 500 composite index as the market 
returns. The returns are in percentages. Our illustration starts with a simple market 
model 

rt =a + PrM<t+et, et ~ N(0, a/), (11.86) 

for t = 1,..., 168. This is a fixed-coefficient model and can easily be estimated 
by the ordinary least-squares (OLS) method. Denote the GM stock return and the 
market return by gm and sp, respectively. The result follows: 

> da=read.table(''m-gmsp-excess-9003.txt'',header=F) 
> gm=da[,l] 
> sp=da[,2] 
> f it=OLS (gm~sp) 
> summary(fit) 
Call: 
OLS (formula = gm~sp) 
Coefficients: 

Value Std. Error t value Pr(>|t|) 

(Intercept) 0.1982 0.6302 0.3145 0.7535 
sp 1.0457 0.1453 7.1962 0.0000 

Regression Diagnostics: 

APPLICATION 

603 

R-Squared 0.2378 

Adjusted R-Squared 0.2332 

Durbin-Watson Stat 2.0290 

Residual Diagnostics: 

Stat P-Value 

Jarque-Bera 2.5348 0.2816 

Ljung-Box 24.2132 0.3362 

Residual standard error: 8.13 on 166 degrees of freedom 

Thus, the fitted model is 

r, = 0.20+ 1.0457rM,f + et, oe = 8.13. 

Based on the residual diagnostics, the model appears to be adequate for the GM 
stock returns with adjusted R2 — 23.3%. 

As shown in Section 11.3, model (11.86) is a special case of the state-space 

model. We then estimate the model using SsfPack. The result is as follows: 

> reg.m=function(parm,mX=NULL){ 

+ parm=exp(parm) % log(sigma.e) is used to ensure 

positiveness. 

+ ssf.reg=GetSsfReg(mX) 

+ ssf.reg$mOmega[3,3]=parm[l] 

+ CheckSsf(ssf.reg) 
+ 1 
> c.start=c (10) 

> X.mtx=cbind(rep(1,168),sp) 

> reg.fit=SsfFit(c.start,gm,"reg.m",mX=X.mtx) 

RELATIVE FUNCTION CONVERGENCE 

> names(reg.fit) 

[1] "parameters" "objective" "message" "grad.norm" 

"iterations" 

[6] "f.evals" "g.evals" "hessian" "scale" "aux" 

[11] "call" "vcov" 

> sqrt(exp(reg.fit$parameters)) 

[1] 8.130114 

> ssf.reg$mOmega[3,3]=exp(reg.fit$parameters) 

> reg.s=SsfMomentEst(gm,ssf.reg,task="STSMO") 

> reg.s$state.moment[10,] 

state.1 state.2 

0.1982025 1.045702 

> sqrt(reg.s$state.variance[10 , ] ) 

state.1 state.2 

0.6302091 0.1453139 

As expected, the result is in total agreement with that of the OLS method. 

604 

STATE-SPACE MODELS AND KALMAN FILTER 

Finally, we entertain the time-varying CAPM of Section 11.3.1. The estimation 

result, including time plot of the smoothed response variable, is given below. The 

command SsfCondDens is used to compute the smoothed estimates of the state 

vector and observation without variance estimation. 

> tv.capm =function(parm,mX=NULL){ %setup model for estimation 
+ parm=exp(parm) %parameterize in log for positiveness. 
+ Phi.t = rbind(diag(2),rep(0,2)) 
+ Omega=diag(parm) 
+ JPhi=matrix(-1,3,2) 
+ JPhi[3,1]=1 
+ JPhi[3,2]=2 
+ Sigma=-Phi.t 
+ ssf.tv=list(mPhi=Phi.t, 
+ mOmega=Omega, 
+ mJPhi=JPhi, 
+ mSigma=Sigma, 
+ mX=mX) 
+ CheckSsf(ssf. tv) 

+ ) 
> tv.start=c(0,0,0) % starting values 
> tv.mle=SsfFit(tv.start,gm,"tv.capm",mX=X.mtx) % estimation 
> sigma.mle=sqrt(exp(tv.mle$parameters)) 
> sigma.mle 
[1] 4.907845e-05 1.219885e-02 8.125213e+00 
% Smoothing 

> smoEst.tv=SsfCondDens(gm,tv.capm(tv.mle$parameters,mX=X. 
mtx) , 
+ task="STSMO") 
> names(smoEst. tv) 
[1] "state" "response" "task" 
> par(mfcol=c(2,2)) % plotting 

> plot(gm,type='1',ylab='excess return') 
> title(main='(a) Monthly simple excess returns') 
> plot(smoEst.tv$response,type='1',ylab='rtn') 
> title(main='(b) Expected returns') 
> plot(smoEst.tv$state[,1],type='1',ylab='value') 
> title(main='(c) Alpha(t)') 

> plot(smoEst.tv$state[,2],type='1',ylab='value') 
> title(main='(d) Beta(t)') 

Note that estimates of an and crs are 4.91 x 10~5 and 1.22 x 10~2, respectively. 

These estimates are close to zero, indicating that at and /3, of the time-varying 

market model are essentially constant for the GM stock returns. This is in agreement 

with the fact that the fixed-coefficient market model fits the data well. Figure 11.5 

shows some plots for the time-varying CAPM fit. Part (a) is the monthly simple 

excess returns of GM stock from January 1990 to December 2003. Part (b) is the 

expected returns of GM stock, that is, rt\T, where T = 168 is the sample size. Parts 

APPLICATION 

605 

o 

LD 

<I) 
E o 
"53 m 
IT I 
o 
T 
in 

CD 
O 

CD 
n ^ 
CO O 
> T“ 

0 

50 100 150 

50 100 150 

(b) 

(d) 

Figure 11.5 Time plots of some statistics for time-varying CAPM applied to monthly simple excess 

returns of General Motors stock. S&P 500 composite index return is used as market return: (a) monthly 

simple excess return, (b) expected returns rt\r, (c) at estimate, and (d) p, estimate. 

(c) and (d) are the time plots of the estimates of at and f3t. Given the tightness in 
the vertical scale, these two time plots confirm the assertion that a fixed-coefficient 
market model is adequate for the monthly GM stock return. 

Example 11.3. In this example we reanalyze the series of quarterly earnings 
per share of Johnson & Johnson from 1960 to 1980 using the unobserved com¬ 
ponent model; see Chapter 2 for details of the data. The model considered is 

yt = Ht + Yt+et, et~N(0,cr]-), (11.87) 

where yt is the logarithm of the observed earnings per share, pc, is the local trend 
component satisfying 

Ht+i = fit + rit, r)t ~ N(0, cr*), 

and yt is the seasonal component that satisfies 

(1 + B 4- B" + ZT )yt — u>t, cot ~ (0, o'C0), 

that is, yt = — Xw=1 Yt-j +cot. This model has three parameters—ae, o^, and 
ctw—and is a simple unobserved component model. It can be put in a state-space 

606 

form as 

STATE-SPACE MODELS AND KALMAN FILTER 

" AO+l 
Yt+1 
Yt 
Yt-1 

' 1 
0 
0 
0 

0 
-1 
1 
0 

0 
-1 
0 
1 

0 
-1 
0 
0 

txt 
Yt 
Yt-1 
Yt-2 

+ 

0 " 
" 1 
1 
0 
0 
0 
0  0 

Vt 
a>t 

where the covariance matrix of (r]t, co,)' is diagfcr2, cr2}, and yt = [1,1,0, 0]s, + et; 
see Section 11.3. This is a special case of the structural time series in SsfPack and 
can easily be specified using the command GetSsf Stsm. Performing the maximum- 
likelihood estimation, we obtain (oe, an, aw) = (2.04 x 10-6, 7.27 x 10-2, 2.93 x 
10“2). 

> jnj=scan(file='q-jnj.txt') 
> y=log(jnj) 
% Estimation 
> jnj.m=function(parm){ 
+ parm=exp(parm) 
+ jnj.sea=GetSsfStsm(irregular=parm[1],level=parm[2], 
+ seasonalDummy=c(parm[3],4)) 
+ CheckSsf(jnj.sea) 
+ ) 
> 

> c.start=c(0,0,0) % Starting values 
> jnj.est=SsfFit(c.start,y,"jnj.m") 
> names(jnj.est) 

[1] "parameters" "objective" "message" "grad.norm" "itera¬ 

tions " 

[6] "f.evals" "g.evals" "hessian" "scale" "aux" 

[11] "call" 

> jnjest=exp(jnj.est$parameters) 
> jnjest % estimates 

[1] 2.044516e-06 7.269655e-02 2.931691e-02 
> jnj.ssf=GetSsfStsm(irregular=jnj est[1],level = jnjest[2], 
+ seasonalDummy=c(jnjest[3],4)) % specify the model with esti¬ 
mates 

> CheckSsf(jnj.ssf) 
$mPhi: 

[,1] 
1 
0 
0 
0 
1 

[ / 2 ] 
0 
-1 
1 
0 
1 

[ / 3 ] 
0 
-1 
0 
1 
0 

[, 4] 
0 
-1 
0 
0 
0 

[1,1 
[2, ] 
[3, ] 
[4,] 
[5, ] 

APPLICATION 

$mOmega: 

[,1] 
[1,] 0.005284788 
[2,] 0.000000000 
[3,] 0.000000000 
[4,] 0.000000000 
[5,] 0.000000000 

t,2] [,3] 

0.000000000 0 
0.000859481 0 
0.000000000 0 
0.000000000 0 
0.000000000 0 

[ / 4] [, 5] 
0 0.000000e+00 
0 0.000000e+00 
0 0.000000e+00 
0 0.000000e+00 
0 4.180047e-12 

607 

$mSigma: 

[,1] t/2] [ 

[1,] -1 0 
[2,] 0 -1 
[3,] 0 0 
t4(] 0 0 
[5,] 0 0 

3] [,4] 
0 0 
0 0 
-1 0 
0 -1 
0 0 

$mDelta: 

[,1] 
[1, ] 0 
[2, ] 0 
[3, ] 0 
[4, ] 0 
[5, ] 0 

$mJPhi: 
[1] 0 
$mJOmega: 

[1] 0 
$mJDelta: 

[1] 0 
$mX: 

[1] 0 
$CT: 
[1] 0 
$cX : 

[1] 0 
$cY: 
[1] 1 
$cSt: 

[1] 4 
attr(, "class"): 
[1] "ssf" % below: smoothed components 
> jnj.smo=SsfMomentEst(y,jnj.ssf,task="STSMO") 
> upl=jnj.smo$state.moment[,1]+ 
+ 2*sqrt(jnj.smo$state.variance[,1]) 
> lwl=jnj.smo$state.moment[,1]- 
+ 2*sqrt(jnj.smo$state.variance[,1]) 
> max(upl) % obtain the range for plotting 

608 

STATE-SPACE MODELS AND KALMAN FILTER 

[1] 2.795702 
> min(lwl) 
[1] -0.5948943 
> up=jnj.smo$state.moment[,2]+ 
+ 2 *sqrt(jnj.smo$state.variance[, 2 ] ) 
> lw=jnj.smo$state.moment[,2]- 
+ 2*sqrt(jnj.smo$state.variance[,2]) 
> max(up) 
[1] 0.3788652 
> min(lw) 
[1] -0.3552441 
> par(mfcol=c(2,1)) % plotting 
> plot(tdx,jnj.smo$state.moment[,1],type='1',xlab='year', 
+ ylab='value',ylim=c(-1,3)) 
> lines(tdx,upl,lty=2) 
> lines(tdx,lwl,lty=2) 
> title(main='(a) Trend component') 
> plot(tdx,jnj.smo$state.moment[,2],type='1',xlab='year', 
+ ylab='value',ylim=c(-.5,.5)) 
> lines(tdx,up,lty=2) 
> lines(tdx,lw,lty=2) 
> title(main='(b) Seasonal component') 
% Filtering and smoothing 
> jnj.fil=KalmanFil(y,jnj.ssf,task="STFIL") 
> jnj.smol=KalmanSmo(jnj.fil,jnj.ssf) 
> plot(tdx,jnj.fil$mOut[,1],type='1',xlab='year',ylab='resi') 
> title(main='(a) 1-Step forecast error') 
> plot(tdx,jnj.smol$response.residuals[2:85],type='1', 
+ xlab='year',ylab='resi') 
> title(main='(b) Smoothing residual') 

Figure 11.6 shows the smoothed estimates of the trend and seasonal compo¬ 
nents, that is, fxt\j and yt\j with T = 84, of the data. Of particular interest is that 
the seasonal pattern seems to evolve over time. Also shown are 95% pointwise 
confidence regions of the unobserved components. Figure 11.7 shows the residual 
plots, where part (a) gives the 1-step-ahead forecast errors computed by Kalman 
filter and part (b) is the smoothed response residuals of the fitted model. Thus, 
state-space modeling provides an alternative approach for analyzing seasonal time 
series. It should be noted that the estimated components in Figure 11.6 are not 
unique. They depend on the model specified and constraints used. In fact, there 
are infinitely many ways to decompose an observed time series into unobserved 
components. For instance, one can use a different specification for the seasonal 
component, for example, sesonalTrig in SsfPack, to obtain another decomposi¬ 
tion for the earnings series of Johnson & Johnson. Thus, care must be exercised 
in interpreting the estimated components. However, for forecasting purposes, the 
choice of decomposition does not matter provided that the chosen one is a valid 
decomposition. 

EXERCISES 

609 

Figure 11.6 Smoothed components of fitting model (11.87) to logarithm of quarterly earnings per 

share of Johnson & Johnson from 1960 to 1980: (a) trend component and (b) seasonal component. 

Dotted lines indicate pointwise 95% confidence regions. 

EXERCISES 

11.1. Consider the ARMA(1,1) model yt — 0.8y,_i = at + 0.4af_i with 
at ~ A(0,0.49). Convert the model into a state-space form using (a) 
Akaike’s method, (b) Harvey’s approach, and (c) Aoki’s approach. 

11.2. The file aa-rv-2 0m. txt contains the realized daily volatility series of Alcoa 
stock returns from January 2, 2003, to May 7, 2004; see the example in 
Section 11.1. The volatility series is constructed using 20-minute intradaily 
log returns. 

(a) Fit an ARIMA(0,1,1) model to the log volatility series and write down 

the model. 

(b) Estimate the local trend model in Eqs. (11.1) and (11.2) for the log 
volatility series. What are the estimates of ae and cr^? Obtain time plots 
for the filtered and smoothed state variables with pointwise 95% confi¬ 
dence interval. 

11.3. Consider the monthly simple excess returns of Pfizer stock and the S&P 500 
composite index from January 1990 to December 2003. The excess returns 
are in m-pfesp-ex9003 . txt with Pfizer stock returns in the first column. 

610 STATE-SPACE MODELS AND KALMAN FILTER 

(a) 

I960 1965 1970 1975 1980 

Year 

(b) 

Figure 11.7 Residual series of fitting model (11.87) to logarithm of quarterly earnings per share of 

Johnson & Johnson from 1960 to 1980: (a) 1-step-ahead forecast error v, and (b) smoothed residuals 
of response variable. 

(a) Fit a fixed-coefficient market model to the Pfizer stock return. Write 

down the fitted model. 

(b) Fit a time-varying CAPM to the Pfizer stock return. What are the esti¬ 
mated standard errors of the innovations to the a, and /3? series? Obtain 
time plots of the smoothed estimates of at and fit. 

11.4. Consider the AR(3) model 

xt — (f>]Xt-i + 02Xt-2 + 03*r-3 + , at ~ N(0, 

and suppose that the observed data are 

yt=xt+et, e, ~ (V(0, a]), 

where {e,} and [a,} are independent and the initial values of xj with j < 0 
are independent of e, and a, for t > 0. 

(a) Convert the model into a state-space form. 

(b) If E(et) = c, which is not zero, what is the corresponding state-space 

form for the system? 

REFERENCES 

611 

11.5. The file m-ppiaco4709. txt contains year, month, day, and U.S. producer 
price index (PPI) from January 1947 to November 2009. The index is for all 
commodities and not seasonally adjusted. Let zt = ln(Z,) - ln(Zf_i), where 
Z, is the observed monthly PPI. It turns out that an AR(3) model is adequate 
for it if the minor seasonal dependence is ignored. Let yt be the sample 
mean-corrected series of Zt- 

(a) Fit an AR(3) model to yt and write down the fitted model. 

(b) Suppose that y, has independent measurement errors so that yt = xt + et, 
where xt is a zero-mean AR(3) process and Var(e;) = a2. Use a state- 
space form to estimate parameters, including the innovational variances 
to the state and cr2. Write down the fitted model and obtain a time plot of 
the smoothed estimate of xt. Also, show the time plot of filtered response 
residuals of the fitted state-space model. 

REFERENCES 

Akaike, H. (1975). Markovian representation of stochastic processes by canonical variables. 

SIAM Journal on Control 13: 162-173. 

Aoki, M. (1987). State Space Modeling of Time Series. Springer, New York. 

Anderson, B. D. O. and Moore, J. B. (1979). Optimal Filtering. Prentice-Hall, Englewood 

Cliffs, NJ. 

Chan, N. H. (2002). Time Series: Applications to Finance. Wiley, Hoboken, NJ. 

de Jong, P. (1989). Smoothing and interpolation with the state space model. Journal of the 

American Statistical Association 84: 1085-1088. 

Durbin, J. and Koopman, S. J. (2001). Time Series Analysis by State Space Methods. Oxford 

University Press, Oxford, UK. 

Hamilton, J. (1994). Time Series Analysis. Princeton University Press, Princeton, NJ. 

Harvey, A. C. (1993). Time Series Models, 2nd ed. Harvester Wheatsheaf, Hemel Hempstead, 

UK. 

Kalman, R. E. (1960). A new approach to linear filtering and prediction problems. Journal 

of Basic Engineering, Transactions ASMA, Series D 82: 35-45. 

Kim, C. J. and Nelson, C. R. (1999). State Space Models with Regime Switching. Academic, 

New York. 

Kitagawa, G. and Gersch, W. (1996). Smoothness Priors Analysis of Time Series. Springer, 

New York. 

Koopman, S. J. (1993). Disturbance smoother for state space models. Biometrika 80: 

117-126. 

Koopman, S. J., Shephard, N., and Doomik, J. A. (1999). Statistical algorithms for models in 
state-space form using SsfPack 2.2. Econometrics Journal 2: 113-166. Also available 
at http://www.ssfpack.com/. 

Shumway, R. H. and Staffer, D. S. (2000). Time Series Analysis and Its Applications. 

Springer, New York. 

West, M. and Harrison, J. (1997). Bayesian Forecasting and Dynamic Models, 2nd ed. 

Springer, New York. 

' 

■ 

CHAPTER 12 

Markov Chain Monte Carlo Methods 
with Applications 

Advances in computing facilities and computational methods have dramatically 
increased our ability to solve complicated problems. The advances also extend the 
applicability of many existing econometric and statistical methods. Examples of 
such achievements in statistics include the Markov chain Monte Carlo (MCMC) 
method and data augmentation. These techniques enable us to make some statistical 
inference that was not feasible just a few years ago. In this chapter, we introduce 
the ideas of MCMC methods and data augmentation that are widely applicable 
in finance. In particular, we discuss Bayesian inference via Gibbs sampling and 
demonstrate various applications of MCMC methods. Rapid developments in the 
MCMC methodology make it impossible to cover all the new methods available in 
the literature. Interested readers are referred to some recent books on Bayesian and 
empirical Bayesian statistics (e.g., Carlin and Louis, 2000; Gelman, Carlin, Stern, 
and Rubin, 2003). 

For applications, we focus on issues related to financial econometrics. The 
demonstrations shown in this chapter represent only a small fraction of all possible 
applications of the techniques in finance. As a matter of fact, it is fair to say that 
Bayesian inference and the MCMC methods discussed here are applicable to most, 
if not all, of the studies in financial econometrics. 

We begin the chapter by reviewing the concept of a Markov process. Consider 
a stochastic process {Xr}, where each Xt assumes a value in the space 0. The 
process {Xt} is a Markov process if it has the property that, given the value of Xt, 
the values of Xh, h>t, do not depend on the values Xs, 5 < t. In other words, 
{Xt} is a Markov process if its conditional distribution function satisfies 

P(Xh\Xt,s<t) = P(Xh\Xt), h>t. 

Analysis of Financial Time Series, Third Edition, By Ruey S. Tsay 
Copyright © 2010 John Wiley & Sons, Inc. 

613 

614 

MARKOV CHAIN MONTE CARLO METHODS WITH APPLICATIONS 

If {X,} is a discrete-time stochastic process, then the prior property becomes 

P(Xh\Xt,Xt„u...) = P(Xh\Xt), h>t. 

Let A be a subset of 0. The function 

P,(0,h,A) = P(Xh e A\Xt =0), h>t 

is called the transition probability function of the Markov process. If the transi¬ 
tion probability depends on h — t, but not on t, then the process has a stationary 
transition distribution. 

12.1 MARKOV CHAIN SIMULATION 

Consider an inference problem with parameter vector 0 and data X, where 0 e 0. 
To make inference, we need to know the distribution P(0\X). The idea of Markov 
chain simulation is to simulate a Markov process on 0, which converges to a 
stationary transition distribution that is P{0\X). 

The key to Markov chain simulation is to create a Markov process whose station¬ 
ary transition distribution is a specified P(0\X) and run the simulation sufficiently 
long so that the distribution of the current values of the process is close enough to 
the stationary transition distribution. It turns out that, for a given P{0\X), many 
Markov chains with the desired property can be constructed. We refer to methods 
that use Markov chain simulation to obtain the distribution F(0|X) as MCMC 
methods. 

The development of MCMC methods took place in various forms in the sta¬ 
tistical literature. Consider the problem of “missing value” in data analysis. Most 
statistical methods discussed in this book were developed under the assumption of 
“complete data” (i.e., there is no missing value). For example, in modeling daily 
volatility of an asset return, we assume that the return data are available for all 
trading days in the sample period. What should we do if there is a missing value? 
Dempster, Laird, and Rubin (1977) suggest an iterative method called the 
Expectation-Maximization (EM) algorithm to solve the problem. The method 
consists of two steps. First, if the missing value were available, then we could use 
methods of complete-data analysis to build a volatility model. Second, given the 
available data and the fitted model, we can derive the statistical distribution of the 
missing value. A simple way to fill in the missing value is to use the conditional 
expectation of the derived distribution of the missing value. In practice, one can 
start the method with an arbitrary value for the missing value and iterate the 
procedure for many many times until convergence. The first step of the prior 
procedure involves performing the maximum-likelihood estimation of a specified 
model and is called the M-step. The second step is to compute the conditional 
expectation of the missing value and is called the E-step. 

Tanner and Wong (1987) generalize the EM algorithm in two ways. First, they 
introduce the idea of iterative simulation. For instance, instead of using the con¬ 
ditional expectation, one can simply replace the missing value by a random draw 

GIBBS SAMPLING 

615 

from its derived conditional distribution. Second, they extend the applicability of 
the EM algorithm by using the concept of data augmentation. By data augmenta¬ 
tion, we mean adding auxiliary variables to the problem under study. It turns out 
that many of the simulation methods can often be simplified or speeded up by data 
augmentation; see the application sections of this chapter. 

12.2 GIBBS SAMPLING 

Gibbs sampling (or Gibbs sampler) of Geman and Geman (1984) and Gelfand and 
Smith (1990) is perhaps the most popular MCMC method. We introduce the idea 
of Gibbs sampling by using a simple problem with three parameters. Here the word 
parameter is used in a very general sense. A missing data point can be regarded 
as a parameter under the MCMC framework. Similarly, an unobservable variable 
such as the “true” price of an asset can be regarded as N parameters when there 
are N transaction prices available. This concept of parameter is related to data 
augmentation and becomes apparent when we discuss applications of the MCMC 
methods. 

Denote the three parameters by d\, 02, and 6*3. Let X be the collection of available 
data and M the entertained model. The goal here is to estimate the parameters so 
that the fitted model can be used to make inference. Suppose that the likelihood 
function of the model is hard to obtain, but the three conditional distributions of 
a single parameter given the others are available. In other words, we assume that 
the following three conditional distributions are known: 

/l (01102, 03, X, M), /z(02103, 0!, X, M), /3(03101,02, X, M), (12.1) 

where fi{9i\6j^i, X, M) denotes the conditional distribution of the parameter 9, 
given the data, the model, and the other two parameters. In application, we do not 
need to know the exact forms of the conditional distributions. What is needed is the 
ability to draw a random number from each of the three conditional distributions. 
Let 02,0 and 03q be two arbitrary starting values of O2 and $3. The Gibbs sampler 

proceeds as follows: 

1. Draw a random sample from f\ (0j |02,o, 03,0, X, M). Denote the random draw 

by 0i,i. 

2. Draw a random sample from /2(02|03,o, 0i,i, X, M). Denote the random draw 

by 02,i. 

3. Draw a random sample from /3 (03 |0i, 1,02,1, X, M). Denote the random draw 

by 03,L 

This completes a Gibbs iteration and the parameters become 6\j, 02,1, and 03j. 

Next, using the new parameters as starting values and repeating the prior itera¬ 
tion of random draws, we complete another Gibbs iteration to obtain the updated 

616 

MARKOV CHAIN MONTE CARLO METHODS WITH APPLICATIONS 

parameters 0],2, 6*2,2, and 6*3,2- We can repeat the previous iterations for m times to 
obtain a sequence of random draws: 

02,1, 6*3,1), . . . , (01,m, 02,m, 03,m)- 

Under some regularity conditions, it can be shown that, for a sufficiently large 
m, (0i,m, 02,m, 03,m) is approximately equivalent to a random draw from the joint 
distribution /(0i, 02, 9j,\X, M) of the three parameters. The regularity conditions 
are weak; they essentially require that for an arbitrary starting value (0i,o, 02,o, 03,o), 
the prior Gibbs iterations have a chance to visit the full parameter space. The actual 
convergence theorem involves using the Markov chain theory; see Tierney (1994). 
In practice, we use a sufficiently large n and discard the first m random draws 

of the Gibbs iterations to form a Gibbs sample, say, 

(01 ,m+1,02,m+l, 03,m+l), • • • , (01,n, 02,n, 03,n)- 

(12.2) 

Since the previous realizations form a random sample from the joint distribution 
/(0i> 02 , 031^6, M), they can be used to make inference. For example, a point 
estimate of 0,- and its variance are 

0/ = 

m  E e‘.j- 
j=m+1 

07 

m 

n 

y E Wij-Qi)2- 02-3) 
j=m+1 

The Gibbs sample in Eq. (12.2) can be used in many ways. For example, if 
we are interested in testing the null hypothesis Hq : By = 02 versus the alternative 
hypothesis Ha : 0\ 76 6*2, then we can simply obtain the point estimate of 0 = 
01 — 02 and its variance as 

m  E ~ 
j=m +1 

n — m — 1  E (»1.i - ft J - 5)2. 

j=m+1 

The null hypothesis can then be tested by using the conventional r-ratio statistic 
t = 0/a. 

Remark. The first m random draws of a Gibbs sampling, which are discarded, 
are commonly referred to as the burn-in sample. The burn-ins are used to ensure 
that the Gibbs sample in Eq. (12.2) is indeed close enough to a random sample 
from the joint distribution /(0i, 02, 03|Z, M). □ 

Remark. The method discussed before consists of running a single long chain 
and keeping all random draws after the bum-ins to obtain a Gibbs sample. Alter¬ 
natively, one can mn many relatively short chains using different starting values 
and a relatively small n. The random draw of the last Gibbs iteration in each chain 
is then used to form a Gibbs sample. □ 

BAYESIAN INFERENCE 

617 

From the prior introduction, Gibbs sampling has the advantage of decomposing 
a high-dimensional estimation problem into several lower dimensional ones via 
full conditional distributions of the parameters. At the extreme, a high-dimensional 
problem with N parameters can be solved iteratively by using N univariate con¬ 
ditional distributions. This property makes the Gibbs sampling simple and widely 
applicable. However, it is often not efficient to reduce all the Gibbs draws into a 
univariate problem. When parameters are highly correlated, it pays to draw them 
jointly. Consider the three-parameter illustrative example. If 0\ and #2 are highly 
correlated, then one should employ the conditional distributions f(0\, $21#3, X, M) 
and f-i(0s\6\, 62, X, M) whenever possible. A Gibbs iteration then consists of (a) 
drawing jointly ((fi, @2) given 62, and (b) drawing 63 given (6\, 62). For more infor¬ 
mation on the impact of parameter correlations on the convergence rate of a Gibbs 
sampler, see Liu, Wong, and Kong (1994). 

In practice, convergence of a Gibbs sample is an important issue. The theory only 
states that the convergence occurs when the number of iterations m is sufficiently 
large. It provides no specific guidance for choosing m. Many methods have been 
devised in the literature for checking the convergence of a Gibbs sample. But there 
is no consensus on which method performs best. In fact, none of the available 
methods can guarantee 100% that the Gibbs sample under study has converged for 
all applications. Performance of a checking method often depends on the problem 
at hand. Care must be exercised in a real application to ensure that there is no 
obvious violation of the convergence requirement; see Carlin and Louis (2000) 
and Gelman et al. (2003) for convergence checking methods. In application, it is 
important to repeat the Gibbs sampling several times with different starting values 
to ensure that the algorithm has converged. 

12.3 BAYESIAN INFERENCE 

Conditional distributions play a key role in Gibbs sampling. In the statistical 
literature, these conditional distributions are referred to as conditional posterior 
distributions because they are distributions of parameters given the data, other 
parameter values, and the entertained model. In this section, we review some well- 
known posterior distributions that are useful in using MCMC methods. 

12.3.1 Posterior Distributions 

There are two approaches to statistical inference. The first approach is the classical 
approach based on the maximum-likelihood principle. Here a model is estimated by 
maximizing the likelihood function of the data, and the fitted model is used to make 
inference. The other approach is Bayesian inference that combines prior belief with 
data to obtain posterior distributions on which statistical inference is based. Histor¬ 
ically, there were heated debates between the two schools of statistical inference. 
Yet both approaches have proved to be useful and are now widely accepted. The 
methods discussed so far in this book belong to the classical approach. However, 

618 

MARKOV CHAIN MONTE CARLO METHODS WITH APPLICATIONS 

Bayesian solutions exist for all of the problems considered. This is particularly so in 
recent years with the advances in MCMC methods, which greatly improve the fea¬ 
sibility of Bayesian analysis. Readers can revisit the previous chapters and derive 
MCMC solutions for the problems considered. In most cases, the Bayesian solu¬ 
tions are similar to what we had before. In some cases, the Bayesian solutions might 
be advantageous. For example, consider the calculation of value at risk in Chapter 
7. A Bayesian solution can easily take into consideration the parameter uncertainty 
in VaR calculation. However, the approach requires intensive computation. 

Let 0 be the vector of unknown parameters of an entertained model and X 
be the data. Bayesian analysis seeks to combine knowledge about the parameters 
with the data to make inference. Knowledge of the parameters is expressed by 
specifying a prior distribution for the parameters, which is denoted by P(0). For 
a given model, denote the likelihood function of the data by f(X\0). Then by the 
definition of conditional probability, 

f(0\X) = 

f(B,X) 

f(X\0)P(0) 

f{X) 

f(X) 

(12.4) 

where the marginal distribution f(X) can be obtained by 

f(X) = J f(X,0)dO = J f(X\0)P(0)d0. 

The distribution f(0\X) in Eq. (12.4) is called the posterior distribution of 0. In 
general, we can use Bayes’s rule to obtain 

f(0\X)txf(X\0)P(0), (12.5) 

where P(0) is the prior distribution and f(X\0) is the likelihood function. From 
Eq. (12.5), making statistical inference based on the likelihood function f(X\0) 
amounts to using a Bayesian approach with a constant prior distribution. 

12.3.2 Conjugate Prior Distributions 

Obtaining the posterior distribution in Eq. (12.4) is not simple in general, but there 
are cases in which the prior and posterior distributions belong to the same family 
ot distributions. Such a prior distribution is called a conjugate prior distribution. 
For MCMC methods, use of conjugate priors means that a closed-form solution 
for the conditional posterior distributions is available. Random draws of the Gibbs 
sampler can then be obtained by using the commonly available computer routines 
of probability distributions. In what follows, we review some well-known conjugate 
priors. For more information, readers are referred to textbooks on Bayesian statistics 
(e.g., DeGroot 1970, Chapter 9). 

Result 12.1. Suppose that x\,...,xn form a random sample from a normal 
distribution with mean p, which is unknown, and variance a1, which is known 

BAYESIAN INFERENCE 

619 

and positive. Suppose that the prior distribution of /x is a normal distribution with 
mean /x0 and variance a2. Then the posterior distribution of p given the data and 
prior is normal with mean /x* and variance a2 given by 

P* 

cr2p0 + no2x 

o2 + no2 

and 

2 _ °1(yl 

o - o ’ 
<7Z + not: 

where x = Xa=i xi/n 's the sample mean. 

In Bayesian analysis, it is often convenient to use the precision parameter r] = 
1/cr2 (i.e., the inverse of the variance a2). Denote the precision parameter of the 
prior distribution by r/0 = 1/cr2 and that of the posterior distribution by 17* = 1/cr2. 
Then Result 12.1 can be rewritten as 

. , Vo nr] _ 
V* = V0 + nrj and /x* = — x p0 H-x a. 

V* V* 

For the normal random sample considered, data information about /x is contained in 
the sample mean x, which is the sufficient statistic of /x. The precision of x is n/o2 
= nr]. Consequently, Result 12.1 says that (a) precision of the posterior distribution 
is the sum of the precisions of the prior and the data, and (b) the posterior mean is 
a weighted average of the prior mean and sample mean with weight proportional 
to the precision. The two formulas also show that the contribution of the prior 
distribution is diminishing as the sample size n increases. 

A multivariate version of Result 12.1 is particularly useful in MCMC methods 

when linear regression models are involved; see Box and Tiao (1973). 

Result 12.1a. Suppose that x\, ...,xn form a random sample from a multi¬ 
variate normal distribution with mean vector /t and a known covariance matrix 
E. Suppose also that the prior distribution of /x is multivariate normal with mean 
vector p0 and covariance matrix E0. Then the posterior distribution of ft is also 
multivariate normal with mean vector /x* and covariance matrix E*, where 

E*1 = E"1 +n'Z~1 and /x* = £*( Y,-'fi0 + n?,~lx), 

where x = x,/n is the sample mean, which is distributed as a multivariate 
normal with mean p and covariance matrix 'L/n. Note that nl is the precision 
matrix of x and E'1 is the precision matrix of the prior distribution. 

A random variable r] has a gamma distribution with positive parameters a and 

if its probability density function is 

f(r]\a, 0) = r]a~xe~p\ rj > 0, 

r» 

where T(q;) is a gamma function. For this distribution, E(r]) — a/fi and Var(?7) = 

a/?2. 

620 

MARKOV CHAIN MONTE CARLO METHODS WITH APPLICATIONS 

Result 12.2. Suppose that x\,...,xn form a random sample from a normal 
distribution with a given mean p, and an unknown precision r]. If the prior distri¬ 
bution of r] is a gamma distribution with positive parameters a and ft, then the 
posterior distribution of 17 is a gamma distribution with parameters a + (n/2) and 
P + E/U'fe - ^)2/2- 

A random variable 9 has a beta distribution with positive parameters a and ft 

if its probability density function is 

f(m P) = F7E7^“~1(1 ~ 0 < e < 1. 

r(a)ro0) 

The mean and variance of 9 are E{9) = a/(a + ft) and Var(9) = or/3/[(or + ft)2(a + 
p + m 

Result 12.3. Suppose that x\,...,xn form a random sample from a Bernoulli 
distribution with parameter 9. If the prior distribution of 9 is a beta distribution 
with given positive parameters a and ft, then the posterior of 6 is a beta distribution 
with parameters a + ]T"=i xt and ft + n - ]T"=i xt. 

Result 12.4. Suppose that x\,...,xn form a random sample from a Poisson 
distribution with parameter A. Suppose also that the prior distribution of A is a 
gamma distribution with given positive parameters a and ft. Then the posterior 
distribution of A is a gamma distribution with parameters a + xt and ft + n. 

Result 12.5. Suppose that x\, ...,xn form a random sample from an exponential 
distribution with parameter A. If the prior distribution of A is a gamma distribution 
with given positive parameters a and ft, then the posterior distribution of A is a 
gamma distribution with parameters a + n and ft + Ya=\ xi- 

A random variable X has a negative binomial distribution with parameters m 

and A, where m > 0 and 0 < A < 1, if X has a probability mass function 

p(n\m, A) = 

m + n — 1 
n 

^ Am(l — A)” 

if n = 0, 1,..., 

0 

otherwise. 

A simple example of negative binomial distribution in finance is how many MBA 
graduates a firm must interview before finding exactly m “right candidates” for its 
m openings, assuming that the applicants are independent and each applicant has 
a probability A of being a perfect fit. Denote the total number of interviews by Y. 
Then X = Y — m is distributed as a negative binomial with parameters m and A. 

Result 12.6. Suppose that x\,...,xn form a random sample from a negative 
binomial distribution with parameters m and A, where m is positive and fixed. If 

BAYESIAN INFERENCE 

621 

the prior distribution of A. is a beta distribution with positive parameters a and p, 
then the posterior distribution of A is a beta distribution with parameters a + mn 
and p + Ya=\ Xi. 

Next we consider the case of a normal distribution with an unknown mean ji 
and an unknown precision rj. The two-dimensional prior distribution is partitioned 
as P(/x, 77) = P(jx\rj)P(rj). 

Result 12.7. Suppose that x\,... ,xn form a random sample from a normal dis¬ 
tribution with an unknown mean /x and an unknown precision rj. Suppose also that 
the conditional distribution of /x given rj = rj0 is a normal distribution with mean 
ji0 and precision r0r\0 and the marginal distribution of rj is a gamma distribution 
with positive parameters a and ft. Then the conditional posterior distribution of /x 
given rj = ij0 is a normal distribution with mean /x* and precision rj*, 

/x* =- and rj* = (r0 + n)rj0, 

Tojio + nx 

T0 +n 

where x = Xa=i xi/n is the sample mean, and the marginal posterior distribution 
of rj is a gamma distribution with parameters a + (n/2) and /3*, where 

P* — ft + \ ~ x)2 + 

r0n(x - ji0)2 

2(z0 + n) 

When the conditional variance of a random variable is of interest, an inverted 
chi-squared distribution (or inverse chi-squared) is often used. A random variable Y 
has an inverted chi-squared distribution with v degrees of freedom if 1 /Y follows a 
chi-squared distribution with the same degrees of freedom. The probability density 

function of Y is 

f(y\v) = 

n-v/2 

__ —(u/2+l) —1/(2 y) 
r(v/2)y 

y > 0. 

For this distribution, we have E{Y) = \/{v — 2) if v >2 and Var(F) = 2/[{v — 

2)2(v — 4)] if v>4. 

Result 12.8. Suppose that a\,...,an form a random sample from a normal 
distribution with mean zero and variance a2. Suppose also that the prior dis¬ 
tribution of a2 is an inverted chi-squared distribution with v degrees of free¬ 
dom [i.e., (vX)/o2 ~ xjf, where A > 0], Then the posterior distribution of a2 is 
also an inverted chi-squared distribution with v + n degrees of freedom—that is, 

(vX + XXi a2)/a2 ~ Xv+n- 

622 

MARKOV CHAIN MONTE CARLO METHODS WITH APPLICATIONS 

12.4 ALTERNATIVE ALGORITHMS 

In many applications, there are no closed-form solutions for the conditional poste¬ 
rior distributions. But many clever alternative algorithms have been devised in the 
statistical literature to overcome this difficulty. In this section, we discuss some of 
these algorithms. 

12.4.1 Metropolis Algorithm 

This algorithm is applicable when the conditional posterior distribution is known 
except for a normalization constant; see Metropolis and Ulam (1949) and Metropo¬ 
lis et al. (1953). Suppose that we want to draw a random sample from the distribu¬ 
tion f(0\X), which contains a complicated normalization constant so that a direct 
draw is either too time-consuming or infeasible. But there exists an approximate 
distribution for which random draws are easily available. The Metropolis algorithm 
generates a sequence of random draws from the approximate distribution whose 
distributions converge to f(0\X). The algorithm proceeds as follows: 

1. Draw a random starting value 0O such that f(0o\X) > 0. 

2. For t = 1,2,..., 

a. Draw a candidate sample 0* from a known distribution at iteration t given 
the previous draw 0f_i. Denote the known distribution by Jt(0t\0t-X), 
which is called a jumping distribution in Gelman et al. (2003). It is also 
referred to as a proposal distribution. The jumping distribution must be 
symmetric—that is, 7,(0,|0y) = J,{Oj\0i) for all 0;, 0j, and t. 

b. Calculate the ratio 

no*\x) 
f{o,-x\xy 

c. Set 

0t = 

0* with probability min(r, 1), 
0 t—i otherwise. 

Under some regularity conditions, the sequence {0;} converges in distribution to 
f(0\X)\ see Gelman et al. (2003). 

Implementation of the algorithm requires the ability to calculate the ratio r for 
all 0* and 0t_x, to draw 0* from the jumping distribution, and to draw a random 
iealization from a uniform distribution to determine the acceptance or rejection of 
0*. The normalization constant of f(0\X) is not needed because only a ratio is 
used. 

The acceptance and rejection rule of the algorithm can be stated as follows: 
(i) if the jump from 0f_[ to 0* increases the conditional posterior density, then 
accept 0* as 0t; (ii) if the jump decreases the posterior density, then set 0, = 0* 

ALTERNATIVE ALGORITHMS 

623 

with probability equal to the density ratio r, and set 0, = 0t-\ otherwise. Such a 
procedure seems reasonable. 

Examples of symmetric jumping distributions include the normal and Student- 
t distributions for the mean parameter. For a given covariance matrix, we have 
f{6i 16 j) = f(0j 10j), where f{0\0O) denotes a multivariate normal density function 
with mean vector 0O. 

12.4.2 Metropolis-Hasting Algorithm 

Hasting (1970) generalizes the Metropolis algorithm in two ways. First, the jumping 
distribution does not have to be symmetric. Second, the jumping rule is modified to 

f{0*\X)/Jt(9*\Ot-i) = f(0*\X)Jt(0t-i\0*) 

f(6t-l |X)M(*f-l|0*) f(0t-i\X)Jt(0*\0t-i)' 

This modified algorithm is referred to as the Metropolis-Hasting algorithm. Tierney 
(1994) discusses methods to improve computational efficiency of the algorithm. 

12.4.3 Griddy Gibbs 

In financial applications, an entertained model may contain some nonlinear param¬ 
eters (e.g., the moving-average parameters in an ARMA model or the GARCH 
parameters in a volatility model). Since conditional posterior distributions of nonlin¬ 
ear parameters do not have a closed-form expression, implementing a Gibbs sampler 
in this situation may become complicated even with the Metropolis-Hasting algo¬ 
rithm. Tanner (1996) describes a simple procedure to obtain random draws in 
a Gibbs sampling when the conditional posterior distribution is univariate. The 
method is called the Griddy Gibbs sampler and is widely applicable. However, the 
method could be inefficient in a real application. 

Fet Qi be a scalar parameter with conditional posterior distribution 
f{0i\X, 0-i), where 0-i is the parameter vector after removing (9,. For instance, 
if 0 = {6\, 62, OsY, then 0-\ = (6*2, 63)'. The Griddy Gibbs proceeds as follows: 

1. Select a grid of points from a properly selected interval of 9,, say, 0t\ < 
0i2 < ■ ■ ■ < dim• Evaluate the conditional posterior density function to obtain 
wj = f(Pij\X> °-i) for j = 1,..., m. 

2. Use w\, ... ,wm to obtain an approximation to the inverse cumulative distri¬ 

bution function (CDF) of f(9j \X, 0-i). 

3. Draw a uniform (0,1) random variate and transform the observation via the 

approximate inverse CDF to obtain a random draw for 0;. 

Some remarks on the Griddy Gibbs are in order. First, the normalization con¬ 
stant of the conditional posterior distribution /(0,|A, 0-i) is not needed because 
the inverse CDF can be obtained from {Wj}J=l directly. Second, a simple approx¬ 
imation to the inverse CDF is a discrete distribution for {0/7}J=1 with probabil¬ 

ity p(0ij) = u)j/J2v=1 Wv- Third, in a real application, selection of the interval 

624 

MARKOV CHAIN MONTE CARLO METHODS WITH APPLICATIONS 

[On, 0im\ for the parameter 9j must be checked carefully. A simple checking proce¬ 
dure is to consider the histogram of the Gibbs draws of Oj. If the histogram indicates 
substantial probability around On or 0im, then the interval must be expanded. How¬ 
ever, if the histogram shows a concentration of probability inside the interval 
[0n,0jm], then the interval is too wide and can be shortened. If the interval is 
too wide, then the Griddy Gibbs becomes inefficient because most of wj would be 
zero. Finally, the Griddy Gibbs or Metropolis-Hasting algorithm can be used in a 
Gibbs sampling to obtain random draws of some parameters. 

12.5 LINEAR REGRESSION WITH TIME SERIES ERRORS 

We are ready to consider some specific applications of MCMC methods. Examples 
discussed in the next few sections are for illustrative purposes only. The goal here 
is to highlight the applicability and usefulness of the methods. Understanding these 
examples can help readers gain insights into applications of MCMC methods in 
finance. 

The first example is to estimate a regression model with serially correlated 
errors. This is a topic discussed in Chapter 2, where we use SCA to perform the 
estimation. A simple version of the model is 

yt — A) + P\Xu + ■ • • + PkXkt + Zt, 

Zt = <pzt-1 + at, 

where yt is the dependent variable, Xjt are explanatory variables that may contain 
lagged values of yt, and zt follows a simple AR(1) model with {a,} being a sequence 
of independent and identically distributed normal random variables with mean zero 
and variance a2. Denote the parameters of the model by 0 = {f}', 0, a2)', where 
ft = (A), fti, • • • , and let x, = (1, x\t, ..., x^t)' be the vector of all regressors 
at time t, including a constant of unity. The model becomes 

yt=x'tp+zt, Zt =4>zt-i +at, f = (12.6) 

where n is the sample size. 

A natural way to implement Gibbs sampling in this case is to iterate between 
regression estimation and time series estimation. If the time series model is known, 
we can estimate the regression model easily by using the least-squares method. 
However, if the regression model is known, we can obtain the time series zt by 
using zt =yt - x'rfi and use the series to estimate the AR(1) model. Therefore, we 
need the following conditional posterior distributions: 

f(P\Y,X,(j),cr2), f{<f>\Y,X, 0,CT2), f (cr21Y, X, /}, 0), 

where Y = (yi,...,yn) and X denotes the collection of all observations of 
explanatory variables. 

LINEAR REGRESSION WITH TIME SERIES ERRORS 

625 

We use conjugate prior distributions to obtain closed-form expressions for the 

conditional posterior distributions. The prior distributions are 

P ~ N(fi0, T0), 0 ~ N(cp0, <7%), V-\ ~ Xv> (12.7) 

cr^ 

where again ~ denotes distribution, and Pa, T0, X, v, 0O, and a2 are known quan¬ 
tities. These quantities are referred to as hyperparameters in Bayesian inference. 
Their exact values depend on the problem at hand. Typically, we assume that Pa 
= 0, (f>0 = 0, and T0 is a diagonal matrix with large diagonal elements. The prior 
distributions in Eq. (12.7) are assumed to be independent of each other. Thus, we 
use independent priors based on the partition of the parameter vector 6. 

The conditional posterior distribution /(P\Y, X, 0, cr2) can be obtained by using 

Result 12.1a of Section 12.3. Specifically, given 0, we define 

y0,t = yt - <f>yt-u x0tt =xt 

Using Eq. (12.6), we have 

ya,t = P'x0,t +«r> t = 2,...,n. (12.8) 

Under the assumption of {at}, Eq. (12.8) is a multiple linear regression. Therefore, 
information of the data about the parameter vector ft is contained in its least-squares 

estimate 

-l 

x0,tx0tt 

0=2 

which has a multivariate normal distribution 

~ /V 

E 

0=2 

Xo,tX0t t 

Using Result 12.1a, the posterior distribution of /?, given the data, 0, and a2, is 

multivariate normal. We write the result as 

(0\Y,X,<l>,cr)~N(P„T*), 

(12.9) 

where the parameters are given by 

v-i _ Za=2*QTX0T , T-i 
— 9 ‘ ^ O ’ 

P* — 

^2t=2Xo,txo,t'£ , ^,-1 

-P + KlPo 

a1 

626 

MARKOV CHAIN MONTE CARLO METHODS WITH APPLICATIONS 

Next, consider the conditional posterior distribution of 0 given /?, cr1 2 3 4 5, and the 

data. Because P is given, we can calculate zt = yt — P'xt for all. t and consider 
the AR(1) model 

Zt — 4>zt-i + 

t = 2,..., n. 

The information of the likelihood function about 0 is contained in the least-squares 
estimate 

which is normally distributed with mean 0 and variance cr2(Y^=2 zf-j)-1- Based 
on Result 12.1, the posterior distribution of 0 is also normal with mean 0* and 
variance a2, where 

ec. = 

r" 72 
2—,t=2 zt-1 

cr- 

+ cr; 

0* = < 

' V” 72 

Z^r=2 U- -0 + ct0 20o 

(12.10) 

Finally, turn to the posterior distribution of o2 given /?, 0, and the data. Because 

P and 0 are known, we can calculate 

at — zt — <pzt-\, Zt=yt-P'xt, t — 

By Result 12.8, the posterior distribution of a2 is an inverted chi-squared 
distribution—that is, 

VX + a} 2 
-~ Xv+(n~-D> (12.11) 

where denotes a chi-squared distribution with k degrees of freedom. 

Using the three conditional posterior distributions in Eqs. (12.9)—(12.11), we 

can estimate Eq. (12.6) via Gibbs sampling as follows: 

1. Specify the hyperparameter values of the priors in Eq. (12.7). 

2. Specify arbitrary starting values for p, 0, and cr2 (e.g., the ordinary least- 

squares estimate of p without time series errors). 

3. Use the multivariate normal distribution in Eq. (12.9) to draw a random 

realization for p. 

4. Use the univariate normal distribution in Eq. (12.10) to draw a random real¬ 

ization for 0. 

5. Use the chi-squared distribution in Eq. (12.11) to draw a random realization 

for cr2. 

LINEAR REGRESSION WITH TIME SERIES ERRORS 

627 

Repeat steps 3-5 for many iterations to obtain a Gibbs sample. The sample means 
are then used as point estimates of the parameters of model (12.6). 

Example 12.1. As an illustration, we revisit the example of U.S. weekly inter¬ 
est rates of Chapter 2. The data are the 1-year and 3-year Treasury constant maturity 
rates from January 5, 1962, to April 10, 2009, and are obtained from the Federal 
Reserve Bank of St. Louis. Because of unit-root nonstationarity, the dependent and 
independent variables are 

1. c3t — r3t — which is the weekly change in 3-year maturity rate, 

2. c\t = rit — 1, which is the weekly change in 1-year maturity rate, 

where the original interest rates rlt are measured in percentages. In Chapter 2, we 
employed a linear regression model with an MA(1) error for the data. Here we 
consider an AR(2) model for the error process. Using the traditional approach in 
R, we obtain the model 

c3t = 0.782cir + zt, zt = 0.183zf_i - 0.036zf_2 + at, (12.12) 

where oa — 0.068. Standard errors of the coefficient estimates of Eq. (12.12) 
are 0.0075, 0.0201, and 0.0201, respectively. Except for a marginally significant 
residual ACF at lags 4 and 6, the prior model seems adequate. 

Writing the model as 

c3t=Pcit+zt, zt = <p\zt-i + (f>2Zt-2 + at, (12.13) 

where {at} is an independent sequence of N(0, o2) random variables, we estimate 
the parameters by Gibbs sampling. The prior distributions used are 

~ N(0, 4), 0 ~ N[0, diag(0.25, 0.16)], (vk)/a2 - (10 x 0.05)/ct2 ~ Xio- 

The initial parameter estimates are obtained by the ordinary least-squares method 
[i.e., by using a two-step procedure of fitting the linear regression model first, then 
fitting an AR(2) model to the regression residuals]. Since the sample size 2466 is 
large, the initial estimates are close to those given in Eq. (12.12). We iterated the 
Gibbs sampling for 2100 iterations but discard results of the first 100 iterations. 
Table 12.1 gives the posterior means and standard errors of the parameters. From 
the table, the posterior mean of a is approximately 0.069. Figure 12.1 shows the 
time plots of the 2000 Gibbs draws of the parameters. The plots show that the draws 
are stable. Figure 12.2 gives the histogram of the marginal posterior distribution of 

each parameter. 

We repeated the Gibbs sampling with different initial values but obtained similar 
results. The Gibbs sampling appears to have converged. From Table 12.1, the 
posterior means are close to the estimates of Eq. (12.12).This is expected as the 
sample size is large and the model is relatively simple. 

628 

MARKOV CHAIN MONTE CARLO METHODS WITH APPLICATIONS 

TABLE 12.1 Posterior Means and Standard Errors of Model (12.13) 
Estimated by Gibbs Sampling with 2100 Iterations" 

Parameter 

Mean 
Standard error 

P 

0.793 
0.008 

01 

0.184 
0.019 

02 

-0.036 
0.021 

a2 

0.00479 
0.00013 

“The results are based on the last 2000 iterations, and the prior distributions are given 
in the text. 

Iterations 

Iterations 

(M 

Iterations Iterations 

Figure 12.1 Time plots of Gibbs draws for the model in Eq. (12.13) with 2100 iterations. Results are 
based on last 2000 draws. Prior distributions and starting parameter values are given in text. 

12.6 MISSING VALUES AND OUTLIERS 

In this section, we discuss MCMC methods for handling missing values and detect¬ 
ing additive outliers. Let {yt}"=1 be an observed time series. A data point yh is an 
additive outlier if 

( Xh + co if t — h, 
} xt otherwise, 

(12.14) 

where co is the magnitude of the outlier and xt is an outlier-free time series. 
Examples of additive outliers include recording errors (e.g., typos and measurement 
errors). Outliers can seriously affect time series analysis because they may induce 
substantial biases in parameter estimation and lead to model misspecification. 

MISSING VALUES AND OUTLIERS 

629 

Figure 12.2 Histograms of Gibbs draws for model in Eq. (12.13) with 2100 iterations. Results are 

based on last 2000 draws. Prior distributions and starting parameter values are given in text. 

Consider a time series xt and a fixed time index h. We can learn a lot about Xh by 
treating it as a missing value. If the model of xt were known, then we could derive 
the conditional distribution of Xh given the other values of the series. By comparing 
the observed value yh with the derived distribution of Xh, we can determine whether 
yh can be classified as an additive outlier. Specifically, if yh is a value that is likely 
to occur under the derived distribution, then yh is not an additive outlier. However, 
if the chance to observe yh is very small under the derived distribution, then yh 
can be classified as an additive outlier. Therefore, detection of additive outliers and 
treatment of missing values in time series analysis are based on the same idea. 

In the literature, missing values in a time series can be handled by using either 
the Kalman filter or MCMC methods; see Jones (1980), Chapter 11, and McCulloch 
and Tsay (1994a). Outlier detection has also been carefully investigated; see Chang, 
Tiao, and Chen (1988), Tsay (1988), Tsay, Pena, and Pankratz (2000), and the 
references therein. The outliers are classified into four categories depending on the 
nature of their impacts on the time series. Here we focus on additive outliers. 

12.6.1 Missing Values 

For ease in presentation, consider an AR(/?) time series 

xt = (j)\Xt-\ + • • • + (ppXt-p + at, 

(12.15) 

630 

MARKOV CHAIN MONTE CARLO METHODS WITH APPLICATIONS 

where {a,} is a Gaussian white noise series with mean zero and variance a2. 
Suppose that the sampling period is from t = 1 to t = n, but the observation Xh is 
missing, where 1 < h < n. Our goal is to estimate the model in the presence of a 
missing value. 

In this particular instance, the parameters are 0 = (0\ Xh, cr2)', where 0 — 
(01,..., (ppY. Thus, we treat the missing value Xh as an unknown parameter. If we 
assume that the prior distributions are 

0 ~ 7V(0O, E0), xh ~ N(/x0, a2), — ~ *2, 

X) 

<7a 

where the hyperparameters are known, then the conditional posterior distributions 
/(0|X, xh, a2) and f(cr2\X, xh, 0) are exactly as those given in the previous 
section, where X denotes the observed data. The conditional posterior distribution 
f(*h\X, 0, a2) is univariate normal with mean /r* and variance cr%. These two 
parameters can be obtained by using a linear regression model. Specifically, given 
the model and the data, xh is only related to {xh_p, xh-i,xh+i, ..., xh+p}. 
Keeping in mind that Xh is an unknown parameter, we can write the relationship 
as follows: 

1. For t = h, the model says 

Xh — 01 Xh—i + • • • + (ppXh-p + ah. 

Letting yh = 0i^_i H-1- (ppXh-p and bh = —the prior equation can 
be written as 

yh =Xh+ bh = 00 Xh + bh, 

where 0O = 1. 

2. For t = h + 1, we have 

xh+1 = 01 Xh + 02*6-1 H-b (ppXh+\-p +ah+1. 

Letting yh+\ = *6+1 — 02*/i-i — • • • — 0^*6+\-P and bh+1 = «6+i> the prior 
equation can be written as 

3. In general, for t = h + j with j = 1,..., p, we have 

yh+l — 01*6 + ^6 + 1- 

Xh+j = 01*A+j—l H F 07*6 + 07 + 1*6-1 H-f (ppXh+j-p + ah+j. 

Let yh+j = *6+7 —01*6+7-1 07-1*6+1 —07 + 1*6-1-4>pXh+j-p 
and bh+j = ah+j. The prior equation reduces to 

yh+j — <PjXh + bh+j. 

MISSING VALUES AND OUTLIERS 

631 

Consequently, for an AR(p) model, the missing value Xf, is related to the model, 
and the data in p + 1 equations 

yh+j = (pjXh + bh+j, j=0,...,p, (12.16) 

where </>0 = 1- Since a normal distribution is symmetric with respect to its mean, ah 
and —ah have the same distribution. Consequently, Eq. (12.16) is a special simple 
linear regression model with p + 1 data points. The least-squares estimate of Xh 
and its variance are 

Ey=o fijyh+j 

xh 

Var(!xh) = 

For instance, when p = 1, we have Xh = [0i/(l + 02)]Orii-i + *h+1), which is 
referred to as the filtered value of Xh- Because a Gaussian AR(1) model is time 
reversible, equal weights are applied to the two neighboring observations of Xh to 
obtain the filtered value. 

Finally, using Result 12.1, we obtain that the posterior distribution of Xh is 

normal with mean pi* and variance a*, where 

<r2Ho + o-02(Ey=o tfYxh 
^2 + ^2(E;=o 0j) 

2 2 
a °o 

= 

(12.17) 

Missing values may occur in patches, resulting in the situation of multiple con¬ 
secutive missing values. These missing values can be handled in two ways. First, 
we can generalize the prior method directly to obtain a solution for multiple fil¬ 
tered values. Consider, for instance, the case that Xh and Xh+\ are missing. These 
missing values are related to {xh-p, ■ ■ ■, Xh-\\ Xh+2, • • •, *h+P+i}- We can define a 
dependent variable yh+j in a similar manner as before to set up a multiple linear 
regression with parameters Xh and Xh+i- The least-squares method is then used to 
obtain estimates of xh and xh+\. Combining with the specified prior distributions, 
we have a bivariate normal posterior distribution for (*/,, Xh+1)'. In Gibbs sampling, 
this approach draws the consecutive missing values jointly. Second, we can apply 
the result of a single missing value in Eq. (12.17) multiple times within a Gibbs 
iteration. Again consider the case of missing Xh and Xh+\. We can employ the 
conditional posterior distributions f(xh\X, xh+\, <t>, cr2) and f(xh+\\X, xh, 0, o2) 
separately. In Gibbs sampling, this means that we draw the missing value one at a 

time. 

Because xh and Xh+\ are correlated in a time series, drawing them jointly is 
preferred in a Gibbs sampling. This is particularly so if the number of consecutive 
missing values is large. Drawing one missing value at a time works well if the 

number of missing values is small. 

Remark. In the previous discussion, we assumed h - p > 1 and h + p <n. 
If h is close to the end points of the sample period, the number of data points 
available in the linear regression model must be adjusted. □ 

632 

MARKOV CHAIN MONTE CARLO METHODS WITH APPLICATIONS 

12.6.2 Outlier Detection 

Detection of additive outliers in Eq. (12.14) becomes straightforward under the 
MCMC framework. Except for the case of a patch of additive outliers with similar 
magnitudes, the simple Gibbs sampler of McCulloch and Tsay (1994a) seems to 
work well; see Justel, Pena, and Tsay (2001). Again we use an AR model to 
illustrate the problem. The method applies equally well to other time series models 
when the Metropolis-Hasting algorithm or the Griddy Gibbs is used to draw values 
of nonlinear parameters. 

Assume that the observed time series is yt, which may contain some additive 
outliers whose locations and magnitudes are unknown. We write the model for 
y, as 

yt=St/3t+xt, t = (12.18) 

where {<?>,.} is a sequence of independent Bernoulli random variables such that 
P(8t = 1) = e and P(8t = 0) = 1 — e, e is a constant between 0 and 1, {pt} is a 
sequence of independent random variables from a given distribution, and x, is an 
outlier-free AR(p) time series, 

xt=<fo + <P\xt-x H-b (ppXt-p + at, 

where {at} is a Gaussian white noise with mean zero and variance a2. This model 
seems complicated, but it allows additive outliers to occur at every time point. The 
chance of being an outlier for each observation is e. 

Under the model in Eq. (12.18), we have n data points, but there are 2n + p + 3 
parameters—namely, 0 = (0O,..., 0,)', 8 = (<$,,..., 8n)', p = (ft,..., pn)', a2, 
and €. The binary parameters 8t are governed by € and the fit are determined by the 
specified distribution. The parameters 8 and /? are introduced by using the idea of 
data augmentation with 8t denoting the presence or absence of an additive outlier 
at time t, and pt is the magnitude of the outlier at time t when it is present. 

Assume that the prior distributions are 

0 ~ 7V(0O, Z0), e~Beta(yi,K2), 0, £2), 

\)h 

where the hyperparameters are known. These are conjugate prior distributions. To 
implement Gibbs sampling for model estimation with outlier detection, we need to 
consider the conditional posterior distributions of 

/(0|y,a,/?,a2), /(^|y,5_ft,)8,0,or2), f(Ph\Y,8,p_h,(l>,a2), 

f(e\Y,8), f (cr2|y, 0, 8, P), 

where 1 < h < n, Y denotes the data, and 0denotes that the ith element of 9 is 
removed. 

MISSING VALUES AND OUTLIERS 

633 

Conditioned on 8 and /?, the outlier-free time series xt can be obtained by 
xt — yt — 8tpt. Information of the data about 0 is then contained in the least-squares 
estimate 

?= I 

,/=/>+! 

where xt-\ = (1, xt-\, ..., xt-p)', which is normally distributed with mean 0 and 
covariance matrix 

X = 

The conditional posterior distribution of 0 is therefore multivariate normal with 
mean 0* and covariance matrix X*, which are given in Eq. (12.9) with P being 
replaced by 0 and xOJ by xt-\. Similarly, the conditional posterior distribution of 
o'2 is an inverted chi-squared distribution—that is, 

v^ + IZ”=p+l af _ ,2 

^2 ~ Xv+(n-p)’ 

where at = xt — 0xr_i and xt = yt — 8tpt. 

The conditional posterior distribution of 8h can be obtained as follows. First, 8^ is 
only related to {yj, ^itjth-p with J # h> and °2- More specifically, 
we have 

Xj=yj-8jPj, j^h. 

Second, Xh can assume two possible values: Xh = yh - Ph if fa = 1 and xh = yh, 
otherwise. Define 

Wj = X* 00 01 **_l-(ppX*j_p, j = h,..., h + p, 

where x* = xj if j # h and x*h - yh. The two possible values of xh give rise to 

two situations: 

• Case I: 8h = 0. Here the /zth observation is not an outlier and x£ = yh = xh. 

Hence, wj = aj for j = h,... ,h + p. In other words, we have 

w,  N(0, cr2), 

j =h,...,h + p. 

634 

MARKOV CHAIN MONTE CARLO METHODS WITH APPLICATIONS 

• Case II: <5^ = 1. Now the fith observation is an outlier and = yh — Xh + Ph- 

The wj defined before is contaminated by /3/,. In fact, we have 

wh ~ N(ph,o2) and wj ~ N(-(pj_hph, a2), j = h + 1,..., h + p. 

If we define \J/q = — 1 and ^ = (pi for i = 1, ..., p, then we have wj ~ 

^(~^j-hPh, ff2) for j = h,..., h + p. 

Based on the prior discussion, we can summarize the situation as follows: 

1. Case I: Sh = 0 with probability 1 - e. In this case, Wj ~ N(0, a2) for j = 

h,..., h + p. 

2. Case II: &h = 1 with probability e. Here Wj ~ N(—\pj_h/3h, a2) for j = 

h,..., h + p. 

Since there are n data points, j cannot be greater than n. Let m = min(n, h + p). 
The posterior distribution of Sh is therefore 

P(8h = l\Y,8-h,p,<p,a2) 

__ e exPt- Z7=h(»>j + fj-hPh)2/{2a2)\ 

~ e exp[— Z7=h^j + fj-hPh)2/(2o-2)] + (1 - €) exp[- Y™=h w2/(2a^)]' 

(12.19) 

This posterior distribution is simply to compare the weighted values of the likeli¬ 
hood function under the two situations with weight being the probability of each 
situation. 

Finally, the posterior distribution of f5h is as follows. 

• If Sh = 0, then yh is not an outlier and ph ~ N(0, £2). 

• If <5/, = 1, then yh is contaminated by an outlier with magnitude f}h. The 
variable Wj defined before contains information of /3/, for j = h,h + 1, 
. ..,min (h + p,n). Specifically, we have wj ~ a2) for 
j = h, h + l,, min(fi + p, n). The information can be put in a linear 
regression framework as 

wj ~ - fj-hPh +aj, j = h,h + 1, ..., min(/z + p, n). 

Consequently, the information is embedded in the least-squares estimate 

- _ -'I'j-hWj 

m — min(h + p, n), 

MISSING VALUES AND OUTLIERS 

635 

which is normally distributed with mean fa and variance cr2/(^]J=^ i/r2^). 
By Result 12.1, the posterior distribution of fa is normal with mean fa and 
variance cr^, where 

= -(LJ^j-h^2 2 = 

h + CL?U *2-h^2 ’ + (£?U *U^2' 

Example 12.2. Consider the weekly change series of U.S. 3-year Treasury 
constant maturity interest rate from March 18, 1988, to September 10, 1999, for 600 
observations. The interest rate is in percentage and is a subseries of the dependent 
variable c3? of Example 12.1. The time series is shown in Figure 12.3(a). If AR 
models are entertained for the series, the partial autocorrelation function suggests 
an AR(3) model and we obtain 

C3; = 0.227c3>f_i + 0.006c3,,_2 T 0.114c3j—j T £6, — 0.0128, 

(a) 

Figure 12.3 Time plots of weekly change series of U.S. 3-year Treasury constant maturity interest 

rate from March 18, 1988, to September 10, 1999: (a) data, (b) posterior probability of being an outlier, 

and (c) posterior mean of outlier size. Estimation is based on Gibbs sampling with 1050 iterations with 

first 50 iterations as burn-ins. 

636 

MARKOV CHAIN MONTE CARLO METHODS WITH APPLICATIONS 

where standard errors of the coefficients are 0.041, 0.042, and 0.041, respectively. 
The Ljung-Box statistics of the residuals show £>(12) = 11.4, which is insignificant 
at the 5% level. 

Next, we apply the Gibbs sampling to estimate the AR(3) model and to detect 

simultaneously possible additive outliers. The prior distributions used are 

0  N(0, 0.25/3), 

vX 

5 x 0.00256 , 
-2-~X5. Ti=5, Y2 — 95, £2=0.1, 

where 0.00256 ~ a2/5 and £2 « 9a2. The expected number of additive outliers is 
5%. Using initial values 6 = 0.05, a2 = 0.012, 0i = 0.2, 02 = 0.02, and </>3 = 
0.1, we run the Gibbs sampling for 1050 iterations but discard results of the first 
50 iterations. Using posterior means of the coefficients as parameter estimates, we 
obtain the fitted model 

cjt — 0.252c3)f_i + 0.003c3j,_2 + 0.110c3i,_2 + at, a2 = 0.0118, 

where posterior standard deviations of the parameters are 0.046, 0.045, 0.046, and 
0.0008, respectively. Thus, the Gibbs sampling produces results similar to that 
of the maximum-likelihood method. Figure 12.3(b) shows the time plot of poste¬ 
rior probability of each observation being an additive outlier, and Figure 12.3(c) 
plots the posterior mean of outlier magnitude. From the probability plot, some 
observations have high probabilities of being an outlier. In particular, t — 323 has 
a probability of 0.83 and the associated posterior mean of outlier magnitude is 
-0.304. This point corresponds to May 20, 1994, when the c3t changed from 0.24 
to -0.34 (i.e., about a 0.6% drop in the weekly interest rate within 2 weeks). The 
point with second highest posterior probability of being an outlier is t = 201, which 
is January 17, 1992. The outlying posterior probability is 0.58 and the estimated 
outlier size is 0.176. At this particular time point, c3t changed from -0.02 to 0.33, 
corresponding to a jump of about 0.35% in the weekly interest rate. 

Remark. Outlier detection via Gibbs sampling requires intensive computation 
but the approach performs a joint estimation of model parameters and outliers. Yet 
the traditional approach to outlier detection separates estimation from detection. It 
is much faster in computation, but may produce spurious detections when multiple 
outliers are present. For the data in Example 12.2, the SCA program also identifies 
t = 323 and t — 201 as the two most significant additive outliers. The estimated 
outlier sizes are -0.39 and 0.36, respectively. □ 

12.7 STOCHASTIC VOLATILITY MODELS 

An important financial application of MCMC methods is the estimation of stochas¬ 
tic volatility models, see Jacquier, Poison, and Rossi (1994) and the references 

STOCHASTIC VOLATILITY MODELS 

637 

therein. We start with a univariate stochastic volatility model. The mean and volatil¬ 
ity equations of an asset return rt are 

ft — fio + P\x\t + • • • + PpXpt + at, at — y/htet, (12.20) 

In ht = tx.Q + co In h,-\ + v,, (12.21) 

where {xu\i = 1, ...,/?} are explanatory variables available at time t — 1, the f3j 
are parameters, {er} is a Gaussian white noise sequence with mean 0 and variance 
1, {u,} is also a Gaussian white noise sequence with mean 0 and variance oand 
{e,} and {urj are independent. The log transformation is used to ensure that h, is 
positive for all t. The explanatory variables x,-f may include lagged values of the 
return (e.g., Xu = rt-i). In Eq. (12.21), we assume that |o;i| < 1 so that the log 
volatility process In ht is stationary. If necessary, a higher order AR(p) model can 
be used for In ht. 

Denote the coefficient vector of the mean equation by /? = (fo, /fi,..., ftp)' 
and the parameter vector of the volatility equation by co = (ao, or, a^)'. Suppose 
that R = (ri, ..., rn)' is the collection of observed returns and X is the collection 
of explanatory variables. Let H — (h\, ..., hn)' be the vector of unobservable 
volatilities. Here ft and co are the “traditional” parameters of the model and H 
is an auxiliary variable. Estimation of the model would be complicated via the 
maximum-likelihood method because the likelihood function is a mixture over the 
n-dimensional H distribution as 

f(R\X,fi,a>) = I f(R\X,/},H)f(H\co)dH. 

However, under the Bayesian framework, the volatility vector H consists of aug¬ 
mented parameters. Conditioning on H, we can focus on the probability distribution 
functions f(R\H, P) and f(H\oo) and the prior distribution /?(/?, co). We assume 
that the prior distribution can be partitioned as p{P,co) — p(f})p(6o); that is, prior 
distributions for the mean and volatility equations are independent. A Gibbs sam¬ 
pling approach to estimating the stochastic volatility in Eqs. (12.20) and (12.21) 
then involves drawing random samples from the following conditional posterior 

distributions: 

f(P\R, X, H, co), f(H\R, X, /?, cu), f(co\R, X, /?, H). 

In what follows, we give details of practical implementation of the Gibbs sampling 

used. 

12.7.1 Estimation of Univariate Models 

Given H, the mean equation in (12.20) is a nonhomogeneous linear regression. 
Dividing the equation by y[hx, we can write the model as 

fo,t — xotP + 

t = 1, 

(12.22) 

638 

MARKOV CHAIN MONTE CARLO METHODS WITH APPLICATIONS 

where rQtt = rt/*/ht and x0tt — xt/y/h~t, with x, = (1, x\t,..., xpt)' being the vec¬ 
tor of explanatory variables. Suppose that the prior distribution of P is multivariate 
normal with mean P0 and covariance matrix A0. Then the posterior distribution of 
(i is also multivariate normal with mean /?* and covariance matrix A*. These two 
quantities can be obtained as before via Result 12.1a, and they are 

n / n 

~ ^^xo,tx0}t + A0 , /?* = A* | xotrot + Aa ($0 

t=1 \r=l 

where it is understood that the summation starts with p + 1 if is the highest 
lagged return used in the explanatory variables. 

The volatility vector H is drawn element by element. The necessary conditional 
posterior distribution is f(ht\R, X, p, co), which is produced by the normal 
distribution of at and the lognormal distribution of the volatility, 

a f(a,\ht, rt, xt, p)f(ht\h,-U a>)f(ht+i\ht, a) 

a h~0-5 exp[-(r, - x'tp)2/{2ht)]h;x exp[-(ln ht - ^)2/(2a2)] 

a h~L5 exp[—(rr - x'tP)2/(2ht) - (In ht - p,)2/(2a2)], (12.23) 

where pt = [o;o(l — a\) + o;i(ln ht+\ + In + a2) and a2 = cr2/(\ + 
a2). Here we have used the following properties: (a) at\ht ~ N(0, ht); (b) In h,\ 
In ht^\ ~ N(ao + a\ \nht^,a2); (c) In h,+\ \ In h, ~ A^(o:o + a\ In ht,a2)\ (d) 
d In ht = ht dhr, where d denotes differentiation; and (e) the equality 

(x - a)2 A + (x- b)2C = (x- cf(A + C) + (a - b)2AC/(A + C), 

where c = (Aa + Cb)/(A + C) provided that A + C ^ 0. This equality is a scalar 
version of Lemma 1 of Box and Tiao (1973, p. 418). In our application, A = l, a = 
a0 + In ht-1, C = a2, and b = (In ht+1 - a0)/ai. The term (a - b)2AC/(A + C) 
does not contain the random variable h, and, hence, is integrated out in the deriva¬ 
tion of the conditional posterior distribution. Jacquier, Poison, and Rossi (1994) use 
the Metropolis algorithm to draw ht. We use Griddy Gibbs in this section, and the 
range of ht is chosen to be a multiple of the unconditional sample variance of rt. 
To draw random samples of (o, we partition the parameters as a — (uq,u\)' 
and a2. The prior distribution of a> is also partitioned accordingly [i.e., p(co) = 
p(a)p(cr2)]. The conditional posterior distributions needed are 

• f(a\Y, X, H, P, a2) = f(a\H, a2): Given H, In ht follows an AR(1) 
model. Therefore, the result of AR models discussed in the previous two 
sections applies. Specifically, if the prior distribution of a is multivariate 

STOCHASTIC VOLATILITY MODELS 

639 

normal with mean a0 and covariance matrix Ca, then f(a.\H,a%) is 
multivariate normal with mean a* and covariance matrix C*, where 

C~l = 

cr,: 

+ c: 

— c. 

EL2 Zt In h,  + Cn 

cr,: 

where zf = (1, Inht-\)'. 

• f(oZ\Y,X,H,p,a) = f(at\H, a): Given H and a, we can calculate vt = 
In /zf — a0 - In /if_i for t = 2.n. Therefore, if the prior distribution 
of al is {mX)jo1 ~ xL then the conditional posterior distribution of of is 
an inverted chi-squared distribution with m + n — 1 degrees of freedom; that 

is, 

+ EL2 _ 2 

2 X-m+n— l’ 

av 

Remark. Formula (12.23) is for 1 < t < n, where n is the sample size. For 
the two end data points h\ and hn, some modifications are needed. A simple 
approach is to assume that h i is fixed so that the drawing of ht starts with t = 2. 
For t = n, one uses the result In hn ~ («o + «i In hn-\, a„). Alternatively, one 
can employ a forecast of hn+\ and a backward prediction of h0 and continue to 
apply the formula. Since hn is the variable of interest, we forecast hn+\ by using 
a 2-step-ahead forecast at the forecast origin n — 1. For the model in Eq. (12.21), 

the forecast of hn+\ is 

hn-1 (2) = a0 4- ai (ao + aq In hn-1). 

The backward prediction of h0 is based on the time reversibility of the model 

(In ht — r}) = cm (In ht-1 - r]) + vt, 

where 77 = or0/(l — «i) and |ori | < 1. The model of the reversed series is 

(In ht - rj) = cm (In ht+1 - rf) + v*, 

where {v*} is also a Gaussian white noise series with mean zero and variance o]. 
Consequently, the 2-step-backward prediction of h0 at time t — 2 is 

M-2) = ajf(ln h2 - rj). □ 

Remark. Formula (12.23) can also be obtained by using results of a missing 
value in an AR(1) model; see Section 12.6.1. Specifically, assume that In h, is 
missing. For the AR(1) model in Eq. (12.21), this missing value is related to 

In ht-1 and In ht+1 for 1 < t < n. From the model, we have 

In ht = ao + cm In ht-\ + at. 

640 

MARKOV CHAIN MONTE CARLO METHODS WITH APPLICATIONS 

Define yt = ao + ct\yt-1, xt = 1, and b, = —at. Then we obtain 

y, = xt In ht + bt. (12.24) 

Next, from 

In ht+i = a0 + ai In ht + at+x, 

we define yt+x = In /if+i — «o, Jcf+i = aj, and bt+x = ar+i and obtain 

yt+i —-ri+i In h,+1 + bt-|_i. (12.25) 

Now Eqs. (12.24) and (12.25) form a special simple linear regression with two 
observations and an unknown parameter In ht. Note that bt and bt+x have the 
same distribution because — at is also N(0, a2). The least-squares estimate of In h, 
is then 

1^ _ xtyt + x,+xyt+x __ «p(l - ai) + oq(ln ht+1 + In ht-\) 
xf +^f2+i 1 + aj 

which is precisely the conditional mean of In h, given in Eq. (12.23). In addition, 
this estimate is normally distributed with mean In h, and variance of/0 + «2)- For¬ 
mula (12.23) is simply the product of at ~ 7V(0, ht) and ln~^r ~ N[ln h,, a;/(l + 
“]■)] with the transformation d In ht = ht 1 dht. This regression approach general¬ 
izes easily to other AR(/?) models for In ht. We use this approach and assume that 

i are fixed for a stochastic volatility AR(p) model. □ 

Remark. Starting value of ht can be obtained by fitting a volatility model of 

Chapter 3 to the return series. □ 

Example 12.3. Consider the monthly log returns of the S&P 500 index from 
January 1962 to December 2009 for 575 observations. The returns are computed 
using the first adjusted closing index of each month, that is, the closing index 
of the first trading day of each month. Figure 12.4(a) shows the time plot of the 
log level of the index, whereas Figure 12.4(b) shows the log returns measured in 
percentage. If GARCH models are entertained for the series, we obtain a Gaussian 
GARCH(1,1) model 

r, — 0.552 + at, a, = y/h~tet, 

h, = 0.878 + 0.125ar2_! + 0.837/z,_1, (12.26) 

where / ratios of the coefficients are all greater than 2.56. The Ljung-Box statistics 
of the standardized residuals and their squared series fail to indicate any model inad¬ 
equacy. Specifically, we have <2(12) = 10.04(0.61) and 6.14(0.91), respectively, 
for the standardized residuals and their squared series. 

STOCHASTIC VOLATILITY MODELS 

641 

Figure 12.4 Time plot of monthly S&P 500 index from 1962 to 2009: (a) log level and (b) log return 

in percentage. 

Next, consider the stochastic volatility model 

rt = n + at, at = y/fTt€t, 

In ht = ao + a\ In ht~\ + vt, (12.27) 

where the vt are iid N(0, of). To implement the Gibbs sampling, we use the prior 
distributions 

ix ~ N(0, 4), a ~ N[a0, diag(0.25, 0.04)], -—-/10, 

10x0.1 , 

av 

where a0 = (0, 0.6)'. For initial parameter values, we use the fitted values of the 
GARCH(1,1) model in Eq. (12.26) for {ht}, that is, h0t = ht, and set a and of 
to the least-squares estimate of \n(hot). The initial value of /x is the sample mean 
of the log returns. The volatility ht is drawn by the Griddy Gibbs with 400 grid 
points. The possible range of ht for the jth Gibbs iteration is [r]\t, rj2t], where 
rjlt = 0.6 x maxQij-i't, h0t) and rj2t = 1.4 x min(/i;_i,f, h0t), where /zy—i,r and 
hot denote, respectively, the estimate of ht for the (j - l)th iteration and initial 

value. 

We ran the Gibbs sampling for 2500 iterations but discarded results of the first 
500 iterations. Figure 12.5 shows the density functions of the prior and posterior 

642 

MARKOV CHAIN MONTE CARLO METHODS WITH APPLICATIONS 

>? 
Tr> 
c 
CD 
Q 

'</) 
c Q) 
Q 

-2-10 1 

2 

0.0 0.2 0.4 0.6 0.8 1.0 

Ob 

Figure 12.5 Density functions of prior and posterior distributions of parameters in stochastic volatility 

model for monthly log returns of S&P 500 index. Dashed line denotes prior density and solid line the 

posterior density, which is based on results of Gibbs sampling with 2000 iterations. See text for more 
details. 

distributions of the four coefficient parameters. The prior distributions used are 
relatively noninformative. The posterior distributions are concentrated especially 
for (x and ay2. Figure 12.6 shows the time plots of fitted volatilities. The upper 
panel shows the posterior mean of h, over the 5000 iterations for each time point, 
whereas the lower panel shows the fitted values of the GARCH(1,1) model in Eq. 
(12.26). The two plots exhibit a similar pattern. 

The posterior mean and standard error of the four coefficients are as follows: 

Parameter 

a0 

OC] 

Mean 

Standard error 

0.409 

0.157 

0.454 

0.068 

0.837 

0.025 

0.086 

0.007 

The posterior mean of a\ is 0.837, confirming strong serial dependence in the 
volatility series. This value is smaller than that obtained by Jacquier, Poison, and 
Rossi (1994) who used daily returns of the S&P 500 index. Finally, we have used 
different initial values, priors, and numbers of iterations for the Gibbs sampler. The 

STOCHASTIC VOLATILITY MODELS 

643 

1970 1980 1990 2000 2010 

Year 

Figure 12.6 Time plots of fitted volatilities for monthly log returns of S&P 500 index from 1962 to 

2009. Lower panel shows posterior means of a Gibbs sampler with 2000 iterations. Upper panel shows 

results of a Gaussian GARCH(1,1) model. 

results are stable. Of course, as expected, the results and efficiency of the Griddy 
Gibbs algorithm depend on the specification of the range for ht. 

12.7.2 Multivariate Stochastic Volatility Models 

In this section, we study multivariate stochastic volatility models using the Cholesky 
decomposition of Chapter 10. We focus on the bivariate case, but the methods 
discussed also apply to the higher dimensional case. Based on the Cholesky decom¬ 
position, the innovation at of a return series r, is transformed into b, such that 

b\t—a\t, b2t = ci2t — q2\,tb\t, 

where b2t and <721,1 can be interpreted as the residual and least-squares estimate of 
the linear regression 

a-2t = qiijau +bit- 

The conditional covariance matrix of at is parameterized by {gn>f, g22,t} and {<721,1} 

as 

or i,t 

<712, t 

&21 ,f 

<722,f 

1 

0 ' 

gll.f 

0 

<721 ,t 

1 

0 

822, t 

1 <721,1 

0 1 

(12.28) 

644 

MARKOV CHAIN MONTE CARLO METHODS WITH APPLICATIONS 

where gatt = Var(£>,7|F,_i) and b\, Lb2t. Thus, the quantities of interest are 

£ii,d 822,t and <721,0 

A simple bivariate stochastic volatility model for the return r, = (r\t, r2t)' is as 

follows: 

rt — Po + P\Xt + at, (12.29) 

In guj = Qf/o + In gtt'f-i + vit, i = 1,2, (12.30) 

#21,* — 7o + Kl<721,f-1 + (12.31) 

where {a,} is a sequence of serially uncorrelated Gaussian random vectors with 
mean zero and conditional covariance matrix T.t given by Eq. (12.28), /?0 is a 
two-dimensional constant vector, xt denotes the explanatory variables, and {tqr}, 
{V2t}, and {u,} are three independent Gaussian white noise series such that Var(i;(f) 
= afv and Var(w,) = cr2. Again log transformation is used in Eq. (12.30) to ensure 
the positiveness of gaj. 

Let Gi = (giitgu,ny, G = [Gi,G2\, and Q = , q2l,n)'- The 
“traditional” parameters of the model in Eqs. (12.29)—(12.31) are /? = (/?0, ^j), 
= (of«o.a«i)', and <yfv for i = 1, 2, and y = (yo, y\)' and cr2. The augmented 
parameters are Q, Gi, and G2- To estimate such a bivariate stochastic volatility 
model via Gibbs sampling, we use results of the univariate model in the previous 
section and two additional conditional posterior distributions. Specifically, we can 
draw random samples of 

1. fi0 and row by row using the result (12.22) 

2. gu,t using Eq. (12.23) with a, being replaced by a\t 

3. a\ and using exactly the same methods as those of the univariate case 

with at replaced by a\t 

To draw random samples of a2, cr2,, and g22j, we need to compute b2t. But 
this is easy because b2t = a2t — q2\,ja\t given the augmented parameter vector Q. 
Furthermore, b2t is normally distributed with mean 0 and conditional variance g22,t. 

It remains to consider the conditional posterior distributions 

f(y\Q,cr^), f(ol\Q,y), f(,qi\,t\A,G, Q-t,y,al), 

where A denotes the collection of a,, which is known if R, X, fi0, and are given. 
Given Q and a2, model (12.31) is a simple Gaussian AR(1) model. Therefore, if 
the prior distribution of y is bivariate normal with mean y0 and covariance matrix 
D0, then the conditional posterior distribution of y is also bivariate normal with 
mean y * and covariance matrix D*, where 

En / 
t=2ZtZ't 

at 

+ D -1 

y* = d* 

En 

1=2 Ztq2\,t 

+ Kly* 

STOCHASTIC VOLATILITY MODELS 

645 

where zt = (1, <gr2i,r—i)r- Similarly, if the prior distribution of cr2 is (mX)/o2 ~ y2, 
then the conditional posterior distribution of cr2 is 

mX + E"= 2 _ 2 

2 X-m+n— 1’ 

where m, = ^2i,/ - Yo - m2l,f-i- Finally, 
/(^2l,r I A, G, Q^t,a2, y) 

OC /(&2<|g22,f)/(02lTl021,f-l, T, )/(^21 ,r+1 l<?21,r, O'2) 

a g22°f exp[-(a2, - q2l,tau)2/(2g22j)] exp[-(q2i,t - ju,)2(2cr2)], (12.32) 

where /xf = [y0(l - 7i) + tt0?2i,f-i + 02i,H-i)]/(l + Y\) and “2 = “«/(! + y2). 
In general, nt and cr2 can be obtained by using the results of a missing value in an 
AR(p) process. It turns out that Eq. (12.32) has a closed-form distribution for q2\j. 
Specifically, the first term of Eq. (12.32), which is the conditional distribution of 
q2i,t given g22j and at, is normal with mean a2t/a\, and variance g22j / (a\t)2 ■ The 
second term of the equation is also normal with mean /xt and variance a2. Con¬ 
sequently, by Result 12.1, the conditional posterior distribution of q2\<t is normal 
with mean 11* and variance cr2, where 

1 _ a\t , ! + n2 

r* g22,t cr2 

x lit + 

«21 

fir 

X 
g22,t a\t 

where /it is defined in Eq. (12.32). 

Example 12.4. In this example, we study bivariate volatility models for the 
monthly log returns of IBM stock and the S&P 500 index from January 1962 
to December 2009. This is an expanded version of Example 12.3 by adding the 
IBM returns. Figure 12.7 shows the time plots of the two return series. Let rt = 
(IBM;, SP;)'. If time-varying correlation GARCH models with Cholesky decom¬ 
position of Chapter 10 are entertained, we obtain the model 

rt = P0 +«,, (12.33) 

gii.r = “10 + «ngn,f-i + «12«2(_i. (12.34) 

g22,t = “20 + “22^2, r-l» (12.35) 

021, t = Yo, (12.36) 

where b2t = a2t — q2\,ta\t and the estimates and their standard errors are given in 
Table 12.2(a). For comparison purpose, we also fit a BEKK(1,1) model and obtain 
jg0 = (0.70, 0.54)' and the coefficient matrices 

A = 

' 0.80 
_ 0.83 0.01 _ 

, = 

0.07 
-0.06 

0.33 ' 
0.43 _ 

, B i = 

' 1.00 
0.01 

-0.12 ' 
0.90 

where the matrices are defined in Eq. (10.6) of Chapter 10. 

646 

MARKOV CHAIN MONTE CARLO METHODS WITH APPLICATIONS 

(a) 

1970 1980 1990 2000 2010 

Year 

(b) 

Figure 12.7 Time plots of monthly log returns of (a) IBM stock and (b) S&P 500 index from 1962 
to 2009. 

For stochastic volatility model, we employ the same mean equation in Eq. 
(12.33) and a stochastic volatility model similar to that in Eqs. (12.34)—(12.36). 
The volatility equations are 

In gn,t = aio + an In gn,t_i + vu, 

Var(uu) = o\v. 

(12.37) 

It1 822,t = «20 + «21 In g22,t-\ + V2t, 

Var (V2t) = cr2v, 

(12.38) 

<721,r = Yo + Ut, Var(iq) = oj. 

(12.39) 

The prior distributions used are 

PiO ~ N(0, 4), 

a, ~ N[(0, 0.7)', diag(0.25, 0.04)], 

Vo ~ N(0, 1), 

10x0.1 2 5x0.2 2 
- '■N-' y f v ^ 

where i = 1 and 2. These prior distributions are relatively noninformative. We 
obtained the initial values of {gjU, g22j, qn.i} from the results of the BEKK(1,1) 
model. In addition, we set the values of quantities at t = 1 as given. We then 
ran the Gibbs sampling for 2500 iterations but discarded results of the first 500 
iterations. The random samples of giUt were drawn by Griddy Gibbs with 500 grid 
points in the intervals [rjith, qi<2t] where the lower and upper bounds are set by 

STOCHASTIC VOLATILITY MODELS 

647 

TABLE 12.2 Estimation of Bivariate Volatility Models for Monthly Log Returns of 
IBM Stock and S&P 500 Index from January 1962 to December 2009" 

(a) Bivariate GARCH(1,1) Model With Time-Varying Correlations 

Parameter 

Pei 

P02 

QTo 

an 

an 

«20 

“22 

Ye> 

Estimate 
Standard error 

0.69  0.49  3.98  0.80  0.12 
1.22  0.04  0.03 
0.30  0.18 

10.67  0.12  0.37 
0.04  0.01 
0.53 

(b) Stochastic Volatility Model 

Parameter 

P01 

Pea 

“10 

an 

°\v 

a 20 

«21 

<4 

Yo 

Posterior mean  0.53 
Standard error 

0.51 
0.26  0.17 

0.75  0.80  0.07 
0.03 
0.11 
0.01 

0.43 
0.06 

0.81 
0.03  0.01 

0.07  0.38 
0.03 

al 

0.07 
0.01 

“The stochastic volatility models are based on the last 2000 iterations of a Gibbs sampling with 2500 
total iterations. 

the same method as those of Example 12.3. Posterior means and standard errors of 
the “traditional” parameters of the bivariate stochastic volatility model are given 
in Table 12.2(b). 

To check for convergence of the Gibbs sampling, we ran the procedure several 
times with different starting values and numbers of iterations. The results are stable. 
For illustration, Figure 12.8 shows the scatterplots of various quantities for two 
different Gibbs samples. The first Gibbs sample is based on 500 + 2000 iterations, 
and the second Gibbs sample is based on 500+ 1000 iterations, where M + N 
denotes that the total number of Gibbs iterations is M + N, but results of the 
first M iterations are discarded. The scatterplots shown are posterior means of 

gnj, 822,ti g2i,t, 022,D 021,0 and the correlation p2\,t- The line y = x is added to 
each plot to show the closeness of the posterior means. The stability of the Gibbs 
sampling results is clearly seen. 

It is informative to compare the BEKK model and the GARCH model with time- 
varying correlations in Eqs. (12.33)—(12.36) with the stochastic volatility model. 
First, as expected, the mean equations of the three models are essentially iden¬ 
tical. Second, Figure 12.9 shows the time plots of the conditional variance for 
IBM stock return. Figure 12.9(a) is for the GARCH model, Figure 12.9(b) is from 
the BEKK model, and Figure 12.9(c) shows the posterior mean of the stochas¬ 
tic volatility model. The three models show similar volatility characteristics; they 
exhibit volatility clustering and indicate an increasing trend in volatility. How¬ 
ever, the GARCH model produces higher peak volatility values and an additional 
peak in 1993. Third, Figure 12.10 shows the time plots of conditional variance 
for the S&P 500 index return. The GARCH model produces an extra volatility 
peak around 1993. This additional peak does not appear in the univariate analy¬ 
sis shown in Figure 12.6. It seems that for this particular instance the bivariate 
GARCH model produces a spurious volatility peak. This spurious peak is induced 
by its dependence on IBM returns and does not appear in the stochastic volatility 
model or the BEKK model. Indeed, the fitted volatilities of the S&P 500 index 

648 

MARKOV CHAIN MONTE CARLO METHODS WITH APPLICATIONS 

Figure 12.8 Scatterplots of posterior means of various statistics of two different Gibbs samples for 

bivariate stochastic volatility model for monthly log returns of IBM stock and S&P 500 index. The x 

axis denotes results based on 500 + 2000 iterations and the y axis denotes results based on 500 + 1000 
iterations. Notation is defined in text. 

return by the bivariate stochastic volatility model are similar to that of the uni¬ 
variate analysis. Fourth, Figure 12.11 shows the time plots of fitted conditional 
correlations. Here the three models differ substantially. The correlations of the 
GARCH model with Cholesky decomposition are relatively smooth and always 
positive with mean value 0.59 and standard deviation 0.07. The range of the cor¬ 
relations is (0.411,0.849). The correlations of the BEKK(1,1) model assume small 
negative values around 1993 and are more variable with mean 0.59, standard devi¬ 
ation 0.13 and range (—0.020, 0.877). However, the correlations produced by the 
stochastic volatility model vary markedly from one month to another with mean 
value 0.60, standard deviation 0.14, and range (-0.161,0.839). Furthermore, the 
negative correlations occur in several isolated periods. The difference is under¬ 
standable because q2\,t contains the random shock u, in the stochastic volatility 
model. 

Remark. The Gibbs sampling estimation applies to other bivariate stochas¬ 
tic volatility models. The conditional posterior distributions needed require some 
extensions of those discussed in this section, but they are based on the same ideas. 
The BEKK model is estimated by using Matlab. □ 

NEW APPROACH TO SV ESTIMATION 

649 

Figure 12.9 Time plots of fitted conditional variance for monthly log returns of IBM stock from 1962 

to 2009: (a) GARCH model with time-varying correlations, (b) BEKK(l.l) model, and (c) bivariate 

stochastic volatility model estimated by Gibbs sampling with 500 + 2000 iterations. 

12.8 NEW APPROACH TO SV ESTIMATION 

In this section, we discuss an alternative procedure to estimate stochastic volatility 
(SV) models. This approach makes use of the technique of forward filtering and 
backward sampling (FFBS) within the Kalman filter framework to improve the 
efficiency of Gibbs sampling. It can dramatically reduce the computing time by 
drawing the volatility process jointly with the help of a mixture of normal distri¬ 
butions. In fact, the approach can be used to estimate many stochastic diffusion 
models with leverage effects and jumps. 

For ease in presentation, we reparameterize the univariate stochastic volatility 

model in Eqs. (12.20) and (12.21) as 

rt = x'tfi + ct0 exp (y) €t, 

Zt+i = azt + rjt, 

(12.40) 

(12.41) 

MARKOV CHAIN MONTE CARLO METHODS WITH APPLICATIONS 

650 

O 

O 

O 
CO 

1970 1980 1990 2000 2010 

Year 

(a) 

(b) 

1970 1980 1990 2000 2010 

Year 

(c) 

Figure 12.10 Time plots of conditional variance for monthly log returns of S&P 500 index from 1962 

to 2009: (a) GARCH model with time-varying correlations, (b) BEKK(1,1) model, and (c) bivariate 
stochastic volatility model estimated by Gibbs sampling with 500 + 2000 iterations. 

where xt = (1, x\t,..., xpt)', /3 = (/So, /Si,..., /3P)', cto > 0, {zf} is a zero-mean 
log volatility series, and {er} and {rjt} are bivariate normal distributions with mean 
zero and covariance matrix 

1 po^ 

P°r, a* 

The parameter p is the correlation between et and rp and represents the leverage 
effect of the asset return rt. Typically, p is negative signifying that a negative return 
tends to increase the volatility of an asset price. 

Compared with the model in Eqs. (12.22) and (12.20), we have zt = 
\n(ht) — ln(o-Q) and = exp{£’[ln(/i,)]}. That is, zt is a mean-adjusted log volatil¬ 
ity series. This new parameterization has some nice characteristics. For example, 
the volatility series is cto exp(z,/2), which is always positive. More importantly, rp 
is the innovation of zt+\ and is independent of zt. This simple time shift enables 
us to handle the leverage effect. If one postulates zt = azt-\ + tjt for Eq. (12.41), 
then rjt and et cannot be correlated because a nonzero correlation implies that Zt 
and are correlated in Eq. (12.40), which would lead to some identifiability issues. 

NEW APPROACH TO SV ESTIMATION 

651 

Figure 12.11 Time plots of fitted correlation coefficients between monthly log returns of IBM stock 

and S&P 500 index from 1962 to 2009: (a) GARCH model with time-varying correlations, (b) 

BEKK(1,1) model, and (c) bivariate stochastic volatility model estimated by Gibbs sampling with 

500 + 2000 iterations. 

Remark. Alternatively, one can write the stochastic volatility model as 

rt = x'tp + (T0 exp J €t, 

Zt = azt-i + rj t, 

where (et, r)t)' is a bivariate normal distribution as before. Yet another equivalent 
parameterization is 

rt =x'tp + exp 

z* = oi o + az*_ [ + rjt, 

where E(z*) =ao/(l — a) is not zero. □ 

Parameters of the stochastic volatility model in Eqs. (12.40) and (12.41) are 
P, cr0, a, p, or,, and z — (zi, ..., Zn)', where n is the sample size. For simplicity, we 

652 

MARKOV CHAIN MONTE CARLO METHODS WITH APPLICATIONS 

assume z 1 is known. To estimate these parameters via MCMC methods, we need 
their conditional posterior distributions. In what follows, we discuss the needed 
conditional posterior distributions. 

1. Given z and ao and a normal prior distribution, /? has the same conditional 
posterior distribution as that in Section 12.7.1 with «fht replaced by ao exp(z,/2); 
see Eq. (12.22). 

2. Given z and a2, a is a simple AR(1) coefficient. Thus, with an approximate 
normal prior, the conditional posterior distribution of a is readily available; see 
Section 12.7.1. 

3. Given /? and z, we define vt = (r, — jc'jS) exp(-z,/2) = a0et. Thus, {iy} 
is a sequence of iid normal random variables with mean zero and variance <Jq. 
If the prior distribution of is (mX)/oQ ~ x2, tihen the conditional posterior 
distribution of a£ is an inverted chi-squared distribution with m + n degrees of 
freedom; that is, 

mk + E"=i _ 2 

2 %m+n' 

4. Given /?, ao, z, and a, we can easily obtain the bivariate innovation bT = 
(G, Vt)' for t = 2, ..., n. The likelihood function of (p, ois readily available as 

Up. o2n) = fj /(MS) <x lEf^-'Wexp ^-1 

t=2 

a 15; j—(«—i)/2 exp 

■itr 

1=2 

where tr(A) denotes trace of the matrix A. However, this joint distribution is 
complicated because one cannot separate p and a^2. We adopt the technique of 
Jacquier, Poison, and Rossi (2004) and reparameterize the covariance matrix as 

E = 

1 par 

. P°'i 

1 cp 
cp 00 + cp2 

where a> = a2(l — p2). It is easy to see that |E| = co and 

X 

1 

cp2 -cp 

CO 

. -<p 1 

+ 

0 ' 

■ 1 
_ 0  0 

1 _ 

'

I
I
I

1

■

T

t

3

+

' 1 
0 ‘ 
_ 0  0 _ 

where 5 contains cp only. Let e = (e2,..., €„)' and )/ = (m,..., rj„y be the inno¬ 
vations of the model in Eqs. (12.40) and (12.41). The likelihood function then 
becomes (keeping terms related to parameters only) 

l(<p, co) oc co 

1)/2 exp  ~tr (SR) 

2 co 

 
 
 
 
 
NEW APPROACH TO SV ESTIMATION 

653 

where R = YTt=2 btb't — (e, r\)'{e, r]), which is the 2 x 2 cross-product matrix of 
the innovations. For simplicity, we use conjugate priors such that co is inverse 
gamma (IG) with hyperparameters (y0/2, yi/2); that is, co ~ IG(y0/2, yi/2), and 
<p\co ~ N(0, co/2). Then, after some algebraic manipulation, the joint posterior 
distribution of (cp, co) can be decomposed into a normal and an inverse gamma 
distribution. Specifically, 

where <p = e1 Jj / (2 + e'e), and 

In Gibbs sampling, once (p and co are available, we can obtain p and o2 easily 

because a2 = co + cp2 and p = <p/crv. Note that the probability density function of 
an IG(a, /J) random variable co is 

where a >2 and /3 > 0. 

5. Finally, we consider the joint distribution of the log volatility z given the 

data and other parameters. From Eq. (12.40), we have 

Therefore, letting yt = ln[(rr — jc'/?)2/<Tq], we obtain 

ao 

yt = Zt + €*, 

(12.42) 

where e* = ln(e2). Since ef ~ y2, e* is not normally distributed. Treating Eq. 
(12.42) as an observation equation and Eq. (12.40) as the state equation, we have 
the form of a state-space model except that e* is not Gaussian; see Eqs. (11.26) and 
(11.27). To overcome the difficulty associated with nonnormality, Kim, Shephard, 
and Chib (1998) use a mixture of seven normal distributions to approximate the 
distribution of €*. Specifically, we have 

7 

/(ef)« !><*(/*<. "?)> 

where pt, /x,, and m2 are given in Table 12.3. See also Chib, Nardari, and Shephard 
(2002). 

654 

MARKOV CHAIN MONTE CARLO METHODS WITH APPLICATIONS 

TABLE 12.3 Seven Components of Normal Distributions 

Component i 

Probability /?, 

Mean /z,- 

var .'cof 

1 
2 
3 
4 
5 
6 
7 

0.00730 
0.10556 
0.00002 
0.04395 
0.34001 
0.24566 
0.25750 

-11.4004 
-5.2432 
-9.8373 
1.5075 
-0.6510 
0.5248 
-2.3586 

5.7960 
2.6137 
5.1795 
0.1674 
0.6401 
0.3402 
1.2626 

-25 -20 -15 -10 -5 0 5 
x 

Figure 12.12 Density functions of log(/2), solid line, and that of a mixture of seven normal distribu¬ 
tions, dashed line. Results are based on 100,000 observations. 

To demonstrate the adequacy of the approximation, Figure 12.12 shows the 
density function of e* (solid line) and that of the mixture of seven normals (dashed 
line) in Table 12.3. These densities are obtained using simulations with 100,000 
observations. From the plot, the approximation by the mixture of seven normals is 
very good. 

Why is it important to have a Gaussian state-space model? The answer is that 
such a Gaussian model enables us to draw the log volatility series z jointly and 
efficiently. To see this, consider the following special Gaussian state-space model, 

NEW APPROACH TO SV ESTIMATION 

where rjt and et are uncorrelated (i.e., no leverage effects): 

Zt+i = uzt + r\t, rjt ~iid N(0, tf), 

yt=ct+zt + et, et N(0, Ht), 

655 

(12.43) 

(12.44) 

where, as will be seen later, (ct, Ht) assumes the value (/x,-, urf) of Table 12.3 for 
some i. For this special state-space model, we have the Kalman filter algorithm 

vt =yt- yt\t-i = yt-ct- zt\t-i, 

Vt — + Ht, 

(12.45) 

where V, = Var(uf) is the variance of the 1-step-ahead prediction error vt of yt 
given Ft-\ = (yi,..., yr_i), and Zj\i and T,j\i are, respectively, the conditional 
expectation and variance of the state variable Zj given Ft. See the Kalman filter 
discussion of Chapter 11. 

Forward Filtering and Backward Sampling 
Let p(z\Fn) be the joint conditional posterior distribution of z given the return 
data and other parameters, where for simplicity the parameters are omitted from 
the condition set. We can partition the distribution as 

p(z\Fn) = P(Z2, Z3, ■■■, zn\Fn) 

= p(Zn\Fn)p(Zn-l\Zn, Fn)p(zn-2\zn-l, Zn, Fn) • • • p(Zi\Z3, ■ ■ ■ , Zn, Fn) 

= p(Zn\F„)p(zn-i\zn, Fn)p(zn-2\zn-1, Fn) • • • p(zi\zi, Fn), (12.46) 

where the last equality holds because zt in Eq. (12.43) is a Markov process so that 
conditioned on zt+1, Zt is independent of Zt+j for j > 1. 

From the Kalman filter in Eq. (12.45), we obtain that p(zn\Fn) is normal with 
mean zn\n and variance £„|„. Next, consider the second term p(zn-11Zn, Fn) of Eq. 
(12.46). We have 

Pi.Zn—1 \Zm Fn) = p(Zn—l \Zm Fn—i, yn) = p{zn — i |zn, Fn—\, vn), (12.47) 

where vn = yn — yn\n-\ is the 1-step-ahead prediction error of yn. From the state- 
space model in Eqs. (12.43) and (12.44), zn-1 is independent of vn. Therefore, 

p(Zn-l\zn, Fn) = p(zn-l\zn, Fn-1). 

(12.48) 

656 

MARKOV CHAIN MONTE CARLO METHODS WITH APPLICATIONS 

This is an important property because it implies that we can derive the poste¬ 
rior distribution p(zn-i\zn, Fn) from the joint distribution of (zn-1, Zn) given Fn_i 
via Theorem 11.1. First, the joint distribution is bivariate normal under the Gaus¬ 
sian assumption. Second, the conditional mean and covariance matrix of (zn-1, zn) 
given Fn-\ are readily available from the Kalman filter algorithm in Eq. (12.45). 
Specifically, we have 

Zn—1 
Zn 

~ N ( 
V 

Zn — \\rt— 1 
Zn\n— 1 

& ^n —l|n —1 

^n\n-l 

(12.49) 

where the covariance is obtained by (i) multiplying zn-1 by Eq. (12.43) and (ii) 
taking conditional expectation. Note that all quantities involved in Eq. (12.49) are 
available from the Kalman filter. Consequently, by Theorem 11.1, we have 

P(z.n — 1 \Zn> Fn) ~ N(/Xn_j, En_j), (12.50) 

where 

F'n—1 Z.n- 1 |n— 1 4" (Zn Zn\n—l)> 

^n-1 = ~ 

Next, for the conditional posterior distribution p(zn-2\zn-i, Fn), we have 

Pi.Zn—2\Zn—1> Fn) p(Zn—2\Zn — 1 > Fn—2, yn — l, yn) 

P(z,n—2\Zn — 1 j Fn—2i E/i —1> P«) 

— P(z.n—2\Zn— 1, Fn —2). 

Consequently, we can obtain p(zn-2\zn-i, Fn) from the bivariate normal distribu¬ 
tion of p(zn-2, zn-i\Fn-2) as before. In general, we have 

P(Zt\zt+i, Fn) = p(zt\zt+i, Ft), for 1 < t < n. 

Furthermore, from the Kalman filter, p(zt, zt+\\Ft) is bivariate normal as 

Consequently, 

where 

Zt 
Zt+1 

~ N ( 

Zt\t 
Ft V  Zt+\\t 

» 

ffE,| t 
£rlr 
a^t\t  S/+l|r 

P(z,\zt+u Ft) ~ N(jil Ef), 

P-t — Zt\t + o'Et|rEf+11|;(z?+i — 2i+i|r), 

NEW APPROACH TO SV ESTIMATION 

657 

The prior derivation implies that we can draw the volatility series z jointly 
by a recursive method using quantities readily available from the Kalman filter 
algorithm. That is, given the initial values zi|o and £i|o, one uses the Kalman 
filter in Eq. (12.45) to process the return data forward, then applies the recursive 
backward method to draw a realization of the volatility series z. This scheme is 
referred to as forward filtering and backward sampling (FFBS); see Carter and 
Kohn (1994) and Fruhwirth-Schnatter (1994). Because the volatility {zt} is serially 
correlated, drawing the series jointly is more efficient. 

Remark. The FFBS procedure applies to general linear Gaussian state-space 
models. The main idea is to make use of the Markov property of the model and 
the structure of the state transition equation so that 

p(St\St+i, Fn) = p(St\St+u Ft, vt+i, ...,vn) = p(St\St+\, Ft), 

where St denotes the state variable at time t and Vj is the 1-step-ahead prediction 
error. This identity enables us to apply Theorem 11.1 to derive a recursive method 
to draw the state vectors jointly. □ 

Return to the estimation of the SV model. As in Eq. (12.42), let yt = ln[(rr — 
ctq]. To implement FFBS, one must determine ct and Ht of Eq. (12.44) 
so that the mixture of normals provides a good approximation to the distribution 
of €*. To this end, we augment the model with a series of independent indicator 
variables {It}, where It assumes a value in {1,..., 7} such that P(It = i) = pn 

with Y?i=\ Pit — 1 f°r each F In practice, conditioned on {zt}, we can determine 
ct and Ht as follows. Let 

qn = ^[(y? -Zt- for i = 1, ..., 7, 

where jxL and ujj are the mean and standard error of the normal distributions given 
in Table 12.3 and <t>(-) denotes the cumulative distribution function of the standard 
normal random variable. These probabilities qn are the likelihood function of I, 
given yt and zt. The probabilities p, of Table 12.3 form a prior distribution of I,. 
Therefore, the posterior distribution of I, is 

— 7 ’ 1 • • •, '• 

£y=l pm 

We can draw a realization of It using this posterior distribution. If the random 
draw is It — j, then we define ct — ptj and Ht = mj. In summary, conditioned 
on the return data and other parameters of the model, we employ the approximate 
linear Gaussian state-space model in Eqs. (12.43) and (12.44) to draw jointly the 
log volatility series z. It turns out that the resulting Gibbs sampling is efficient in 
estimating univariate stochastic volatility models. 

658 

MARKOV CHAIN MONTE CARLO METHODS WITH APPLICATIONS 

On the other hand, the square transformation involved in Eq. (12.42) fails to 
retain the correlation between r/t and e, if it exists, making the approximate state- 
space model in Eqs. (12.43) and (12.44) incapable of estimating the leverage effect. 
To overcome this inadequacy, Artigas and Tsay (2004) propose using a time-varying 
state-space model that maintains the leverage effect. Specifically, when p ^ 0, we 
have 

rjt = pa^t + rj*, 

where rj* is a normal random variable independent of et and Var(^*) = cr2 (1 — p2). 
The state transition equation of Eq. (12.43) then becomes 

zt+1 = uzt + panet + rj*. 

Substituting €, = (1 /a0)(rt - exp(-z,/2), we obtain 

zt+1 = azt + 

po^rt -x't/3) 
-exp 

ao 

+ 4; 

= G(Zt) +1?; 

(12.52) 

where G(zt) = oizt + pa^(rt — x't/}) exp(—zf/2)/<ro- This is a nonlinear transition 
equation for the state variable zt. The Kalman filter in Eq. (12.45) is no longer 
applicable. To overcome this difficulty, Artigas and Tsay (2004) use a time-varying 
linear Kalman filter to approximate the system. Specifically, the last two equations 
of Eq. (12.45) are modified as 

Zt-\-i\t G(ztit')) 

£/+i|f = g(zt\t)2Zt\t + 0-2(1 - p2), (12.53) 

where g(zt\t) = dG(x)/dx\x=Zt]t is the first-order derivative of G(zt) evaluated at 
the smoothed state zt\t- 

Example 12.5. To demonstrate the FFBS procedure, we consider the monthly 
log returns of the S&P 500 index from January 1962 to November 2004 for 515 
observations. This is a subseries of the data used in Example 12.3. See Figure 12.4 
for time plots of the index and its log return. We consider two stochastic volatility 
models in the form: 

rt = M + cr0exp(zf/2)e,, et ~iid N(0, 1), (12.54) 

Zt+i = otZt + r]t, T}t ~iid N(0, a2). 

In model 1, {et} and {ijt} are two independent Gaussian white noise series. That 
is, there is no leverage effect in the model. In model 2, we assume that corr(£,, et) 
= p, which denotes the leverage effect. 

NEW APPROACH TO SV ESTIMATION 

659 

TABLE 12.4 Estimation of Stochastic Volatility Model in Eq. (12.54) for Monthly 
Log Returns of S&P 500 Index from January 1962 to November 2004 Using Gibbs 
Sampling with FFBS Algorithm" 

Parameter 

Estimate 

Standard error 

Estimate 

Standard error 

Oo 

O' 

With Leverage Effect 

0.0081 

0.0274 

0.0080 

0.0279 

0.0764 

0.0255 

-0.0616 

0.1186 

Without Leverage Effect 

0.0775 

0.0266 

-0.0613 

0.1164 

2.5639 

0.3924 

2.5827 

0.3783 

P 

-0.3892 

0.0292 

“The results are based on 2000+8000 iterations with the first 2000 iterations as bum-ins. 

We estimate the models via the FFBS procedure using a program written in 
Matlab. The Gibbs sampling was run for 2000+8000 iterations with the first 2000 
iterations as bum-ins. Table 12.4 gives the posterior means and standard errors of 
the parameter estimates. In particular, we have p = —0.39, which is close to the 
value commonly seen in the literature. Figure 12.13 shows the time plots of the 
posterior means of the estimated volatility. As expected, the two volatility series 
are very close. Compared with the results of Example 12.3, which uses a shorter 

Figure 12.13 Estimated volatility of monthly log returns of S&P 500 index from January 1962 to 

November 2004 using stochastic volatility models: (a) with leverage effect and (b) without leverage 

effect. 

660 

MARKOV CHAIN MONTE CARLO METHODS WITH APPLICATIONS 

series, the estimated volatility series exhibit similar patterns and are in the same 
magnitude. Note that the volatility shown in Figure 12.6 is conditional variance 
of percentage log returns whereas the volatility in Figure 12.13 is the conditional 
standard error of log returns. 

12.9 MARKOV SWITCHING MODELS 

The Markov switching model is another econometric model for which MCMC 
methods enjoy many advantages over the traditional likelihood method. McCulloch 
and Tsay (1994b) discuss a Gibbs sampling procedure to estimate such a model 
when the volatility in each state is constant over time. These authors applied the 
procedure to estimate a Markov switching model with different dynamics and mean 
levels for different states to the quarterly growth rate of U.S. real gross national 
product, seasonally adjusted, and obtained some interesting results. For instance, the 
dynamics of the growth rate are significantly different between periods of economic 
“contraction ’ and “expansion.” Since this chapter is concerned with asset returns, 
we focus on models with volatility switching. 

Suppose that an asset return rt follows a simple two-state Markov switching 

model with different risk premiums and different GARCH dynamics: 

P\yfht + Vhtet, 

ht — a10 + ocuht-i + ai2fl(2_j if st = 1, 

K = a20 + U2\ht-\ + a22a^_x if s, = 2, 

(12.55) 

where at — *Jh~tet, {et} is a sequence of Gaussian white noises with mean zero 
and variance 1, and the parameters satisfy some regularity conditions so that 
the unconditional variance of a, exists. The probability transition from one state to 
another is governed by 

P(st = 2|$,_! = 1) = eu P(st = l\st-i=2) = e2, (12.56) 

where 0 <<?,•< 1. A small et means that the return series has a tendency to stay 
in the ith state with expected duration 1/*?,-. For the model in Eq. (12.55) to be 
identifiable, we assume that /32 > P\ so that state 2 is associated with higher risk 
premium. This is not a critical restriction because it is used to achieve uniqueness 
in labeling the states. A special case of the model results if aly- = a2j for all j 
so that the model assumes a GARCH model for all states. However, if piy/ht is 
replaced by then model (12.55) reduces to a simple Markov switching GARCH 
model. 

Model (12.55) is a Markov switching GARCH-M model. For simplicity, we 
assume that the initial volatility h i is given with value equal to the sample variance 
of rt. A more sophisticated analysis is to treat h\ as a parameter and estimate it 
jointly with other parameters. We expect the effect of fixing hx will be negligible 
in most applications, especially when the sample size is large. The “traditional” 

MARKOV SWITCHING MODELS 

661 

parameters of the Markov switching GARCH-M model are P = {P\, P2)1, a, = 
(af/o. Qffi, oia)' for i = 1 and 2, and the transition probabilities e = (e\, e2)' ■ The 
state vector S — (s\, s2, ..., s,,)' contains the augmented parameters. The volatility 
vector H = (h2, ...,hn)' can be computed recursively if h\, a,, and the state 
vector 5 are given. 

Dependence of the return on volatility in model (12.55) implies that the return 
is also serially correlated. The model thus has some predictability in the return. 
However, states of the future returns are unknown and a prediction produced by 
the model is necessarily a mixture of those over possible state configurations. This 
often results in high uncertainty in point prediction of future returns. 

Turn to estimation. The likelihood function of model (12.55) is complicated as it 
is a mixture over all possible state configurations. Yet the Gibbs sampling approach 
only requires the following conditional posterior distributions: 

f(P\R, S, H, «!, a2), S, H, cttfi), 

P(S\R, h\, a\, a2), f(et\S), 1 = 1,2, 

where R is the collection of observed returns. For simplicity, we use conjugate 
prior distributions discussed in Section 12.3—that is, 

Pi ~ N(pi0, of0), e, ~ Beta(y, i, yi2). 

The prior distribution of parameter atj is uniform over a properly specified interval. 
Since is a nonlinear parameter of the likelihood function, we use the Griddy 
Gibbs to draw its random realizations. A uniform prior distribution simplifies the 
computation involved. Details of the prior conditional posterior distributions follow: 

1. The posterior distribution of Pi only depends on the data in state i. Define 

Then we have 

_ rt/\fht if st = i, 

0 otherwise. 

nt = pi+€t, for st=i. 

Therefore, information of the data on Pi is contained in the sample mean of rit. Let 

fj = (J2Sr=i rit)/ni> where the summation is over all data points in state i and 
is the number of data points in state i. Then the conditional posterior distribution 
of Pi is normal with mean P* and variance af*, where 

~y — ni H y> 
aL °to 

Pi = ai* {n‘r‘ + Pio/crfo) , 

i - 1,2. 

662 

MARKOV CHAIN MONTE CARLO METHODS WITH APPLICATIONS 

2. Next, the parameters or;;- can be drawn one by one using the Griddy Gibbs 
method. Given h\, S, av^i, and otjv with v ^ j, the conditional posterior distribu¬ 
tion function of otjj does not correspond to a well-known distribution, but it can 
be evaluated easily as 

/(“«7 !•) a ~2 

In h, + 

(ft ~ PiVhj) 

2l 

ht 

if s, = i, 

where ht contains atJ. We evaluate this function at a grid of points for atj over a 
properly specified interval. For example, 0 < an < 1 - a12. 

3. The conditional posterior distribution of a only involves 5. Let l\ be the 
number of switches from state 1 to state 2 and f2 be the number of switches from 
state 2 to state 1 in S. Also, let ft, be the number of data points in state i. Then 
by Result 12.3 of conjugate prior distributions, the posterior distribution of et is 
Beta(y,i +£it yi2 + m -€,•). 

4. Finally, elements of S can be drawn one by one. Let S-j be the vector 
obtained by removing Sj from 5. Given S-j and other information, sj can assume 
two possibilities (i.e., sj = 1 or sj — 2), and its conditional posterior distribution is 

P(Sjl-) oc n f (at\H)P(Sj\S-j). 

t=j 

The probability 

P(sj = i\S-j) = P(sj = i\sj-i,sJ+i), i = l,2 

can be computed by the Markov transition probabilities in Eq. (12.56). In addition, 
assuming sj = i, one can compute ht for t > j recursively. The relevant likelihood 
function, denoted by L(sj), is given by 

L(sj = i) = ]~[ f(at\H) (x exp(/),), 

t=j 

n 1 

,2~i 

In (ht) + ~~ 
ht. 

t=j 

for i — 1 and 2, where a, — rt — yfht if st = 1 and a, = rt — fcy/ht otherwise. 
Consequently, the conditional posterior probability of sj = 1 is 

P(s = 11-) — _P(sj = l\sj-uSj+i)L(sj = 1)_ 

P(Sj = 1|Sj-usj+i)L(sj = 1) + P(Sj = 2\Sj-i,sJ+1)L(sj = 2)' 

The state Sj can then be drawn easily using a uniform distribution on the unit 
interval [0, 1], 

MARKOV SWITCHING MODELS 

663 

Remark. Since sj and Sj+1 are highly correlated when e\ and ej are small, it 
is more efficient to draw several sj jointly. However, the computation involved in 
enumerating the possible state configurations increases quickly with the number of 
states drawn jointly. □ 

Example 12.6. In this example, we consider the monthly log stock returns of 
General Electric Company from January 1926 to December 1999 for 888 observa¬ 
tions. The returns are in percentages and shown in Figure 12.14(a). For comparison 
purposes, we start with a GARCH-M model for the series and obtain 

rt = 0.182-y/^r + at, at = y/JTtet, 

ht = 0.546 + 1.740fc,_i - 0.775ht-2 + 0.025at2_1, (12.57) 

where rt is the monthly log return and {et} is a sequence of independent Gaussian 
white noises with mean zero and variance 1. All parameter estimates are highly 
significant with p values less than 0.0006. The Ljung-Box statistics of the stan¬ 
dardized residuals and their squared series fail to suggest any model inadequacy. It 

1940 1960 1980 2000 

Year 

(a) 

Figure 12.14 (a) Time plot of monthly log returns, in percentages, of GE stock from 1926 to 1999. 

(b) Time plot of the posterior probability of being in state 2 based on results of last 2000 iterations 

of Gibbs sampling with 5000 + 2000 total iterations. Model used is two-state Markov switching 

GARCH-M model. 

664 

MARKOV CHAIN MONTE CARLO METHODS WITH APPLICATIONS 

is reassuring to see that the risk premium is positive and significant. The GARCH 
model in Eq. (12.57) can be written as 

(1 - 1.7655 + 0.775B2)a? = 0.546 + (1 - 0.025B)ry, 

where ry = aj — ht and 5 is the back-shift operator such that Baf = aj_l. As 
discussed in Chapter 3, the prior equation can be regarded as an ARMA(2,1) model 
with nonhomogeneous innovations for the squared series a?. The AR polynomial 
can be factorized as (1 — 0.945 B) (1 — 0.8205), indicating two real characteristic 
roots with magnitudes less than 1. Consequently, the unconditional variance of rt 
is finite and equal to 0.546/(1 — 1.765 + 0.775) ^ 49.64. 

Turn to Markov switching models. We use the following prior distributions: 

j8i ~ AT (0.3, 0.09), p2 ~ JV(1.3, 0.09), e,- ~ Beta(5, 95). 

The initial parameter values used are (a) et =0.1, (b) s\ is a Bernoulli trial with 
equal probabilities and st is generated sequentially using the initial transition prob¬ 
abilities, and (c) ofi = (1.0, 0.6, 0.2)' and a2 = (2, 0.7, 0.1)'. Gibbs samples of aij 
are drawn using the Griddy Gibbs with 400 grid points, equally spaced over the 
following ranges: ai0 € [0, 6.0], an e [0, 1], and ai2 e [0, 0.5], In addition, we 
implement the constraints an + ai2 < 1 for i = 1, 2. The Gibbs sampler is run 
for 5000 + 2000 iterations, but only results of the last 2000 iterations are used to 
make inference. 

Table 12.5 shows the posterior means and standard deviations of parameters 
of the Markov switching GARCH-M model in Eq. (12.55). In particular, it also 
contains some statistics showing the difference between the two states such as 
^ = Pi — Pi- The difference between the risk premiums is statistically significant 
at the 5% level. The differences in posterior means of the volatility parameters 
between the two states appear to be insignificant. Yet the posterior distributions of 
volatility parameters show some different characteristics. Figures 12.15 and 12.16 
show the histograms of all parameters in the Markov switching GARCH-M model. 
They exhibit some differences between the two states. Figure 12.17 shows the 
time plot of the persistent parameter an + ai2 for the two states. It shows that the 
persistent parameter of state 1 reaches the boundary 1.0 frequently, but that of state 
2 does not. The expected durations of the two states are about 11 and 9 months, 
respectively. Figure 12.14(b) shows the posterior probability of being in state 2 for 
each observation. 

Finally, we compare the fitted volatility series of the simple GARCH-M model 
in Eq. (12.57) and the Markov switching GARCH-M model in Eq. (12.55). The two 
fitted volatility series (Figure 12.18) show similar patterns and are consistent with 
the behavior of the squared log returns. The simple GARCH-M model produces a 
smoother volatility series with lower estimated volatilities. 

MARKOV SWITCHING MODELS 

665 

TABLE 12.5 Fitted Markov Switching GARCH-M Model for Monthly Log Returns 
of GE Stock from January 1926 to December 1999“ 

Parameter 

Posterior mean 
Posterior standard error 

Parameter 

Posterior mean 
Posterior standard Error 

fh 

0.111 
0.043 

h 

0.247 
0.050 

State 1 

e\ 

0.089 
0.012 

State 2 

«2 

0.112 
0.014 

£*io 

2.070 
1.001 

£*20 

2.740 
1.073 

Difference Between States 

£*11 

0.844 
0.038 

£*21 

0.869 
0.031 

£*12 

0.033 
0.033 

£*22 

0.068 
0.024 

Parameter 

- P\ 

£?2 — e\ 

£*20 — £*10 

C*21 - C*ll 

£*22 — £*12 

Posterior mean 
Posterior standard error 

0.135 
0.063 

0.023 
0.019 

0.670 
1.608 

0.026 
0.050 

-0.064 
0.043 

aThe numbers shown are the posterior means and standard deviations based on a Gibbs sampling with 

5000 + 2000 iterations. Results of the first 5000 iterations are discarded. The prior distributions and 

initial parameter estimates are given in the text. 

Figure 12.15 Histograms of risk premium and transition probabilities of a two-state Markov switching 

GARCH-M model for monthly log returns of GE stock from 1926 to 1999. Results based on last 2000 

iterations of Gibbs sampling with 5000 + 2000 total iterations. 

666 

MARKOV CHAIN MONTE CARLO METHODS WITH APPLICATIONS 

0.75 0.80 0.85 0.90 0.95 

0.05 0.10 0.15 0.20 0.25 

«12 

0.70 0.75 0.80 0.85 0.90 0.95 

«21 

°22 

Figure 12.16 Histograms of volatility parameters of two-state Markov switching GARCH-M model 

for monthly log returns of GE stock from 1926 to 1999. Results based on last 2000 iterations of Gibbs 
sampling with 5000 + 2000 total iterations. 

12.10 FORECASTING 

Forecasting under the MCMC framework can be done easily. The procedure is 
simply to use the fitted model in each Gibbs iteration to generate samples for 
the forecasting period. In a sense, forecasting here is done by using the fitted 
model to simulate realizations for the forecasting period. We use the univariate 
stochastic volatility model to illustrate the procedure; forecasts of other models 
can be obtained by the same method. 

Consider the stochastic volatility model in Eqs. (12.20) and (12.21). Suppose that 
there are n returns available and we are interested in predicting the return rn+i and 
volatility hn+i for i = 1,..., i, where l > 0. Assume that the explanatory variables 
xjt in Eq. (12.20) are either available or can be predicted sequentially during the 
forecasting period. Recall that estimation of the model under the MCMC framework 
is done by Gibbs sampling, which draws parameter values from their conditional 
posterior distributions iteratively. Denote the parameters by = (/J0 j,j 

aj — («o,;, ai,j)\ and &yj for the jth Gibbs iteration. In other words, at the jth 

FORECASTING 

667 

c 
0 
o 
in 
0 
o o 

E 
13 
CO 

o 
o 

LO o> 
o 
o 
03 
o 
ID 
00 
d 
o 
00 
o 

Iterations 

(b) 

Figure 12.17 Time plots of persistent parameter an + a, 2 of two-state Markov switching GARCH-M 
model for monthly log returns of GE stock from 1926 to 1999. Results based on last 2000 iterations of 

Gibbs sampling with 5000 + 2000 total iterations. 

Gibbs iteration, the model is 

ft — fhj + + • • • + fipjXpt + at, (12.58) 

In ht = a0J + aij In ht-1 + vt, Var(uf) = (12.59) 

We can use this model to generate a realization of rn+i and hn+i for i — 1 
Denote the simulated realizations by rn+ij and hn+ij, respectively. These realiza¬ 

tions are generated as follows: 

• Draw a random sample vn+\ from N(0, cr^j) and use Eq. (12.59) to compute 

hn+l,j ■ 

• Draw a random sample en+i from A^(0, 1) to obtain an+\j = ^/hn+ij€n+\ 

and use Eq. (12.58) to compute rn+ij. 

• Repeat the prior two steps sequentially for n + i with i = 2, ..., l. 

If we run a Gibbs sampling for M + N iterations in model estimation, we only 
need to compute the forecasts for the last N iterations. This results in a random 

668 

MARKOV CHAIN MONTE CARLO METHODS WITH APPLICATIONS 

1940 

1960 

1980 

2000 

Year 

(a) 

Figure 12.18 Fitted volatility series for monthly log returns of GE stock from 1926 to 1999: (a) squared 

log returns, (b) GARCH-M model in Eq. (12.59), and (c) two-state Markov switching GARCH-M model 
in Eq. (12.57). 

sample for rn+i and hn+i. More specifically, we obtain 

\rn+\J, ■ ■ ■ , rn+t,j}Ij=\, [hn + \ j, . . . , hn+ij}y_J. 

These two random samples can be used to make inference. For example, point 
forecasts of the return rn+, and volatility hn+j are simply the sample means of the 
two random samples. Similarly, the sample standard deviations can be used as the 
variances of forecast errors. To improve the computational efficiency in volatility 
forecast, importance sampling can be used; see Gelman, Carlin, Stem, and Rubin 
(2003). 

Example 12.7. (Example 12.3 continued) As a demonstration, we consider the 
monthly log return series of the S&P 500 index from 1962 to 1999. Table 12.6 
gives the point forecasts of the return and its volatility for five forecast horizons 
starting with December 1999. Both the GARCH model in Eq. (12.26) and the 

OTHER APPLICATIONS 

669 

TABLE 12.6 Volatility Forecasts for Monthly Log Return of S&P 500 Index" 

Horizon 

1 

2 

3 

4 

5 

GARCH 
SVM 

GARCH 
SVM 

0.66 
0.53 

17.98 
19.31 

Log Return 

0.66 
0.78 

Volatility 

18.12 
19.36 

0.66 
0.92 

18.24 
19.35 

0.66 
0.88 

18.34 
19.65 

0.66 
0.84 

18.42 
20.13 

“The data span is from January 1962 to December 1999 and the forecast origin is December 1999. 

Forecasts of the stochastic volatility model are obtained by a Gibbs sampling with 2000 + 2000 

iterations. 

stochastic volatility model in Eq. (12.27) are used in the forecasting. The volatility 
forecasts of the GARCH(1,1) model increase gradually with the forecast horizon 
to the unconditional variance 3.349/(1 —0.086 — 0.735) = 18.78. The volatility 
forecasts of the stochastic volatility model are higher than those of the GARCH 
model. This is understandable because the stochastic volatility model takes into 
consideration the parameter uncertainty in producing forecasts. In contrast, the 
GARCH model assumes that the parameters are fixed and given in Eq. (12.26). 
This is an important difference and is one of the reasons that GARCH models tend 
to underestimate the volatility in comparison with the implied volatility obtained 
from derivative pricing. 

Remark. Besides the advantage of taking into consideration parameter uncer¬ 
tainty in forecast, the MCMC method produces in effect a predictive distribution 
of the volatility of interest. The predictive distribution is more informative than a 
simple point forecast. It can be used, for instance, to obtain the quantiles needed 
in value at risk calculation. □ 

12.11 OTHER APPLICATIONS 

The MCMC method is applicable to many other financial problems. For example, 
Zhang, Russell, and Tsay (2008) use it to analyze information determinants of 
bid and ask quotes, McCulloch and Tsay (2001) use the method to estimate a 
hierarchical model for IBM transaction data, and Eraker (2001) and Elerian, Chib, 
and Shephard (2001) use it to estimate diffusion equations. The method is also 
useful in value at risk calculation because it provides a natural way to evaluate 
predictive distributions. The main question is not whether the methods can be used 
in most financial applications, but how efficient the methods can become. Only 
time and experience can provide an adequate answer to the question. 

670 

MARKOV CHAIN MONTE CARLO METHODS WITH APPLICATIONS 

EXERCISES 

12.1. Suppose that x is normally distributed with mean \x and variance 4. Assume 
that the prior distribution of /r is also normal with mean 0 and variance 25. 
What is the posterior distribution of ji given the data point xl 

12.2. Consider the linear regression model with time series errors in Section 12.5. 

Assume that zt is an AR(p) process (i.e., zt = (t>\Zt-\ H-b <PPzt-p + at). 
Let 0 = (0i, ..., (pp)' be the vector of AR parameters. Derive the condi¬ 
tional posterior distributions of f(fi\Y, X, 0, a2), f(</>\Y,X,/3,a2), and 
/(a2|y, X, /?, 0) using the conjugate prior distributions, that is, the pri¬ 
ors are 

P ~ N(Po’ ^o), 0 ~ N(0O, A0), (vX)/a2 ~ xl- 

12.3. Consider the linear AR(p) model in Section 12.6.1. Suppose that Xh and 
Xh+1 are two missing values with a joint prior distribution being multivariate 
normal with mean fi() and covariance matrix Y0. Other prior distributions 
are the same as that in the text. What is the conditional posterior distribution 
of the two missing values? 

12.4. Consider the monthly log returns of Ford Motors stock from January 1965 
to December 2008: (a) Build a GARCH model for the series, (b) build 
a stochastic volatility model for the series, and (c) compare and discuss 
the two volatility models. The simple returns of the stock are in the file 
m-fsp6508.txt. 

12.5. Build a stochastic volatility model for the daily log return of Cisco Systems 
stock from January 2001 to December 2008. You may download the simple 
return of the stock from the CRSP database or the file d-csco0108.txt. 
Transform the data into log returns in percentage. Use the model to obtain 
a predictive distribution for 1-step-ahead volatility forecast at the forecast 
origin December 31, 2008. Finally, use the predictive distribution to com¬ 
pute the value at risk of a long position worth $1 million with probability 
0.01 for the next trading day. 

12.6. Build a bivariate stochastic volatility model for the monthly log returns of 
Ford Motors stock and the S&P composite index for the sample period from 
January 1965 to December 2008. Discuss the relationship between the two 
volatility processes and compute the time-varying beta for the Ford stock. 

12.7. Consider the monthly log returns of Procter & Gamble stock and the value- 
weighted index from January 1965 to December 2008. The simple returns 
are given in the file m-pgvw650 8 . txt. Transform the data into log returns 
in percentages, (a) Build a bivariate stochastic volatility model for the two 
return series, (b) Build a BEKK(1,1) model for the two series, (c) Compare 
and discuss the two models. 

12.8. Consider the monthly data of 30-year mortgage rate and the 3-month Trea¬ 
sury Bill rate of the secondary market from April 1971 to September 2009. 

REFERENCES 

671 

The data are in m-mort3mtb7109. txt. (a) Build a regression model with 
time series error to study the effect of 3-month Treasury Bill rate on the 
mortgage rate, (b) Reestimate the model using MCMC method, (c) Compare 
and discuss the two fitted models. 

REFERENCES 

Artigas, J. C. and Tsay, R. S. (2004). Effective estimation of stochastic diffusion models with 
leverage effects and jumps. Working paper. Graduate School of Business, University 
of Chicago. 

Box, G. E. P. and Tiao, G. C. (1973). Bayesian Inference in Statistical Analysis. Addison- 

Wesley, Reading, MA. 

Carlin, B. P. and Louis, T. A. (2000). Bayes and Empirical Bayes Methods for Data Analysis, 

2nd ed. Chapman and Hall, London. 

Carter, C. K. and Kohn, R. (1994). On Gibbs sampling for state space models. Biometrika 

81: 541-553. 

Chang, I., Tiao, G. C., and Chen, C. (1988). Estimation of time series parameters in the 

presence of outliers. Technometrics 30: 193-204. 

Chib, S., Nardari, F., and Shephard, N. (2002). Markov chain Monte Carlo methods for 

stochastic volatility models. Journal of Econometrics 108: 281-316. 

DeGroot, M. H. (1970). Optimal Statistical Decisions. McGraw-Hill, New York. 

Dempster, A. P., Laird, N. M., and Rubin, D. B. (1977). Maximum likelihood from incom¬ 
plete data via the EM algorithm (with discussion). Journal of the Royal Statistical 

Society Series B 39: 1-38. 

Elerian, O., Chib, S. and Shephard, N. (2001). Likelihood inference for discretely observed 

nonlinear diffusions. Econometrica 69: 959-993. 

Eraker, B. (2001). Markov Chain Monte Carlo analysis of diffusion with application to 

finance. Journal of Business & Economic Statistics 19: 177-191. 

Fruhwirth-Schnatter, S. (1994). Data augmentation and dynamic linear models. Journal of 

Time Series Analysis 15: 183-202. 

Gelfand, A. E. and Smith, A. F. M. (1990). Sampling-based approaches to calculating 
marginal densities. Journal of the American Statistical Association 85: 398-409. 

Gelman, A., Carlin, J. B., Stem, H. S., and Rubin, D. B. (2003). Bayesian Data Analysis, 

2nd ed. Chapman and Hall/CRC, London. 

Geman, S. and Geman, D. (1984). Stochastic relaxation, Gibbs distributions, and the 
Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine 

Intelligence 6: 721—741. 

Hasting, W. K. (1970). Monte Carlo sampling methods using Markov chains and their 

applications. Biometrika 57: 97-109. 

Jacquier, E., Poison, N. G., and Rossi, P. E. (1994). Bayesian analysis of stochastic volatility 
models (with discussion). Journal of Business & Economic Statistics 12: 371-417. 

Jacquier, E., Poison, N. G., and Rossi, P. E. (2004). Bayesian analysis of stochastic volatility 
models with fat-tails and correlated errors. Journal of Econometrics 122: 185-212. 

672 

MARKOV CHAIN MONTE CARLO METHODS WITH APPLICATIONS 

Jones, R. H. (1980). Maximum likelihood fitting of ARMA models to time series with 

missing observations. Technometrics 22: 389-395. 

Justel, A., Pena, D., and Tsay, R. S. (2001). Detection of outlier patches in autoregressive 

time series. Statistica Sinica 11: 651-673. 

Kim, S., Shephard, N., and Chib, S. (1998). Stochastic volatility: Likelihood inference and 

comparison with ARCH models. Review of Economic Studies 65: 361-393. 

Liu, J., Wong, W. H., and Kong, A. (1994). Correlation structure and convergence rate of 
the Gibbs samplers. I. Applications to the comparison of estimators and augmentation 
schemes. Biometrika 81: 27-40. 

McCulloch, R. E. and Tsay, R. S. (1994a). Bayesian analysis of autoregressive time series 

via the Gibbs sampler. Journal of Time Series Analysis 15: 235-250. 

McCulloch, R. E. and Tsay, R. S. (1994b). Statistical analysis of economic time series via 

Markov switching models. Journal of Time Series Analysis 15: 523-539. 

McCulloch, R. E. and Tsay, R. S. (2001). Nonlinearity in high-frequency financial data and 
hierarchical models. Studies in Nonlinear Dynamics and Econometrics 5: 1 — 17. 

Metropolis, N. and Ulam, S. (1949). The Monte Carlo method. Journal of the American 

Statistical Association 44: 335-341. 

Metropolis, N„ Rosenbluth, A. W„ Rosenbluth, M. N., Teller, A. H„ and Teller, E. (1953). 
Equation of state calculations by fast computing machines. Journal of Chemical Physics 
21 1087-1092. 

Tanner, M. A. (1996). Tools for Statistical Inference: Methods for the Exploration of Posterior 

Distributions and Likelihood Functions, 3rd ed. Springer, New York. 

Tanner, M. A. and Wong, W. H. (1987). The calculation of posterior distributions by data 
augmentation (with discussion). Journal of the American Statistical Association 82' 
528-550. 

Tierney, L. (1994). Markov chains for exploring posterior distributions (with discussion). 

Annals of Statistics 22: 1701-1762. 

Tsay, R. S. (1988). Outliers, level shifts, and variance changes in time series. Journal of 

Forecasting 7: 1-20. 

Tsay, R. S„ Pena, D., and Pankratz, A. (2000). Outliers in multivariate time series. Biometrika 

87: 789-804. 

Zhang, M. Y., Russell, J. R., and Tsay, R. S. (2008). Determinants of bid and ask quotes 

and implications for the cost of trading. Journal of Empirical Finance 15: 656-678. 

Index 

D(un) condition, 378, 379 

ACD model, 255 

exponential, 256 

generalized Gamma, 257 

threshold, 264 

Weibull, 256 

Activation function, see Neural network, 

200 

Airline model, 84 

Akaike information criterion (AIC), 48, 

406 

Arbitrage, 443 

ARCH effect, 114 

ARCH model, 115 

estimation, 120 

t distribution, 121 

GED innovation, 122 

normal, 120 

Arranged autoregression, 212 

Augmented Dickey-Fuller test, 77 

Autocorrelation function (ACF), 31 

Autocovariance, 30 

Autoregressive integrated moving-average 

(ARIMA) model, 76 

Autoregressive model, 37 

estimation, 49 

forecasting, 54 

order, 47 

stationarity, 46 

Autoregressive moving-average (ARMA) 

model, 64 

forecasting, 68 

Back propagation 

neural network, 203 

Back-shift operator, 41 

Bartlett’s formula, 32 
Bayesian information criterion (BIC), 48 

Bid-ask bounce, 236 

Bid-ask spread, 236 

Bilinear model, 177 

Black-Scholes 

differential equation, 299 

Black-Scholes formula 

European call option, 301, 109 

European put option, 302 

Brownian motion, 290 

geometric, 294 

standard, 289 

Business cycle, 42 

Canonical correlation analysis, 437 

Characteristic equation, 46 

Characteristic root, 42, 46 

CHARMA model, 150 
Cholesky decomposition, 400, 458, 

517 

Cointegration, 91, 428 

Cointegration test 

maximum eigenvalue, 437 

trace, 437 

Common factor, 543 

Common trend, 430 

Companion matrix, 404 
Compounding, 4 

Conditional distribution, 8 

Analysis of Financial Time Series, Third Edition, By Ruey S. Tsay 
Copyright © 2010 John Wiley & Sons, Inc. 

673 

674 

Conditional forecast, 55 

Conditional heteroscedasticity, 97 
HAC covariance estimator, 98 

Conditional-likelihood method, 61 

Conjugate prior, see Distribution, 618 
Correlation 

coefficient, 30 

constant, 522 

time-varying, 525 

Cost-of-carry model, 443 

Covariance matrix, 390 

Cross-correlation matrix, 390, 391 
Cross-validation, 193 
CVaR 

Conditional value at risk, 328 

Data 

3M stock return, 21, 67, 74, 185 

Bank of America stock return, 485 
BHP daily price, 449 

Cisco stock return, 296, 538, 546 
Citi-Group stock return, 21 

Civilian employment number, 475 
Consumer price index, 475 

equal-weighted index, 21, 60, 61, 102, 178, 

213 

GE stock return, 663 

Hewlett-Packard stock return, 485 
Hong Kong market index, 507 

IBM stock return, 21, 33, 145, 154, 155, 182, 

203, 213, 295, 330, 334, 338, 340, 348, 

355, 364, 393, 485, 524, 544, 645 

IBM transactions, 238, 241, 246, 250, 262, 268 
Intel stock return, 21, 111, 123, 339, 485, 538, 

546 

J.P. Morgan Chase stock return, 485 
Japan market index, 507 

Johnson & Johnson’s earnings, 81 
Mark/dollar exchange rate, 116 
Microsoft sock return, 21 

SP 500 excess return. 134, 151 

SP 500 index futures, 443, 445 

INDEX 

Data augmentation, 615 
Decomposition model, 248 

Descriptive statistics, 21 
Diagonal VEC model, 510 

Dickey-Fuller test, 77 

Differencing 76 

seasonal, 84 

Distribution 
beta, 620 

double exponential, 312 
Frechet family, 344 

gamma, 277, 619 

generalized error, 122 

generalized extreme value, 343 

Generalized gamma, 278 

generalized Pareto, 361, 373 
inverted chi-squared, 621 
Laplacian, 312 

multivariate t, 548 

multivariate normal, 460, 619 
negative binomial, 620 
Poisson, 620 
posterior, 618 
prior, 618 

conjugate, 618 

skew-Student-/, 122 
Weibull, 278 

Diurnal pattern, 238 

Donsker’s theorem, 290 
Duration 

between trades, 239 
model, 253 

Durbin-Watson statistic, 97 

Dynamic conditional correlation model, 531 

EG ARCH model, 143 
forecasting, 147 

Eigenvalue, 457 

Eigenvector, 457 

EM algorithm, 614 

Error correction model, 431 
Estimation 

SP 500 index return, 154, 158, 160, 393, 524, 

extreme value parameter, 345 

538, 544, 640, 645, 658 

SP 500 spot price, 445 

Exact-likelihood method, 61 
Exceedance, 359 

U.S. 3-month Treasury bill rate, 196 

Exceeding times, 359 

U.S. government bond, 23, 395, 494 
U.S. interest rate, 23, 90, 627, 635 

U.S. monthly unemployment rate, 181 
U.S. real GNP, 42, 188 

U.S. unemployment rate, 218 
Vale daily price, 449 

value-weighted index, 21, 145 

value-weighted index, 34, 47, 102, 213 

Excess return, 5 

Expected shortfall, 333 

Extended autocorrelation function, 66 
Extremal index, 377, 380 

Extreme value theory, 342 

Factor analysis, 489 

Factor mimicking portfolio, 482 

675 

INDEX 

Factor model 

common factor, 468 

estimation, 491 

factor loading, 468 

specific factor, 468 

Factor rotation 

varimax, 492 

Filtering, 561 

Forecast 

horizon, 54 

origin, 54 

Forecast updating formula, 581 

Forecasting 

MCMC method, 666 

Forward filtering and backward sampling, 

Fractional differencing, 101 

GARCH model, 131 

Cholesky decomposition, 528 

multivariate, 521 

diagonal, 523 

time-varying correlation, 526 

GARCH-M model, 142, 660 

Generalized least squares, 478 

Generalized Pareto Distribution, 361 

Geometric ergodicity, 179 

Gibbs sampling, 615 

Global minimum variance portfolio, 473 

Griddy Gibbs, 623 

Half-life, 56 

Hazard function, 279 

Hh function, 318 

Hill estimator, 348 

Hyperparameter, 625 

Identifiability, 422 
IGARCH model, 140, 329 

Implied volatility, 110 

Impulse response function, 71, 413 

Innovation, 36 

Inverted yield curve, 91 

Invertibility, 60, 431 

Invertible ARMA model, 70 

Ito process, 292 

Ito’s lemma, 294 

multivariate, 310 

Joint distribution function, 7 

Jump diffusion, 311 

Kalman filter, 563, 593 

Kalman gain, 563, 593 

Kernel, 190 

bandwidth, 192 

Epanechnikov, 191 

Gaussian, 191 

Kernel regression, 190 

Kurtosis, 9 

excess, 9 

Lag operator, 41 

Lead-lag relationship, 391 

Leptokurtic, 9 

Leverage effect, 111, 144, 650 

Likelihood function, 19 

Linear time series, 36 

Liquidity, 235 

Ljung-Box statistic, 114, 32 

multivariate, 397 

Local linear regression, 195 

Local trend model, 558 

Log return, 5 

Logit model, 267 

Long position, 6 

Long-memory 

stochastic volatility, 154 

time series, 101 

Marginal distribution, 7 

Market model, 470 

Markov process, 613 

Markov property, 37 

Markov switching model, 187, 660 

Martingale difference, 132 

Maximum-likelihood estimate 

exact, 420 

MCMC method, 199 

Mean equation, 113 

Mean excess function, 362 

Mean excess plot, 362 

Mean residual life plot, 363 

Mean reversion, 56, 71 

half-life, 56 

Metropolis algorithm, 622 

Metropolis-Hasting algorithm, 623 

Missing value, 600, 628 

Model checking, 51 

Moment 

of a random variable, 8 

Moving-average model, 57 

Nadaraya-Watson estimator, 191 

Neural network, 199 

activation function, 200 

feed-forward, 200 

INDEX 

676 

Neural network {Continued.) 

skip layer, 202 

Neuron, see Neural network, 200 

Node, see Neural network, 200 

Nonlinearity test, 205 

BDS, 208 

bispectral, 207 

F test, 211 

Keenan, 210 

RESET, 209 

Tar-F, 213 

Nonstationarity 

unit-root, 72 

Nonsynchronous trading, 232 

Nuisance parameter, 211 

Options 

American, 288 

at-the-money, 288 

European call, 109 

in-the-money, 288 

out-of-the-money, 288 
stock, 288 

strike price, 109, 288 

Order statistics, 339 

Ordered probit model, 245 

Orthogonal factor model, 489 
Outlier 

additive, 628 

detection, 632 

OX command 

garchOxFit, 130 

Pairs trading, 446 

Parametric bootstrap, 215 

Partial autoregressive function (PACF), 

46 

PCD model, 265 

Peaks over Thresholds, 359 
n weight, 70 

Pickands estimator, 348 

Platykurtic, 9 

Poisson process, 311 

inhomogeneous, 371 

intensity function, 363 

Portmanteau test 32, see Ljung-Box statistic, 

397 

Positive-definite matrix, 458 
Prediction, 561 

Present value, 5 

Principal component analysis, 483, 543 
Vf weight, 36 

Put-call parity, 302 

Quantile, 8 

definition, 327 

R command 
ar, 49 

exindex, 384 

factanal, 495 

garchFit, 127 
gev, 351 

glm, 252 

gpd, 367 
hill, 351 

lowess, 196 
meplot, 363 

nnet, 223 
optim, 186 

pot, 366 

princomp, 488 
qt, 335 

quantile, 341 
read.table, 24 

riskmeasures, 370 
ts, 24 

R package, 24 
R-square, 54 

Adjusted, 54 

Random coefficient (RCA) model, 153 
Random walk, 72 
with drift, 73 

Realized volatility, 559, 162 

Reduced-form model, 399 
Regression 

with time series errors, 90 

Return level, 358 

stress period, 358 

RiskMetrics, 328 

S-Plus command 

archTest, 115 

autocorTest, 115 
coint, 439 

ewmal, 331 
garch, 124 

mfactor, 500 
mgarch, 507 
OLS, 97 
tslag, 97 

VAR, 414 

VECM, 439 

Sample autocorrelation, 31 
Scree plot, 488 

Seasonal adjustment, 82 
Seasonal model, 81 

multiplicative, 84 

INDEX 

Shape parameter 

of a distribution, 343 

Shock, 36, 55, 113 

Short position, 6 

Simple return, 3 

Skewness, 9 

Smoothed disturbance, 597 

Smoothing, 189, 561 

Square root of time rule, 330 

Standard Brownian motion, 77 

State-space model, 576 

nonlinear, 199 

Stationarity, 30 

weak, 390 

Statistical arbitrage, 446 

Steady state, 594 

Stochastic diffusion equation, 292 

Stochastic volatility model, 153, 637 

multivariate, 643 

Structural equation, 400 

Structural form, 400 

Structural Time Series, 590 

Structural time series model, 558 

Student-r distribution 

standardized, 121 

Survival function, 363 

Tail index, 343 

TGARCH model, 149 

general form, 182 

Threshold, 180 
Threshold autoregressive model 

multivariate, 444 

self-exciting, 180 

smooth, 184 

677 

Threshold cointegration, 444 

Time plot, 19 

Transactions data, 237 

Trend stationary model, 75 

Unit-root test, 76 

Unit-root time series, 72 

Unobserved component model, 590 

Value at risk, 326, 546 

VaR 

econometric approach, 333 

homogeneous Poisson process, 365 
inhomogeneous Poisson process, 371 

RiskMetrics, 329 
traditional extreme value, 353 

Vector AR model, 399 

Vector ARM A model, 422 

marginal models, 427 

Vector MA model, 417 

VIX Volatility Index, 110 

Volatility, 109 
Volatility equation, 113 

Volatility model 
factor, 543 

Volatility smile, 311 

Weighted least squares, 477 

White noise, 36 

Wiener process, 289 

generalized, 291 

Yule-Walker equation 

multivariate, 404 

' 

. 

■ 

WILEY SERIES IN PROBABILITY AND STATISTICS 

ESTABLISHED BY WALTER A. SHEWHART AND SAMUEL S. WlLKS 

Editors: David J. Balding, Noel A. C. Cressie, Garrett M. Fitzmaurice, 
Iain M. Johnstone, Geert Molenberghs, David W. Scott, Adrian F. M. Smith, 
Ruey S. Tsay, Sanford Weis berg 
Editors Emeriti: Vic Barnett, J. Stuart Hunter, Jay Kadane, JozefL. Teugels 

The Wiley Series in Probability and Statistics is well established and authoritative. It covers 
many topics of current research interest in both pure and applied statistics and probability 
theory. Written by leading statisticians and institutions, the titles span both state-of-the-art 
developments in the field and classical methods. 

Reflecting the wide range of current research in statistics, the series encompasses applied, 
methodological and theoretical statistics, ranging from applications and new techniques 
made possible by advances in computerized practice to rigorous treatment of theoretical 
approaches. 

This series provides essential and invaluable reading for all statisticians, whether in aca¬ 

demia, industry, government, or research. 

t ABRAHAM and LEDOLTER • Statistical Methods for Forecasting 
AGRESTI • Analysis of Ordinal Categorical Data, Second Edition 
AGRESTI • An Introduction to Categorical Data Analysis, Second Edition 
AGRESTI • Categorical Data Analysis, Second Edition 
ALTMAN, GILL, and McDONALD • Numerical Issues in Statistical Computing for the 

Social Scientist 

AMARATUNGA and CABRERA • Exploration and Analysis of DNA Microarray and 

Protein Array Data 

ANDEL ■ Mathematics of Chance 
ANDERSON • An Introduction to Multivariate Statistical Analysis, Third Edition 

* ANDERSON • The Statistical Analysis of Time Series 

ANDERSON, AUQUIER, HAUCK, OAKES, VANDAELE, and WEISBERG • 

Statistical Methods for Comparative Studies 

ANDERSON and LOYNES • The Teaching of Practical Statistics 
ARMITAGE and DAVID (editors) • Advances in Biometry 
ARNOLD, BALAKRISHNAN, and NAGARAJA • Records 

* ARTHANARI and DODGE • Mathematical Programming in Statistics 
* BAILEY • The Elements of Stochastic Processes with Applications to the Natural 

Sciences 

BALAKRISHNAN and KOUTRAS • Runs and Scans with Applications 
BALAKRISHNAN and NG • Precedence-Type Tests and Applications 
BARNETT • Comparative Statistical Inference, Third Edition 
BARNETT • Environmental Statistics 
BARNETT and LEWIS • Outliers in Statistical Data, Third Edition 
BARTOSZYNSKI and NIEWIADOMSKA-BUGAJ • Probability and Statistical Inference 
BASILEVSKY ■ Statistical Factor Analysis and Related Methods: Theory and 

Applications 

BASU and RIGDON • Statistical Methods for the Reliability of Repairable Systems 
BATES and WATTS • Nonlinear Regression Analysis and Its Applications 
BECHHOFER, SANTNER, and GOLDSMAN • Design and Analysis of Experiments for 

Statistical Selection, Screening, and Multiple Comparisons 

*Now available in a lower priced paperback edition in the Wiley Classics Library. 
tyjow available in a lower priced paperback edition in the Wiley-Interscience Paperback Series. 

BELSLEY • Conditioning Diagnostics: Collinearity and Weak Data in Regression 
t BELSLEY, KETH, and WELSCH ■ Regression Diagnostics: Identifying Influential 

Data and Sources of Collinearity 

BEND AT and PIERSOL • Random Data: Analysis and Measurement Procedures, 

Fourth Edition 

BERRY, CHALONER, and GEWEKE • Bayesian Analysis in Statistics and 

Econometrics: Essays in Honor of Arnold Zellner 

BERNARDO and SMITH • Bayesian Theory 
BHAT and MILLER • Elements of Applied Stochastic Processes, Third Edition 
BHATTACHARYA and WAYMIRE • Stochastic Processes with Applications 
BILLINGSLEY • Convergence of Probability Measures, Second Edition 
BILLINGSLEY • Probability and Measure, Third Edition 
BIRKES and DODGE • Alternative Methods of Regression 
BISWAS, DATTA, FINE, and SEGAL • Statistical Advances in the Biomedical Sciences: 

Clinical Trials, Epidemiology, Survival Analysis, and Bioinformatics 

BLISCHKE AND MURTHY (editors) • Case Studies in Reliability and Maintenance 
BLISCHKE AND MURTHY * Reliability: Modeling, Prediction, and Optimization 
BLOOMFIELD • Fourier Analysis of Time Series: An Introduction, Second Edition 
BOLLEN • Structural Equations with Latent Variables 
BOLLEN and CURRAN • Latent Curve Models: A Structural Equation Perspective 
BOROVKOV • Ergodicity and Stability of Stochastic Processes 
BOULEAU • Numerical Methods for Stochastic Processes 
BOX • Bayesian Inference in Statistical Analysis 
BOX • R. A. Fisher, the Life of a Scientist 
BOX and DRAPER • Response Surfaces, Mixtures, and Ridge Analyses, Second Edition 

* BOX and DRAPER • Evolutionary Operation: A Statistical Method for Process 

Improvement 

BOX and FRIENDS • Improving Almost Anything, Revised Edition 
BOX, HUNTER, and HUNTER • Statistics for Experimenters: Design, Innovation, 

and Discovery, Second Editon 

BOX, JENKINS, and REINSEL • Time Series Analysis: Forcasting and Control, Fourth 

Edition 

BOX, LUCENO, and PANIAGUA-QUINONES • Statistical Control by Monitoring 

and Adjustment, Second Edition 

BRANDIMARTE • Numerical Methods in Finance: A MATLAB-Based Introduction 

' BROWN and HOLLANDER • Statistics: A Biomedical Introduction 

BRUNNER, DOMHOF, and LANGER • Nonparametric Analysis of Longitudinal Data in 

Factorial Experiments 

BUCKLEW • Large Deviation Techniques in Decision, Simulation, and Estimation 
CAIROLI and DALANG • Sequential Stochastic Optimization 
CASTILLO, HADI, BALAKRISHNAN, and SARABIA • Extreme Value and Related 

Models with Applications in Engineering and Science 

CHAN • Time Series: Applications to Finance, Second Edition 
CHARALAMBIDES • Combinatorial Methods in Discrete Distributions 
CHATTERJEE and HADI • Regression Analysis by Example, Fourth Edition 
CHATTERJEE and HADI • Sensitivity Analysis in Linear Regression 
CHERNICK ■ Bootstrap Methods: A Guide for Practitioners and Researchers, 

Second Edition 

CHERNICK and FRIIS • Introductory Biostatistics for the Health Sciences 
CHILES and DELFINER • Geostatistics: Modeling Spatial Uncertainty 
CHOW and LIU • Design and Analysis of Clinical Trials: Concepts and Methodologies, 

Second Edition 

CLARKE • Linear Models: The Theory and Application of Analysis of Variance 

*Now available in a lower priced paperback edition in the Wiley Classics Library. 
'Now available in a lower priced paperback edition in the Wiley-Interscience Paperback Series. 

CLARKE and DISNEY • Probability and Random Processes: A First Course with 

Applications, Second Edition 

* COCHRAN and COX • Experimental Designs, Second Edition 

COLLINS and LANZA • Latent Class and Latent Transition Analysis: With Applications 

in the Social, Behavioral, and Health Sciences 

CONGDON ■ Applied Bayesian Modelling 
CONGDON • Bayesian Models for Categorical Data 
CONGDON • Bayesian Statistical Modelling 
CONOVER • Practical Nonparametric Statistics, Third Edition 
COOK • Regression Graphics 
COOK and WEISBERG • Applied Regression Including Computing and Graphics 
COOK and WEISBERG • An Introduction to Regression Graphics 
CORNELL • Experiments with Mixtures, Designs, Models, and the Analysis of Mixture 

Data, Third Edition 

COVER and THOMAS • Elements of Information Theory 
COX • A Handbook of Introductory Statistical Methods 

* COX ■ Planning of Experiments 

CRESSIE • Statistics for Spatial Data, Revised Edition 
CSORGO and HORVATH • Limit Theorems in Change Point Analysis 
DANIEL • Applications of Statistics to Industrial Experimentation 
DANIEL • Biostatistics: A Foundation for Analysis in the Health Sciences, Eighth Edition 

* DANIEL • Fitting Equations to Data: Computer Analysis of Multifactor Data, 

Second Edition 

DASU and JOHNSON • Exploratory Data Mining and Data Cleaning 
DAVID and NAGARAJA • Order Statistics, Third Edition 

* DEGROOT, FIENBERG, and KADANE • Statistics and the Law 

DEL CASTILLO • Statistical Process Adjustment for Quality Control 
DeMARIS • Regression with Social Data: Modeling Continuous and Limited Response 

Variables 

DEMIDENKO • Mixed Models: Theory and Applications 
DENISON, HOLMES, MALLICK and SMITH • Bayesian Methods for Nonlinear 

Classification and Regression 

DETTE and STUDDEN • The Theory of Canonical Moments with Applications in 

Statistics, Probability, and Analysis 

DEY and MUKERJEE • Fractional Factorial Plans 
DILLON and GOLDSTEIN • Multivariate Analysis: Methods and Applications 
DODGE • Alternative Methods of Regression 

* DODGE and ROMIG ■ Sampling Inspection Tables, Second Edition 
* DOOB • Stochastic Processes 

DOWDY, WEARDEN, and CHILKO • Statistics for Research, Third Edition 
DRAPER and SMITH • Applied Regression Analysis, Third Edition 
DRYDEN and MARDIA • Statistical Shape Analysis 
DUDEWICZ and MISHRA • Modem Mathematical Statistics 
DUNN and CLARK • Basic Statistics: A Primer for the Biomedical Sciences, 

Third Edition 

DUPUIS and ELLIS • A Weak Convergence Approach to the Theory of Large Deviations 
EDLER and KITSOS • Recent Advances in Quantitative Methods in Cancer and Human 

Health Risk Assessment 

* ELANDT-JOHNSON and JOHNSON • Survival Models and Data Analysis 

ENDERS • Applied Econometric Time Series 

t ETHIER and KURTZ • Markov Processes: Characterization and Convergence 

EVANS, HASTINGS, and PEACOCK • Statistical Distributions, Third Edition 

*Now available in a lower priced paperback edition in the Wiley Classics Library. 

tNow available in a lower priced paperback edition in the Wiley-Interscience Paperback Series. 

FELLER • An Introduction to Probability Theory and Its Applications, Volume I, 

Third Edition, Revised; Volume II, Second Edition 

FISHER and VAN BELLE • Biostatistics: A Methodology for the Health Sciences 
FITZMAURICE, LAIRD, and WARE • Applied Longitudinal Analysis 

* FLEISS • The Design and Analysis of Clinical Experiments 

FLEISS • Statistical Methods for Rates and Proportions, Third Edition 
FLEMING and HARRINGTON • Counting Processes and Survival Analysis 
FUJIKOSHI, ULYANOV, and SHIMIZU • Multivariate Statistics: High-Dimensional and 

Large-Sample Approximations 

FULLER • Introduction to Statistical Time Series, Second Edition 

' FULLER • Measurement Error Models 

GALLANT • Nonlinear Statistical Models 
GEISSER • Modes of Parametric Statistical Inference 
GELMAN and MENG • Applied Bayesian Modeling and Causal Inference from 

Incomplete-Data Perspectives 

GEWEKE • Contemporary Bayesian Econometrics and Statistics 
GHOSH, MUKHOPADHYAY, and SEN • Sequential Estimation 
GIESBRECHT and GUMPERTZ • Planning, Construction, and Statistical Analysis of 

Comparative Experiments 

GIFI • Nonlinear Multivariate Analysis 
GIVENS and HOETING • Computational Statistics 
GLASSERMAN and YAO • Monotone Structure in Discrete-Event Systems 
GNANADESIKAN • Methods for Statistical Data Analysis of Multivariate Observations, 

Second Edition 

GOLDSTEIN and LEWIS • Assessment: Problems, Development, and Statistical Issues 
GREENWOOD and NIKULIN ■ A Guide to Chi-Squared Testing 
GROSS, SHORTLE, THOMPSON, and HARRIS • Fundamentals of Queueing Theory, 

Fourth Edition 

GROSS, SHORTLE, THOMPSON, and HARRIS • Solutions Manual to Accompany 

Fundamentals of Queueing Theory, Fourth Edition 
* HAHN and SHAPIRO ■ Statistical Models in Engineering 

HAHN and MEEKER • Statistical Intervals: A Guide for Practitioners 
HALD • A History of Probability and Statistics and their Applications Before 1750 
HALD • A History of Mathematical Statistics from 1750 to 1930 
HAMPEL • Robust Statistics: The Approach Based on Influence Functions 
HANNAN and DEISTLER • The Statistical Theory of Linear Systems 
HARTUNG, KNAPP, and SINHA • Statistical Meta-Analysis with Applications 
HEIBERGER • Computation for the Analysis of Designed Experiments 
HEDAYAT and SINHA • Design and Inference in Finite Population Sampling 
HEDEKER and GIBBONS • Longitudinal Data Analysis 
HELLER • MACSYMA for Statisticians 

HINKELMANN and KEMPTHORNE • Design and Analysis of Experiments, Volume 1: 

Introduction to Experimental Design, Second Edition 

HINKELMANN and KEMPTHORNE • Design and Analysis of Experiments, Volume 2: 

Advanced Experimental Design 

HOAGLIN, MOSTELLER, and TUKEY • Fundamentals of Exploratory Analysis 

of Variance 

* HOAGLIN, MOSTELLER, and TUKEY • Exploring Data Tables, Trends and Shapes 
* HOAGLIN, MOSTELLER, and TUKEY • Understanding Robust and Exploratory 

Data Analysis 

HOCHBERG and TAMHANE • Multiple Comparison Procedures 
HOCKING Methods and Applications of Linear Models: Regression and the Analysis 

of Variance, Second Edition 

*Now available in a lower priced paperback edition in the Wiley Classics Library. 
' Now available in a lower priced paperback edition in the Wiley-Interscience Paperback Series. 

HOEL • Introduction to Mathematical Statistics, Fifth Edition 
HOGG and KLUGMAN • Loss Distributions 
HOLLANDER and WOLFE • Nonparametric Statistical Methods, Second Edition 
HOSMER and LEMESHOW • Applied Logistic Regression, Second Edition 
HOSMER, LEMESHOW, and MAY • Applied Survival Analysis: Regression Modeling 

of Time-to-Event Data, Second Edition 

t HUBER and RONCHETTI • Robust Statistics, Second Edition 

HUBERTY • Applied Discriminant Analysis 
HUBERTY and OLEJNIK • Applied MANOVA and Discriminant Analysis, 

Second Edition 

HUNT and KENNEDY • Financial Derivatives in Theory and Practice, Revised Edition 
HURD and MIAMEE • Periodically Correlated Random Sequences: Spectral Theory 

and Practice 

HUSKOVA, BERAN, and DUPAC • Collected Works of Jaroslav Hajek— 

with Commentary 

HUZURBAZAR • Flowgraph Models for Multistate Time-to-Event Data 
IMAN and CONOVER • A Modem Approach to Statistics 

^ JACKSON • A User’s Guide to Principle Components 

JOHN • Statistical Methods in Engineering and Quality Assurance 
JOHNSON • Multivariate Statistical Simulation 
JOHNSON and BALAKRISHNAN • Advances in the Theory and Practice of Statistics: A 

Volume in Honor of Samuel Kotz 

JOHNSON and BHATTACHARYYA • Statistics: Principles and Methods, Fifth Edition 
JOHNSON and KOTZ ■ Distributions in Statistics 
JOHNSON and KOTZ (editors) • Leading Personalities in Statistical Sciences: From the 

Seventeenth Century to the Present 

JOHNSON, KOTZ, and BALAKRISHNAN ■ Continuous Univariate Distributions, 

Volume 1, Second Edition 

JOHNSON, KOTZ, and BALAKRISHNAN • Continuous Univariate Distributions, 

Volume 2, Second Edition 

JOHNSON, KOTZ, and BALAKRISHNAN • Discrete Multivariate Distributions 
JOHNSON, KEMP, and KOTZ • Univariate Discrete Distributions, Third Edition 
JUDGE, GRIFFITHS, HILL, LUTKEPOHL, and LEE • The Theory and Practice of 

Econometrics, Second Edition 

JURECKOVA and SEN • Robust Statistical Procedures: Aymptotics and Interrelations 
JUREK and MASON • Operator-Limit Distributions in Probability Theory 
KADANE • Bayesian Methods and Ethics in a Clinical Trial Design 
KADANE AND SCHUM • A Probabilistic Analysis of the Sacco and Vanzetti Evidence 
KALBFLEISCH and PRENTICE • The Statistical Analysis of Failure Time Data, Second 

Edition 

KARIYA and KURATA • Generalized Least Squares 
KASS and VOS • Geometrical Foundations of Asymptotic Inference 

t KAUFMAN and ROUSSEEUW • Finding Groups in Data: An Introduction to Cluster 

Analysis 

KEDEM and FOKIANOS • Regression Models for Time Series Analysis 
KENDALL, BARDEN, CARNE, and LE • Shape and Shape Theory 
KHURI • Advanced Calculus with Applications in Statistics, Second Edition 
KHURI, MATHEW, and SINHA • Statistical Tests for Mixed Linear Models 
KLEIBER and KOTZ • Statistical Size Distributions in Economics and Actuarial Sciences 
KLEMELA • Smoothing of Multivariate Data: Density Estimation and Visualization 
KLUGMAN, PANJER, and WILLMOT • Loss Models: From Data to Decisions, 

Third Edition 

KLUGMAN, PANJER, and WILLMOT • Solutions Manual to Accompany Loss Models: 

From Data to Decisions, Third Edition 

*Now available in a lower priced paperback edition in the Wiley Classics Library. 
tNow available in a lower priced paperback edition in the Wiley-Interscience Paperback Series. 

KOTZ, BALAKRISHNAN, and JOHNSON • Continuous Multivariate Distributions, 

Volume 1, Second Edition 

KOVALENKO, KUZNETZOV, and PEGG • Mathematical Theory of Reliability of 

Time-Dependent Systems with Practical Applications 

KOWALSKI and TU • Modem Applied U-Statistics 
KRISHNAMOORTHY and MATHEW • Statistical Tolerance Regions: Theory, 

Applications, and Computation 

KROONENBERG • Applied Multiway Data Analysis 
KVAM and VIDAKOVIC • Nonparametric Statistics with Applications to Science 

and Engineering 

LACHIN • Biostatistical Methods: The Assessment of Relative Risks 
LAD • Operational Subjective Statistical Methods: A Mathematical, Philosophical, and 

Historical Introduction 

LAMPERTI • Probability: A Survey of the Mathematical Theory, Second Edition 
LANGE, RYAN, BILLARD, BRILLINGER, CONQUEST, and GREENHOUSE • 

Case Studies in Biometry 

LARSON • Introduction to Probability Theory and Statistical Inference, Third Edition 
LAWLESS • Statistical Models and Methods for Lifetime Data, Second Edition 
LAWSON • Statistical Methods in Spatial Epidemiology 
LE • Applied Categorical Data Analysis 
LE • Applied Survival Analysis 
LEE and WANG • Statistical Methods for Survival Data Analysis, Third Edition 
LePAGE and BILLARD • Exploring the Limits of Bootstrap 
LEYLAND and GOLDSTEIN (editors) • Multilevel Modelling of Health Statistics 
LIAO ■ Statistical Group Comparison 
LINDVALL • Lectures on the Coupling Method 
LIN • Introductory Stochastic Analysis for Finance and Insurance 
LINHART and ZUCCHINI • Model Selection 
LITTLE and RUBIN • Statistical Analysis with Missing Data, Second Edition 
LLOYD • The Statistical Analysis of Categorical Data 
LOWEN and TEICH • Fractal-Based Point Processes 
MAGNUS and NEUDECKER • Matrix Differential Calculus with Applications in 

Statistics and Econometrics, Revised Edition 

MALLER and ZHOU • Survival Analysis with Long Term Survivors 
MALLOWS • Design, Data, and Analysis by Some Friends of Cuthbert Daniel 
MANN, SCHAFER, and SINGPURWALLA • Methods for Statistical Analysis of 

Reliability and Life Data 

MANTON, WOODBURY, and TOLLEY • Statistical Applications Using Fuzzy Sets 
MARCHETTE • Random Graphs for Statistical Pattern Recognition 
MARDIA and JUPP • Directional Statistics 
MASON, GUNST, and HESS • Statistical Design and Analysis of Experiments with 

Applications to Engineering and Science, Second Edition 

McCULLOCH, SEARLE, and NEUHAUS • Generalized, Linear, and Mixed Models, 

Second Edition 

McFADDEN • Management of Data in Clinical Trials, Second Edition 
* McLACHLAN • Discriminant Analysis and Statistical Pattern Recognition 

McLACHLAN, DO, and AMBROISE • Analyzing Microarray Gene Expression Data 
McLACHLAN and KRISHNAN • The EM Algorithm and Extensions, Second Edition 
McLACHLAN and PEEL • Finite Mixture Models 
McNEIL • Epidemiological Research Methods 
MEEKER and ESCOBAR • Statistical Methods for Reliability Data 
MEERSCHAERT and SCHEFFLER • Limit Distributions for Sums of Independent 

Random Vectors: Heavy Tails in Theory and Practice 

*Now available in a lower priced paperback edition in the Wiley Classics Library. 
TNow available in a lower priced paperback edition in the Wiley-Interscience Paperback Series. 

MICKEY, DUNN, and CLARK • Applied Statistics: Analysis of Variance and 

Regression, Third Edition 

* MILLER • Survival Analysis, Second Edition 

MONTGOMERY, JENNINGS, and KULAHCI • Introduction to Time Series Analysis 

and Forecasting 

MONTGOMERY, PECK, and VINING • Introduction to Linear Regression Analysis, 

Fourth Edition 

MORGENTHALER and TUKEY • Configural Polysampling: A Route to Practical 

Robustness 

MUIRHEAD • Aspects of Multivariate Statistical Theory 
MULLER and STOYAN • Comparison Methods for Stochastic Models and Risks 
MURRAY • X-STAT 2.0 Statistical Experimentation, Design Data Analysis, and 

Nonlinear Optimization 

MURTHY, XIE, and JIANG • Weibull Models 
MYERS, MONTGOMERY, and ANDERSON-COOK • Response Surface Methodology: 

Process and Product Optimization Using Designed Experiments, Third Edition 

MYERS, MONTGOMERY, VINING, and ROBINSON • Generalized Linear Models. 

With Applications in Engineering and the Sciences, Second Edition 

t NELSON • Accelerated Testing, Statistical Models, Test Plans, and Data Analyses 
t NELSON • Applied Life Data Analysis 

NEWMAN • Biostatistical Methods in Epidemiology 
OCHI • Applied Probability and Stochastic Processes in Engineering and Physical 

Sciences 

OKABE, BOOTS, SUGIHARA, and CHIU • Spatial Tesselations: Concepts and 

Applications of Voronoi Diagrams, Second Edition 

OLIVER and SMITH • Influence Diagrams, Belief Nets and Decision Analysis 
PALTA • Quantitative Methods in Population Health: Extensions of Ordinary Regressions 
PANJER • Operational Risk: Modeling and Analytics 
PANKRATZ • Forecasting with Dynamic Regression Models 
PANKRATZ • Forecasting with Univariate Box-Jenkins Models: Concepts and Cases 

* PARZEN • Modem Probability Theory and Its Applications 

PENA, TIAO, and TSAY • A Course in Time Series Analysis 
PIANTADOSI • Clinical Trials: A Methodologic Perspective 
PORT • Theoretical Probability for Applications 
POURAHMADI • Foundations of Time Series Analysis and Prediction Theory 
POWELL • Approximate Dynamic Programming: Solving the Curses of Dimensionality 
PRESS • Bayesian Statistics: Principles, Models, and Applications 
PRESS • Subjective and Objective Bayesian Statistics, Second Edition 
PRESS and TANUR • The Subjectivity of Scientists and the Bayesian Approach 
PUKELSHEIM • Optimal Experimental Design 
PURI, VILAPLANA, and WERTZ • New Perspectives in Theoretical and Applied 

Statistics 

t PUTERMAN • Markov Decision Processes: Discrete Stochastic Dynamic Programming 

QIU • Image Processing and Jump Regression Analysis 

* RAO • Linear Statistical Inference and Its Applications, Second Edition 

RAUSAND and H0YLAND • System Reliability Theory: Models, Statistical Methods, 

and Applications, Second Edition 

RENCHER • Linear Models in Statistics 
RENCHER • Methods of Multivariate Analysis, Second Edition 
RENCHER • Multivariate Statistical Inference with Applications 

* RIPLEY • Spatial Statistics 
* RIPLEY • Stochastic Simulation 

ROBINSON • Practical Strategies for Experimenting 

*Now available in a lower priced paperback edition in the Wiley Classics Library. 
tNow available in a lower priced paperback edition in the Wiley-Interscience Paperback Series. 

ROHATGI and SALEH • An Introduction to Probability and Statistics, Second Edition 
ROLSKI, SCHMIDLI, SCHMIDT, and TEUGELS • Stochastic Processes'for Insurance 

and Finance 

ROSENBERGER and LACHIN ■ Randomization in Clinical Trials: Theory and Practice 
ROSS • Introduction to Probability and Statistics for Engineers and Scientists 
ROSSI, ALLENBY, and McCULLOCH • Bayesian Statistics and Marketing 

' ROUSSEEUW and LEROY • Robust Regression and Outlier Detection 
* RUBIN • Multiple Imputation for Nonresponse in Surveys 

RUBINSTEIN and KROESE • Simulation and the Monte Carlo Method, Second Edition 
RUBINSTEIN and MELAMED • Modem Simulation and Modeling 
RYAN • Modem Engineering Statistics 
RYAN • Modem Experimental Design 
RYAN • Modem Regression Methods, Second Edition 
RYAN • Statistical Methods for Quality Improvement, Second Edition 
SALEH • Theory of Preliminary Test and Stein-Type Estimation with Applications 

* SCHEFFE • The Analysis of Variance 

SCHIMEK • Smoothing and Regression: Approaches, Computation, and Application 
SCHOTT • Matrix Analysis for Statistics, Second Edition 
SCHOUTENS • Levy Processes in Finance: Pricing Financial Derivatives 
SCHUSS • Theory and Applications of Stochastic Differential Equations 
SCOTT • Multivariate Density Estimation: Theory, Practice, and Visualization 

| SEARLE • Linear Models for Unbalanced Data 
j SEARLE • Matrix Algebra Useful for Statistics 

SEARLE, CASELLA, and McCULLOCH • Variance Components 
SEARLE and WILLETT • Matrix Algebra for Applied Economics 
SEBER • A Matrix Handbook For Statisticians 

' SEBER • Multivariate Observations 

SEBER and LEE • Linear Regression Analysis, Second Edition 

' SEBER and WILD • Nonlinear Regression 

SENNOTT • Stochastic Dynamic Programming and the Control of Queueing Systems 

* SERFLING • Approximation Theorems of Mathematical Statistics 
SHAFER and VOVK ■ Probability and Finance: It’s Only a Game! 
SILVAPULLE and SEN • Constrained Statistical Inference: Inequality, Order, and Shape 

Restrictions 

SMALL and McLEISH • Hilbert Space Methods in Probability and Statistical Inference 
SRIVASTAVA • Methods of Multivariate Statistics 
STAPLETON • Linear Statistical Models, Second Edition 
STAPLETON • Models for Probability and Statistical Inference: Theory and Applications 
STAUDTE and SHEATHER • Robust Estimation and Testing 
STOYAN, KENDALL, and MECKE • Stochastic Geometry and Its Applications, Second 

Edition 

STOYAN and STOYAN • Fractals, Random Shapes and Point Fields: Methods of 

Geometrical Statistics 

STREET and BURGESS • The Construction of Optimal Stated Choice Experiments: 

Theory and Methods 

STYAN • The Collected Papers of T. W. Anderson: 1943-1985 
SUTTON, ABRAMS, JONES, SHELDON, and SONG • Methods for Meta-Analysis in 

Medical Research 

TAKEZAWA • Introduction to Nonparametric Regression 
TAMHANE • Statistical Analysis of Designed Experiments: Theory and Applications 
TANAKA • Time Series Analysis: Nonstationary and Noninvertible Distribution Theory 
THOMPSON • Empirical Model Building 
THOMPSON • Sampling, Second Edition 
THOMPSON • Simulation: A Modeler’s Approach 

*Now available in a lower priced paperback edition in the Wiley Classics Library. 

Now available in a lower priced paperback edition in the Wi ley-Interscience Paperback Series. 

THOMPSON and SEBER • Adaptive Sampling 
THOMPSON, WILLIAMS, and FINDLAY • Models for Investors in Real World Markets 
TIAO, BISGAARD, HILL, PENA, and STIGLER (editors) • Box on Quality and 

Discovery: with Design, Control, and Robustness 

TIERNEY • LISP-STAT: An Object-Oriented Environment for Statistical Computing 

and Dynamic Graphics 

TSAY • Analysis of Financial Time Series, Third Edition 
UPTON and FINGLETON • Spatial Data Analysis by Example, Volume II: 

Categorical and Directional Data 

t VAN BELLE • Statistical Rules of Thumb, Second Edition 

VAN BELLE, FISHER, HEAGERTY, and LUMLEY • Biostatistics: A Methodology for 

the Health Sciences, Second Edition 

VESTRUP • The Theory of Measures and Integration 
VEDAKOVIC • Statistical Modeling by Wavelets 
VINOD and REAGLE • Preparing for the Worst: Incorporating Downside Risk in Stock 

Market Investments 

WALLER and GOTWAY • Applied Spatial Statistics for Public Health Data 
WEERAHANDI • Generalized Inference in Repeated Measures: Exact Methods in 

MANOVA and Mixed Models 

WEISBERG ■ Applied Linear Regression, Third Edition 
WELSH • Aspects of Statistical Inference 
WESTFALL and YOUNG • Resampling-Based Multiple Testing: Examples and 

Methods forp-Value Adjustment 

WHITTAKER • Graphical Models in Applied Multivariate Statistics 
WINKER • Optimization Heuristics in Economics: Applications of Threshold Accepting 
WONNACOTT and WONNACOTT • Econometrics, Second Edition 
WOODING • Planning Pharmaceutical Clinical Trials: Basic Statistical Principles 
WOODWORTH • Biostatistics: A Bayesian Introduction 
WOOLSON and CLARKE • Statistical Methods for the Analysis of Biomedical Data, 

Second Edition 

WU and HAMADA • Experiments: Planning, Analysis, and Parameter Design 

Optimization, Second Edition 

WU and ZHANG • Nonparametric Regression Methods for Longitudinal Data Analysis 
YANG • The Construction Theory of Denumerable Markov Processes 
YOUNG, VALERO-MORA, and FRIENDLY • Visual Statistics: Seeing Data with 

Dynamic Interactive Graphics 

ZACKS • Stage-Wise Adaptive Designs 
ZELTERMAN • Discrete Distributions—Applications in the Health Sciences 

* ZELLNER • An Introduction to Bayesian Inference in Econometrics 

ZHOU, OBUCHOWSKI, and McCLISH • Statistical Methods in Diagnostic Medicine 

*Now available in a lower priced paperback edition in the Wiley Classics Library. 
tjvfow available in a lower priced paperback edition in the Wiley-Interscience Paperback Series. 

' 

■ 

. 

• LIBRARY 

N|H 

l,l|^ Amazing Research. 

Amazing Help. 

http://nihllbrary.nih.gov 

10 Center Drive 
Bethesda, MD 20892-1150 
301-496-1080 

Praise for the Second Edi\ 

“. . . too wonderful a book to be missed by anyone who w< 

NIH LIBRARY 

11 III | III II III Will III III 

3 1496 01032  C

O

—Journal of Statistical Computation and Simulation 

“All in all this is an excellent account on financial time series...with plenty of intuitive 
insight of how exactly these models work...” —MAA Reviews 

Since publication of the first edition, Analysis of Financial Time Series has served as one of the 
most influential and prominent works on the subject. This Third Edition now utilizes the freely 
available R software package to explore empirical financial data and illustrate related computa¬ 
tion and analyses using real-world examples. Retaining the fundamental and hands-on style 
of its predecessor, this new edition continues to serve as the cornerstone for understanding the 
important statistical methods and techniques for working with financial data. 

Accessible explanations and numerous interesting examples assist readers with understand¬ 
ing analysis and application of univariate financial time series; return series of multiple assets; 
and Bayesian inference in finance methods. The latest developments in financial econometrics 
are explored in-depth, such as realized volatility, volatility with skew innovations, conditional 
value at risk, statistical arbitrage, and applications of duration and dynamic-correlation models. 
Additional features of the Third Edition include: 

Applications of nonlinear duration models throughout all discussion of high-frequency 
data analysis and market microstructure 

Newly added applications of nonlinear models and methods 

An updated chapter on multivariate time series analysis that explores the relevance of 
cointegration to pairs trading 

A new, unified approach to value at risk (VaR) via loss function 

An introduction to extremal index for dependence data in the discussion of extreme values, 
quantiles, and value at risk 

The use of both R and S-PLUS"1 software with the book’s numerous examples and exercises en¬ 
sures that readers can reproduce the results shown in the book and apply the detailed steps and 
procedures to their own work. New and updated exercises throughout provide opportunities to 
test comprehension of the presented material, and a related Web site houses additional data sets 
and related software programs. 

Analysis of Financial Time Series, Third Edition is an ideal book for introductory courses on 
time series at the graduate level and a valuable supplement for statistics courses in time series 
at the upper-undergraduate level. It also serves as an indispensible reference for researchers and 
practitioners working in business and finance. 

is H. G. B. Alexander Professor of Econometrics and Statistics at the 
University of Chicago Booth School of Business. Dr. Tsay has written over 100 published 
articles in the areas of business and economic forecasting, data analysis, risk management, 
and piocess control, and he is the coauthor of A Course in Time Series Analysis (Wiley). 
Dr. Tsay is a Fellow of the American Statistical Association, the Institute of Mathematical 
Statistics, the Royal Statistical Society, and Academia Sinica. 

Subscribe to our free Statistics eNewsletter at 

wiley.com/enewsletters 

Visit wiley.com/statistics | 

©WILEY 

wiley.com 

 
