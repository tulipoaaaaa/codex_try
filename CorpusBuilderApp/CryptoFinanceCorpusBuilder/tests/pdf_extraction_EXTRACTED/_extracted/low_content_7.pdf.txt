z 
> 

s 

n
e
 C
2 

e
e
 S
o
r
 g
e

®

e
e

°

a
y

.

 a
e

e
h

James D 

Ham 

ilton 

 
 
 
 
 
 
 
™ 

| 

Time  Series  Analysis 

A 

» 

~ 

Time Series  Analysis 

ysl DorteridieS  se90thS  sepphatl  ant] 

- 

ie 
‘ 
yj we 

sl 
aie 

> 

i 

We  eee  eee 

oe 

’ 

— 

TIME  SERIES  ANALYSIS 
by James D. Hamilton 

~ 

Copyright  © Princeton University  Press 

. 

3 

5. 

ee  yy 

“This In Indian Edition Published by 

Levant  Books 

surchased— 

|A 
oo 

ai a = 

3k 36 5 

bad  2ravan  Rao Meigiri”| 
Nap sai Law Libr rary  | 

Contents 

PREFACE  xiii 

2 

ew 

P 

~ 

ae 

,  sel  bogyU 

- . 

310%  iammnqO 

a 

| 

serene BO ogous? ilese  Ad  MOMAISA 
bial 
é  ae  a  agaist bean cg 

3.6. 
3.7. 

The  Autocovariance-Generating  Function  61 
 Invertibility  64 

APPENDIX  3.A.  Convergence  Results for Infinite-Order 
Moving  Average  Processes  69 
Exercises  70 

References  71 

4 

Forecasting 

: 

72 

4.1. 
4.2. 

4.3. 

4.4. 

Principles  of Forecasting  72 
Forecasts  Based  on  an  Infinite  Number 
of Observations  77 
Forecasts  Based  on  a  Finite  Number 
of Observations  85 
The  Triangular  Factorization  of a  Positive  Definite 
Symmetric  Matrix  87  | 

4.5.  ~Updating  a  Linear  Projection  92 
4.6. 
4.7. 
4.8. 

Optimal  Forecasts  for  Gaussian  Processes  100 
Sums  of ARMA.  Processes  102 
_Wold’s  Decomposition  and  the  Box-Jenkins 
Modeling  Philosophy  108 

APPENDIX  4.A.  Parallel  Between  OLS  Regression 
and Linear  Projection  113 
APPENDIX  4.B.  Triangular  Factorization  of the Covariance 
Matrix for an  MA(1)  Process  114 
Exercises  115 

References  116 

5 

Maximum  Likelihood  Estimation 

117 

5.1. 
5.2. 

5.3. 

5.4. 

5.5. 
| 
5.6. 

5.7. 

Introduction  117 
The  Likelihood  Function  for  a Gaussian  AR(1) 
Process  118 
The  Likelihood  Function  for  a  Gaussian  AR(p) 
Process  123 
The  Likelihood  Function  for  a Gaussian  MA(1) 
Process  127 
The  Likelihood  Function  for  a Gaussian  MA(q) 
Process  130 
The  Likelihood  Function  for a Gaussian  ARMA(p,  q) 
Process  132 
Numerical  Optimization  133 

vi  Contents 

5.8. 

5.9. 

Statistical  Inference  with  Maximum  Likelihood 
Estimation  142 
Inequality  Constraints  146 

APPENDIX  S.A.  Proofs  of Chapter  5 Propositions  148 
Exercises  150 
References  150 

6 

Spectral  Analysis 

152 

6.1. 
6.2. 
6.3. 
6.4. 

The  Population  Spectrum  152 
The  Sample  Periodogram  158 
Estimating  the  Population  Spectrum  163 
Uses  of Spectral  Analysis  167 

APPENDIX  6.A.  Proofs  of Chapter  6 Propositions  172 
References  178 
Exercises  178 

7 

Asymptotic  Distribution  Theory  - 

180 

7.1. 
7.2. 

Review  of Asymptotic  Distribution  Theory  180 
Limit  Theorems  for  Serially  Dependent 
Observations  186 

APPENDIX  7.A.  Proofs  of Chapter  7 Propositions  195 
References  199 
Exercises  198 

8 

8.1. 

8.2. 

8.3. 

Linear  Regression  Models 

200 

Review  of Ordinary  Least  Squares 
with  Deterministic  Regressors  and  i.i.d.  Gaussian 
Disturbances  200 
Ordinary  Least  Squares  Under  More  General 
Conditions  207 
Generalized  Least  Squares  220 

APPENDIX  8.A.  Proofs  of Chapter 8 Propositions  228 
References  231 
Exercises  230 

9 

9.1. 
9.2. 

Linear  Systems  of Simultaneous  Equations 
Simultaneous  Equations  Bias  233 
Instrumental  Variables  and  Two-Stage  Least 
Squares  238 

233 

| 

Contents 

vii 

9.3. 
9.4 

9.5. 
9.6. 

Identification  243 
 Full-Information  Maximum  Likelihood 
Estimation  247 
Estimation  Based  on  the  Reduced  Form  250 
Overview  of Simultaneous  Equations  Bias  252 

APPENDIX  9.A.  Proofs  of Chapter  9 Proposition  253 
References  256 
Exercise  255 

10 

Covariance-Stationary  Vector  Processes 

257 

10.1.  Introduction  to  Vector  Autoregressions  257 
10.2.  Autocovariances  and  Convergence  Results 

for  Vector  Processes  261 

10.3.  The  Autocovariance-Generating  Function 

for  Vector  Processes  266 

10.4.  The  Spectrum  for  Vector  Processes  268 
10.5.  The  Sample  Mean  of a  Vector  Process  279 

| 

APPENDIX  10.A.  Proofs  of Chapter  10 Propositions  285 
References  290 
Exercises  290 

11 

Vector  Autoregressions 

291 

11.1.  Maximum  Likelihood  Estimation  and  Hypothesis 

Testing  for  an  Unrestricted  Vector 
Autoregression  291 
11.2. 
Bivariate  Granger  Causality  Tests  302 
11.3.  Maximum  Likelihood  Estimation  of Restricted 

Vector  Autoregressions  309 
11.4.  The  Impulse-Response  Function  318 
11.5.  Variance  Decomposition  323 
11.6.  Vector  Rapprogresnous, and  Structural  Econometric 

Models  324 

11.7.  Standard  Errors  for  Impulse-Response 

Functions  336 

APPENDIX  11.A.  Proofs  of Chapter 11 Propositions  340 
APPENDIX  11.B.  Calculation  of Analytic  Derivatives  344 
Exercises  348 
References  349 

viii 

Contents 

EE EE, 
12 

Bayesian  Analysis 

35] 

12.1.  Introduction  to  Bayesian  Analysis  351 
12.2.  Bayesian  Analysis  of Vector  Autoregressions  360 
12.3.  Numerical  Bayesian  Methods  362 

APPENDIX  12.A.  Proofs  of Chapter  12  Propositions  366 

Exercise  370 

References  370 

13. 

The  Kalman  Filter 

372 

13.1.  The  State-Space  Representation  of  a Dynamic 

System  372 

13.2..  Derivation  of the  Kalman  Filter  377 
13.3.  Forecasts  Based  on  the  State-Space 

Representation  381 

13.4.  Maximum  Likelihood  Estimation 

of Parameters  385 

13.5.  The  Steady-State  Kalman  Filter  389 
13.6.  Smoothing  394 
13.7. 
13.8.  Time-Varying  Parameters  399 

Statistical  Inference  with  the  Kalman  Filter  397 

APPENDIX  13.A.  Proofs  of Chapter  13 Propositions  403 
References  407 
Exercises  406 

14. 

Generalized  Method  of Moments 

409 

14.1.  Estimation  by the  Generalized  Method 

of Moments  409 

14.2.  Examples  415 
14.3.  Extensions  424 
14.4.  GMM  and  Maximum  Likelihood  Estimation  427 

APPENDIX  14.A.  Proofs  of Chapter  14 Propositions  431 
References  433 
Exercise  432 

eee JOS BiNTO9 2S) 22280  GS  Bad 
15 

Models  of Nonstationary  Time  Series 

435 

15.1.  Introduction  435 
15.2.  Why  Linear  Time  Trends  and  Unit  Roots?  438 

Contents 

ix 

15.3.  Comparison  of Trend-Stationary  and  Unit  Root 

Processes  438 

15.4.  The  Meaning  of Tests  for  Unit  Roots  444 
15.5. 

Other  Approaches  to  Trended  Time  Series  447 

APPENDIX  15.A.  Derivation  of Selected  Equations 
for Chapter  15  451 
References  452 

16 

Processes  with  Deterministic  Time  Trends 

454 

16.1.  Asymptotic  Distribution  of OLS  Estimates 
of the  Simple  Time  Trend  Model  454 
16.2.  Hypothesis  Testing  for  the  Simple  Time  Trend 

Model  461 

16.3.  Asymptotic  Inference  for  an  Autoregressive 

Process  Around  a  Deterministic  Time Trend  463 

APPENDIX  16.A.  Derivation  of Selected  Equations 
for Chapter  16  472 
Exercises  474 

References  474 

17. 

Univariate  Processes  with  Unit  Roots 

475 

17.1.  Introduction  475 
17.2.  Brownian  Motion  477 
17.3.  The  Functional  Central  Limit  Theorem  479 
17.4.  Asymptotic  Properties  of a  First-Order 

Autoregression  when  the  True  Coefficient  Is 
Unity  486 

17.5.  Asymptotic  Results  for  Unit  Root  Processes 
with  General  Serial  Correlation  504 
17.6.  Phillips-Perron  Tests  for  Unit  Roots  506 
17.7.  Asymptotic  Properties  of a pth-Order 

Autoregression  and  the  Augmented  Dickey-Fuller 
Tests  for  Unit  Roots  516 

17.8.  Other  Approaches  to Testing  for  Unit  Roots  531 
17.9.  Bayesian  Analysis  and  Unit  Roots  532 

APPENDIX  17.A.  Proofs  of Chapter 17 Propositions  534 
Exercises  537 
References  541 

x 

Contents 

18 

Unit  Roots  in  Multivariate  Time  Series 

544 

18.1.  Asymptotic  Results  for  Nonstationary  Vector 

Processes  544 

18.2.  Vector  Autoregressions  Containing  Unit  Roots  549 
18.3.  Spurious  Regressions  557 

APPENDIX  18.A.  Proofs  of Chapter  18  Propositions  562 
Exercises  568 
References  569 

19 

Cointegration 

571 

19.1.  Introduction  571 
19.2.  Testing  the  Null  Hypothesis  of No 

Cointegration  582 

19.3.  Testing  Hypotheses  About  the  Cointegrating 

Vector  601 

APPENDIX  19.A.  Proofs  of Chapter  19 Propositions  618 
Exercises  625 
References  627 

20  = Full-Information  Maximum  Likelihood 
Analysis  of Cointegrated  Systems 

630 

20.1.  Canonical  Correlation  630 
20.2.  Maximum  Likelihood  Estimation  635 
20.3.  Hypothesis  Testing  645 
20.4.  Overview  of Unit  Roots—To  Difference 

or  Not  to  Difference?  651 

APPENDIX  20.A.  Proofs  of Chapter  20 Propositions  653 
References  655 
Exercises  655 

Time  Series  Models  of Heteroskedasticity 
21. 
21.1.  Autoregressive  Conditional  Heteroskedasticity 

657 

(ARCH)  657  © 
21.2.  Extensions  665 

APPENDIX  21.A.  Derivation  of Selected  Equations 
for Chapter 21  673 
References  674 

Contents 

xi 

a ee 
22 

Modeling  Time  Series  with  Changes 
in  Regime 

677 

Soa 
IA 
22.3. 

22.4. 

Introduction  677 
Markov  Chains  678 
Statistical  Analysis  of 1.i.d.  Mixture 
Distributions  685 
Time  Series  Models  of Changes  in  Regime  690 

APPENDIX  22.A.  Derivation  of Selected  Equations 
for Chapter  22  699 
Exercise  702 

References  702 

Mathematical  Review 

Trigonometry  704 
Complex  Numbers  708 
Calculus  711 
Matrix  Algebra  721 
Probability  and  Statistics  739 

References  750 

Statistical  Tables 

Answers  to  Selected  Exercises 

Greek  Letters  and  Mathematical  Symbols 
Used  in the  Text 

704 

751 

769 

786 

AUTHOR  INDEX  789 

SUBJECT  INDEX  792 

xii 

Contents 

Preface 

Much  of  economics  is  concerned  with  modeling  dynamics.  There  has  been  an 
explosion  of research  in this  area  in the  last  decade,  as  “‘time  series  econometrics” 
has  practically  come  to  be  synonymous  with  “empirical  macroeconomics.” 

Several  texts  provide  good  coverage  of the  advances  in the  economic  analysis 
of  dynamic  systems,  while  others  summarize  the  earlier  literature  on  statistical 
inference  for  time  series  data.  There  seemed a use  for  a  text  that  could  integrate 
the  theoretical  and  empirical  issues  as  well  as  incorporate  the  many  advances  of 
the  last  decade,  such  as  the  analysis  of vector  autoregressions,  estimation  by gen- 
eralized  method  of moments,  and  statistical  inference  for  nonstationary  data.  This 
is the  goal  of  Time  Series  Analysis. 

A principal  anticipated  use  of the  book would  be as  a textbook  for a graduate 
econometrics  course  in time  series  analysis.  The  book  aims  for maximum  flexibility 
through  what  might be described  as  an integrated  modular  structure.  As an  example 
of this,  the first  three  sections  of Chapter  13 on  the  Kalman  filter  could  be covered 
right  after  Chapter  4, if desired.  Alternatively,  Chapter  13  could  be  skipped  al- 
together  without  loss  of comprehension.  Despite  this  flexibility,  state-space  ideas 
are  fully  integrated  into  the  text  beginning  with  Chapter  1, where  a  state-space 
representation  is used  (without  any jargon or formalism)  to introduce  the key results 
concerning  difference  equations.  Thus,  when  the  reader  encounters  the  formal 
development  of the  state-space  framework  and  the  Kalman  filter  in  Chapter  13, 
the  notation  and  key ideas  should  already  be  quite  familiar. 

Spectral  analysis  (Chapter  6) is another  topic that  could  be covered  at a point 
of the reader’s  choosing  or skipped  altogether.  In this  case,  the  integrated  modular 
structure  is achieved  by the early introduction  and use  of autocovariance-generating 
functions  and  filters.  Wherever  possible,  results  are  described  in  terms  of  these 
rather  than  the  spectrum. 

Although  the  book  is  designed  with  an  econometrics  course  in  time  series 
methods  in  mind,  the  book  should  be  useful  for  several  other  purposes.  It  is 
completely  self-contained,  starting  from  basic  principles  accessible  to  first-year 
graduate  students  and including  an  extensive  math  review  appendix. Thus  the book 
would  be  quite  suitable  for  a  first-year  graduate  course  in  macroeconomics  or 
dynamic  methods  that  has no  econometric  content.  Such  a course  might  use  seal 
ters  1 and  2, Sections  3.1  through  3.5,  and  Sections  4.1  and  4.2. 

Yet  another  intended  use  for  the  book  would  be  in  a  conventional  econo- 
metrics  course  without  an  explicit time  series  focus.  The popular econometrics  texts 
do not have much discussion  of such topics as numerical  methods;  asymptotic  results 
for  serially  dependent,  heterogeneously  distributed  observations;  estimation  of 
models  with  distributed  lags;  autocorrelation-  and  heteroskedasticity-consistent 

xiii 

standard  errors;  Bayesian  analysis;  or  generalized  method  of moments.  All  of these 
topics  receive  extensive  treatment  in  Time  Series  Analysis.  Thus,  an  econometrics 
course  without  an  explicit  focus  on  time  series  might  make  use  of  Sections  3.1 
through  3.5,  Chapters  7 through  9.  and  Chapter  14,  and  perhaps  any  of Chapters 
5.  11,  and  12  as  well.  Again,  the  text  is  self-contained,  with  a  fairly  complete 
discussion  of conventional  simultaneous  equations  methods  in Chapter  9.  Indeed, 
a  very  important  goal  of  the  text  is to  develop  the  parallels  between  (1)  the  tra- 
ditional  econometric  approach  to  simultaneous  equations  and  (2) the  current  pop- 
ularity  of vector  autoregressions  and  generalized  method  of moments  estimation. 
Finally,  the  book  attempts  to  provide  a  rigorous  motivation  for  the  methods 
and  yet  still  be  accessible  for  researchers  with  purely  applied  interests.  This  is 
achieved  by relegation  of many  details  to  mathematical  appendixes  at  the  ends  of 
chapters.  and  by inclusion  of  numerous  examples  that  illustrate  exactly  how  the 
theoretical  results  are  used  and  applied  in practice. 

The  book  developed  out  of  my  lectures  at  the  University  of  Virginia.  I am 
grateful  first  and  foremost  to  my  many  students  over  the  years  whose  questions 
and  comments  have  shaped  the  course  of the  manuscript.  I also  have  an  enormous 
debt  to  numerous  colleagues  who  have  kindly  offered  many  useful  suggestions, 
and  would  like  to  thank  in  particular  Donald  W.  K.  Andrews,  Jushan  Bai,  Peter 
Bearse,  Stephen  R.  Blough,  John  Cochrane,  George  Davis,  Michael  Dotsey,  John 
Elder,  Robert  Engle.  T.  Wake  Epps,  Marjorie  Flavin,  John  Geweke,  Eric  Ghysels, 
Carlo  Giannini,  Clive  W.  J.  Granger,  Alastair  Hall,  Bruce  E.  Hansen,  Kevin 
Hassett,  Tomoo  Inoue,  Ravi  Jagannathan,  Kenneth  F.  Kroner,  Jaime  Marquez, 
Rocco  Mosconi,  Edward  Nelson,  Masao  Ogaki,  Adrian  Pagan,  Peter  C.  B.  Phillips, 
Peter  Rappoport,  Glenn  Rudebusch,  Raul  Susmel,  Mark Watson,  Kenneth  D. West, 
Halbert  White,  and Jeffrey  M. Wooldridge.  I would  also  like to thank  Pok-sang  Lam 
and  John  Rogers  for graciously  sharing  their  data.  Thanks  also  go to Keith  Sill  and 
Christopher  Stomberg  for  assistance  with  the  figures,  to  Rita  Chen  for  assistance 
with  the  statistical  tables  in Appendix  B, and  to Richard  Mickey  for a superb job of 
copy  editing. 

James  D. Hamilton 

iv  = Preface 

ke Conerrocd  With  the  ciynenne  ccaseau: 
pa  variable. whos  voir 

of 

-evisns 
¥. 

o 
oyu: 

fa 
Vihe are S  takes Of  at ites i  )  2uetie:  vari 

late  (as 

decnated 

whe 

oe tok co  totes feet 

>t 

iz 

iad 

eke zane? SHIT oe 

ar  tie 

sy 

he ee >, 

hs  2s  ,  SStpeessS  > 

ao  Sirs  tame 

eet.  i prone: sae  cheeses 
are ;  Se  Pte 

Difference  Equations 

1.1.  First-Order  Difference  Equations 

This  book  is concerned  with  the  dynamic  consequences  of events  over  time.  Let’s 
say  we  are  studying  a variable  whose  value  at  date  ¢ is denoted  y,.  Suppose  we  are 
given  a dynamic  equation  relating  the  value  y takes  on  at date  ¢ to  another  variable 
w, and  to  the  value  y took  on  in the  previous  period: 

Ye  =  PYr-1  +  W,. 

[1.1.1] 

Equation  [1.1.1]  is a linear first-order  difference  equation.  A difference  equation  is 
an  expression  relating  a  variable  y,  to  its  previous  values.  This  is  a  first-order 
difference  equation  because  only  the  first  lag of the  variable  (y,_,)  appears  in the 
equation.  Note  that  it expresses  y, as  a  linear  function  of y,_,  and  w,. 

An  example  of  [1.1.1]  is Goldfeld’s  (1973)  estimated  money  demand  function 
for the  United  States.  Goldfeld’s  model  related  the  log of the  real  money  holdings  of 
the public  (m,) to  the  log of aggregate  real  income  (J,), the  log of the  interest  rate  on 
bank  accounts  (r,,),  and  the  log of the  interest  rate  on  commercial  paper  (r,,): 

m,  =  0.27  +  0.72m,_,  +  0.19J,  —  0.045r,,  —  0.019r,,. 

[1.1.2] 

This  is a  special  case  of [1.1.1] with  y,  =  m,,  @ =  0.72,  and 

w,  =  0.27  +  0.19/,  —  0.045r,,  —  0.019r,,. 

For  purposes  of analyzing  the  dynamics  of such  a system,  it simplifies  the  algebra 
a  little  to  summarize  the  effects  of all  the  input  variables  (/,, r,,,.and  r,,) in terms 
of a  scalar  w, as  here. 

In  Chapter  3 the  input  variable  w, will  be  regarded  as  a  random  variable,  and 
the  implications  of [1.1.1]  for  the  statistical  properties  of the  output  series  y, will  be 
explored.  In preparation  for  this  discussion,  it is necessary  first  to  understand  the 
mechanics  of difference  equations.  For  the  discussion  in Chapters  | and  2, the  values 
.  .} will  simply  be  regarded  as  a  sequence  of deter- 
for the  input  variable  {w,,  w.,  . 
ministic  numbers.  Our  goal is to  answer  the  following  question:  If a dynamic  system 
is described  by [1.1.1],  what  are  the  effects  on  y of changes  in the  value  of w? 

Solving  a  Difference  Equation  by Recursive  Substitution 
The presumption  is that  the  dynamic  equation  [1.1.1]  governs  the  behavior 
of y for all dates  t. Thus,  for each  date  we  have  an  equation  relating  the  value  of 
1 

y  for  that  date  to  its  previous  value  and  the  current  value  of  w: 

Date 

Equation 

0 

1 

2 

Yo  =  Py-1  +  Wo 

yi  =  Yo  +  wy 

y2  =  $y,  +  Wz 

[1.1.3] 

[1.1.4] 

[1.1.5] 

t 

ae  a  PY,-1+  W,. 

[1.1.6] 

If we  know  the  starting  value  of y for  date  t  =  —1  and  the  value  of  w  for 
dates  t  =  0,1,  2,...  ,  then  it is possible  to  simulate  this  dynamic  system  to  find 
the  value  of y for  any  date.  For  example,  if we  know  the  value  of y fort  =  —1 
and  the  value  of  w  for  t  =  0, we  can  calculate  the  value  of y for  t  =  0 directly 
from  [1.1.3].  Given  this  value  of yo and  the  value  of w  for  t  =  1, we  can  calculate 
the  value  of y for  ¢ =  1 from  [1.1.4]: 

Yi  =  hyo  +  W,  =  P(py_;  +  Wo)  +  Wy, 

or 

Yi  =  by 1,  +  Gm  +  Wy. 

Given  this  value  of y,  and  the  value  of w  for  t  =  2, we  can  calculate  the  value  of 
y for  t =  2 from  [1.1.5]: 

y2  =  dyi  +  w.  =  G(d*y_,  +  GWo  +  Wi)  +  Wo, 

or 

yo  =  Py_1  +  GW  +  gw,  +  wy. 

Continuing  recursively  in this  fashion,  the  value  that  y takes  on  at  date  ¢ can  be 
described  as  a  function  of its initial  value  y_,  and  the  history  of w  between  date 
0 and  date  t: 

yr  =  Ptty_,  +  Pwo  +  7m  Te  ot  se  w,_,  +  W,. 

[1.1.7] 
This  procedure  is known  as  solving  the  difference  equation  [1.1.1]  by recursive 
substitution. 

Dynamic  Multipliers 
Note that  [1.1.7]  expresses  y, as  a linear function  of the initial  value  y_,  and 
the  historical  values  of w.  This  makes  it very  easy  to calculate  the  effect  of Wo on 
y,.  If Wo  were  to  change  with  y_,  and  w,,  W2,...,  W, taken  as  unaffected,  the 
effect  on  y, would  be given  by 

| 

rte ¢'. 

[1.1.8] 
Note that the calculations  would  be exactly the same  if the dynamic simulation 
were  started  at  date  ¢ (taking  y,_,  as  given);  then  Y,+;  Could  be  described  as  a 
2  Chapter  1  | Difference  Equations 

function  of y,_,  and  Wir  Wists  es  + 

>  Wray: 

Vitj  =  o/*'y,_,  +  b/w,  +  gp)” Wea  +  db)  *W,42 

eo  ae 

ag  Ow, 4-1  af  Wi 4) 

B 

The  effect  of w, on  y,, ; is given  by 

at 
OVerj 

j 

[1.1.9] 

[1.1.10] 

Thus the dynamic multiplier [1.1.10] depends only on j, the length of time separating 
the disturbance  to  the  input  (w,) and  the  observed  value  of the  output  (y,, j). The 
multiplier  does  not  depend  on  1; that  is,  it does  not  depend  on  the  dates  of  the 
observations  themselves.  This is  true  of any  linear  difference  equation. 

As an  example  of calculating  a dynamic  multiplier,  consider  again  Goldfeld’s 
pee demand  specification  [1.1.2].  Suppose  we  want  to  know  what  will  happen 
a re demand  two  quarters from  now  if current  income  J, were  to  increase  by 
ond it fh with  future income  /,,,  and Ths unaffected: 

=  OM 42  OAi2 
¥  Tae; 
{| I al, 
“eS 

ss  aw, 

« 

sox 

We  -™ 

al, HE Hite 

a it increase  in /, will increase  w, by 0.19  units,  meaning ‘that 

" ince  @ =  0. 42, we calculate 

. 

=f 2)°0. 19) = 0,098, 

Saw ne 

Eg  erg  pe  ings  Agee  mt 

A  5- 

ir  es  Lm pt  se af am  opm 
y  te  w.  If  < p< 1, the m 7  A  oe scometri 

|  peed aL 
: 

Tse 
ae 
bette  sot  cen ao sae  . 

ariZ ney 

st  ee  See 

So 

rer  aig  ty 

(b)  ¢ =  -0.8 

(c) 

@=  1.1 

(d)  ¢ =  -1.1 

FIGURE  1.1  Dynamic  multiplier  for  first-order  difference  equation  for different 
values  of ¢ (plot  of dy,, ;/aw,  =  d/ as  a  function  of the  lag /). 

-- 

Y:+2, 
time  ¢ is given  by 

-  and  a  constant  interest  rate’  r >  0, the  present  value  of  the  stream  at 

Vr+t 
+ 

yr42 
Ali  iP had  EE 

Vr43 
(1  +  r)3 

Ayaris  © 

7 

plindek 

t 

Let  B denote  the  discount  factor: 

B=  1/1  +  7). 

Note  that  0 <  B <  1. Then  the  present  value  [1.1.12]  can  be written  as 

pa BY 4  ;: 
j=0 

[1.1.13] 

Consider  what  would  happen  if there  were  a  one-unit  increase  in  w,  with 
Wisi»  W425.  .  unaffected.  The  consequences  of this  change  for the present  value 
of y are  found  by differentiating  [1.1.13]  with  respect  to  w, and  then  using [1.1.10] 

'The  interest  rate  is measured  here as  a  fraction  of 1; thus  r  =  0.1  corresponds  to a 10%  interest 

rate. 

‘ 

4  Chapter  1  | Difference  Equations 

to  evaluate  each  derivative: 

>  aj Miti  _ & 
2 B’ tt 

} 

=  2 Bb  =  1(1  —  Bq), 

[1.1.14] 

provided  that  |B¢|  <  1. 

In  calculating  the  dynamic  multipliers  [1.1.10]  or  [1.1.14],  we  were  asking 
»  Wa; 
what  would  happen  if w,  were  to  increase  by one  unit  with  w,,,,  Wi42,--  - 
unaffected.  We  were  thus  finding  the  effect  of  a  purely  transitory  change  in  w. 
Panel  (a) of Figure  1.2  shows  the  time  path  of w  associated  with  this  question,  and 
panel  (b)  shows  the  implied  path  for  y.  Because  the  dynamic  multiplier  [1.1.10] 
calculates  the  response  of y to  a  single  impulse  in  w,  it is  also  referred  to  as  the 
impulse-response  function. 

1.2 

1.2 

Time 

(a)  Value  of w 

Time 

(b)  Value  of y 

FIGURE  1.2  Paths  of input  variable  (w,)  and  output  variable  (y,)  assumed for 
dynamic  multiplier  and  present-value  calculations. 

Lui  First-Order Difference  Equations  § 

Sometimes  we  might  instead  be interested  in the  consequences  of a permanent 
,  and  w,,,  would 
change  in  w.  A  permanent  change  in  w  means  that  w,,  w,,,,  - 
all  incr  ase  by one  unit,  as  in  Figure  1.3.  From  formula  [1.1.10],  the  effect  on  y,+, 
of  a  permanent  change  in  w  beginning  in  period ¢ is given  by 

. 

re] t+ 

Ay  ]  +  oy  J  a  oy  J  oy 

0  t+j 

0  t+) 

Ow, 

OW, 44 

OW, +42 

or  -  t+) 

0 

OW, 4; 

=  Git git  pire  --  +  o4+1 

When  |¢| <  1, the  limit  of this  expression  as j goes  to  infinity  is sometimes  described 
as  the  “long-run”  effect  of  w  on  y: 

l+@+Pt-::: 

[1.1.15] 

(1 — ¢). 

= 

Time 

(a)  Value  of w 

Time 

(b)  Value  of y 
FIGURE  1.3  Paths  of input  variable  (w,)  and  output  variable  (y,)  assumed  for 
long-run  effect  calculations. 

6  Chapter  1  | Difference  Equations 

For ener. the long-run income  elasticity  of money  demand in  the system  [1.1.2] 
is given  by 

0.19 

“ro  0.48. 

A permanent  1%  increase  in income  will  eventually  lead  to  a  0.68%  increase  in 
money  demand. 

Another  related  question  concerns  the  cumulative  consequences  for  y of a 
one-time  change in  w.  Here  we  consider  a transitory  disturbance  to w as  in panel 
(a) of Figure  1.2,  but wish  to calculate  the sum  of the  consequences  for  all future 
values  of y. Another  way  to  think  of this is  as  the  effect  on  the  present  value  of y 
[1.1.13] with the discount  rate es =  1. Setting B =  1 in[1.1.14]  shows this cumulative 
effect  to be equal  to 

SP  sins ua ~  4), 

| 

[1.1.16] 

provided that  |¢| <  1. Note  that the cumulative effect  on  y of a transitory  change 
in w  (expression  [1.1.16]) is  the  same  as  the  long-run  effect  on  y of a permanent 
change  i inw y  seapeceith iy. a seb. 

For  example,  for p  =  4, F refers  to  the  following  4  x  4 matrix: 

p>;  2  3  % 

10 

gprs 

Od 
F-)o  £00 
we 

ae  ae 

0 
104 

For p =  1 (the first-order  difference  equation  [1.1.1]),  F is just the scalar  ¢. Finally, 
define  the  (p  x  1) vector  v, by 

v,=| 

W, 

0]. 
0 

[1.2.4] 

Consider  the  following  first-order  vector  difference  equation: 

E,  i  | 

+  Vi; 

[1.2.5] 

or 

Yr 
Rez, 

a 2 

od,  gd  G3  ... 
ames 

aes: |  eae 
1  0  aes 

= 

0 

bp-1  d, 

Yr-1 
0  = 
¥31 
0 

Ww, 
0 
*  0 

0 
0 

7 ped 

O 

OFs@icasa 

1 

0  = 

0 

This  is a  system  of p equations.  The  first  equation in this  system  is identical  to 
equation  [1.2.1].  The  second  equation  is simply  the  identity 

eer  Sica  Yr-1> 

owing  to  the  fact  that  the  second  element  of &, is the  same  as  the  first  element  of 
€,_,. The  third  equation  in [1.2.5]  states  that  y,_.  =  y,_2;  the pth equation  states 

that Yr-p+1  =  Ye-p+i: 

Thus,  the  first-order  vector  system  (1. 2.5] is  simply  an  alternative  represen- 
tation  of the pth-order  scalar  system  [1.2.1].  The  advantage  of rewriting  the  pth- 
order  system  [1.2.1]  in  the  form  of a  first-order  system  [1.2.5]  is that  first-order 
systems  are  often  easier  to  work  with  than  pth-order  systems.  — 

A dynamic  multiplier  for  [1.2.5]  can  be found  in exactly  the same  way  as was 
done  for  the  first-order  scalar  system  of Section  1.1.  If we  knew  the  value  of the 
vector  € for date  ¢ = 
and  of v for  date  t =  0, we  could  find  the  value  of & for 
date  0 from 

—1 

& =  Fé_,  +  Vo: 

The  value  of &€ for  date 1 is 

&, =  FE  +  v,  =  F(FE_,  +  %)  +  v,  = F€_,  +  Fy  +  ¥,. 

Proceeding  recursively  in this  fashion  produces  a generalization  of [1.1.7]: 

E,  FeFie,  -  F'vy  +  F*=y,  a  F'-2y,  a  - ie  i  Fv,_;  -  V,. 

[1.2.6] 

8  Chapter  I  | Difference  Equations  . 

Writing  this  out  in terms  of the  definitions  of  € and  v, 

ve 

= 

Wo 

WwW) 

¥-2 
Yr-1 
Yyi-2  | =  Ft!  y-3| 

0 
+  FY  0 

0 
P-"  O01 

+ 

Yi-p+l 

yp 

Wes 

0 

0 
i 

0 

0 

[1.2.7] 

Consider  the first  equation  of this  system,  which  characterizes  the  value  of y,.  Let 
f§2 denote  the (1, 1) element  of  F’, f{9.the  (i, 2) element  of F’, and  so  on.  Then 
the first equation of [1.2.7] states that  — 

een. 

» >  fi y-1  +  Hy. 2+:  hess. +  fw wo 

128) 

g 

2  fir Pwy +  +  ARs RS tok —— 

oh 

Y= 

, 

q  sot shamare  104 

Bs: 

This describes  the value of  y at date ¢ 

7 
q  8  ‘ oe  es leat 
: 

= of p initial  values of y 

since  time 0 (wo, w,, 
t  variable  w 
was needed 
for y (the value y_,) 

in the  case  of a 

ee 

first-order  differenc  equ ap initia vane  Y <ot  ag 
of a  pth-order difference  equation. — 

»)  are seas Hae 

eralization of [1. al i 

Bay  = PE + Pe + BP epi “Ayeo to 

| 

+ 

-  (1.29) 
hon rs  Versa +  Mees oi aa  in rs atc Fin aura  te owt  sAT 

By, 

m 

ti 

e 

r 

f  \ 9  im  a 
f 
oak 
eae  ts 
A. 

Ps 
>  491 

, 

* 

#  a  mae 

geben  gt  cas 

ge  { 

mF 

7"  a  f 
» 
‘ 
iy 

Direct  multiplication  of  [1.2.3]  reveals  that  the  (1,  1) element  of F? is (db;  +  2), 

SO 

oo 
IVs 42 
——  = 

2 

<A 

+ 

Se 

in  a pth-order  system. 

For  larger values  of j, an  easy  way  to obtain  a numerical  value  for the dynamic 
multiplier  dy,, ;/dw,  is to  simulate  the  system.  This  is done  as  follows.  Set  y_;  = 
y-2  =***  =  y_,  =  0,  Wo  =  1, and  set  the  value  of w for  all  other  dates  to  0. 
Then  use  [1.2.1]  to  calculate  the  value  of  y,  for  ¢ =  0 (namely,  yp  =  1).  Next 
substitute  this  value  along with y,_1,  ¥,-2,-  - 
»Yr—p+1  back  into  [1.2.1]  to calculate 
Y-+1,  and  continue  recursively  in  this  fashion.  The  value  of y at  step  ¢ gives  the 
effect  of a  one-unit  change  in  wo  on  y,. 

- 

Although  numerical  simulation  may  be  adequate  for  many  circumstances,  it 
is also  useful  to  have  a  simple  analytical  characterization  of dy,, ;/dw,,  which,  we 
know  from  [1.2.11],  is given by the  (1, 1) element  of F’. This  is fairly easy  to obtain 
in terms  of the  eigenvalues  of the  matrix  F.  Recall  that  the  eigenvalues  of a  matrix 
F are  those  numbers  A for  which 

IF —  AL|  =  0. 

[1.2.12] 

For  example,  for  p =  2 the  eigenvalues  are  the  solutions  to 

>,  o2|  _|A  0 
O  A 
1 

O 

=  0 

or 

* ae  x)  2 
—A 

1 

=  2  -—  $A  -  ¢  =  0. 

[1.2.13] 

The  two  eigenvalues  of F for  a second-order  difference  equation  are  thus  given  by 

A, 

hy 

i  +  4, 
: 
b  ~  VOF a, 
Az  =  ee 

(1.2.14] 

[1.2.15] 

For  a  general  pth-order  system,  the  determinant  in  [1.2.12]  is a  pth-order  poly- 
nomial  in A whose p solutions  characterize  the p eigenvalues  of F. This polynomial 
turns  out  to take  a very  similar  form  to  [1.2.13].  The  following  result  is proved  in 
Appendix  1.A  at  the  end  of this  chapter. 

Proposition  1.1:  The eigenvalues  of the matrix  F defined in equation  [1.2.3] are  the 
values  of d that satisfy 

MP  —  bP“)  —  GP?  —  +--+  =  g  A -  §, =  0. 

[1.2.16] 

Once  we know  the  eigenvalues,  it is straightforward  to characterize  the  dy- 
namic  behavior  of the  system.  First  we  consider  the  case  when  the eigenvalues  of 
that 
F are  distinct;  for  example,  we  require 
that  A, and  A, in  [1.2.14]  and  [1.2.15]  be 
Sian  sates 

quire 

4 

10  Chapter 1  | Difference  Equations 

General Solution  of a pth-Order  Difference  Equation 
with  Distinct  Eigenvalues 

Recall’  that  if the eigenvalues  of a (p x  p) matrix  F are  distinct,  there  exists 

a nonsingular  (p  x  p) matrix  T such  that 

F  =  TAT"! 

[1.2.17] 

where  A is a (p X  p) matrix  with  the eigenvalues  of F along the principal  diagonal 
and  zeros  elsewhere: 

——— 

A;  0  0  =  eee 

0  Ay  @ fees 

0 

0 

Od  DyiOzi  $8 to  Aphis. 

[1.2.18] 

This enables us Suhideacseride the dynamic multiplier  (the (1, 1) element  of 

F’ in  [1.2.11])  very easily.  For example,  from  [1.2.17]  we  can  write F? as 

4  RR  TATH  TATE 

fIS.S.4) 
ie ii. A.  anda 

= 

=Tx  A x  {F-'T)  x  A a  ie 
Tx  AK  eax  TO 

fh, 
| 

| 
_ SIO 
78 
aaa — mid 3a ear tov 

: 

Jaa 

at  ee, TF Io momis (1 JI}pom) 
4 tidy 

oP ay 
Sie  beck der  a7  stamns. lia 
& Sapper firs: 

6  nage a4 Mec  #  janie  tO  41.2.  160s  ae 

.  0  silgmi = +4 i, sted oe 

at >sd® fe estiquni  {$S.5. 

= 

Fo es  a 

stinger 

TAT  a aie. = 

Let  4, denote  the  row  {, column j element  of T and  let  tr’ denote  the  row /, column 
j element of T~'. Equation  [1.2.19]  written  out  explicitly  becomes 

ty, 
hy 
Ser 

to 
by 
eas 

¢** 

fo 
bp 

Ts 

ae  ee 

[Ae 
o 
Set 

Ay  2  2%. 

fi  Go  oe 
jer  a  ee 
fa 

x 

boi  C2 

bop 

0 

0  0  eae 

uM, 

tP?} 

tP2  oe. 

yA,  byAS 
tA,  bypAQ 

* 

°° 
+++ 

bipAD 
j 
bapAD 

a? 
t 
A 
ie 

Fe?  Sy 
tip 
t 
ee 
Ca 
PP 

fA,  b2AS 

*? 

(oe  a 

Yat  i... 

; 

PP 

from  which  the  (1,  1) element  of F’ is given  by 

FY = (eG  +  [42046  +--+  +  (Ape? tas 

or 

where 

fi)  = ae AneGyhh  H+  ter 

CAd, 

[1.2.20]  © 

=  [t,¢"']. 

[1.2.21] 

Note  that  the  sum  of the  c; terms  has  the  following  interpretation: 

CG,  Ft 

eS  fa  eS  ae 

Pr  pep 

ost) 

which  is  the  (1,  1) element  of  T-T~'.  Since  T-T~'  is just  the  (p  xX  p) identity  | 
matrix,  [1.2.22]  implies  that  the  c; terms  sum  to  unity: 

C+  Cc,  =  t24-e  ge  1. 

[1.2.23] 

Substituting  [1.2.20]  into  [1.2.11]  gives  the  form  of  the  dynamic  multiplier 

for  a pth-order  difference  equation: 

OV +; 
Ow, 

=  C,A4,  fe  C>A4  = re 

gaa 

ci. 

[1.2.24] 
. 

Equation  [1.2.24]  characterizes  the  dynamic  multiplier  as  a  weighted  average  of 
each  of the p eigenvalues  raised  to  the jth power. 

The  following  result  provides  a closed-form  expression  for  the  constants  (c,, 
» 
9: 

9  Sge)s 

Cay,  « 

Proposition  1.2:  If the eigenvalues  (A,, Az,  . 
distinct,  then  the  magnitude  c; in  [1.2.21]  can  be written 

« 

. 

,  Ap) J the matrix  F in [1.2.3]  are 

Apo} 

C¢, = 

fl (A,  a  Ay) 

[1.2.25] 

To  summarize,  the pth-order  difference  equation  [1.2.1]  implies  that 

Jey  *  fir ?y,. eee  pe ay  PE 

+  Wrap  +  WWea jaa  t  — + 

+  Buren  +  By. 

RRS.  Fie  teen 
1 

[1.2.26] 

12  Chapter 1 | Difference  Equations 

The  dynamic  multiplier 

OY, + 
i = 

[1.2.27] 

is given  by the  (1,  1) element  of F’: 

{1.2.28} 
A  closed-form  expression  for  y; can  be  obtained  by finding  the  eigenvalues  of  F, 
or  the  values  of A satisfying  [1.2.16].  Denoting  these  p values  by (A,,  A,,...  ,  A,) 
and  assuming  them  to  be  distinct,  the  dynamic  multiplier  is given  by 

by =  fi. 

[1.2.29] 
sya C2,...,  Cp) is a  set  of constants  summing  to  unity  given  by expression 

Wi =  CA,  +  AS  +--+  +  CAL 

,  i a  first-order  system  (p  =  1), this  rule  would  have  us  solve  [1.2.16], 

which  has  the  single  solution 

X=  Gy  =  0, 

According to [1.2.29],  the  dynamic  multiplier  is given  by 

A,  =  4. 

[1.2.30] 

Bet = cM, 

1.2.34] 

From  [1.2.23],  c,  =  1.  Substituting  this  and  [1.2.30]  into  [1.2.31]  gives 

; 

Orsi  _ 
aw,  7  $4, 

j 

or  the  same  result  found  in Section  1.1. 

For  higher-order  systems,  [1.2.29]  allows  a  variety  of more  complicated  dy- 
namics.  Suppose  first  that  all  the  eigenvalues  of  F  (or solutions  to  [1.2.16])  are 
real.  This  would  be  the  case,  for  example,  if  p =  2 and  $7  +  4¢,  > 0 in  the 
solutions  [1.2.14]  and  [1.2.15]  for  the  second-order  system.  If, furthermore,  all of 
the  eigenvalues  are  less  than  1 in absolute  value,  then  the  system  is stable,  and  its 
dynamics  are  represented  as  a  weighted  average  of decaying  exponentials  or de- 
caying exponentials  oscillating  in sign.  For example,  consider  the following  second- . 
order  difference  equation: 

ys  =  0.6y,_;  +  .0.2y,-2  +  W,. 
From  equations  [1.2.14]  and  [1.2.15],  the  eigenvalues  of this  system  are  given  by 

F 

A, 

Fe  =  (84 

0.6  +  V(0.6)?  +  4(0.2) 
2 
0.6 —  V0.6)" + 40.2) 

Ry  5  ene a ON, 

From  [1.2.25],  we  have 

c, = A,/(A, —  Az) =  0.778 

CG  =  A2/(Az  ai  A,)  =  0.222. 

The dynamic  multiplier  for this  system, 

C) Sa =  eM  +  aM, 

1.2.  pth-Order  Difference  Equations 

13 

is  plotted  as  a  function  of  j in  panel  (a)  of  Figure  1.4.°  Note  that  as  j becomes 
larger,  the  pattern  is  dominated  by  the  larger  eigenvalue  (A,),  approximating  a 
simple  geometric  decay  at  rate  A). 

If the  eigenvalues  (the solutions  to  [1.2.16])  are  real  but  at  least  one  is greater 
than  unity  in absolute  value,  the  system  is explosive.  If A, denotes  the  eigenvalue 
that  is largest  in absolute  value,  the  dynamic  multiplier  is eventually  dominated  by 
an  exponential  function  of that  eigenvalue: 

lim  i+) .  i ery 
pan  Ware 

Atods 

Other  interesting  possibilities  arise  if some  of the  eigenvalues  are  complex. 
Whenever  this  is  the  case,  they  appear  as  complex  conjugates.  For  example,  if 
p  =  2 and  ¢?  +  4¢,  <  0, then  the  solutions  A, and  A, in [1.2.14]  and  [1.2.15]  are 
complex  conjugates.  Suppose  that  A, and  A, are  complex  conjugates,  written  as 

. 

A,  =a+t  bi 
A,  =a  —  bi. 

[1.2.32] 
id 33) 

For  the p  =  2 case  of [1.2.14]  and  [1.2.15],  we  would  have 

) 
[1.2:34] 
[1.2.35] 
Our  goal  is to  characterize  the  contribution  to  the  dynamic  multiplier  c,A4 
when  A, is a complex  number  as  in [1.2.32].  Recall  that  to  raise  a complex  number 
to  a  power,  we  rewrite  [1.2.32]  in polar  coordinate  form: 

a =  $,/2 
b  =  (12)V-& —- 40> 

| 

A,  =  R-[cos(6)  +  i-sin(6)], 

[1.2.36] 

where  @ and R are  defined  in terms  of a  and b by the  following  equations: 

R=V@te 

cos(@)  =  a/R 
sin(@)  =  D/R. 

Note  that  R is equal  to  the  modulus  of the  complex  number  A,. 

The  eigenvalue  A, in [1.2.36]  can  be written  as‘  | 

and  so 

A,  =  R[e”], 

My  =  R’[e"]  =  R’[cos(6j)  +  i-sin(6j)). 

[1.2.37] 

Analogously,  if A, is the  complex  conjugate  of A,, then 
A,  =  R{cos(@)  —  i-sin(6)], 

which  can  be written® 

A,  =  R{e-"*). 

Thus 

AL =  Ri[e~'®]  =  R’[cos(6j)  —  i-sin(6j)}. 

[1.2.38] 

is numerical  simulation  of the  system. 

* Again,  if one’s  purpose  is solely to generate  a numerical  plot as in Fi 
“See  equation  [A.3.25]  in the  Mathematical  Réview  (Appendix  A) at the end  of the  book. 
*See  equation  [A.3.26]. 

aie 

; 

. 

1.4,  the easiest  approach 

14  Chapter I  | Difference  Equations 

10 
Lag  (j) 

(a)  $,  =  0.6,  ¢,  =  0.2 

20 

+ 

6 
Lag  (j) 

20 

(b)  $i  a  0.5,  dg? =  —0.8 

FIGURE  1.4  Dynamic  multiplier for second-order  difference  equation  for differ- 
ent  values  of ¢, and ¢, (plot of dy,,;/dw,  as  a function  of the  lag j). 

Substituting  [1.2.37]  and  [1.2.38]  into[ 1.2.29} gives  the contribution  of the complex 
conjugates  to  the  dynamic  multiplier dy, , ;/dw,: 

cA,  +  coA4  =  c,R’[cos(6j)  +  i-sin(6j)]  + c2R’[cos(@j)  —  i-sin(@)] 

=  [c,  +  ¢]-R/cos(@j)  +  i:[e,  —  ¢2)-R’sin(4)). 

[1.2.39] 
. 

The  appearance  of  the  imaginary  number i in  [1.2.39]  may  seem a little 
troubling.  After  all, this  calculation  was  intended  to give the  effect  of a change  in 
the real-valued  variable  w, on  the real-valued  variable  y,, ; as predicted by the real- 
valued  system  [1.2.1],  and  it would  be odd  indeed  if the  correct  answer  involved 
the  imaginary  number  i! Fortunately,  it turns  out  from  [1.2.25]  that  if A, and  A, 
are  complex  conjugates,  then  c,  and  c, are  complex  conjugates;  that  is, they can 

1.2.  pth-Order  Difference  Equations 

15 

i 

be  written  as 

Cc, 

=art  Bi 

Co  *  2.  Bi 

for  some  real  numbers  a  and  B. Substituting  these  expressions  into  [1.2.39]  yields 

cA,  +  c,A4  =  [(a +  Bi) +  (a —  Bi)]-R/cos(Oj)  +  i-[(a + Bi) —  (a@ —  Bi)]-R’sin(4)) 

=  [2a]-R/cos(6j)  +  i-[2Bi]-R/sin( 4) 
=  2aR/cos(6j)  —  2BR’sin(9), 

which  is strictly  real. 

Thus,  when  some  of  the  eigenvalues  are  complex,  they  contribute  terms 
proportional  to  R/ cos(@j)  and  R/ sin(@)  to  the  dynamic  multiplier  dy,, ,/aw,.  Note 
that  if  R =  1—that  is, if the  complex  eigenvalues  have  unit  modulus—the  mul- 
tipliers  are  periodic  sine  and  cosine  functions  of j. A given  increase  in w, increases 
y.+;  for  some  ranges  of j and  decreases  y,,;  over  other  ranges,  with  the  impulse 
never  dying  out  as  j —  ©.  If the  complex  eigenvalues  are  less  than  1 in  modulus 
(R <  1), the  impulse  again  follows  a sinusoidal  pattern  though  its amplitude  decays 
at  the  rate  R’.  If the  complex  eigenvalues  are  greater  than  1 in modulus  (R >  1), 
the  amplitude  of the  sinusoids  explodes  at  the  rate  R’. 

For  an  example  of  dynamic  behavior  characterized  by decaying  sinusoids, 

consider  the  second-order  system 

yr  >  0.5y,_1  a  0.8y,_>  +  W,. 

The  eigenvalues  for  this  system  are  given  from  [1.2.14]  and  [1.2.15]: 
0.5  +  V(0.5)° — 4(0.8) 
2 
0.5  —  V(O.5)* — 4(0.8) 

 —  0.25 

—  ———— 

ey 

A=  se  =  0.25  —  0.86i, 

with  modulus 

R  =  V(.25)" 

+ (0.86)  =  0.9. 

Since  R <  1, the dynamic  multiplier  follows  a pattern  of damped  oscillation  plotted 
in  panel  (b)  of  Figure  1.4.  The  frequency®  of  these  oscillations  is given  by the 
parameter  @ in [1.2.39],  which  was  defined  implicitly  by 

cos(@)  =  a/R  =  (0.25)/(0.9)  =  0.28 

or 

@ =  1.29. 
The  cycles  associated  with  the  dynamic  multiplier  function  [1.2.39]  thus  have  a 
period  of 

2m _  (2)(3.14159)  _ 
ee 
0 

4.9; 

i 
that is, the peaks in the pattern  in panel (b) of Figure  1.4 appear  about five periods 
"apart. 

*See  Section  A.1  of the  Mathematical  Review  (Appendix  A) at the end of the book for a discussion 

of the frequency  and  period  of a sinusoidal  function. 

16  Chapter 1  | Difference Equations 

Solution  of a  Second-Order  Difference  Equation 
with  Distinct  Eigenvalues 

The  second-order  difference  equation  (p  =  2) comes  up  sufficiently  often 
that  it  is  useful  to  summarize  the  properties  of  the  solution  as  a  general  function 
of ¢,  and  ¢,,  which  we  now  do.’ 

The  eigenvalues  A, and  A, in  [1.2.14]  and  [1.2.15]  are  complex  whenever 

or  whenever  (¢,,  ¢2) lies  below  the  parabola  indicated  in  Figure  1.5.  For  the  case 
of complex  eigenvalues,  the  modulus R satisfies 

7  +  4,  <  0, 

or,  from  [1.2.34]  and  [1.2.35], 

R?  =  q@*  +  b’, 

R?  =  ($,/2)?  —  (bf  +  4¢2)/4  = 

—  dp. 

Thus,  a  system  with  complex  eigenvalues  is explosive  whenever  ¢,  <  —  1. Also, 
when  the  eigenvalues  are  complex,  the  frequency  of oscillations  is given  by 

@ =  cos~'(a/R)  =  cos~{o,/(2  V—¢,)], 
where  “‘cos~  '(x)’’ denotes  the  inverse  of the cosine  function,  or  the  radian  measure 
of an  angle  whose  cosine  is x. 

\  * ~, on sie os . 0.6 — 6 Ko  Yi 

<-1 

A,>1 

(-2,-1 

JAj<1 

complex  eigenvalues 
JAf>1 

2,-1) 

. 

9° + 49,=0 
{ 
' 

i] 

' 

FIGURE  1.5  Summary  of dynamics  for  a second-order  difference  equation. 

This discussion  closely  follows  Sargent  (1987,  pp.  188-89). 

1.2.  pth-Order  Difference  Equations 

17 

For  the  case  of real  eigenvalues,  the  arithmetically  larger  eigenvalue  (A,) will 

be  greater  than  unity  whenever 

Or 

gd,  +  Y  Bi 

IB?  sory 

2 

VE + 46, >  2  -  dy. 

Assuming  that  A, is real,  the  left  side  of this  expression  is a  positive  number  and 
the  inequality  would  be  satisfied  for  any  value  of  ¢,  >  2.  If,  on  the other  hand, 
g@, <  2, we  can  square  both  sides  to  conclude  that  A, will  exceed  unity  whenever 

or 

$i  +  46,  >4  -— 44,  +  Oj 

d,>1— $y. 

Thus,  in the  real  region,  A, will  be greater  than  unity either  if ¢, >  2 or  if (¢,,  ) 
lies northeast  of the line  ¢, =  1 —  @, in Figure  1.5.  Similarly,  with  real eigenvalues, 
the  arithmetically  smaller  eigenvalue  (A) will  be  less  than  —1  whenever 

Or —  V¢di  +  4,  gga 
2 

| 

-  VET FAG; <  -2  -  4; 
VO} + 46, >2  +  gy. 

Again,  if ¢;  <  —2,  this  must  be  satisfied,  and  in the  case  when  ¢, >  —2,  we  can 
square  both  sides: 

b}  +  4, > 4+  4d,  +  62 
o,>1+  4. 

Thus,  in the  real  region,  A, will  be  less  than  —  1 if either  6, <  —2  or  (¢;,  ¢5) lies 
to  the  northwest  of the  line  ¢,  =  1  +  ¢, in Figure  1.5. 

The  system  is thus  stable  whenever  (¢,, ¢,) lies  within  the  triangular  region 
. 

of Figure  1.5. 

General  Solution  of a  pth-Order  Difference  Equation 
with  Repeated  Eigenvalues 
In  the  more  general  case  of a  difference  equation  for  which  F has  repeated 
eigenvalues  and  s  < p linearly  independent  eigenvectors,  result  {1.2.17]  is gener- 
alized  by using  the  Jordan  decomposition, 

F  =  MJM"! 

[1.2.40] 

where  M is a  (p  X  p) matrix  and  J takes  the  form  - 

a 

sh  «::*s 

irons 

18  Chapter 1  | Difference  Equations 

SS 

0 0 
Dice bene 
0  0 
oe 
Oe (02.A,  ete 
oY 

ie 

on  a peices vibe 
C18  . 0: »  9%, 0.2, 

1.2.41] 

for A, an  eigenvalue  of F. If [1.2.17] is  replaced  by [1.2.40],  then  equation  [1.2.19] 
generalizes  to 

=  MJ/M-! 

[1.2.42] 

where 

. 

AS ed = © a ee 

aad 

qn 

, 

Moo,  om 1.24,  dmeson 

rn) then Si  tant}  babrvoig 
[a  (ar  0) Va fh « ers Pd  ditey  ie gemg  on) 

ae, 

vee 
oS  . 
Pe 

ee ca 

Gee 
ac  — 

ee ke 
Soe 
ae: 
a 

= 

[1.2.43] 

| 

é 

J  Hts a jursbacbatatr trem!  qyotietes Faso TH.  SHD  SE 
HizoqgOIg  sniwoliol  o:? 

Sa  {i-»  Ee  eee  Eh 

eel 

=n 

7: 

ge 
’  _—-  ua ” 

think  of  a  “solution”  of y,  in  terms  of the  infinite  history  of  w, 

),  =  a  WW, _  |  ¥ 3  YoW,_2  T  YW, _3  eS  we 

[1.2.44] 

where  y, is given  by the  (1, 1) element  of F’ and  takes  the  particular  form  of [1.2.29] 
in  the  case  of distinct  eigenvalues. 

It  is also  straightforward  to  calculate  the  effect  on  the  present  value  of y of 
a  transitory  increase  in  w.  This  is simplest  to  find  if we  first  consider  the  slightly 
more  general  problem  of the  hypothetical  consequences  of a change  in any  element 
of the  vector  v, on  any  element  of &,,; in  a  general  system  of the  form  of  [1.2.5]. 
The  answer  to  this  more  general  problem  can  be inferred  immediately  from  [1.2.9]: 

Oiesy 
Ov, 

wy 

[1.2.45] 

The  true  dynamic  multiplier  of interest,  dy,, ;/dw,,  is just the  (1,  1) element  of the 
(p  X  p) matrix  in  [1.2.45].  The  effect  on  the  present  value  of € of a  change  in  v 
is given  by 

0»  BE. 
j=0 

ov, 

=  > piF! =  (1, -  BR), 

a 

[1.2.46] 

provided  that  the  eigenvalues  of F are  all  less  than  B~'  in modulus.  The  effect  on 
the  present  value  of y of a  change  in  w, 

a> BY; 

j=0 

ow, 

’ 

is thus  the  (1,  1) element  of the  (p  x  p) matrix  in [1.2.46].  This  value  is given  by 
the  following  proposition. 

Proposition  1.3: 
If the  eigenvalues  of the  (p  x  p) matrix  F defined  in  [1.2.3]  are 
all less  than  B~'  in  modulus,  then  the  matrix  (1,  —  BF)~'  exists  and  the  effect  of 
w  on  the present  value  of y is given  by its  (1, 1) element: 

V/(1  1  Oras  $28?  Tye  4 

pale 

a  $,B"). 

Note  that  Proposition  1.3  includes  the  earlier  result  for  a  first-order  system 

(equation  [1.1.14])  as  a  special  case. 

The  cumulative  effect  of  a  one-time  change  in  w,  on  Vie  Vint,  -  «.  Con  be 
considered  a  special  case  of Proposition  1.3  with  no  discounting.  Setting  B =  1 in 
Proposition  1.3  shows  that,  provided  the  eigenvalues  of F are  all  less  than  1 in 
modulus,  the  cumulative  effect  of a  one-time  change  in w  on y is given  by 

Oi  4; 
2 Sey  =  VG  =  Ou  Gas  +:  +  —  +). 
j=0  Ow, 

[1.2.47] 

Notice  again  that  [1.2.47]  can  alternatively  be interpreted  as giving the even- 

tual  long-run  effect  on  y of a permanent  change  in w: 

‘ 

lim 

OY, 4 

f 

dy 

ht  RE ee  Abd 

dy 

Ras 

20  Chapter 1 | Difference  Equations 

0 

eth 

; 

s 

pal 

sa:  6 

APPENDIX  1.A.  Proofs  of Chapter  1 Propositions 
@  Proof of Proposition  1.1.  The  eigenvalues  of F satisfy 

For  the  matrix  F defined  in equation  [1.2.3],  this  determinant  would  be 

\F —  Al,|  =  0. 

[1.A.]] 

%,  d  3  -°- 
it 
Bee 
Ge 
6  +--+ 
Se 

i 

d, -1 
ae 
O 

>, 
0 
Of  —~f0-0°a-- 

AvADoDes)  2-  0:0 
0A.  0  --°00 
0  0 

a 

6. 

»® 

1 

0 

0  0  0 

OA 

¢; 
(¢,  —  A)  o, 
-A 
6 
)  | 

1 
0 

= 

d,-1 
0 
0 

dg, 
0 
0 

[1.A.2] 

Oe 

0 

0 

oe 

1 

—A 

Recall  that if we multiply  a column of a matrix by a constant  and add  the result  to another 
ix  is unchanged.  If ne multiply the pth column  of the 
a, the eet 

[1.A.2] 

by  (1 

d  add th  Beast 24 the LP  Oe  eaten  ate 

is that  i  fl. A.2] a 
ce ,  ‘<  a  bran  bp “om _ 

i 

; 

;  ti fo 
ae 

rakes  a  WF)  lls. 
# 
ee 

Supple be un 
tee 

a8Qe +: os  Wg 

a  tet  | 1  Saithte| is 

2 

A 

; 
>. 

-  oO  ?  oe <M wae  ~  Ay  aoe  »  Ta  :  ww; 

. 

Cf!  & sPeiecOraiac-ot  ¥, Oth 

nid  et  Dy 

yon aan yt  asst os  mi  Nasal 

*  @eats,  2  Giese 

o- 

Tic 

oe 

— 

a 

The  eigenvalues  of F are  thus  the  values  of A for which  [1.A.3] is zero, or for which 

AP — pdr-'  —  gh"? — --- 

—  &  =  0, 

: 

as  asserted  in  Proposition  1.1.  @ 

w 

@  Proof  of Proposition  1.2.  Assuming  that  the  eigenvalues  (A,, A2,.  . 
,  Ap) are  distinct, 
the  matrix  T in  equation  [1.2.17]  can  be  constructed  from  the  eigenvectors  of F  Let  t, 
denote  the  following  (p  X  1) vector, 

- 

[1.4.4] 

. 

| 

fe 
J 
ack  $,  or  eee  Ne 3 ed 

OY 

“ae  is 

at  1G-areeigs 
; 
biti 
Sittgen 6 24 om Set, é Bou che nay  eS)  4 abe i 

.  beatae tote  2i 

oe  i ale re  =  YO 

zy  ki 

|e  Oa 
yEe 

ms 

ants 

; 

Poe 

Re  engl 

i 

Wiig 

Sw  ti twat!  tte  A 
MmisisD  al Bs we  ; 

rc  [ACT 

ae ‘a 

nietisish  omas  sett  dee 

so  bea, 

|  ayy 

Be 

2 

tahisoe  te 

[arte arts oars  rele  | 

ee  ee  i  ‘= 

1 

is 

: 

[1.A.5] 

iad 

: 

| ean  % 

where T is given  by [1.A.4]  and  [1.A. 8]. Writing  out  the  first  column  of the  matrix  system 
of equations  [1.4.9]  explicitly,  we  have 
APm?  ABThos  xy 
we 
Ag?  ARF  oe 
Ag->  Aen-3.--- 

age" 
Asm? 
Ag-3 

em 
p 
pf) 

l 
0 
0 

Al 

1 

wh 

oo 

ELE 

pe-d 

vp} 

0 

0 

This  gives  a  system  of p linear  equations  in the  p unknowns  (¢"',  f?',  . 
that  the  A, are  all  distinct,  the  solution  can  be shown  to  be? 

se  Sl a 

. 

. 

,  t').  Provided 

(A,  —  A2)(A,  —  As) °°  *  (AY  -  A,) 

P= 

: 

(A,  —  A) - As) °°  i A,  - 

y) 

: 

oA 

; 

Consider  postmultiplying  this  system  of  equations  by a  matrix  with  1s  along  the  principal 
diagonal,  8 in  the  row  p,  column  p  —  | position,  and  Os  elsewhere: 

1  0 

0. 

0  0 

0  0 

|  cc  ae 

The  effect  of this  operation  is to  multiply  the pth column  of a matrix  by B and  add  the  result 
to  the  (p  —  1)th  column: 

| a  Soc Te 

1-  6d, 

-Bd,  --: 

—Bd¢,-,—-  B64,  —Bd, 

—B 

1 

ee 

: 
0 

ee 
. 
—_— 

0 

: 
0 

0 

i 

=(1  0---0  Oj 

Next  multiply  the  (p  —  1)th  column  by £ and  add  the  result  to  the  (p  —  2)th  column. 
Proceeding  in this  fashion,  we  arrive  at 

fey  ee  OS 

_[1-Bd, - Bg. -- - 

--  B?-'¢,-,-B’¢,  —Bd2—B’b;-----P?-'¢,  --- 
vee 
0 

ao 

—Bb,-1—B°¢, 
0 

—Bd, 
0 

eee 

r 

aa 

The  first  equation  in [1.A.13]  states  that 

| 

2  6fl  —  BO,  —  Be  —  = 

28-  PMs  eo B’¢,)  == 

=[{1  0  --- 

O  Oj. 

[1.4.13] 

or 

as  claimed  in Proposition.1.3.  @ 

ha =  11  -  Bd,  ~  B’d,  Ci  aa  B¢,), 

Chapter  I References 

Chiang,  Chin  Long.  1980.  An  Introduction  to  Stochastic  Processes  and  Their  Applications. 
Huntington,  N.Y.:  Krieger. 
Goldfeld,  Stephen  M.  1973.  ‘‘The  Demand  for  Money  Revisited,”  Brookings  Papers  on 
Economic  Activity  3:577-638. 
Sargent,  Thomas  J.  1987.  Macroeconomic  Theory,  2d  ed.  Boston:  Academic  Press. 

24  Chapter  1  | Difference  Equations 

Lag  Operators 

2.1.  Introduction 

The  previous  chapter  analyzed  the  dynamics  of  linear  difference  equations  using - 
matrix  algebra.  This  chapter  develops  some  of the  same  results  using  time  series 
operators.  We  begin  with  some  introductory  remarks  on  some  useful  time  series 
operators. 

g 

A  time  series  is  a  collection  of  observatfons  indexed  by the  date  of  each 
observation.  Usually  we  have  collected  data  beginning  at some  particular  date  (say, 
t  =  1) and  ending at  another  (say,  ¢  =  7): 

(V1,  Yas»  + 

+ 

>  V7): 

We  often  imagine  that  we  could  have  obtained  earlier  observations  (yo,  y_1, 
.  -) had  the  process  been  observed 
y_2,-.-.)  or  later  observations  (y7,;,  Yr.2,  - 
,  yr) could  then  be  viewed  as  a 
- 
for  more  time.  The  observed  sample  (y,, y2,  . 
finite  segment  of a  doubly  infinite  sequence,  denoted  {y,}7_  _  .: 

. 

{yD x =  {2 - 

-  Yaa  Yor  Yur  Yar 

©  Vr  Vras  Yr+29  > 

+  + 

ed 
observed  sample 

Typically,  a  time  series  {y,}7 _.  is identified  by describing  the  ‘th  element. 
For  example, a time  trend  is a series  whose  value  at  date  ¢ is simply  the  date  of the 
observation: 

We  could  also  consider  a  time  series  in which  each  element  is equal  to  a  constant 
c,  regardless  of the  date  of the  observation  f: 

y=. 

y=, 

Another  important  time  series  is  a Gaussian  white  noise  process,  denoted 

Vr  =  &15 

4 

where  {e,}* _..  is  a sequence  of independent  random  variables  each  of which  has 
a N(0,  a7) distribution. 

We  are  used  to thinking  of a function  such  as y =  f(x) or y  =  g(x,  w) as  an 
operation  that  accepts  as  input  a  number  (x) or  group  of  numbers  (x,  w)  and 
produces the output  (y).  A time series operator  transforms  one  time series or group 

25 

of  time  series  into  a  new  time  series.  It  accepts  as  input  a  sequence  such  as 
}*_.  or  a  group  of sequences  such  as  ({x}7_  _.,  {w,7_  -.)  and  has  as  output  a 
{x 
new  sequence  {y,}7_  _..  Again,  the  operator  is summarized  by describing  the  value 
of  a  typical  element  of  {yj}7_.  in  terms  of  the  corresponding  elements  of 
{X}r=  -  =: 

An  example  of  a  time  series  operator  is  the  multiplication  operator,  repre- 

sented  as 

Ye  *  Bx.,. 

{2.1.1} 

Although  it is written  exactly  the same  way  as  simple scalar  multiplication,  equation 
{2.1.1]  is  actually  shorthand  for  an  infinite  sequence  of  multiplications,  one  for 
each  date  ¢.  The  operator  multiplies  the  value  x  takes  on  at  any  date  t by some 
constant  6 to  generate  the  value  of y for  that  date. 

Another  example  of a  time  series  operator  is the  addition  operator: 

y,  =  X,  +  W,. 

Here  the  value  of y at  any  date  ¢ is the  sum  of the  values  that  x  and  w  take  on  for 
that  date. 

Since  the  multiplication  or  addition  operators  amount  to element-by-element 
multiplication  or  addition,  they obey all the standard  rules  of algebra.  For example, 
if  we  multiply  each  observation  of  {x,}=_.  by  B  and  each  observation  of 
{w,}=_ _.  by B and  add  the  results, 

Bx,  +  Bw,, 
the  outcome  is  the  same  as  if we  had  first  added  {x= _.  to  {w}=_.  and  then 
multiplied  each  element  of the  resulting  series  by B: 

B(x,  +  w,). 

A  highly  useful  operator  is the  lag operator.  Suppose  that  we  start  with  a 
sequence  {x,}’_ _.  and  generate  a new  sequence  {y,}~ _.,  where  the  value  of y for 
date  ¢ is equal  to  the  value  x  took  on  at  date  t  —  1: 

[2.1.2] 
Vr  =  X21- 
This  is described  as  applying  the  lag operator  to  {x,}7_ _..  The  operation  is repre- 
sented  by the  symbol  L: 

LZ MP  Kes. vy 

{2.1.3] 

Consider  the  result  of applying  the  lag operator  twice  to  a series: 

Such  a double  application  of the  lag operator  is indicated  by, “E>”: 

L(Lx,)  =\1Ax,  41)  =  X%>. 

In general,  for  any  integer  k, 

Lejos  expe 

[2.1.4] 
Notice  that  if we  first  apply  the  multiplication  operator  and  then  the  lag 

A  ee  ee 

operator,  as  in 

x, —> Bx, > Bx,_1, 

the  result  will  be exactly  the  same  as  if we  had  appli 
then  the  multiplication  operator: 

“aah 

. 

t 

ad: 

. 

plied  the  lag  o 

or. 
:  om mee  ane. 

first 

26  Chapter 2  | Lag Operators 

x, ees X,-1  —  |  ae 

Thus  the  lag operator  and  multiplication  operator  are  commutative: 

L(Bx,)  =  B-Lx,. 

Similarly,  if we  first  add  two  series  and  then  apply  the  lag operator  to  the  result, 

(x,,  W,)  >  X,  +  W,—>  X,_,  +  W,_,, 

the  result  is the  same  as  if we  had  applied  the  lag operator  before  adding: 

Thus,  the  lag operator  is distributive  over  the  addition  operator: 

(x,,  w,) he (x,-  1>  W,_1)  —>  X,-3  +  W,-1- 

L(x,  +  w,)  =  Lx,  +  Lw,. 

We  thus  see  that  the  lag operator  follows  exactly  the  same  algebraic  rules  as 
the  multiplication  operator.  For  this  reason,  it is tempting  to  use  the  expression 
“multiply  y,  by L”  rather  than  ‘“‘operate  on  {y,}7 _.  by L.”  Although  the  latter 
expression  is technically  more  correct,  this  text  will  often  use  the  former shorthand 
expression  to  facilitate  the  exposition. 

Faced  with  a time  series  defined in  terms  of compound  operators,  we  are  free 
to  use  the  standard  commutative,  associative,  and  distributive  algebraic  laws  for 
multiplication  and  addition  to  expres$  the  compound  operator  in  an  alternative 
form.  For  example,  the  process  defined  by 

is exactly  the  same  as 

y,  =  (a +  bDL)Lx, 

=  (aL  +  bL?)x,  =  ax;_,  +  bx,~>. 

To  take  another  example, 

(1  —  A,L)(1  —  A,L)x,  =  (1  —  AL  —  ALL  +  A,A,L7)x, 

(1  —  [A,  +  AJL  +  A,A,L?)x, 

[2.1.5] 

=  x,  —  (Ay  +  Ag)X~1  +  (AyA2)%-2- 

An  expression  such  as  (aL  +  bL?) is  referred  to  as  a polynomial  in  the  lag 
operator.  It is algebraically  similar  to  a  simple  polynomial  (az  +  bz?)  where z is 
a  scalar.  The  difference  is  that  the  simple  polynomial  (az  +  bz?)  refers  to  a 
particular  number,  whereas  a polynomial  in the  lag operator  (aL  +  bL’)  refers  to 
an operator  that  would  be  applied  to  one  time  series  {x,}7__.  to  produce  a  new 
time  series  {y,}7 _.. 

Notice  that  if {x,}* _.  is just a  series  of constants, 

then  the  lag operator  applied  to x, produces  the  same  series  of constants: 

xX,=C 

for  all  t, 

Thus,  for example, 

Lx,  =  %)-1  =  ¢. 

(aL  +  BL?  +  yL?)c  =  (a  + B +  y) °C. 

{2.1.6} 

2.2.  First-Order  Difference  Equations 

.  _ Let  us now return  to the first-order  difference equation  analyzed: in. Section Ll: 

=  Py,-,;  + 

[2.2.1] 

2.2.  First-Order  Difference  Equations 

27 

Equation 

(2.2.1]  can  be  rewritten  using  the  lag operator  [2.1.3]  as 

y,  =  ply,  +  Wr. 

This  equation,  in  turn,  can  be  rearranged  using  standard  algebra, 

ns  oLy,  =  Wy 

or 

(J  ze  gL)y,  =  We 

[2.2.2] 

Next  consider  “multiplying”  both  sides  of  [2.2.2]  by the  following  operator: 
[2.2.3] 

(14+  @bL+  PL?  +  PL?  +---  +  PL’. 

The  result  would  be 

(1  +  oL  +  dL?  +  ¢L?  Hoty 

tt  ¢'L')(1  ee  oL)y, 

pe  (1  Be oL  ze  gL?  +  ¢L?  +eee  +t  ¢'L')w,. 

[2.2.4] 

Expanding  out  the  compound  operator  on  the  left  side  of  [2.2.4]  results  in 
(1+  6L  +  PL?  +  PL?  +--+  +  PL)(1  —  oL) 
=(1+¢@L+¢@L?+ 
PL? +---+  GL) 

=  (1 +  oL  +  ¢?L?  +  ¢L*  t+--°+  ¢'L')oL 

=(l+o@Lt+¢@LV?+ 
PL? +--+  GPL) 

x4  (oL  +  o7L?  +  &L>  fee  et  ¢'L'  +  ” Sie  Seg 

[2.2.5] 

=  (1 =  rd  Slt) 

Substituting  [2.2.5]  into  [2.2.4]  yields 

(1  —  pt'L'*!)y,  =  (1  +  OL  +  ¢?Ll?  +  PL?  +-:-:  +  PL')w,. 

[2.2.6] 

Writing  [2.2.6]  out  explicitly  using  [2.1.4]  produces 

,  fi  Gy  ee)  =w,  +  ow,_1  a  g¢?w,_2  a  ¢?w,_3  =  geacemagn 

q'w,_; 

or 

[2.2.7] 
y,  =  Gly,  +  w,  +  bw  +  w,-2  +  Pwe3  t+  +  PHWo- 
Notice  that  equation  [2.2.7]  is  identical  to  equation  [1.1.7].  Applying  the 
operator  [2.2.3]  is performing  exactly  the  same  set  of recursive  substitutions  that 
were  employed  in  the  previous  chapter  to  arrive  at  ebpouk 

It is interesting  to  reflect  on  the  nature  of the  operator  [2.2.3]  as  t becomes 

large.  We  saw  in  [2.2.5]  that 

(1+  OL  +  PL?  +  PL  +--+  +  PL  —  L)y,  =  y  —  O*'y-1- 
That  is, (1  +  OL  +  @L?  +  PL?  +  ---  +  PL'\(1  —  $L)y,  differs  from  y, by 
the  term  o'*'y_,.  If |¢| <  1 and  if y_,  is a  finite  number,  this  residual  $'*"y_, 
will  become  negligible  as  t becomes  large: 

(1+  oL  +  @L?  +  PL?  +--+  +  PL)  -  OL)y,  =» 

for  ¢ large. 

A sequence  {y,}*_  _,  is said  to  be  bounded  if there  exists  a  finite  number  y such 
that 

ly|<y 

for  all ¢. 

Thus, when  || <  1 and when  we  are considering  applying an operator  to a bounded 
sequence,  we  can  think  of 

(1  +  OL  +  @L?  +  PL?  +--+  +  PIL’) 

28  Chapter 2  | Lag Operators 

as  approximating  the  inverse  of  the  operator  (1  —  @L),  with  this  approximation 
made  arbitrarily  accurate  by choosing  j sufficiently  large: 

(1  —  @L)-'  =  lim  (1  +  @L  +  @L?  +  #13  +--+  +  PL’). 

[2.2.8] 

This  operator  (1  —  ¢L)~'  has  the  property 

where  “1”  denotes  the  identity  operator: 

(1  —  @L)"'(1  —  L)  =  1, 

Ly,  =  yj. 

The  following  chapter  discusses  stochastic  sequences  rather  than  the  deter- 
ministic  sequences  studied  here.  There  we  will  speak  of mean  square  convergence 
and  stationary  stochastic  processes  in  place  of  limits  of  bounded  deterministic 
sequences,  though  the  practical  meaning  of  [2.2.8]  will  be  little  changed. 

Provided  that  |¢|  <  1 and  we  restrict  ourselves  to  bounded  sequences  or 
stationary  stochastic  processes,  both  sides  of [2.2.2]  can  be  “‘divided”  by (1  —  @L) 
to  obtain 

or 

y=  TH  oL)~*w, 

ys  =  W,  +  Pw,  +  dw, _2  €  pw, +3  >  ahaa  : 

[2.2.9] 
It should  be emphasized  that  if we  were  not  restricted  to considering  bounded 
sequences  or  stationary  stochastic  processes  {w,}_ _.  and { y,}*_ _.,  then  expression 
[2.2.9] would  not  be a necessary  implication  of [2.2.1].  Equation  [2.2.9] is consistent 
with  [2.2.1],  but  adding  a  term  a,¢', 

y,  =  a,b  +  w,  +  bw,_,  +  $W,_2  +  PwWu3  +67, 

{2.2.10} 

produces  another  series  consistent  with  [2.2.1]  for  any  constant  a,.  To  verify  that 
[2.2.10]  is consistent  with  [2.2.1],  multiply  [2.2.10]  by (1  —  L): 

(1 —  gL)y,  =  (1  —  $L)a.g'  +  (1  —  PL)(1  —  $L)'w, 

ap’  —  p:a,¢'—*  +  Ww, 

=  W,, 

so  that  [2.2.10]  is consistent  with  [2.2.1]  for  any  constant  a,. 

Although  any  process  of the  form  of [2.2.10]  is consistent  with  the  difference 

equation  [2.2.1],  notice  that  since  |¢| <  1, 

la,¢|>- 

as  ta. 

Thus, even  if {w,}_ _. is a bounded  sequence,  the solution  { y,}7_ _.  given by [2.2.10] 
is unbounded  unless  a,  =  0 in  [2.2.10].  Thus,  there  was  a  particular  reason  for 
defining  the operator  [2.2.8]  to  be the  inverse  of (1 —  ¢L)—namely,  (1 —  #L)~' 
defined  in [2.2.8]  is the  unique  operator  satisfying 

(1 —  @L)""(1  —  oL)  =  1 

that  maps  a  bounded  sequence  {w,}/_ _,  into  a  bounded  sequence  {yJr.-k- 

The  nature  of (1  —  #L)~'  when  |¢| =  1 will  be discussed  in Section  2.5. 

2.3.  Second-Order  Difference  Equations 
Consider  next  a second-order  difference  equation: 

Yr  =  PiYe-1  +  P2Yr-2  +  We 

[2.3.1] 

2.3.  Second-Order  Difference  Equations 

29 

Rewriting  this  in  lag  operator  form  produces 

[2.3.2] 
The  left  side  of (2. 3.2] contains  a second-order  2  Seal in  the  lag operator 

(1  —  OL  —  d2L?)y, =  w

.

,

L.  Suppose  we  factor  this  polynomial,  that  is, find  numbers  A, and  A, such  that 

(1 —  ,L  —  $1?)  =  (1 —  AyL) (1 —  AgL)  =  (1  [Ar +  Ag)L  +  ArA2L?). 

[2.3.3] 

This  is just  the  operation  in [2.1.5]  in reverse.  Giyen  values  for ¢, and  ¢,, we  seek 
numbers  A, and  A, with  the  properties  that 

and 

A, +A,  = 

For  example,  if ¢,  =  0.6  and  ¢,  =  —0.08,  then  we  should  choose  A,  =  0.4  and 
; 
A,  =  0.2: 

(1  —  0.6L  +  0.08L2)  =  (1 —  0.4L)(1  —  0.2L). 

[2.3.4] 

It is easy  enough  to  see  that  these  values  of A, and  A, work  for this  numerical 
example,  but  how  are  A, and  A, found  in general?  The  task  is to  choose  A,  and  A, 
so  as  to  make  sure  that  the operator  on  the  right  side of [2.3.3]  is identical  to  that 
on  the  left  side.  This  will  be  true  whenever  the  following  represent  the  identical 
functions  of z: 

| 

(1  —  $,z  —  dz”)  =  (1 —  A,z)(1  —  A,2z). 

[2.3.5] 

This ‘equation  simply  replaces  the  lag operator  L in  [2.3.3]  with  a  scalar  z.  What 
is the  point  of doing  so?  With  [2.3.5],  we  can  now  ask,  For  what  values  of z is the 
right  side  of [2.3.5]  equal  to  zero?  The  answer  is, if either  z  =  Aj!  or  z  =  Az’, 
then  the  right side  of [2.3.5]  would  be zero.  It would  not  have  made  sense  to  ask 
an  analogous  question  of [2.3.3] —L  denotes  a particular  operator,  not  a number, 
and  L  =  A;'  is not  a  sensible  statement. 

Why should  we  care  that  the  right side  of [2.3.5]  is zero  if  z  =  Aj! orifz  = 
Az '? Recall  that  the  goal  was  to  choose  A, and  A, so  that  the  two  sides  of [2.3.5] 
represented  the identical  polynomial  in z.  This  means  that  for any particular  value 
z  the  two  functions  must  produce  the  same  number.  If we  find  a  value  of z  that 
sets  the  right  side  to  zero,  that  same  value  of z  must  set  the  left  side  to  zero  as 
well.  But  the  values  of z  that  set  the  left  side  to  zero, 

are  given  by the  quadratic  formula: 

(1  —  $,z  —  $2?)  =  0, 

iu  >  Digs 

o,  —  Voi + 46, 

$,  +  Voi  +  44, 
Zz,  =  oe 

{2.3.6} 

(2.3.7] 

{2.3.8] 

Setting  z  =  z,  or  z,  makes  the  left  side of [2.3.5]  zero,  while  z  =  Aj!  or 

Az '  sets  the  right  side  of [2.3.5]  to zero.  Thus 

Ap  =  2 
Ay!  =  22. 

[2.3.9] 
[2.3.10] 

30  Chapter  2 

Lag Operators 

 
Returning  to  the  numerical  example  [2.3.4]  in which  ¢,  =  0.6  and  ¢,  =  —0.08, 
we  would  calculate 

_  0.6  ~ 

VOBF—  4008), 

<i  de 

2(0.08) 
_  0.6  +  VOOR = X008) 
2(0.08) 

— 

aabiada 

vie 

and  so 

A,  =  1/(2.5)  =  0.4 
A>  = 1/(5.0)  =  0.2, 

as  was  found  in [2.3.4]. 

, 
3 

When  ¢j  +  4¢,  <  0, the  values  z, and  z, are  complex  conjugates,  and  their 
reciprocals  A, and  A, can  be  found  by first  writing  the  complex  number  in polar 
coordinate  form.  Specifically,  write 

a 

| 

} 

23.=.4.+  bi 

,=R: ‘{cos(6) 3  sin(0) = -  R-e, 

Pilea ee  6  to 25u pi 2 ai}  snitslusico  nsowted  ssishaacssno 
esoubovini 1 ravse% 
ta 

Lenin =. p  Spf spare)  = =  ésin  otilog & 37h y262 

ent, 

odt 

Seige 

aes 

Ht 

29d 01 overt’  iS 

_  “ely, .  i  dimsicgs dass method for calculating th  pen  seh 
7 

6 ROD NOS  Sits  tise  stdiztc 

16 sotun2  10niE 

A> 

fre 

f 
sides of [2.3.5] by Btiiox>  fas  2u 
aioe be"  4, 27 Bes She ‘ae ai A aye” eG  fis: =745)*! 

both 

e 

a 

define  A to be the  variable  z Zz 

‘ 

yur 

nite  at  ii  texts  2i sili  Ainen afi  sho  Hay sdt  aj 

3 iti ae ri bimaet nt sharon mitsimonyiog  fj  sdtiw  yiosih! “i 

x 

intc 

9 

weln  rnc 

ices a8, & ce. ort  ey aa: 

: 

A  -} 

=  Key 

ps. 11} 

ae 

It  is  instructive  to  compare  these  results  with  those  in  Chapter  1.  There  the 
dynamics  of  the  second-order  difference  equation  [2.3.1]  were  summarized  by 
calculating  the  eigenvalues  of  the  matrix  F given  by 

F  =  ie ef 

[2.3.17] 

The  eigenvalues  of  F  were  seen  to  be  the  two  values  of  A  that  satisfy  equation 
[1-2. 135: 

(A?  —  A  —  2)  =  9. 
But  this  is the  same  calculation  as  in  [2.3.14].  This  finding  is summarized  in the 
following  proposition. 

Proposition  2.1: 

Factoring  the polynomial  (1  —  $,L  —  ,L7)  as 
(2.3.18] 
(4  2  ob  —-6L2)  =  0  Dewy 
is the  same  calculation  as  finding  the  eigenvalues  of the  matrix  F in  [2.3.17|.  The 
eigenvalues  d, and  d, of F are  the same  as  the parameters  i, and  A, in [2.3.18],  and 
are  given  by equations  [2.3.15]  and  [2.3.16]. 

The  correspondence  between  calculating  the  eigenvalues  of  a  matrix  and 
factoring  a polynomial  in the lag operator  is very  instructive.  However,  it introduces 
one  minor  source  of possible  semantic  confusion  about  which  we  have  to be careful. 
Recall  from  Chapter  1 that  the  system  [2.3.1]  is stable  if both  A, and  A, are  less 
than  1 in  modulus  and  explosive  if either  A, or  A, is  greater  than  1 in  modulus. 
Sometimes  this  is described  as  the  requirement  that  the  roots  of 

[2.3.19] 
(A?  —  $,A  —  ¢)  =  0 
lie  inside  the  unit  circle.  The  possible  confusion  is that  it is often  convenient  to 
work  directly  with  the  polynomial  in the  form  in which  it appears  in [2.3.2], 

(1  —  oz  —  ¢)z7)  =  0, 

[2.3.20] 

whose  roots,  we  have  seen,  are  the  reciprocals  of those  of [2.3.19],  Thus,  we  could 
say  with  equal  accuracy  that  “‘the  difference  equation  [2.3.1]  is stable  whenever 
the  roots  of  [2.3.19]  lie  inside  the-unit  circle”  or  that  “the  difference  equation 
[2.3.1]  is stable  whenever  the  roots  of [2.3.20]  lie outside  the  unit  circle.”  The  two 
statements  mean  exactly  the  same  thing.  Some  scholars  refer  simply  to  the  “roots 
of the  difference  equation  [2.3.1],”  though  this  raises  the  possibility  of confusion 
between  [2.3.19]  and  [2.3.20].  This  book  will  follow  the  convention  of using  the 
term  “eigenvalues”  to  refer  to the  roots  of [2.3.19].  Wherever  the  term  “roots”  is 
used,  we  will  indicate  explicitly  the  equation  whose  roots  are  being  described. 

From  here  on  in  this  section,  it is assumed  that  the  second-order  difference 
equation  is stable,  with  the  eigenvalues  A, and  A, distinct  and  both  inside  the  unit 
circle.  Where  this  is the  case,  the  inverses 

@  =  AL)"  ste  aA  AIL  4  « 
(l=  AzL)"'  =  1 +  AGL  +  AZL*  +  AZL®  +  --  - 

are  well  defined  for  bounded  sequences.  Write  [2.3.2]  in factored  form: 

(1  —  AL)  =  AZL)y,  =  w, 
and  operate  on  both  sides  by (1  —  A,L)~'(1  —  A,L)~!: 
y  =  (1  —  Agh)a8@  —  AgL)~'w,. 

32  Chapter  2  | Lag Operators 

[2.3.21] 

Following  Sargent  (1987,  p. 184),  when  A 1  #A we  can  use  the  following  operator: 

A,  — 

Ae!  {—L-  -  —2— 

A 

A 

| 

0. 

Notice  that  this  is simply another  way  of writing  the  operator  in (2.3.21]: 

9 
ty 

—  . y-yJAs(1  —  AsL)  —  A,(1  -  A,L) 
are  : (I =  A,L)  «(1  =  A,L) 

ants 

1 
~  (P=  AL)  +  (1 =  aL) 

Thus,  [2.3.21]  can  be written  as 

“ere 

<  fl  =~ ~ 

-— 

|  o ~ 

y.)71  al  Se  w 
Pak  1  AL)" 

as  WAL  aes ikl  eS 

os) 

tg  we. AN  po Ne 

= 

st  hia’  a iy  ash  | 

:  ra pe aarcie ie MLS die o. st} re4  cf ni (4 
) ae 

3 
| 
*  itt Powee: dy) erdeunsyts aA 4 stdeic  #i re § he a noHey; ro  Soateisthib  = 5) 

(a  Te  dws +  Leaks  + Gals + LAE + Calla  5 oy 

vr lia  asad Pres  ae  - 

4 

OW 160) bar 
St J sada Ueno 

t=; 
7 
y 

sivric  tins  EA 

c.  +1¢5  teas  sft  eH Die  wg  % 

is  odds  tect  gris wale: 24] 

abt fena2  oF nee ecirtey 

a & — 4  ay 

{0.#.4] 

-- 

As  in  the  second-order  system,  we  multiply  both  sides  of this  equation  by z~”  and 
define A =  z7!: 

| 

(AP  —  QyAP~!  —  GydP  8 

be  A  ee 

[2.4.4] 

=  (A  —  AMA  =  Ax)  ++ 

(A  =  A). 

Clearly,  setting  A  =  A, fori  =  1,2,...,  or  p causes  the  right  side  of  [2.4.4]  to 
equal  zero.  Thus  the  values  (A,,  A2,  . 
.  A,) must  be  the  numbers  that  set  the  left 
side  of expression  [2.4.4]  to  zero  as  well: 

. 

. 

AP  a  pA!  wz  2A” ~*  Fe 

te 

te  b,-\A  a  $,  =  0. 

[2.4.5] 

This  expression  again  is identical  to  that  given  in  Proposition  1.1,  which  charac- 
terized  the eigenvalues  (A,, Az, . 
,  A,) of the  matrix  F defined  in equation  [1.2.3]. 
Thus,  Proposition  2.1  readily  generalizes. 

. 

. 

Proposition  2.2:  Factoring  a pth-order  polynomial  in  the  lag operator, 
(1  =  $L  -  dL?  —  +++  -  o£”)  =  (1  -  ALI  ~  AgL)  ++ 

(1  =  A, L), 
is the same  calculation  as finding  the  eigenvalues  of the  matrix  F defined  in  {1.2.3}. 
, 
The  eigenvalues  (A,, A2,.  . 
A,,) in  [2.4.3]  and  are  given  by the  solutions  to  equation  [2.4.5]. 

,  A,) of F are  the same  as  the parameters  (A,, Ax...  . 

- 

The  difference  equation  [2.4.1]  is stable if the eigenvalues  (the roots  of [2.4.5]) 

lie  inside  the  unit  circle,  or  equivalently  if the  roots  of 

: 

by  Ofer  yz?  my  yetiye’ 

2 

[2.4.6] 

lie  outside  the  unit  circle. 

Assuming  that  the  eigenvalues  are  inside  the  unit  circle  and  that  we  are 
restricting  ourselves  to considering  bounded  sequences,  the  inverses  (1 —  A,L)~', 
(1  —  A,L)~',...,  (1  —  A,L)~'  all  exist,  permitting  the  difference  equation 

(l=  AyE}  —'A,£)  ***  (1  —  A, £)y,  =o 

to  be written  as 

Die  Cake  bem:  agdeje"  «« 

«hb  ~  Aga) tay: 

-yaGheea7) 

Provided  further  that  the  eigenvalues  (A,,  A>,  ..  . 
,  A,) are  all  distinct,  the poly- 
nomial  associated  with  the  operator  on  the  right  side  of [2.4.7]  can  again  be  ex-  ~ 
panded  with  partial  fractions: 
| 

1 
(1  —  Ayz)(1  —  Az)  ++  -  (1  =  A,z) 

- 

C 
(1  -A,z) 

+ 

C2 
(1  —  A,2z) 

were 

Cp 
ci:  =  Ag2z). 

{2.4.8] 

Following  Sargent  (1987,  pp.  192-93), the values  of (c,, C2,  . 
.  ,  ¢,) that  make  [2.4.8] 
true  can  be  found  by multiplying  both  sides  by (1  —  A,z)Ql  —  Agz)---  (1  -  A,2): 

. 

1 =  e,(1  -  Agz)(1  —  Agz)  «+ 

+  (1 =  Az) 

+  ,(1  —  Ayz)(1  —  Ayz)  +++  (1  =  A,z)  +  <=> 
+  ¢,(1  —  Ayz)(1  —  Apz)  ++ 

(1 +  A, 12). 

[2.4.9] 

uatio 

Equation  [2.4.9] has to hold  for all values  of z.  Since  it is a (p  - 

p — 1)th-order 
polynomial,  if (c,,  c.,...,  Cp) are  chosen  so  that  [2.4.9]  holds  for P particular 
34  Chapter 2  | Lag Operators 

, 

distinct  valuss of z,  then  [2.4. 9] must hold  for  all z.  To  ensure  that ite 4.9] holds 
at  z  =  A; '  requires  that 

1 =  ¢,(1  —  A,Ar')(1  —  AsAz*)*  ++  (1 —  A,Ar?) 

or 

= 

Oa  — 
For  [2.4.9]  to  hold  for  z  =  Ay',  Ay',...,  Ap' requires 

Gn  Ar  A) 

1 

A) 

G  So,  2.4.10 

Pee 

Ae™* 

ee Se |  RS  OES  gE 

Tsp  STAD  =  AS)  >  a =  Ay) 

Sad 

2.4.11 

Ki 

; 

+3 

‘: 

I 

bee  oat 

; 

a 

5 hearer 

a 

ela 
SS  OA  = ad  Op  =  Ae 
| 
:  —— that test are  identical to expression (1. 2: 25} in op end 1.  Recall from 
B 

UE  HP  CQyw 

Cc,  =  a 

thie discumsion  thierei(tiar'e)*  + by +  Cicit-c 
_*"To‘conclude, [2.4.7] can be written 

2.4.12 

sBf 7  3 

i 

. 

- 

P 

‘ 

= ee Ne  aids ies bs <} moni  has 

Rees  8 
Wty 
Ree tt  Bt are Srenr  “a 
Pa)  <a 
top eRe ij sara +s )w, + eg(1 + Agk + BL? SRE 

. 
==  Pe  $e efi +  LAL? — -w,  y  es 

=  ye 
pis. 

M3 

te 

sig  tae tS cba sto: aaley ye 1a 

.
|
Be  es one et  is  Lot ae = 

Eguesion 

tee + oly + ler +e 

00 ted 4 + fl  4A  e+  AZ  =  ? 

Jag: * Tes eri  tig wgeos 

3 

= 

A 

/ 

eae!  f..- an sit i a cap be Aicases  Betts 

ne 

SA 

‘ 

napetete Bed  Sed wo |

  ag  isktd yy 

operators  0 wi 

 
Notice  that  y; is  the  dynamic  multiplier  [2.4.14].  The  effect  of  w,  on  the  present 
value  of y  is given  by 

aw, 

ides 

=  2» By; 
j=0 

[2.4.18] 

Thinking  of.y(z)  as  a  polynomial  in  a  real  number  Z, 

(Zz)  =  Wy  +  Wz  +  wz?  +  Wz?  +e’, 
it appears  that  the  multiplier  [2.4.18]  is simply  this  polynomial  evaluated  at  z  =  B: 

0  > B’Y, 4; 

j=0 

Ow, 

=  ¥(B)  =  %  +  “HB  +  P?  +  p>  +---. 

[2.4.19] 

But  comparing  [2.4.17]  with  [2.4.7],  it is apparent  that 

W(L)  =  [1  —  AL)(L  —  A2L)  ++  >  A —  APL), 

and  from  [2.4.3]  this  means  that 

We  conclude  that 

W(L)  =  (1 -—  OL  -  bL?  - 

—  LI". 
| 
W(z)  =  [1 —  Giz  —  227  —  +++  —  bz? 

-  ++ 

for  any  value  of z,  so,  in  particular, 

y(B)  =  [1 —  4,8  —  $28?  —  ---  —  $B)’. 

[2.4.20] 

Substituting  [2.4.20]  into  [2.4.19]  reveals  that 

BYr+; 

dD 
a 77 iS Webepbdl  mia oc 

1 

> 

[2.4.21]  | 

reproducing  the  claim  in  Proposition  1.3.  Again,  the  long-run  multiplier  obtains 
as  the  special  case  of [2.4.21]  with  B =  1: 

im  [Beet yee yg  Bes] @  —__t  __ 

j-7~ 

OW,, 

OW, 44 

OW, 4; 

1S  GO OPA  90h  o> 

2.5.  Initial  Conditions  and  Unbounded  Sequences 

Section  1.2 analyzed  the  following  problem.  Given  a pth-order  difference  equation 

Ye  =  PiYr-1  +  PaYr-2  +++  +  OHN-p  +, 

[2.5.1] 

Pp initial  values  of y,  ' 

Y-19  Y-29---  >  Y-ps 
and  a sequence.  of values  for  the  input variable w, 

{Wo,  Wi,  sss  5  Webs 

36  Chapter  2  | Lag Operators 

[2.5.2] 

[2.5.3] 

we  sought  to  calculate  the  sequence  of values  for  the  output  variable  y: 

{Yo,  Yi»  ess  yb. 

Certainly  there  are  systems  where  the  question  is posed  in precisely  this  form.  We 
may  know  the equation  of motion  for  the  system  [2.5.1]  and  its current  state  [2.5.2] 
and  wish  to characterize  the  values  that  { yo,  y,,.  . 
,  y,} might  take  on  for different 
specifications  of {wo,  w,,  . 

,  w}. 

. 

. 

. 

However,  there  are  many  examples  in  economics  and  finance  in  which  a 
theory  specifies  just  the  equation  of  motion  [2.5.1]  and  a  sequence  of  driving 
variables  [2.5.3].  Clearly,  these  two  pieces  of information  alone  are  insufficient  to 
determine  the  sequence  { yo,  y,,  . 
,  y,}, and  some  additional  theory  beyond  that 
contained  in  the  difference  equation  [2.5.1]  is  needed  to  describe  fully  the  de- 
pendence  of y on  w.  These  additional  restrictions  can  be  of  interest  in  their  own 
right  and  also  help  give  some  insight  into  some  of the  technical  details  of manip- 
ulating difference  equations.  For  these  reasons,  this  section  discusses  in some  depth 
an  example  of the  role  of initial  conditions  and  their  implications  for  solving  dif- 
ference  equations. 

. 

. 

Let  P, denote  the  price of a stock  and  D, its dividend  payment.  If an  investor 
buys  the  stock  at  date  ¢ and  sells  it  at  ¢  +  1, the  investor  will  earn  a  yield  of 
D,/P, from  the dividend  and a yield of (P,,,  —  P,)/P, in capital gains.  The  investor’s 
total  return  (r,,,)  is thus 

hiie  Gis 

tl,  +  Os; 

A very  simple  model  of the  stock  market  posits  that  the  return  investors  earn  on 
stocks  is constant  across  time  periods: 

r>0. 

r=(P,,,-—P)/P,+  D/P, 

[2.5.4] 
Equation  [2.5.4]  may  seem  too simplistic  to  be  of much  practical  interest;  it 
assumes  among  other  things that investors  have  perfect foresight  about  future  stock 
prices  and  dividends.  However,  a slightly  more  realistic  model  in which  expected 
stock  returns  are  constant  involves  a  very  similar  set  of technical  issues.  The ad- 
vantage  of the  perfect-foresight  model  [2.5.4]  is that  it can  be  discussed  using  the 
tools  already  in hand  to gain some  further  insight  into  using lag operators  to  solve 
difference  equations. 

Multiply  [2.5.4]  by P, to  arrive  at 

FP, = Prey  7  P, + D, 

or 

[2.5.5] 
P,.,  =  (1  +  OP,  -  D,. 
Equation  [2.5.5] will  be recognized  as a first-order  difference  equation  of the form 
of [1.1.1]  with  y,  =  P,41,  @ =  (1  +  7), and  w,  =  —D,.  From  [1.1.7],  we  know 
that  [2.5.5]  implies  that 

P..,  =  (1 +  n'*'Po  —  (1 +  r)'Do  —  (1 +  nD,  -—  (1 +  r)'-?D, 

-)-  24+  ND,_{  =  D, 

[2.5.6] 

. 

. 

,  D,} and  the  value  of  P, were  given,  then  [2.5.6] 
If the  sequence  {Do,  D,,  . 
could  determine  the values  of {P,, P2,...  ,  P:+1}-  But  if only the  values  {Do,  D,, 
_..,  D} are  given,  then  equation  [2.5.6]  would  not  be  enough  to  pin down  {P,, 
»  P,41}.  There  are  an  infinite  number  of possible  sequences  {P,, P2,..., 
P,,.  . 
P,,;}  consistent  with  [2.5.5]  and  with  a  given  {Do,  D,,  ...,  Dj}.  This  infinite 
number  of possibilities  is indexed  by the  initial  value  Po. 

» 

2.5.  Initial  Conditions and  Unbounded  Sequences 

37 

>. i 

A  further  simplifying  assumption  helps  clarify  the  nature  of  these  different 

paths  for  {P;,  P2,... ; P,,,}.  Suppose  that  dividends  are  constant  over  time: 

Then  [2.5.6]  becomes 

D,  =  D 

for  all  f. 

P..,  =  1  +  vr)  *P,  —-  1G 

7)  +  (oF 

ty  sto  (rt IT 

=  (1  +  r)'t'P,  - 

ee  6 ae eo ade  D 

1-  (Ld 

=  (1  +  r)'*  [Po  —  (Dir)]  +  (Dir). 

[2.5.7] 

Consider  first  the  solution  in which  Py  =  D/r.  If the  initial  stock  price  should 

happen  to  take  this  value,  then  [2.5.7]  implies  that 

P, =  Dir 

[2.5.8] 

for all t. In this  solution,  dividends  are  constant  at  D and  the  stock  price  is constant 
at  D/r.  With  no  change  in  stock  prices,  investors  never  have  any  capital  gains  or 
losses,  and  their  return  is solely  the  dividend  yield  D/P  =  r.  In  a  world  with  no 
changes  in dividends  this seems  to be a sensible  expression  of the theory represented 
by [2.5.4].  Equation  [2.5.8]  is sometimes  described.as  the  ‘market  fundamentals” 
solution  to  [2.5.4]  for the  case  of constant  dividends. 

However,  even  with  constant  dividends,  equation  [2.5.8] is not  the only result 

consistent  with  [2.5.4].  Suppose  that  the  initial  price  exceeded  D/r: 

Py >  Dir. 

Investors  seem  to be valuing  the stock  beyond  the potential  of its constant  dividend 
stream.  From  [2.5.7]  this  could  be  consistent  with  the  asset  pricing  theory  [2.5.4]  _ 
provided  that  P, exceeds  D/r  by an  even  larger  amount.  As  long  as  investors  all 
believe  that prices will continue  to rise over  time,  each  will earn  the required  return 
r  from  the  realized  capital  gain  and  [2.5.4]  will  be  satisfied.  This scenario  has 
reminded  many  economists  of a  speculative  bubble  in stock  prices. 

If such  bubbles  are  to  be ruled  out,  additional  knowledge  about the process 
for {P,};_ .  is required  beyond that contained  in the theory of [2.5 .4]. For example, 
we  might  argue  that  finite  world  resources  put  an  upper  limit  on  feasible  stock 
prices,  as  in 

IP|1<P 

for all ¢. 

»  [2.5.9] 
Then  the only sequence  for {P,}_ _.. consistent  with  both  [2.5.4] and  [2.5.9]  would 
be the  market  fundamentals  solution  [2.5.8]. 

Let  us  now  relax  the  assumption  that  dividends  are  constant  and  replace  it 
with  the  assumption  that  {D}*__.  is  a  bounded  sequence.  What  path  for 
{Pi}. -  in [2.5.6]  is consistent  with  [2.5.9]  in this  case?  The  answer  can  be found 
by returning  to  the  difference  equation  [2.5.5].  We  arrived  at  the  form  [2.5.6]  by 
recursively substituting this equation backward.  That is, we used the fact that [2.5.5] 
held  for dates  t,t  ~1,t-—2,...,0  and recursively  substituted  to arrive  at [2.5.6] 
as  a  logical implication  of [2.5.5].  Equation  [2.5.5]  could  equally  well  be solved  — 
recursively forward.  To do so,  equation  [2.5.5]  is written  as 

= 

a 

| 

Po =  7 [Piet  +  Di. 

[2.5.10] 

38  Chapter 2 | Lag Operators 

An  analogous  equation  must  hold  for  date  ¢  +  1: 

P 

+1 

= 

l  + ale  T  eh 

[2.5.11] 

Substitute  [2.5.11]  into  [2.5.10]  to  deduce 

l 

] 

P,  " 

l+ril+t+r 

wwe]  ae  +  D,,;)  +  D, 

(2.5.12] 

1 

2 
Pao 

2 

] 

] 

; +  | ve  F +  | od  ag  F +  |p, 

Using  [2.5.10]  for  date  ¢  +  2, 

Prs2  > 

l+r 

[Piss  +  Disa], 

and  substituting  into  [2.5.12]  gives 
P,..  + | —— 
1 

P, =  | ——| 
1 

3 

3 

| 
Feel  1+  [a] on  +  [pon  +  [Ao], 

— 
1 

1 

2 

Continuing  in this  fashion  T periods into  the  future  produces 

T 

. 

: 

T-1 

P,.=  Few Peaz  +  Fee Diy 3 +  4]  D,47-2 

1 
P+  7 

1 
Lusor 

7 

‘1 
ey  of 

| 

[2.5.13] 

Mote  be Paty  >. 

If the sequence  {P}7_ _,  is to  satisfy  [2.5.9],  then 

1 

T 

ag wn gO  sap 

If {D,};— _~  is likewise  a  bounded  sequence,  then  the  following  limit  exists: 

T 

1 

j+l1 

oo 2, F +  | Disp 

Thus,  if {P,}7 _.  is to be a bounded  sequence,  then  we  can  take  the limit  of [2.5.13] 
as  T —  ©  to  conclude 

& 

j+l 

1 
Pye  2, F i  | Dis js 

| 
[2.5.14] 

which  is referred  to as the “‘market  fundamentals”  solution  of [2.5.5] for the general 
case  of time-varying  dividends.  Notice that  [2.5.14]  produces  [2.5.8]  as  a special 
case  when  D, = D  for  all t. 

Describing  the value of a variable  at time  ¢ as  a function  of future  realizations 
of  another  variable  as  in  [2.5.14]  may  seem  an  artifact  of  assuming  a  perfect- 
foresight  model  of stock  prices.  However,  an  analogous  set of operations  turns  out 
to  be  appropriate  in  a  system  similar  to  [2.5.4]  in  which  expected  returns  are 
constant.!  In such  systems  [2.5.14]  generalizes  to 

é 

1 

j+1 

P,=  2 E at | E,Di+j, 

jad 

See Sargent  (1987)  and  Whiteman  (1983) for  an  introduction  to  the me te ere of difference 

equations  involving  expectations. 

2.5.  Initial  ilies and  Unbounded  Sequences 

39 

where  E, denotes  an  expectation  of  an  unknown  future  quantity  based  on  infor- 
mation  available  to  investors  at  date  ¢. 

Expression  [2.5.14]  determines  the  particular  value  for  the  initial  price  Pp that 
is consistent  with  the  boundedness  condition  [2.5.9].  Setting  ¢  =  0 in  [2.5.14]  and 
substituting  into  [2.5.6]  produces 

@ 

at 

D 
Py  =(t.+  2  {[ Joo +  [te]  + FG]  2 

t+1 

=~ 

1 

y) 

1 
eee 

3 

ie ' 

+  {|  b+  ||  Di +1 

or 

+  -+-- 

= 

1  + 

(1  + r)'Do 

‘D 

t+1- 

1 
es 

t+2 

1 

—  (1  +  nD,  (hit.  AD,  —  =: 

(1.  +  OD,-;  —  D, 

] 

%  to ij ad Dae  reais ace 

ae 

2 

1 

3 

1 

Thus,  setting  the  initial  condition  Po to  satisfy  [2.5.14]  is sufficient  to  ensure  that 
it holds  for all t. Choosing  Py equal to any other value  would  cause  the consequences 
of each  period’s  dividends  to  accumulate  over  time  so  as  to  lead  to  a  violation  of 
[2.5.9]  eventually. 

: 

It  is  useful  to  discuss  these  same  calculations  from  the  perspective  of  lag 
operators.  In  Section  2.2  the  recursive  substitution  backward  that  led  from  [2.5.5] 
to  [2.5.6]  was  represented  by writing  [2.5.5]  in terms  of lag operators  as 

f1-—(1  + r)LIP,,,  =  —D, 

_(2.5.15] 

and  multiplying  both  sides  of [2.5.15]  by the  following  operator: 

(Bt 

©  (OR  A 

ee 

+  4: 

[2.5.16] 

If (1 +  r) were  less  than  unity,  it would  be natural  to consider  the  limit  of [2.5.16] 
as  [>  ©; 

f1-Q  +L)  '=1+  (+L  +  (1 +  Pl?  +-:-.-. 

In the  case  of the  theory  of stock  returns  discussed  here,  however,  r >  0 and 
this  operator  is  not  defined.  In  this  case,  a  lag operator  representation  can  be 
‘  sought  for the  recursive  substitution  forward  that  led  from  [2.5.5]  to [2.5.13].  This 

is accomplished  using  the  inverse  of the  lag operator, 

-1 

= 
L  W,  7°  Wrats 

which  extends  result  [2.1.4]  to  negative values  of k.  Note  that  L~-!  is indeed  the 
inverse  of the  operator  L: 

L“Lw,)  =  Lo'w,_,  =  Ww, 

In general, 

with  L° defined  as  the  identity  operator: 

L-aLh  a  L)-€, 

40  Chapter 2  | Lag Operators 

L°w, =  w,. 

Now  consider  multiplying  [2.5.15]  by 

(1+  (1+r)L7!  +  (1 +r)-2L-274+---4+  (1+  4  lathe  | 2.5.17] 

x  [-(1  +  vr)  LY 

to  obtain 

ee  te  2”  +  (Cf  +  r)-27L-2  +  -->  +  (1  +  fy  BE 
xi  Gf  +8)  LIP, , 
mit  +  (2  +  rk  +  01  +  Wotlk  8: 
i  Sh  ype Ln)  x  (1  +  P)~'D,,, 

or 

1-  (1 +  r)-"L-")P,,,  =  |-— 

[ 

( 

r) 

1 

5  t+1  F +  a *  F +  | D.+2 

2 

1 
as 

P 

7 

1 

3- 

1 

 & 

F *  | Di +3  + 

Pa 

+  ; ss | Dia7> 

which  is identical  to  [2.5.13]  with  ¢ in  [2.5.13]  replaced  with  ¢  +  1. 

When  r > 0 and  {P}*  _..  is  a bounded  sequence,  the  left side  of the  preceding 
equation  will  approach  P,,,  as  T becomes  large.  Thus,  when  r  >  0 and  {P,}* _. 
and  {D,}7 _.  are  bounded  sequences,  the  limit  of the  operator  in  [2.5.17]  exists 
and  could  be  viewed  as  the  inverse  of the  operator  on  the  left  side  of [2.5.15]: 

f1-(4+yL)'  =  -GQ  +  nL" 

Oe 

a)  es 

ie  eae 

ee ae es ia) ee  ee 

Applying  this limiting  operator  to  [2.5.15]  amounts  to  solving  the  difference  equa- 
tion  forward  as  in [2.5.14]  and  selecting  the  market  fundamentals  solution  among 
the set  of possible  time  paths for {P,}~  _.  given a particular  time  path for dividends 
{D}r- -  =. 

Thus,  given  a  first-order  difference  equation  of the  form 

(1 —  oL)y,  =  w,, 

[2.5.18] 

Sargent’s  (1987)  advice  was  to  solve  the  equation  “backward”  when  |¢| <  1 by 
multiplying  by 

fl  -  @L)-1  =  [1 +  6h  +  @L?  +  PL?  +---] 

[2.5.19] 

and  to  solve  the  equation  “forward”  when  |¢| >  1 by multiplying  by 

oe 
t=  61)? > 
=F  "Ae  iy 
Paws” 

=  —  pL NI + 6  +  b-*L-*  + 6-7?  +  --  ¢j 

[2.5.20] 

Defining  the  inverse  of  [1  —  @L]  in  this  way  amounts  to  selecting  an  operator 
[1 —  L]~!  with  the  properties  that 

[1 —  pL]-'  x  [1 —  GL]  =  1 

(the identity  operator) 

and  that,  when  it is applied  to  a bounded  sequence  {Ww} —  2, 

[1 —  $L)~'w,, 

the  result  is another  bounded  sequence. 

The  conclusion  from  this  discussion  is that  in  applying  an  operator  such  as 
[1 —  @L]~!,  we  are  implicitly  imposing  a boundedness  assumption  that  rules  out 

2.5.  Initial  Conditions  and  Unbounded  Sequences 

41 

phenomena  such  as  the  speculative  bubbles  of equation  [2.5.7]  a  priori.  Where 
that  is our  intention,  so  much  the  better,  though  we  should  not  apply the rules 
[2.5.19]  or  [2.5.20]  without  some  reflection  on  their  economic  content.  _ 

Chapter  2 References 

~ 

Sargent,  Thomas  J. 1987.  Macroeconomic  Theory, 2d ed. Boston:  Academic Press. 
Whiteman,  Charles  H.  1983.  Linear  Rational sepeceons Models: A User's Guide. ‘Min- 
neapolis:  University  of as ies oa Press. 

cit (Stine: = mete ea F. 
etek fee si | 

oz 

s

e

ne  a

12.5  IAl-  is  suthy lent.  te  érmure  that 

:  ber ae aronfetes) op isoitnebbeitghie ccs 
MS  tins 21.9) bee  <  wed Wlation  of 

 
Stationary 
ARMA  Processes 

This  chapter  introduces  univariate  ARMA  processes,  which  provide  a  very  useful 
class  of models  for describing  the dynamics  of an  individual  time  series.  The  chapter 
begins  with  definitions  of  some  of  the  key  concepts  used  in  time  series  analysis. 
Sections  3.2 through  3.5 then investigate  the properties  of various  ARMA  processes. 
Section  3.6  introduces  the  autocovariance-generating  function,  which  is useful  for 
analyzing  the  consequences  of combining  different  time  series  and  for  an  under- 
standing  of the  population  spectrum.  The  chapter  concludes  with  a  discussion  of 
invertibility  (Section  3.7),  which  can  be  important  for  selecting  the  ARMA  rep- 
resentation  of an  observed  time  series  that  is appropriate  given  the uses  to be made 
of the  model. 

3.1.  Expectations,  Stationarity,  and  Ergodicity 

Expectations  and  Stochastic  Processes 

Suppose  we  have  observed  a  sample  of size  T of some  random  variable  Y,: 
(3.1.1} 

{Y1, Yas  +++»  Yzt- 

For  example,  consider  a collection  of T independent  and  identically  distributed  __.. 
(i.i.d.)  variables  e,, 

; 

. 

{€,, > 

Er}, 

[3.1.2] 

with 

e, ~  N(0,  a). 

This  is referred  to  as  a  sample  of size  T from  a  Gaussian  white  noise  process. 

The  observed  sample  [3.1.1]  represents  T particular  numbers,  but  this  set  of 
T numbers  is only one  possible  outcome  of the  underlying  stochastic  process  that 
generated  the data.  Indeed,  even  if we  were  to imagine  having observed  the process 
for an  infinite  period  of time,  arriving  at the  sequence 

To  an rat {. aD  Yoo  Yas Yao-+-->  >  V7,  Vr+1»  YT+29*  «  }, 
the  infinite  sequence  {y,}7 _.  would  still  be viewed  as  a single  realization  from  a 
time  series  process.  For  example,  we  might  set  one  computer  to work  generating 
an  infinite  sequence  of i.i.d.  N(0, a”) variates,  {e{”}/_ _.,  and  a second  computer 
generating  a  separate  sequence,  {e(”}7__..  We  would  then  view  these  as  two 
independent  realizations  of  a Gaussian  white  noise  process. 

43 

Imagine  a  battery  of  /  such  computers  generating  sequences  {y=  _., 
{y2)}=  1...  ,  {yy}  _.,  and  consider  selecting  the  observation  associated  with 
date  ¢ from  each  sequence: 

Lyf,  yi,  ws 

2 

WIMP. 

I 

This  would  be  described  as  a  sample  of / realizations  of the  random  variable  Y,,. 
This  random  variable  has  some  density,  denoted  fy,(y,),  which  is called  the  un- 
conditional  density  of  Y,.  For  example,  for  the  Gaussian  white  noise  process,  this 
density  is given  by 

a 

—yi 

hyn  5  210 exp| =2| 

The  expectation  of the  tth  observation  of a  time  series  refers  to  the  mean  of 

this probability  distribution,  provided  it exists: 

E(Y,)  7  be YF y AY) dy,. 

[3.1.3] 

We  might  view  this.as  the  probability  limit  of the  ensemble  average: 

| 

I 

E(Y,)  =  plim  /D)  > Y®. 

[3.1.4] 

For  example,  if {Y,}*_ _.  represents  the  sum  of a constant  yu plus a Gaussian  white 
noise  process  {e,}*_ _., 

In>x 

i=1 

Y, =p  +t  &, 

then  its mean  is 

E(Y,)  =  uw  +  E(e,)  =  p. 

If Y, is a  time. trend  plus  Gaussian  white  noise, 

then  its mean  is 

Y, =  Bt +  «,, 

E(Y,)  =  pt. 

[3.1.5] 

[3.1.6] 

[3.1.7] 

[3.1.8] 

Sometimes  for  emphasis  the  expectation  E(Y,)  is called  the  unconditional 

__ 
mean  of Y,. The  unconditional  mean  is denoted  y,: 

E(Y,)  =  Py 
Note that this notation  allows the general possibility that the mean  can  be a function: 
of the  date  of the  observation  .  For  the  process  [3.1.7]  involving  the  time  trend, 
the mean  [3.1.8] is a function  of time, whereas for the constant  plus Gaussian  white 
noise,  the  mean  [3.1.6] is not  a function  of time. 
The  variance  of the random  variable  Y, (denoted  yo,) is similarly  defined  as 
Bas) 

O-  wr hood, 

weEY,-  w= 

44  Chapter 3  | Stationary  ARMA  Processes 

For  example,  for  the  process  [3.1.7],  the  variance  is 
o  =  E(Y,  —  Bt)’  =  E(e?)  = 

Autocovariance 

Given  a  particular  realization  such  as  {y{}*___  on  a time  series  process, 
consider  constructing  a  vector  x{!)  associated  with  date  ¢.  This  vector  consists  of 
the  [j +  1] most  recent  observations  on  y as  of date  ¢ for  that  realization: 

We  think  of  each  realization  {y,}7  _.  as  generating  one  particular  value  of  the 
vector  x, and  want  to  calculate  the  probability  distribution  of this  ag x! across 
realization i.  This  distribution  is called  the  joint  distribution  of  (Y,, 
Y,_,,..., 
Y,_;).  From  this distribution  we  can  calculate  the jth autocovariance  of " (denoted 
Yin): 

a  ee  es 

Cee  Ce 

Nobaiistet:tatcd  Maeat>  -  --  Yr  dV, dh  °°.  Gy.  3.1.10] 

: 

=  E(Y,  —  w)(¥.-;  — 

Be -))- 
Note  that  [3.1.10]  has  the  form  of a  covariance  between  two  variables  X and  Y: 
Cov(X,  Y)  =  E(X  —  wx(¥  —  py). 
Thus  [3.1.10]  could  be described  as  the  covariance  of Y, with  its own  lagged  value; 
hence,  the  term  ‘“‘autocovariance.”’  Notice  further  from  [3.1.10]  that  the  Oth  au- 
tocovariance  is just the  variance  of Y,, as  anticipated  by the notation  yo, in [3.1.9]. 
The autocovariance  y,,can  be viewed  as  the (1, j +  1) element  of the variance- 
covariance  matrix  of  the  vector  x,.  For  this  reason,  the  autocovariances  are  de- 
scribed  as  the  second  moments  of the  process  for  Y,. 

Again  it may  be  helpful  to  think  oi the jth autocovariance  as  the  probability  _ 

limit  of an  ensemble  average: 

Tg 

I 

bee C8)  z [Y?  —  mw):  (¥'2;  —  w-- 

[3.1.11] 

As  an  example  of calculating  autocovariances,  note  that  for  the  process  n 

[3.1.5]  the  autocovariances  are  all zero  for j #  0: 

ww  = EY,  —  w)(¥-)  —  w) =  Elec)  =  0 

for j #0. 

Stationarity 
If neither  the  mean  y, nor  the  autocovariances  7;, depend on  the date  f, then  © 

the process  for  Y, is said  to be covariance-stationary  or  weakly stationary: 

E(Y,)  = 
BLY,  =  uy: —p)=y, 

for  all t 
for  all  ¢ and  any j. 

2.4.  Expectations,  Stationarity,  and  Ergodicity 

45 

For  example,  the  process  in  [3.1.5]  is covariance-stationary: 

E(Y,)  =  » 

E(Y,  — u)(Y¥ 
(Fe  — MI 

Saee 

ee.  alae 
FB 
Altea  ea 

By contrast,  the  process  of  [3.1.7]  is not  covariance-stationary,  because  its  mean, 
Bt,  is a  function  of time. 

Notice  that  if a  process  is covariance-stationary,  the  covariance  between  Y, 
and  Y,_,  depends  only  on  j, the  length  of  time  separating  the  observations,  and 
not  on  ¢t,  the  date  of  the  observation.  It  follows  that  for  a  covariance-stationary 
process,  y; and  y_,;  would  represent  the  same  magnitude.  To  see  this,  recall  the 
definition 

[3.1.12] 
y,  =  EY,  —#e%iep  Ta)- 
If the  process  is  covariance-stationary,  then  this  magnitude  is  the  same  for  any 
value  of t we  might  have  chosen;  for  example,  we  can  replace  ¢ with  ¢  +  j: 

Ue  E(Y,4;  es BY e+j-j  bl)  F  E(Y +; =  NY,  =)  =  EO,  =  BM), 4;  —  p). 
But  referring  again to the definition  [3.1.12],  this last expression  is just the definition 
of y_;.  Thus,  for  any  covariance-stationary  process, 

¥,  =  y-; 

for  all  integers  /. 

[3.1.13] 

. 

- 

A different  concept  is that  of strict stationarity.  A process  is said  to  be strictly 
,  j,, the  joint  distribution  of  (Y,,  Y,,;,, 
stationary  if, for  any  values  of j,, j2,  . 
Y,+;,»-  --  »  Y,+;,)  depends  only  on  the  intervals  separating  the  dates  ly 
apis 
j,) and  not  on  the  date  itself  (#). Notice  that  if a  process  is strictly  stationary  with 
finite  second  moments,  then  it must  be covariance-stationary —if the  densities  over 
which  we  are  integrating  in  [3.1.3]  and  [3.1.10]  do  not  depend  on  time,  then  the 
moments  p,  and  y,, will  not  depend  on  time.  However,  it is possible  to  imagine  a 
process  that  is covariance-stationary  but  not  strictly  stationary;  the  mean  and  au- 
tocovariances  could  not  be functions  of time,  but  perhaps  higher  moments  such  as 
E(Y?)  are. 

In  this  text  the  term  ‘‘stationary”’  by itself  is  taken  to  mean  “‘covariance- 

stationary.” 

A process  {Y,} is said  to  be  Gaussian  if the  joint  density  | 

Ie  RanneMiag  hin Vi+jy  os:  49  Vr4j,) 

is Gaussian  for  any  j,, j2,  . 
, j,.  Since  the  mean  and  variance  are  all  that  are 
needed  to parameterize  a multivariate  Gaussian  distribution-completely,  a covariance- 
stationary  Gaussian  process  is strictly  stationary. 

. 

. 

| 

Ergodicity 

We  have  viewed  expectations  of a time  series  in terms  of ensemble  averages 
such  as  [3.1.4]  and  [3.1.11].  These  definitions  may  seem  a bit  contrived,  since 
usually  all one  has available  is a single realization  of size  T from  the process,  which 
we  earlier  denoted  {y(”, y$,  . 
, yP}.  From  these  observations  we  would  cal- 
culate the  sample  mean  Y. This,  of course,  is not  an  ensemble  average  but  rather 
a time  average: 

. 

. 

T 

y= (WT)  & ys”. 

[3.1.14] 

46  Chapter 3  | Stationary  ARMA  Processes 

Whether  time  averages  such  as  [3.1.14] eventually  converge  to the ensemble  concept 
E(Y,)  for a  stationary  process  has  to  do  with  ergodicity.  A  covariance-stationary 
process  is  said  to  be  ergodic  for  the  mean  if [3.1.14]  converges  in  probability  to 
E( Y,) as T’—  ~.'  A  process  will  be  ergodic  for  the  mean  provided  that  the  auto- 
covariance  ; goes  to  zero  sufficiently  quickly  as  j becomes  large.  In  Chapter  7 we 
will  see  that  if the  autocovariances  for  a  covariance-stationary  process  satisfy 

> lyl<@, 
F- 

[3.1.15] 

then  {Y,}  is ergodic  for  the  mean. 

Similarly,  a  covariance-stationary  process  is  said  to  be  ergodic  for  second 

moments  if 

(MTD)  My  -  >  Y 

P 

for  all j. Sufficient  conditions  for  second-moment  ergodicity  will  be  presented  in 
Chapter  7. In the  special  case  where  {Y,} is a stationary  Gaussian  process,  condition 
[3.1.15]  is sufficient  to  ensure  ergodicity  for  all  moments. 

For  many  applications,  stationarity  and  ergodicity  turn  out  to  amount  to  the 
same  requirements.  For  purposes  of  clarifying  the  concepts  of  stationarity  and 
ergodicity,  however,  it may  be  helpful  to  consider  an  example  of a process  that  is 
stationary  but  not  ergodic.  Suppose  the  mean  pw  for  the  ith  realization 
{yM}=  _.  is generated  from  a  N(0,  A’) distribution,  say 

YO =  p© + e,. 

[3.1.16] 

Here  {e,} is  a Gaussian  white  noise  process  with  mean  zero  and  variance  go?  that 
is independent  of uw“.  Notice  that 

Also, 

and 

w,  = E(u)  +  E(e,)  =  0. 

Yor  =  E(u?  +  &)?  =  d?  +  o° 

¥je  =  E(w  +  €)(u  +  €,_;))  = 

~— forj  #0. 

Thus  the  process  of [3.1.16]  is covariance-stationary.  It does  not  satisfy  the  sufficient 
condition  [3.1.15}  for ergodicity  for the mean,  however,  and indeed,  the time  average 

WT) SYP =  G/T) SW  +  2) =n  +  WT) D « 

converges  to 

rather  than  to  zero,  the  mean  of Y,. 

3.2.  White  Noise 
The basic building  block  for all the processes  considered  in this chapter is asequence  | 
{ce} _.  whose  elements  have  mean  zero  and  variance  0”, 

' 

E(¢)  =  0 

3.2.1] 
(3.2.2] 

and  for  which  the  e’s are uncorrelated  across  time: 

E(e?)  =  a’, 

1Often “ergodicity”  is used  in a more  general sense;  see  Anderson  and  Moore  (1979,  p. 319) or 

Hannan  (1970, pp. 201-20). 

3.2.  White Noise  47 

E(e,e,)  =  0 

for  t  #  TF. 

[3.2.3] 

A  process  satisfying  [3.2.1]  through  (3.2. 3] is  described  as  a  white  noise  process. 

We  shall  on  occasion  wish  to replace  [3.2.3] with  the  slightly  stronger  condition 

that  the  e«’s  are  independent  across  time: 

€,,  €,  independent  for  ¢  #  rT. 

[3.2.4] 

Notice  that  [3.2.4]  implies  [3.2.3]  but  [3.2.3]  does  not  imply  [3.2.4].  A  process 
satisfying  [3.2.1]  through  [3.2.4]  is called  an  independent  white  noise  process. 

Finally,  if [3.2.1]  through  [3.2.4]  hold  along  with 

e, ~  N(0,  0°), 

3.2.5] 

then  we  have  the  Gaussian  white  noise  process. 

3.3.  Moving  Average  Processes 

The  First-Order  Moving  Average  Process 

Let  {e,} be  white  noise  as  in [3.2.1]  through  [3.2.3],  and  consider  the  process 

Y, =  Hw  +  E,  +  6€,_1,; 

[3.3.1] 

where  yu and  6 could  be any  constants.  This  time  series  is called a first-order  moving 
average  process,  denoted  MA(1).  The  term  “‘moving  average”  comes  from  the  fact 
that  Y, is constructed  from  a  weighted  sum,  akin  to  an  average,  of  the  two  most 
recent  values  of e. 

The  expectation  of  Y, is given  by 

E(Y,)  =  E(u  +  ¢,  +  66,-,)="  + Ele)  +  OE(e,-;)  =n. 

[3.3.2] 

We  used  the  symbol  y for  the  constant  term  in [3.3.1]  in anticipation  of the  result 
that  this  constant  term  turns  out  to  be  the  mean  “ the  process. 

The  variance  of  Y, is 

E(Y,  =  p)? 

E(e, ay 
E(e?  +;  2086)...  +O  67.4). 
o.+  0 +  Se" 

=  (1  +  6)o?. 

» 

[3.3.3] 

The  first  autocovariance  is 

E(Y, — w)(%-1  -  uw) = Efe,  +  0e,_,)(€,-  1  +  6e,_>) 

=  E(ee,_,  +.0€7_,  +  0€,€,.  +  67€,_,€,-2) 
=  0+  007  +  0+  0. 

[3.3.4] 

Higher  autocovariances  are  all  zero: 
E(Y,  —  w)(Y,-;  —  w) =  E(e, +  0e,_,)(€,_;  +  6e,_;.,)=0 

[3.3.5] 
_  Since  the  mean  and  autocovariances  are  not  functions  of time,  an  MA(1)  process 
is gay  ge: regardless  of the value  of 6. Furthermore,  [3.1.15] is clearly 
Satisfie 

forj>i1. 

2 byl =  (1 +  0)0? +  [00% 

Thus,  if {¢,} is Gaussian  white  noise,  th 
all  moments. 

en  the  MA(1)  process  [3-3-1]  is ergodic  for 

(.-eeieeees 

ete 

48  Chapter 3 | Stationary-ARMA  Processes 

The jth autocorrelation  of a  covariance-stationary  process  (denoted  pj) is de- 

fined  as  its  /th  autocovariance  divided  by the  variance: 

2; =  ¥;/Yo. 
[3.3.6] 
Again  the  terminology  arises  from  the  fact  that  p,  is  the  correlation  between  Y, 
and  Y,_;: 

rr  en  5” 

— 

Var(Y,)  VVar(¥,-,) 
Since  p, is a  correlation,  |p,| =  1 for  all j, by the  Cauchy-Schwarz  inequality.  Notice 
also  that  the  Oth  autocorrelation  py  is equal  to  unity  for  any  covariance-stationary 
process  by definition. 

 VaVm 

From  [3.3.3]  and  [3.3.4],  the  first  autocorrelation  for  an  MA(1)  process  is 

given  by 

60? 

6 

pi  =  (+ &o? =  (+ 6) 

[3.3.7] 

Higher  autocorrelations  are  all  zero. 

The  autocorrelation  p; can  be plotted  as  a function  of j as  in Figure  3.1.  Panel 
(a) shows  the  autocorrelation  function  for  white  noise,  while  panel  (b) gives  the 
autocorrelation  function  for  the  MA(1)  process: 

es ees  +  0.8¢,_\. 

For  different  specifications  of  @ we  would  obtain  different  values  for  the  first 
autocorrelation  p,  in  [3.3.7].  Positive  values  of  @ induce  positive  autocorrelation 
in  the  series.  In  this  case,  an  unusually  large  value  of  Y, is likely  to  be  followed 
by a larger-than-average  value  for  Y,, ,,  just as  a smaller-than-average  Y, may  well 
be followed  by a smaller-than-average  Y,,,.  By contrast,  negative  values  of 6 imply 
negative  autocorrelation—a  large  Y, might  be  expected  to  be followed  by a  small 
value  for  Y,,,. 

: 

The  values  for p,  implied  by different  specifications  of @ are  plotted  in Figure 
3.2.  Notice  that  the  largest  possible  value  for  p,  is 0.5;  this  occurs  if  6 =  1.  The 
smallest  value  for p,  is  —0.5,  which  occurs  if  9  =  —1.  For  any value  of p,  between 
—0.5  and  0.5,  there  are  two  different  values  of  6 that  could  produce  that  auto- 
correlation.  This  is because  the  value  of 6/(1  +  67) is unchanged  if @ is replaced 
by 1/0: 

Re 

Se  ae 

For  example,  the  processes 

and 

Y, =  €,  + 0.Se,_, 

) 

Y, =  €, +  2€,-; 

would  have  the  same  autocorrelation  function: 

ee  ees 
a  (+2)  (1 +0.5%) 

0.4. 

We will  have  more  to  say  about  the  relation  between  two  MA(1)  processes  that 
share  the same  autocorrelation  function  in Section  3.7. 

3.3.  Moving Average Processes 

49 

b 

, 

Log (i 

¥ 

(a)  White  noise:  Y,  =  e, 

12 

| 

0 

1.2 

12 

t] 

‘ 

Loy 

9 

(b)  MA(1):  Y,  =  e,  +  0.8e,_, 

“1.2 

; 

A 

. 0 

4.2 

; 

< 

= 

(c)  MA(4):  Y, =  €,  —  0.6€,_,  +  0.3e,_, 
OSES, 

=O  Sexegu 

(d)  AR(1):  Y, =  0.8Y,_,  +  e, 

1.2 

“4.2 

10 
Log (i) 

(e)  AR(1):  Y,  =  —O.8Y,_,  +  «, 

FIGURE  3.1  Autocorrelation  functions  for  assorted  ARMA  processes. 

The  qth-Order  Moving  Average  Process 

A qth-order  moving  average  process,  denoted  MA(@q),  is characterized  by 
Y,=  m+  e,  +  Oe,_,  +  06,5  +  °°*  +  0,,—9; 
[3.3.8] 
where  {e,} satisfies  [3.2.1]  through  [3.2.3]  and  (,,  6, ...  ,  8,) could  be any  real 
‘numbers.  The  mean  of [3.3.8]  is again given  by p: 

E(Y,) “27  E(e,) “3 6,E(e,_  1) as 6,°E(€,_2)  mi"  6,°E(e,_  4) =  bM. 

The  variance  of an  MA(Gq)  process  is 

Yo =  E(Y, —  w)? =  E(e, + 0,e,_,  + 06,2 

4°  ++  + 6,6...) 

[3.3.9] 

50  Chapter 3  | Stationary  ARMA  Processes 

A 

ay 

FIGURE  3.2  The  first  autocorrelation  (o) for  an  MA(1)  process  possible  for 
different values’ —_ 0. 

git the e’s are uncorrelated, the variance 2B. 3.9] is? 
pe  eet +  60? +  302 +>  -+  Bo? = ON RBS  tS  am [3.3.10] 

sidgmmge  visiuloecs 

io |For j 

=  1,2,. lly  qs  ey  {5 pA h  ak 

PORN,  Page es  48  8 Byerly od 00: an ane 

te  23 INAMAVOCO 

MA  Bas  nBIG?  ort] 

ey BC 

x (,_; + A1e,—j-1 +  0,€,;2  + eo O,f-j-0) 
ee}  i. 
oe a ae chi Quester  11 rs 0,+2036? e?j-2 + pocutt +  0,0, yt ks 
Ter 

hs 
e's at different  dates have been devppad because their product 
Zero, and @, is defined to be unity.  For , j > q, there are no e’s with 
al definition  ¢ of mae ee ) the Sree iS Z zero.  Thus, 

A 

pay 5 hy 

J 

ve 

934 met +  6429 4.  -  + 7) 
; 

nat : Sy Sty ttl  forj>4. 

it o? 

for j =  i; 

+; 

" 

oe ~»q 

r pa es 

. 

670K  ~  ta  oan 

\ @  < fia 

; Ya 

we Ri 02 aeeee APs  e ane ss 

iii  te  creche acta 

- 

_ 

¥ 

~ 

n pas 

7 7” = 4 7 waa, 

i 

Dae a 

. 
' bed 
u 7 es 

yy fer i ta 
on 

with  65-=  1.  Consider  the  process  that  results  as  gq >  %: 

mele 

t. Ys Wie, -;  =  -  t  WoE,  +  WE,  T  Wre,_2  Fe  fot 

[3.3.13] 

j=0 

This  could  be  described  as  an  MA(&)  process.  To  preserve  notational  flexibility 
later,  we  will  use  w’s for the  coefficients  of an  infinite-order  moving  average  process 
and  6’s  for  the  coefficients  of a  finite-order  moving  average  process." 

Appendix  3.A  to  this  chapter  shows  that  the  infinite  sequence  in  [3.3.13] 

generates  a  well  defined  covariance-stationary  process  provided  that 

Sjcm 

y=o~ 

[3.3.14] 

It is often  convenient  to  work  with  a  slightly  stronger  condition  than  [3.3.14]: 

> Wh<e. 
fo 

[3.3.15] 

A  sequence  of numbers  {y}*>  satisfying  {3.3.14]  is said  to  be  square  summable, 
whereas  a  sequence  satisfying  [3.3.15]  is said  to  be absolutely  summable.  Absolute 
summability  implies  square-summability,  but  the  converse  does  not  hold—there 
are  examples  of  square-summable  sequences  that  are  not  absolutely  summable 
(again,  see  Appendix  3.A). 

4 

: 

The  mean  and  autocovariances  of  an  MA()  process  with  absolutely  sum- 
mable  coefficients  can  be calculated  from  a  simple  extrapolation  of the  results  for 
an  MA(q)  process:* 

: 

E(Y,)  =  <e E(u ee  PoE, tf Wie,-1  x  WrE,_2  +  | 

ae  Wre,- 1)  [3.3.16] 

—  MoM 

4 

Ye  SBE 

GY 

2) 

=  — E (Woe,  +  Wye,-1  +  Wye,-2  +  +++  +  Wre-7)? 
=  lim  (3 + YF + 
y =  E(Y,  —  pYi-)—  #) 

+--+  +  YR)? 

T= 

[3.3.17] 

[3.3.18] 

a  CL  a  WW  x  WY. 22  +  W433  + eae  *) 
Moreover,  an  MA(*)  process  with  absolutely  summable  coefficients  has absolutely 
summable  autocovariances: 

PH] <x, 

[3.3.19] 

Hence,  an  MA(=) process  satisfying  [3.3.15]  is ergodic  for the mean  (see Appendix 
3.A).  If the  e’s  are  Gaussian,  then  the  process  is ergodic  for all moments. 

*Absolute summability  of {¥,}7_., and existence  of the second  moment  E(«?) are sufficient  conditions 
to  permit  interchanging  the  order  of integration  and  summation.  Specifically,  if {X7}7.,  is a sequence 
of random  variables  such  that 

‘ 

; 

a  MAAS 

) E|X;|  <  », 
eee 

YONI 

we}  lSnoipey  meee xr} *  rs E(X;). 

See Rao (1973, p.“111); 

: 

52  Chapter 3  | Stationary  ARMA  Processes 

3.4.  Autoregressive  Processes 

The  First-Order  Autoregressive  Process 

A first-order  autoregression,  denoted  AR(1),  satisfies  the  following  difference 

equation: 

Y,=c+  Y,_,  +  €,. 

[3.4.1] 

Again,  {e,} is a  white  noise  sequence  satisfying  [3.2.1]  through  [3.2.3].  Notice  that 
[3.4.1]  takes  the  form  of  the  first-order  difference  equation  [1.1.1]  or  [2.2.1]  in 
which  the  input  variable  w,  is given  by w,  =  c  +  e,.  We  know  from  the  analysis 
of first-order  difference  equations  that  if |¢| =  1, the  consequences  of  the  e’s  for 
Y accumulate  rather  than  die  out  over  time.  It is thus  perhaps  not  surprising  that 
when  |¢| =  1, there  does  not  exist  a covariance-stationary  process  for  Y, with  finite 
variance  that  satisfies  [3.4.1].  In  the  case  when  |¢|  <  1,  there  is  a  covariance- 
stationary  process for  Y, pSAUSIARE [3.4.1].  It is given by the stable  solution  to [3.4.1] 
characterized in  [2.2.9 

Y, =  (c +  €,)  + O(c +  €,_;)  + O(c  +  €,_2)  + (C+  &,_3)  + °° 

[3.4.2] 

>  [c/(1  }  ¢)) +  €,+  P€,_,  +  $7 e,_2  ae,  AY:  Bh 

This  can  be  viewed  as  an  MA(*)  process  as  in [3.3.13]  with  y, given  by ¢/.  When. 
|p| <  1, condition  [3.3.15]  is satisfied: 

> ly,| <  = lol’, 
j=0 

J=0 

which  equals  1/41  —  ||) provided  that  |¢| <  1. The  remainder  of this  discussion. 
of first-order  autoregressive  processes  assumes  that  || <  1. This  ensures  that  the 
MA(~)  representation  exists  and  can  be  manipulated in  the  obvious  way,  and  that 
the  AR(1)  process  is ergodic  for  the  mean. 

Taking  expectations  of [3.4.2],  we  see  that 

E(Y,)  =[c(1  —  @)] +0+0+-:-:-, 

so  that  the  mean  of a stationary  AR(1)  process  is 
w=cK(1—  ¢). 

The  variance  is 

| 

=  E(Y,-  »)’ 
a  E(e, -  GE, i  GE, _2  +  G°e,-3  qo  ry 
=(1+  ¢+¢4+  $  +--+  +): 

7  o7/(1 ”  ¢’), 

while  the jth autocovariance  is 

=  E(Y, ~  w)(¥i-;- #) 
=  Efe, +  be,-,  +  ¢7€,-2  +  +  °°  +  He; +  Oe  -)-1 

+  p)**6,_ jah  +  +] X  [6,-7 +.  O6,=j-1  +  O,.5.9%  "+4 

=[¢/+  it?  soir * +. +  ‘0? 
=p/[1+  9+  ¢4+--:J:0” 
=(¢/(1  —  ¢)]-07. 

[3.4.3] 

[3.4.4] 

[3.4.5] 

3.4,  Autoregressive  Processes 

53 

It  follows  from  [3.4.4]  and  [3.4.5]  that  the  autocorrelation  function, 
p; =  ¥;'Yo  =  ¢’, 

[3.4.6] 

follows  a  pattern  of  geometric  decay  as  in  panel  (d)  of  Figure  3.1.  Indeed,  the 
autocorrelation  function  [3.4.6]  for  a  stationary  AR(1)  process  is identical  to  the 
dynamic  multiplier  or  impulse-response  function  [1.1.10];  the  effect  of a  one-unit 
increase  in  e,  on  Y,,, is equal  to  the  correlation  between  Y, and  Y,,,.  A  positive 
value  of  ¢,  like  a  positive  value  of  @ for  an  MA(1)  process,  implies  positive  cor- 
relation  between  Y, and  Y,,,.  A  negative  value  of  ¢ implies  negative  first-order 
but  positive  second-order  autocorrelation,  as  in  panel  (e) of Figure  3.1. 

Figure  3.3 shows  the effect  on  the appearance  of the  time  series  { y,} of varying 
the  parameter  ¢. The  panels  show  realizations  of the  process  in  [3.4.1]  with  c  =  0 
and  e,  ~  N(0,  1) for  different  values  of the  autoregressive  parameter  ¢.  Panel  (a) 
displays  white  noise  (@  =  0).  A  series  with  no  autocorrelation  looks  choppy  and 
patternless  to  the  eye;  the  value  of one  observation  gives  no  information  about  the 
value  of the  next  observation.  For  ¢ =  0.5  (panel  (b)),  the  series  seems  smoother, 
with  observations  above  or  below  the  mean  often  appearing  in clusters  of modest 
duration.  For  ¢ =  0.9  (panel  (c)),  departures  from  the  mean  can  be  quite  pro- 
longed;  strong  shocks  take  considerable  time  to  die  out. 

The  moments  for  a  stationary  AR(1)  were  derived  above  by viewing  it as an 
MA(®*)  process.  A second  way  to  arrive  at  the  same  results  is to  assume  that  the 
process  is covariance-stationary  and  calculate  the  moments  directly  from  the  dif- 
ference  equation  [3.4.1].  Taking  expectations  of both  sides  of [3.4.1], 

EUY,)  =¢  +  @E(Y,_,) 

Efe): 

[3.4.7] 

Assuming  that  the  process  is covariance-stationary, 

E(Y,)  =  E(Y,_;)  =  p. 

[3.4.8] 

Substituting  [3.4.8]  into  [3.4.7], 

wm=ct+  dGut+O0 

or 

wm  =  cil  —  9), 

[3.4.9] 

reproducing  the  earlier  result  [3.4.3]. 

yea 

Notice  that  formula  [3.4.9]  is clearly  not  generating  a  sensible  statement  if 
|¢| =  1. For  example,  if c  >  0 and  ¢ >  1, then  Y, in [3.4.1]  is equal  to  a positive 
constant  plus  a  positive  number  times  its  lagged  value  plus  a  mean-zero  random 
variable.  Yet  [3.4.9]  seems  to  assert  that  Y, would  be negative  on  average  for such 
a  process!  The  reason  that  formula  [3.4.9]  is not  valid  when  |d| =  1 is that  we 
assumed  in  [3.4.8]  that  Y, is covariance-stationary,  an  assumption  which  is not 
correct  when  |q] =>  1. 
, 
To  find  the  second  moments  of  Y, in  an  analogous  manner,  use  [3.4.3]  to 

| 

rewrite  [3.4.1]  as 

or 

Y, =  w(1  —  $)  +  $Y,_,  +  «, 

(Y,  —  w) =  O(Y,.1  —  w)  +  &. 
Now  square  both  sides  of [3.4. 10] and  take  expectations: 

E(Y,  ~  uP =  @E(Y,.,  —  w)?  +  2GE[(¥,-1  —  we)  +  E(e2). 

[3.4.10] 

[3.4.11] 

54  Chapter 3 | Stationary. ARMA  Processes 

(a)  @ =  0 (white noise) 

| tid} . 
hol  STs:  } 

er 7m cA teal rae = sora: eee 

‘iat mG  HK  ¢ ait? phate ne  the dm  09d tbh phir poe 

~ inka  anes etl (3.4.46} cress 

Recall  from  [3.4.2]  that  (Y,-,  —  #) is a  linear  function  of  €,;,  &-»,.-.: 
(Y,.,  —  #)  =  Gna  HiObae  at  Per-3  +  * 

But  «,  is  uncorrelated  with  e€,_,,  &-2,  -- 
(Y,_,  —  ).  Thus  the  middle  term  on  the  right  side  of [3.4.11] is  zero: 

+,»  SO  &  must  be  uncorrelated  with 

E((Y,-1;  —  “)e)  =  9. 

Again,  assuming  covariance-stationarity,  we  have 

E(Y,  —  »)?  =  E(Y,-1  —  »)*  =  w%- 

Substituting  [3.4.13]  and  [3.4.12]  into  [3.4.11], 

[3.4.12] 

[3.4.13] 

or 

=  ¢*%  +  0+  a’ 

Yo  =  7/1  -  ¢’), 

reproducing  [3.4.4]. 

Similarly,  we  could  multiply  [3.4.10]  by (Y,_;  —  ) and  take  expectations: 

EI,  ot  L)(Y,-;  =  )] 

=  @E((Y,-,  —  »)\(¥,-;  —  eH  +  EledY¥.-;  —  w)I- 

4 

[3.4.14] 

OS 
But  the  term  (Y,_;  — “#)  will  be  a  linear  function  Oh  tps  p53  Sp FB 
which,  for j >  0, will +S; uncorrelated  with  e,.  Thus,  for j >  @, the  last  term  on  the 
right  side  in  [3.4.14]  is zero.  Notice,  moreover,  that  the  expression  appearing  in 
the  first  term  on  the  right  side  of [3.4.14], 

is the  autocovariance  of observations  on  Y separated  by j —  1 periods: 

E((¥,-1  —  #)(%-;  —  »)I), 

é  | @ a  —  MY  1-1-1)  ony )]  =  VFj=1- 

Thus,  for j >  0, [3.4.14]  becomes 

|

Y%  =  OY-1- 
Equation  [3.4.15]  takes  the  form  of a first-order  difference  equation, 
Y,  =  bY-1  +  We 
in which  the  autocovariance  y takes  the  place  of the  variable  y and  in which  the 
subscript j (which indexes  the order of the autocovariance)  replaces ¢ (which indexes 
time).  The  input  w, in [3.4.15]  is identically  equal  to zero.  It is easy  to see  that  the  . 
difference  equation  [3.4.15]  has  the  solution 

[3.4.15] 

Yj  sae $’Yo; 
which  reproduces  [3.4.6].  We  now  see  why  the  impulse-response  function  and  — 
autocorrelation  function  for  an  AR(1)  process  coincide—they  both  represent  the 
solution  to a first-order  difference  equation  with  autoregressive  parameter  ibaa 
initial  value  of saan  ide and  no  subsequent  shocks. 

The  Second-Order  Autoregressive  Process 
A second-order  autoregression,  denoted  AR(2),. satisfies 

Y,=c  +  GY,_;  +  &Y,-2  +  &, 

[3.4.16] 

56  Chapter 3  | Stationary  ARMA  Processes 

 
or,  in lag operator  notation, 

The  difference  equation  [3.4.16]  is stable  provided  that  the  roots  of 

(1  ~  oL  —  LY,  =  ¢  +  «,. 

(1  —  oz  —  ¢,z?)  =  0 

[3.4.17] 

[3.4.18] 

lie Outside  the  unit  circle.  When  this  condition  is satisfied,  the  AR(2)  process  turns 
Out  to  be  covariance-stationary,  and  the  inverse  of the  autoregressive  operator  in 
[3.4.17]  is given  by 

WL)  =  (1  -  OL  -  &L’?)  =e  +  WLlt+  HL?  +  wWL+---.  [3.4.19] 

Recalling  [12.44],  the  vatue  of 
can  be  found  from  the  (1,  1) element  of  the 
matrix  F  raised  to  the  jth  power,  as  in  expression  [1.2.28].  Where  the  roots  of 
[3.4.18]  are  distinct,  a closed-form  expression  for y, is given by [1.2.29]  and  [1.2.25]. 
Exercise  3.3 at the end of this chapter discusses  alternative  algorithms  for calculating 

Multiplying  both  sides  of [3.4.17]  by y(L)  gives 

Y, =  W(L)c  +  W(L)e,. 

It is straightforward  to  show  that 

y(L)c  = (1  —  6,  —  ) 

and 

> lyjl <  ~; 

[3.4.20] 

[3.4.21] 

[3.4.22] 

the  reader  is invited  to  prove  these  claims in Exercises  3.4  and  3.5.  Since  [3.4.20] 
is an  absolutely  summable  MA()  process,  its mean  is given  by the  constant  term: 
[3.4.23] 

w=cl—d-¢).   - 

An alternative  method  for calculating  the mean  is to  assume that the process 

is covariance-stationary  and  take  expectations  of [3.4.16]  directly: 

: 

E(Y,)  =  c+  $,E(Y,_;)  +  $2E(Y,_2)  +  E(¢,),  . 

implying  | 

is 

: 
dp t+ dp + 0, 

w=ct+ 

) 

reproducing  [3.4.23]. 

To  find  second  moments,  write  [3.4.16]  as 

+,  el  —  &  -  $2)  +  $:Y,-1  +  $2Y 1-2  +  &, 

or 

(3.4.24) 
(Y,- w) =  O(¥i1  —  w+ OK  -  wt ee 
Multiplying both sides of [3.4.24]  by (Y,;  —  ») and taking expectations  produces - 

forj  =1,2,..... 

y =  Hy-1+  dey-2. 

[3.4.25] 
Thus,  the  autocovariances  follow  the  same  second-order  difference  equation  as 
‘does  the  process  for  Y,, with  the  difference  equation  for  y, indexed  by the  lag j. 
The  autocovariances  therefore.  behave  just  as  the  solutions  to  the  second-order 
difference  equation  analyzed  in Section  1.2.  An  AR(2)  process  is  covariance- 
stationary  provided  that  ¢, and  ¢, lie within  the  triangular  region of Figure  1.5. 

3.4. Autoregressive  Processes  87 

When  ¢,  and  ¢,  lie  within  the  triangular  region  but  above  the  parabola  in  that 
figure,  the  autocovariance  function  y,  is  the  sum  of  two  decaying  exponential 
functions  of 7.  When  ¢,  and  @,  fall  within  the  triangular  region  but  below  the 
parabola,  y; is  a damped  sinusoidal  function. 

The  autocorrelations  are  found  by dividing  both  sides  of  [3.4.25]  by yo: 

Oh.  iftic a iecicat 

Mia  alld,  24  te 

[3.4.26] 

In  particular,  setting  j) =  1 produces 

or 

For j =  2, 

Pi  =  $,  +  op, 

Pp,  =  $,/(1  —  ¢,). 

Pp.  =  O10,  +  dr. 

[3.4.27] 

[3.4.28] 

The  variance  of a  covariance-stationary  second-order  autoregression  can  be 
found  by multiplying  both  sides  of [3.4.24]  by (Y,  —  «) and  taking  expectations: 

E(Y,  —  uw)?  =  by E(Y,-1  —  w)(Y,  —  »)  +  Gy E(Y,-2  —  w)(Y,  —  ») 

or 

+ E(e,)(Y,  =  H), 

The  last  term  (a7)  in  [3.4.29]  comes  from  noticing  that 

Yo  =  hin  +  d2%2  +  07. 

{3.4.29] 

E(e,)(Y,  —  w)  = E(e,)[(Y,-1  —  »)  +  $(Y,2  —  wv)  +  €,] 

Equation  [3.4.29]  can  be written 

¢,:0  +  ,°0  ae co. 

Substituting  [3.4.27]  and  [3.4.28]  into  [3.4.30]  gives 

. 

Yo  =  PiPi¥o  +  $2P2¥  +  a”. 

[3.4.30] 

ae  Fee $263 

(1 —  ¢.) 

(1  —  ¢y)  +  $3 fr +  0° 

or 

3 
°" 

(1 —  ¢,)o? 

(+  &)G  —  &P  —  oF 

The pth-Order Autoregressive  Process 
A pth-order  autoregression,  denoted  AR(p),  satisfies 

Y,=¢  +  HYi-1  +  &Y,-2  +++  +  +  Yip  +  &. 

Provided  that  the  roots  of 

[3.4.31] 

1  —  oz  —  dz?  -::  ‘=  dz?  =  0 
[3.4.32] 
all  lie  outside  the  unit  circle,  it is nen  etch  to  verify  that  a  covariance- 
stationary  representation  of the form 

Y=  m+  wLye, 

[3.4.33] 

58  Chapter 3  | Stationary  ARMA  Processes 

exists  where 

WL)  =  (1  -  6 L -— oL?  —--+  —  ¢  L?)~! 

and  27, |Wj| <  ©.  Assuming  that  the  stationarity  condition  is satisfied,  one  way 
to  find the  mean  is to  take  expectations  of [3.4.31]: 

or 

=c+  oe  +  dp  t-->  +  dpm, 

w=  cl  —  $  —  do  ---*  —  @). 

Using  [3.4.34],  equation  [3.4.31]  can  be  written 

Y,-p=  $(Y,-1  —  pw)  t+  $2(Y,-2  te)  A  * 

[3.4.34] 

[3.4.35] 

+ bY, -p 

Mt) eA? 

. 

Fl 
taking expectations: — 

SE SO are  found by multiplying both  sides  of [3.4. 35] by ( yy -  “> and 

| 

4 

y,  =  da ee  are 2ts+ 
: 

+  by, 
On  +  doy. ++ °° +  sh  +o? 

forj  =  1,2,... 
for j =  0. 

[3-4-30) 

Using the fact that  y_;  =  ¥;,  the  system  of equations  in  [3.4.36]  for  j =  0,  1, 
bp. It 
for Yo, ¥:, - 
-  ++» p can be solved 
can  be shown‘ that the (p x 1) vector (Yo, Vix  ois  90% p—1)/  48 ee by the first p 

-  - » ¥p.as  functions of 0”, $, $2, .-. 

of the  first column  of the ( p? X  p2) matrix o*[I,>  —  (F  © F)]~! where 
(p x ana defined in equation ft .2.3] and © indicates pe  ee | 

spinner otis’ hel v-tolis at 4 
y  a  is “1 7 xeA% Nfs  “ogeid 

‘a 

YY 

wn 

Li 

. 

x 53 Briel p= stout ee 2  is 30K. 
ianc  se 

ie ora 

Pa 1 F 

: 

oe 

aris VASO nis’ ny Se ise se ononiva ‘Toast ait  ritiw  >  < 

of the cexstiany =  eM BA 

gace  fal aulouysices we: a 

lie  outside  the  unit  circle,  both  sides  of [3.5.2]  can  be divided  by (1  —  $,L  —  $,L? 
~  +++  —  ¢,L*)  to  obtain 

where 

Y,=pt  W(L)e, 

(1  +  0,L  +-@L?  +--+  +  6,L%) 
WL) Se ee  a  ea 
$,L?) 
(1 -  OL  ~ dL 

>, l¥;| <© 
j=0 

w=  ch(l  —  o  —  G2. —*°*  —  dp). 
Thus,  stationarity  of  an  ARMA  process  depends  entirely  on  the  autoregressive 
¢,) and  not  on  the  moving  average  parameters  (6,,  62, 
parameters.  (¢,,  $2,  . 

, 

. 

. 

Op). 

It is often  convenient  to write  the ARMA  process  [3.5.1] m terms  of deviations 

from  the  mean: 

| 

Y,—  #  =  $(Y,-1  —  2)  +  O(% 2 =<)  +. 

°° 

[3.5.4] 

+  $,(Y,-,  —  @)  +  &  +.0,6,-)  +  96,2  +  °°  >  +  O56,_q-. 

Autocovariances  are  found  by multiplying  both  sides  of [3.5.4]  by (Y,_;  —  m) and 
taking  expectations.  For j >  q,  the  resulting  equations  take  the  form 

¥=AWyj-1+  Wyj-2+°:°+¢,7-, 

forj=eqtl.qt  tS  FO 

Thus,  after  q lags the  autocovariance  function  y; (and the  autocorrelation  function 
p;)  follow  the  pth-order.  difference  equation  governed  by  the  autoregressive 
: 
parameters. 

| 

| 

| 
Note  that  (3.5.5]  does  not  hold  for j =  q, owing  to correlation  between  6, _; 
and  Y,_;.  Hence,  an  ARMA(p,  q) process  will  have  more  complicated  autocovar- 
iances  for  lags  1 through  q  than  would  the  corresponding  AR(p)  process.  For 
j >  q with  distinct  autoregressive  roots,  the  autocovariances  will  be  given  by 

¥ =  WA,  +  hyAL  +--+  +  hyd. 
“SB 
This  takes  the  same  form  as  the  autocovariances  for  an  AR(p)  process  [3.4.38], 
though  because  the  initial  conditions  (yo,  7,  ...  ,  Yq) differ  for  the  ARMA  and 
AR  processes,  the  parameters  h, in [3.5.6]  will  not  be the  same  as  the parameters 
8, in [3.4.38]. 

| 

There  is a potential  for redundant  parameterization  with  ARMA  processes. 

Consider,  for example,  a simple  white  noise  process, 

Y, = &. 

“ 

[3.5.7] 

“ 

 - 2-2 

Suppose  both  sides  of [3.5.7]  are  multiplied  by (1 - pL): 

| 
(es -pL)¥peQ  a  phjeies 
3.5.8} 
Clearly,  if [3.5.7]  is a  valid  representation,  then  so  is [3.5.8]  for  any  value  of p. 
Thus,  [3.5.8]  might  be  described  as  an  ARMA(1,  1) process,  with @,  =  p and 
6,.=  —p.  It is important to avoid  such  a  parameterization.  Since  any  value  of p 
_  in [3.5.8]  describes  the  data  equally  well,  we  will  obviously  get into  trouble  trying 
to estimate  the  parameter  p in [3.5.8]  by maximum  likelihood.  Moreover,  theo-  - 
retical  manipulations  based  on  a representation  such  as  [3.5.8]  may  overlook  key 
cancellations.  If we  are  using  an  ARMA(1,  1) model  in which  0, is Close  to —@,, 
then  the  data  might  better  be modeled  as  simple  white  noise. 
: 
60  Chapter 3  | Stationary  ARMA  Processes  - 

_ 

satiate  related overparameterization  can  arise  with  an  ARMA(p,  q) model.  Con- 
sider  factoring  the  lag polynomial  operators  in  {3.5.2}  as  in  [2.4.3]: 
(1  —  AL)  —  AL)  +++  (1  —  A,LYY,  -  p) 

=  (I~ mL)(1 ~  mL)--- (1 -  gba. P?” 
We  assume  that  |A,|  <  1 for  all  i, so  that  the  process  is covariance-stationary.  If 
the  autoregressive  operator  (1  —  ¢,L  —  o,L?  ~ 
—  ,L”)  and  the  moving 
average  operator  (1  +  0,1  +  6,17  +---  +  9,L7)  have  any  roots  in  common, 
” ay S n;  for  some  i and  j,  then  both  sides  of  {3.5.9]  can  be  divided  by 

+--+ 

= [1a - nt, 
[a-anm- 

k=1 
k#i 

k#j 

) 

; 

or 

(lh  —  OL  —  O23 Lio persion  Ae  hen  Xa) 

=  (+  OL  +  OFF?  +--+  OF Lt), 

[3.5.10] 

where 

(1 —  OfL  -  $31?  ---+  =  o3_\L?-') 

=  (1  3  A,L)(1  sas AL)  ~ 

Sieg  (1 ni  A;-,L)Q  Apel)  iter  (1 sit A,L) 

(1  +  OFL  +-O03L7  +--+  +  ey Fah) 

=  (1 —  mL)  =.  nhl):  > 

(1 -  1,L). 
The  stationary  ARMA(p,  q) process  satisfying  [3.5.2]  is clearly  identical  to  the 
stationary  ARMA(p  —  1, q  —  1) process  satisfying  [3.5.10]. 

(1.  -  yh)  -  al)  >: 

3.6.  The  Autocovariance-Generating  Function 

For  each  of the  covariance-stationary  processes  for Y, considered  so  far,  we  cal-. 
culated  the  sequence  of autocovariances  {y;}7_ _..  If this  sequence  is absolutely 
summable,  then  one  way  of summarizing  the  autocovariances  is through a scalar- 
valued  function  called  the autocovariance-generating  function: 

8y(z)  =  Dv 

[3.6.1] 

This  function  is constructed  by taking  the jth autocovariance  and multiplying  it by 
some  number z raised  to  the jth power,  and then  summing  over  all  the  possible 
values  of j. The  argument  of this  function  (z) is taken  to  be  a  complex  scalar. 

Of particular  interest  as  an  argument  for the  autocovariance-generating  func- 

tion  is any  value  of z  that  lies  on  the  complex  unit  circle, 
z  =  cos(w)  —  isin(w)  =  e~™, 
where  i =  V-1  and  w  is the  radian  angle  that  z  makes  with  the  real  axis.  If the 
autocovariance-generating  function  is evaluated  at  z  =  e~™  and  divided  by 27, 
the resulting  function  of w, 

a 

2 

ae  i 

=  jo 
ln gyle  ) ia  lar Pa  ; 

CD  —aple 

nb 

Sy(w) 

is called the population  spectrum  of Y. The  population  spectrum  will  be discussed 

| 

3.6.  The Autocovariance-Generating  Function 

61 

in  detail  in  Chapter  6.  There  it  will  be  shown  that  for  a  process  with  absolutely 
summable  autocovariances,  the  function  s;(w)  exists  and  can  be  used  to  calculate 
all  of  the  autocovariances.  This  means  that  if two  different  processes  share  the 
same  autocovariance-generating  function,  then  the  two  processes  exhibit  the  iden- 
tical  sequence  of  autocovariances. 

As  an  example  of calculating  an  autocovariance-generating  function,  consider 
the  MA(1)  process.  From  equations  [3.3.3]  to [3.3.5],  its autocovariance-generating 
function  is 

gy(z)  =  [00  1z7'  +  [C1  +  @)o?}z°  +  [007]z1=  o?-[6z-1  +  (1  +  &)  +  62). 

Notice  that  this  expression  could  alternatively  be  written 

gy(z)  =  o°(1  +  6z)(1  +  Oz7'). 

[3.6.2] 

The  form  of expression  [3.6.2]  suggests  that  for  the  MA(q)  process, 

Vere:  t:  (lit  Od  At  Od  bocce:  3+  0 L%  6 

the  autocovariance-generating  function  might  be  calculated  as 

Bye)  —  oe  &  Oz  st  2  +  + 

+  6,29) 

[3.6.3] 

Ot  Oz  Oz  2 

-*  +  8  Z 

This  conjecture  can  be  verified  by carrying  out  the  multiplication  in  [3.6.3]  and 
collecting  terms  by powers  of z: 

ClO  2 +  0, 

2542 

=  4  67x  Oia  + O27  +  4-8  G29 

=  (0,)z7  +  (6,_,  +  0,0,)z4-  +  (0,_2 +  0,_ 10, +  0,0,)z9-» 

+++  ++  (6, +  0,0,  +  0,0,  +--+  ++  6,0,_,)z} 
tbh  pt OF F054  =  s502) 2° 
+°(0;  +°0,0,  +  0:6,  +  = 

> 

>  +0,0)_,)27)  +++  *+(6,)z-4%. 

[3.6.4] 

Comparison  of [3.6.4]  with  [3.3.10]  or  [3.3.12]  confirms  that  the  coefficient  on  z/ 
in [3.6.3]  is indeed  the jth autocovariance. 

This  method  for  finding  gy(z)  extends  to  the  MA(~)  case.  If 

with 

and 

then 

Y,=  e+  WLe, 

WL)  =  H+  WbL+yl?  +--- 

> lvl <  %, 

j=0 

For  example,  the  stationary  AR(1) process  can  be written  as 

8y(Z)  =  o7y(z)y(z~'). 

[3.6.5] 

[3.6.6] 

[3.6.7] 

[3.6.8] 

Y,-—  w=  (1 -  $L)~'e,, 
which  is in the  form  of [3.6.5] With Y(L)  =  1(\  —  #L).  The  autocovariance- 
generating  function  for an  AR(1)  process  could  therefore  be calculated  from 

| 

by 

8y(z)  = 

(1  =  $z)(1  —  ¢z7!) 

[3.6.9] 

62  Chapter 3  | Stationary  ARMA  Processes 

To verify  this claim  directly,  expand  out  the  terms  in [3.6.9]: 

a pinbtidleing ati 1 
(1 —  $z)(1  —  z7') 

=  (1  +  oz  +  P22  +  P22  +-->) 

x  (1  +  oz~!  -  ¢*z~?  re ¢°z~3  pe  tos ‘), 

from  which  the  coefficient  on  z/ is 

| 

. 

a7(p/  +  p/*'h  +  p/*2g?  +  +--+)  =  a h/K(1  —  ¢’). 

This  indeed  yields  the jth autocovariance  as  earlier  calculated  in equation  [3.4.5]. 
The  autocovariance-generating  function  for a stationary  ARMA(p,  q) process 

can  be written 

orn  (1 +  Oz +  Paz?  +  a>  + Ogz9(1  +  O27!  +  0.2727  +++  +  +8279 
oe 

OL  Oe  EOE) 

Oe  Gat? 

| 

[3.6.10] 

Sometimes  the data  are  filtered,  or treated  in a  particular way before they 
are 
and we  would  like to summarize  the  effects  of this treatment on the 
-autocovariances. This calculation is particularly simple using  the autocovariance- 
generating function.  For example, suppose that the original data Y, were generated 
from an MA(1) gan 

| 

Y= (1+ OL), 

B64 
variance ecracie function given by [3.6.2] ~w  data 
the previous 
zed,  m represent  the . in Y, over its value 

, 

' 

wsott  | tF AM  ok  3Ot eiisinevnd 

Ke = Y=  Y,-1.=  (12220909 Y(L}AM  ae 2 Sea 

MAG) poses, oe a uaa Sank ound 

(1  —  L)  to  Y, thus  results  in  multiplying  its  autocovariance-generating  function  by 
(1. =  z)(k 

This  principle  readily  generalizes.  Suppose  that  the  original  data  series  {Y,} 

=  27"). 

‘G. 

3 

satisfies  [3.6.5]  through  [3.6.7].  Let’s  say  the  data  are  filtered  according  to 

X,  =  h(L)Y, 

{3.6.16} 

with 

h(L)  =  = h, Li 

J/=-= 

Di<= 

Substituting  [3.6.5]  into  [3.6.16],  the  observed  data  X, are  then  generated  by 

X,  =  A(i)jw  +  A(L)p(L)e,  =  w*  +  P*(L)e,, 

where  u* =  h(1)y  and  #*(L) = pie WAL.) The  sequence  of coefficients  associated 
with  the  compound  operator {*}*  _.  turns  out  to  be  absolutely  summable,*  and 
the  autocovariance-generating  function  of X, can  accordingly  be  calculated  as 

8x(z) = o2y*(z)y*(z-})  =  o7h(z)w(z)W(2~")h(z")  =  h(z)h(z~*)gy(z). 
[3.6.17] 
Applying  the  filter h(L)  to  a  series  thus  results in  multiplying  its  autocovariance- 
generating  function  by A(z)h(z~'). 

3.7.  Invertibility 

Invertibility for the  MA(1)  Process 

Consider  an  MA(1)  process, 

with 

Y,-  w=  (1 +  OL)e,, 

{3.7.1] 

| 
fort=r 
E 
(eve,  {e  otherwise. 

o 

= 

*Specifically, 

we)  =  (3 Az)(3, wz!) 

S08  +  Wit  +  Wit  NS  Sai  fee  Was  Poets 

+  yt!  +  hy  Zi?"  +  ++  \(doz®  +  Wz!  +  yz?  +  +>), 

from  which  the-coefficient  on  z/ is 

y;  = 

May  ?  hy.  Wy  +  h, 3b,  -  ll  tila  > h,. Wy. 

ve=0 

Then 

Zwi-  S 

Pp h,_  Wy, 

je 

ve-0 

= ZZ hwl=  Dil ZW  t=  Dl Stale. 

64  ‘Chapter 3  | Stationary  ARMA  Processes 

Provided  that  |6| <  1,  both  sides  of  [3.7.1]  can  be  multiplied  by (1  +  @L)~'  to 
obtain® 

(3.7.2] 
(1  —  OL  +  @L?  -  @L?  +  ---)(Y,  —  p)  =e, 
which could  be viewed  as  an  AR()  representation.  If  a  moving  average  repre- 
sentation  such  as  [3.7.1] can  be rewritten  as  an  AR(~)  representation  such  as  [3.7.2] 
simply  by inverting  the moving  average  operator  (1  +  OL),  then  the  moving  average 
representation  is said  to  be  invertible.  For  an  MA(1)  process,  invertibility  requires 
|6| <  1; if || =  1, then  the  infinite  sequence  in  [3.7.2]  would  not  be  well  defined. 
Let  us  investigate  what  invertibility  means  in  terms  of  the  first  and  second 
moments  of  the  process.  Recall  that  the  MA(1)  process  [3.7.1]  has  mean  p  and 
autocovariance-generating  function 

8y(z)  =  o7(1  +  Oz)(1  +  Oz~'). 

Now  consider  a  seemingly  different  MA(1)  process, 
Y,-—  w  =  (1  +  OL)é,, 

[3.7.3] 

[3.7.4] 

with 

~?2 

E(éé.)  =  a  for t  F 
otherwise. 

0 

a 

Note  that  Y, has  the  same  mean  (y) as  Y,.  Its  autocovariance-generating  function 
is 

gz)  =  o(1  +  6z)(1  +  6z~') 

=  67{(0~'z~!  +  1)(6z)}  {((-'z  +  1)(6z~!)} 
=  (676)(1  +  O-4z)(1  +  O-3z~3). 

[3.7.5] 

Suppose  that  the  parameters  of [3.7.4],  (6, @?),  are  related  to  those  of [3.7.1]  by 
| 
the  following  equations: 

ee  sie 

=  Fe. 

[3.7.6] 

[3.7.7] 

Then  the  autocovariance-generating  functions  [3.7.3]  and  [3.7.5]  would  be  the 
‘same,  meaning  that  Y, and  Y, would  have  identical  first  and  second  moments. 

Notice  from  [3.7.6]  that  if |6] <  1, then  |6| >  1.  In  other  words,  for  any 
invertible  MA(1)  representation  [3.7.1],  we  have  found  a  noninvertible  MA(1) 
_  representation  [3.7.4]  with  the  same  first  and  second  moments  as  the  invertible 
representation.  Conversely,  given  any  noninvertible  representation  with  |6| >  1, 
there  exists  an  invertible  representation  with  @ =  (1/6) that  has  the  same  first  and 
second  moments  as  the  noninvertible  representation.  In the  borderline  case  where 
=  +1,  there  is only one  representation of the  process,  and  it is noninvertible. 
Not only do the  invertible  and  noninvertible  representations  share  the  same 
moments,  either  representation  [3.7.1] or  [3.7.4]  could be used  as  an  equally. valid 
description  of any given MA(1)  process!  Suppose  a computer  generated  an  infinite 
sequence  of Y’s according  to  [3.7.4]  with  6 >  1. Thus  we  know  for a fact that  the. 
data were  generated  from  an  MA(1)  process  expressed  in terms  of a noninvertible 
representation.  In what sense  could these same  data be associated  with an  invertible 
MA(1)  representation? 

*Note from [2.2.8] that 

(1 +  OL)-'  =  [1 — (-9)L] ' =  1 +  (—O)L  +  (-O"L?  +  (-OPL9  +  >=. 

3.7.  Invertibility 

65 

Imagine  calculating  a  series  {e,}"  _.  defined  by 

e,=(1  +  OL)-“(Y,  - #) 

=  (Y, —  w)  —  (Y,-;  —  uw) +  O(Y,-2  -  4) -—  OM  -3-  aw) +:: 

3.78] 

where  0  =  (1/6)  is  the  moving  average  parameter  associated  with  the  invertible 
MA(1)  representation  that  shares  the  same  moments  as  [3.7.4].  Note  that  since 
|6| <  1, this  produces  a  well-defined,  mean  square  convergent  series  {e,}. 

Furthermore,  the  sequence  {e,} so  generated  is white  noise.  The  simplest  way 
to verify this is to calculate  the  autocovariance-generating  function  of e, and confirm 
that  the  coefficient  on  z/ (the jth  autocovariance)  is equal  to  zero  for  any  j #  0. 
From  [3.7.8]  and  [3.6.17],  the  autocovariance-generating  function  for  e,  is  given 
by 

g(z)  =  (1  +  6z)~(1  +  6z~')~'gy(z). 

[3.7.9] 

Substituting  [3.7.5]  into  [3.7.9], 

g.(z)  =  (1  +  @z)~“"(1  +  6z~')-(676)(1  +  6-1z)(1  +  O-'z7) 

[3.7.10] 

=  662, 

where  the last equality  follows  from  the fact that 6~'  =  @, Since  the autocovariance- 
generating  function  is a  constant,  it follows  that  «, is a  white  noise  process  with 
variance  676. 

: 

Multiplying  both  sides  of [3.7.8]  by (1  +  @L), 

Y,-—  pp  =(1  +  OL)e, 

is  a  perfectly  valid  invertible  MA(1)  representation  of  data  that  were  actually 
generated  from  the  noninvertible  representation  [3.7.4]. 

The  converse  proposition  is  also  true—suppose  that  the  data  were  really 
generated  from  [3.7.1]  with  |@| <  1, an  invertible  representation.  Then  there  exists 
a  noninvertible  representation  with  6 =  1/6  that  describes  these  data  with  equal 
validity.  To  characterize  this  noninvertible  representation,  consider  the  operator 
proposed  in [2.5.20]  as  the  appropriate  inverse  of (1 +  61): 

(6)"*L-[1  =  (6-')L-!  +  (6-)L~?  -  (6-9)L-3  +--+] 

0  ee  ee  ek,  eS  Ye 

Define  é, to  be the  series  that  results  from  applying  this  operator  to  (Y,  —  2), 

E, =  AY 41 —  #)  —  O (Yin.  —  ow)  +  O(Y43  —-  Mh  et  4 

[3.7.11] 

noting  that  this  series  converges  for  |6| <  1. Again  this  series  is white  noise: 

gz)  =  {6z~"[1  —  Oz~'  +  62-2  —  6z-3.  +--+  -}} 

| 

x  {z[1  —  6z'  +  6?z?  —  62>  +  «+  -}}o2(1  +  Oz)(1  +  Oz~') 

=  @o7. 

The coefficient  on  z/ is zero  for  j # 0, so  é, is white  noise  as claimed.  Furthermore, 
by construction, 

Y, —  p  =  (1 +  6L)é,, 
so  that  we  have  found  a  noninvertible  MA(1)  representation  of data  that  were 
actually  generated  by the  invertible  MA(1)  representation  [3.7.1]. 

Either the  invertible  or  the  noninvertible  representation.  could  characterize 
any  given  data  equally  well,  though  there  is a practical  reason  for preferring  the 

66  Chapter 3  | Stationary  ARMA  Processes 

invertible  representation.  To  find  the  value  of  e  for  date  ¢  associated  with  the 
invertible  representation  as  in [3.7.8],  we  need  to  know  current  and  past  values  of 
Y.  By contrast,  to  find  the  value  of é for  date  ¢ associated  with  the  noninvertible 
representation  as  in  [3.7.11],  we  need  to  use  all  of  the  future  values  of  Y!  If the 
intention  is  to  calculate  the  current  value  of  ¢,  using  real-world  data,  it  will  be 
feasible  only  to  work  with  the  invertible  representation.  Also,  as  will  be  noted  in 
Chapters 4 and  5, some  convenient  algorithms  for  estimating  parameters  and  fore- 
casting  are  valid  only  if the  invertible  representation  is  used. 

The  value  of  e,  associated  with  the  invertible  representation  is  sometimes 
called  the  fundamental  innovation  for  Y,.  For  the  borderline  case  when  \a|  =  1, 
the  process  is  noninvertible,  but  the  innovation  «,  for  such  a  process  wil  still  be 
described  as  the  fundamental  innovation  for  Y,. 

Invertibility  for the  MA(q)  Process 

Consider  now  the  MA(q)  process, 

{Y,  —  w)  =  (1+  OL  +  6,1?  +--+  +  0,L%e, 

[3.7.12] 

E 

fort  =.7 
(ee)  {°  otherwise. 

o? 

= 

Provided  that  the  roots of 

(1 +  Oz  +  0.27  +2)  -  +  0.29}  =  0 

[3.7.13] 

lie outside  the  unit  circle,  [3.7.12]  can  be written  as  an  AR(~)  simply  by inverting 
the  MA  operator, 

(1  +  mL  +  mL?  +  Ll?  +  ---)(Y,  —  w)  =  &, 

where 

a +  mL  +  ml?  +  mL?  +--+)  =  (1+  OL  +  OL?  +--+  +  6,L9)-'. 

Where  this.is  the  case,  the MA(q)  representation  [3.7.12]  is invertible. 

Factor  the  moving  average  operator  as 

. 

(1+ OL +  6,1? +--+  +0,L%  =(1-A,L)(1—A,L)*--(1-A,L). 

[3.7.14] 

If |A,| <  1 for all i, then  the  roots  of [3.7.13]  are  all outside  the  unit  circle  and  the 
representation  [3.7.12] is invertible.  If instead  some  of the A; are  outside  (but not 
on)  the  unit  circle,  Hansen  and  Sargent  (1981,  p.  102)  suggested  the  following 
procedure  for finding  an  invertible  representation.  The  autocovariance-generating 
function  of Y, can  be written 

Br(2)  =  0?  (1 = Ae  =  Az)  A 

x  {1  Ae  Or  Aa)  oe  Paz Oy. 

ag 
my 

the A’s so that  (A,, A>, . 

.  ,  A,) are  inside  the  unit circle  and  (A,,,1,  An42, 
Order 
...,  Ag)  are outside  the  unit  circle.  Suppose  o?  in [3.7.15]  is replaced  by o?: 
AZ,  A2,.°  «  -A2; since  complex  A, appear  as  conjugate  pairs,  this  is a  positive  — 
real.number.  Suppose  further  that  (A, 41,  An+2)  - 

»  »  Ag) are replaced  with  their 

. 

- 

3.7.  Invertibility 

67 

reciprocals,  (A, Ls  bz isialy  95 

~!).  The  resulting  function  would  be 
e 

0  -  Bile  aiff (t=  way TL, (limay  ‘a 

a 

{UI (1  —  Ayz~ oH i (ben  aria} 

ffja—so}f fev 9 

n+1 

n+l 

x  tf (1  —  A,;z7 »| 

{ ees 7  wey 

Il  Q N 

—% z. — 

| 

> N 

— 

—— 

ier ak 

Se) 

“4  fl (1  -  wo} fa =  ae}, 

which  is identical  to  [3.7.15]. 

The  implication  is as  follows.  Suppose  a  noninvertible  representation  for  an 

MA(q)  process  is written  in the  form 

Y,=prt +i] (1  —  A,L)é,, 

q 

| 

[3.7.16] 

where 

and 

|A;|  <  1 
ceah 

fort  =  tee) tam 
nopfondiscrtotrdneth 
2ys 8h.5°¢ 

6 

fort= 
P 

E(é,é,)  = 

(2,)  {°  otherwise. 

Then  the  invertible  representation  is given  by 

Y,=pt  if (1  -  wo} tl he -  At), 

(3.7.17] 

= 

where 

E(e,e,) =  4 Ma  Azgoe  AR. 

fort  = 

otherwise. 

Then  [3.7.16]  and  [3.7.17]  have  the  identical  autocovariance-generating  function, 
though  only [3.7.17]  satisfies  the  invertibility  condition. 

From  the  structure  of the  preceding  argument,  it is clear  that  there  are  a 
number  of alternative  MA(q)  el svi  ar ets. of the data  Y, associated  with all the 
possible “flips”  between  A, and A;-'. Only one  of these  has all of the A; on or inside 
the  unit  circle.  The  innovations  associated  with  this = needa are  said  to be 
the fundamental  innovations  for  Y,. 

68  Chapter 3 | Stationary  ARMA  Processes 

APPENDIX  3.A.°  Convergence  Results for Infinite-Order  Moving 
Average  Processes 

This  appendix  proves  the  statements  made  in  the  text  about  convergence  for  the  MA(=) 
process  (3.3.13). 

First we  show  that  absolute  summability  of the moving average  coefficients  implies  square- 
summability.  Suppose  that {y,}7_.  is  absolutely  summable.  Then  there  exists  an  N <  »  such 
that  |y,| <  1 for  all  j =  N,  implying  v7? <  |y| for  all j => N.  Then 

= 

N-1 

>  v=  2, Wr  Dw<  d w?  +  > |v): 

= 

N-1 

= 

j=0 

But  =;,'  y; is finite,  since N is finite,  and  >7~ ~  |¥;| is finite,  since  {W} is absolutely  summable. 
Hence  27» #7 <  ~™,  establishing  that  [3.3.15]  implies  [3.3.14]. 

Next  we  show  that  square-summability  does  not  imply  absolute  summability.  For  an 
example  of a series  that  is square-summable  but  not  absolutely  summable,  consider  yj  = 
1/j for j =  1, 2,....  Notice  that  1/j >  1/x  for  all x  >  j, meaning  that 

puhh 

tt 
1/j S  | (1/x)  dx 
; 

/ 

and  so 

N+ 

N 
> 1/j >  | (1/x)  dx  =  log(N  +  1) —  log(1)  =  log(N  +  1), 
j=) 

which  diverges  to  ©  as  N —  ~,  Hence  {y};-,  is not  absolutely  summable.  It is,  however, 
-Square-summable,  since  1/j?  <  1/x?  for  all x  < j, meaning  that 

. 

fi 

l/j?#<  | (1/x?)  dx 

AL 

and so 

N 
by 1j?><1+ 
j=l 

N 

(1/x?)  dx  =  1+  (-1/x)|*.,  =  2  -  (1/N), 

; 

which  converges  to  2 as  N —  ~.  Hence  {};_,  is square-summable. 

Next  we  show  that  square-summability  of the moving  average  coefficients  implies  that 
the  MA(=)  representation  im [3.3.13]  generates  a mean  square  convergent  random  variable. 
First  recall  what  is meant  by convergence  of a deterministic  sum  such  as  27_, a; where  {a;} 
is just a  sequence  of numbers.  One  criterion  for determining  whether  27_, a; converges  to 
some  finite  number  as  T —  = is  the  Cauchy  criterion.  The  Cauchy  criterion  states  that 
>j~o 4,  converges  if and  only  if, for  zny  e  >  0, there  exists  a  suitably  large  integer  N such 
that,  for  any  integer  M >  N, 

. 

-M 

N 

> a=  » a; 
j=0 

j=0 

<= 8 

In  words,  once  we  have  summed N terms,  calculating  the  sum  out  to a  larger  number  M 
does  not  change  the  total  by any  more  than  an  arbitrarily  small  number  e. 

For  a  stochastic  process  such  as  [3.3.13],  the  comparable  question  is  whether  | 
Zo We,-,  converges  in  mean  square  to  some  random  variable  Y, as T —  ~,  In  this  case 
the  Cauchy  criterion  states  that  27.9  wWe,-,  converges  if and  only  if, for  any  ¢  >  0, there 
exists  a suitably  large  integer  N such  that  for  any  integer  M >  N 

M 

N 
e| S ,-;  =  p> v.-| Sh. 

2 

[3.A.1] 

In words,  once  N terms  have  been  summed,  the  difference  between  that  sum  and  the  one 
obtained  from  summing  to  M  is a  random  variable  whose  mean  and  variance  are  both 
arbitrarily  close  to zero. 

3.A.  Convergence  Results  for Infinite-Order  Moving Average Processes 

69 

Now,  the  left  side  of [3.A.1]  is simply 
E[Wuls-u  +  Wun18r-mer 
to 

=  (Wy  +  Va 

M 

N 

Ons  1%:-n=1 

°° 

+  Wiad 

tit 

7  |S i  S  vi] 0 

j=0 

j=0 

if =. 

W2  converges  as  required  by  [3.3.14],  then  by the  Cauchy  criterion  the  right side 
amas be  vata as  ant as  pase by choice  of a suitably  large N. Thus  the  infinite 
series  in  [3.3.13]  converges  in  mean  square  provided  that  [3.3.14]  is  satisfied.  — 
Finally,  we  show  that  absolute  summability  of the  moving  average  coefficients  implies 

er 

that  the  process  is ergodic  for  the  mean.  Write  [3.3.18]  as 

Then 

y  Ree  o  y 3 Wi  eV: 
k=0 

ly;| 

ao 

» VV 

A  key property  of the  absolute  value  operator  is that 

la +  b  +  c| <|al  +  || + Iel. 

Hence 

and 

Dbl  s  oD 
j=0 

. 

ly) So?  D lyr adel 

k=0 

| 

DY Weel  =  0?  D DY Wael  Wel  =  07  D lel D Wyrel: 

j=0  k=0 

jeOk=0 

k=0 

j=0 

But  there  exists  an M  <  ©  such  that  =o || <  M,  and  therefore  279 |W;..|  <  M for  k  = 
0,1,2,...,  meaning  that 

> ly,| <  o »» ly,|  -  M  <  0?M?2  <o@, 

” aa 

= 

. 

Hence  [3.1.15]  holds  and  the  process  is ergodic  for  the  mean. 

Chapter  3 Exercises 

3.1. 

Is the following  MA(2)  process  covariance-stationary? 

Y, =  (1  +  2.4L  +  0.8L7)e, 

fort  =  fT 
1 
a  ated  {}  otherwise. 

If so,  calculate  its autocovariances. 
3.2. 

Is the  following  AR(2)  process  covariance-stationary? 

(1  —  1.1L  +  0.18L7)Y,  =  e, 

fort  =  Tt 
1 
E(ce.)  =  {}  otherwise. 

If so, calculate  its autocovariances. 
3.3.  A covariance-stationary  AR(p)  process, 

(1  —~ OL  =  GL?  =  «2  -  $,L/)(Y,  -  wv) =  &, 

70  Chapter 3  | Stationary  ARMA  Processes 

has an  MA()  representation  given  by 

with 

or 

(Y,  -  4)  =  W(L)e, 

WL)  =  fl  -  OL  -  lL?  ----  -  61) 

{1-  @L-¢L?----  —  $,17\ (vo  +  WL  +  Ll?  + 

-«]  =  I, 

In order  for this  equation  to  be true,  the  implied  coefficient  on  L° must  be  unity  and  the 
coefficients  on  L', L?, L®, . . .  must  be zero.  Write  out  these  conditions  explicitly  and  show 
that they imply a recursive  algorithm for generating  the  MA()  weights  y%, W,,.  .  . .  Show 
that this recursion  is algebraically  equivalent  to setting  #, equal  to the  (1,  1) element of the 
matrix  F raised  to  the jth power  as  in equation  [1.2.28]. 

3.4.  Derive  [3.4.21]. 
Verify [3.4.22]. 

3.5. 
3: . Suggest a  recursive  algorithm  for calculating  the AR(~)  weights, 

(1  +  nL  +  mL?  +  +--)(¥,  -  sid 

associated with an  invertible MA(q)  process, 

: 

(Y,  —  w) =  (1  +  OL  +  OL?  +  +++  +  6,L4%)e,. 

ier  eae  mame gestae  ih 

(ino  aqdw 

Piée25t0  Sahin  et Whose 

at Exercise 3.6 or ene mata) ‘process.  (HINT: Recall “equation: 

a i  bse. Jul gh it  = 2g } 

eMart as by teats 
Jove i Bi 

se  cculllccenaae 

-psitiedallg  pent. 19 31.  “Formulatin ap 

a 

teen  iat, salen  oF  ee.  Lip, 

oP  ag  ae es  al  fa  ind * vc  ey  A  one 

in  = 

‘ 

j 

Sind  tei  —s 

; 

‘a, 

Pg  os 

> 

7 

4 

:  ‘  ¥ 

< 
a. 
’ 

— 

it 
: 

— 
Care 

id 
aS 
VEEN? 

teal 

= 

Tap  l= 

a 
,  ia. 

‘ae  ap ’ 
= 

Forecasting 

This  chapter  discusses  how  to  forecast  time  series.  Section  4.1  reviews  the  theory 
of forecasting  and  introduces  the  idea  of  a  linear  projection,  which  is a  forecast 
formed  from a linear  function  of past  observations.  Section  4.2 describes  the  fore- 
casts  one  would  use  for  ARMA  models  if an  infinite  number  of past observations 
were  available.  These  results  are  useful  in theoretical  manipulations  and  in under- 
standing  the  formulas  in Section  4.3  for  approximate  optimal  forecasts  when  only 
a finite  number  of observations  are  available. 

Section  4.4  describes  how  to  achieve  a triangular  factorization  and  Cholesky 
factorization  of a variance-covariance  matrix.  These  results  are  used in that  section 
to calculate  exact  optimal  forecasts  based  on a finite  number of observations.  They 
will  also  be  used  in Chapter  11  to  interpret  v-ctor  autoregressions,  in Chapter  13 
to  derive  the  Kalman  filter,  and  in  a number  i other  theoretical  calculations  and 
numerical  methods  appearing  throughout  the  text.  The  triangular  factorization  is 
used  to  derive  a  formula  for  updating  a  forecast  in Section  4.5  and  to  establish  in 
Section  4.6  that  for  Gaussian  processes  the  linear  projection  is better  than  any 
: 
nonlinear  forecast. 
Section  4.7  analyzes  what  kind  of process  results  when  two different  ARMA 
processes  are  added  together.  Section  4.8  states  Wold’s  decomposition,  which 
provides  a  basis  for  using  an  MA(*)  representation  to  characterize  the  linear 
forecast  rule  for  any  covariance-stationary  process.  The  section  also  describes  a° 
popular  empirical  approach  for finding  a  reasonable  approximation  to  this  repre- 
sentation  that  was  developed  by Box  and  Jenkins  (1976). 

4.1.  Principles  of Forecasting 

Forecasts  Based  on  Conditional  Expectation 
Suppose  we  are  interested  in forecasting  the  value  of a  variable  Y,,,  based 
on  a set  of variables  X, observed  at date  ¢.  For example,  we  might  want  to forecast 
Y,,,  based  on  its m  most  recent  values.  In this  case,  X, would  consist  of a constant 
Pius  Fy  Fioas  +.»  pA  Tareas 

Let  Y%, \ denote  a forecast  of Y,,,  based  on  X,. To  evaluate  the  usefulness 
of this forecast,  we  need  to specify  a loss function,  or  a summary  of how concerned 
we  are  if our  forecast  is off  by a  particular  amount.  Very  convenient  results  are 
obtained from  assuming  a quadratic  loss function.  A quadratic  loss function  means 
choosing  the  forecast  Y*, 1, SO  as  to  minimize 

E(Y,.;  ¥  Yrs 1y)*: 

[4.1.1] 

72 

Expression  [4.1.1]  is known  as  the  mean squared  error  associated  with  the  forecast 
Y*, 1,  denoted 

MSE(Y%,  1),) =  E(Y,.,  a  Yi. iy): 

The forecast  with  the  smallest  mean  squared  error  turns  out  to  be  the  ex- 

pectation  of  Y,,,  conditional  on  X,: 

To  verify  this  claim,  consider  basing  Y*, ,, on  any  function  g(X,)  other  than  the 
conditional  expectation, 

ea lj:  ais  ECY,, .  11%:)- 

[4.1.2] 

Yr  =  g(X,). 

[4.1.3] 

For  this  candidate  forecasting  rule,  the  MSE  would  be 

E(Y,.1  ee g(X,)}?  =  E[Y,.1  =! E(Y,,,|X,)  +  E(Y,4:|X,)  +  g(X,)}? 

=  ELY,+1  =  E(Y,,:1X)) 

[4.1.4] 

+  2E{[Y,+1  i  E(Y,,:1X,)]  ([E(Y,.11X,)  a  g(X,)]} 

: 

+  E{[E(Y,, X,) sia g(X,)]?}. 

, 

Write  the  middle  term  on  the  right side  of [4.1.4]  as 

where 

2E[n,+1]; 

: 

[4.1.5] 

Tee = MY  —  (YX) [E(¥ + IX.) —  9 (X,)]}- 
Consider  first the  expectation  of  »,,,  conditional  on  X,. Conditional  on  X,, the 
terms  E(Y,, ,|X,)  and  g(X,)  are  known  constants  and can. be  factored  out  of this 
expectation:! 

. 

| 

E{n,+11X,]  =  [E(Y,.:1X,)  =  g(X,)]  x  Se)  ns E(Y,4:1X)]|X,) 

=  (E(Y,4:/X,)  ¥  g(X,)]  x  0 
=  0. 

By a straightfoi ward  application  of the law of iterated  expectations,  equation  [A.5.10], 
it follows  that 

, 

E[n +1] =  Ex (E[n+11X,])  =  0. 

Substituting  this  back  into  [4.1.4]  gives 
E(Y¥,41  —  8(X)P  =  E[Yi41  —  E(ViIXDP  +  ECE(Y,+  1X)  —  (XD).  [4.1.6] 
The  second  term  on  the  right  side  of [4.1.6]  cannot  be  made  smaller  than  zero, 
_and  the  first  term  does  not  depend  on  g(X,).  The  function  g(X,)  that  makes  the 
mean  squared  error  [4.1.6]  as  small  as  possible  is the  function  that  sets  the  s. sond 
term  in [4.1.6]  to zero: 

[4.1.7] 
E(Y,.:|X,)  =  g(X,). 
Thus  the  forecast  g(X,)  that  minimizes  the  mean  squared  error  is the  conditional 
expectation  E(Y,,,|X,),  as  claimed. 

The  MSE  of this  optimal  forecast  is 

ELY  +1  =  g(X,)}?  =  ELY, +}  oY E(Y 41%). 

[4.1.8] 

'The  conditional  expectation  E(Y,,,|X,)  represents  the conditional  population  moment  of the  ran- 
dom  variable  Y,,,  and  is not  a function  of the random  variable  Y,,,  itself.  For  example,  if Y,, ,|X, ~ 
N(a’X,,  2), then  E(Y,, ,|X,)  =  a'X,,  which  does  not  depend  on  Y,, ,. 

4.1.  Principles  of Forecasting 

73 

Forecasts  Based  on  Linear  Projection 
We  now  restrict  the  class  of  forecasts  considered  by requiring  the  forecast 

Y**, 4,  to  be  a  linear  function  of X,: 

Y?, y=  @'X,. 

[4.1.9] 

Suppose  we  were  to  find  a  value  for  a  such  that  the  forecast  error  (Y,,,;  —  a’X,) 
is uncorrelated  with  X,: 

E[(Y,4,  —  a'X,)X;]  =  0’. 

[4.1.10] 

If [4.1.10]  holds,  then  the  forecast  a’X, is called  the  linear  projection  of  Y,,,  on 

t° 

The  linear  projection  turns  out  to  produce  the  smallest  mean  squared  error 
among  the  class  of linear  forecasting  rules.  The  proof of this  claim  closely  parallels 
the  demonstration  of the  optimality  of the  conditional  expectation  among  the  set 
of all possible  forecasts.  Let  g’X, denote  any  arbitrary  linear  forecasting  rule.  Note 
that  its MSE  is 

E[Y  41  Ti g’X,] 

-=  E[Y,,,  —  a'X,  +  aX,  —  g'X,} 

=  E(Y,+1  a  a’X,)?  +  2E{(Y;.1  is a’X,] [a’X,  =  g'X,]} 

+  E[a'X,  —  g’X,}*. 

| 

{4.1.11] 

As  in  the  case  of [4.1.4],  the  middle  term  on  the  right  side  of [4.1.11]  is zero: 

1218 FPS  a’X,] [a’X,  —  g’X,]) oF (E(Y,.1  eh a’X,|X;)[a  =  g] oe 0'[a rip g]. 

by virtue  of [4.1.10].  Thus  [4.1.11]  simplifies  to 

FLY,.:  —  9X.  =  ELY,.1  -  a’X,)°  +  Ela’X,  —  2'X,). 
[4.1.12] 
The  optimal  linear  forecast  g'X,  is the  value  that  sets  the  second  term  in [4.1.12] 
equal  to  zero: 

on 

4.. 

g'X,  =  a'X,, 

where  a’X, satisfies  [4.1.10]. 

For  a’X, satisfying  [4.1.10],  we  will  use  the  notation 

_ 

OF  sometimes  simply 

PCY, +11X,)  =  a'X,, 

} 

) ren =a’X,, 

to  indicate  the  linear  projection  of  Y,,,  on  X,.  Notice  that 
MSE[P(Y, ,1X,)] = MSE[E(Y,,1X,)], 

since  the  conditional  expectation  offers  the  best  possible  forecast. 
' 

For  most  applications  a constant  term  will be included  in the projection.  We 
will use  the symbol  £ to indicate  a linear projection  on  a vector  of random  variables 
X, along  with  a constant  term: 

E(Y,.,1X,) =  PLY,, if, X,). 

Properties  of Linear  Projection 
It is straightforward  to  use  [4.1.10]  to calculate  the  projection  coeffici 

in terms  of the  moments  of  Y,,;  and  X,: 

eo 

|  nine 

E(Y,.1X;)  = a’ E(X,X/), 

74  Chapter 4 | Forecasting 

or 

a’  =  E(Y,,,X/)[E(X,X/)]7', 
[4.1.13] 
assuming  that  E(X,X/)  is  a  nonsingular  matrix.  When  E(X,X;/)  is  singular,  the 
coefficient  vector  @  is not  uniquely  determined  by [4.1.10],  though  the  product  of 
this vector  with  the explanatory  variables,  a'X,,  is uniquetydetermined  by [4.1.10].? 

The  MSE  associated  with  a  linear  projection  is given  by 

E(Y,,,  —  a'X,)?  =  E(Y,,,)?  —  2E(a@’X,Y,,;)  +  E(a’X,X/a). 

[4.1.14] 

Substituting  [4.1.13]  into  [4.1.14]  produces 

E(Y,.1  7  a'X,)?  ad E(Y,4;)  es 2E(Y,4:X%;)[E(X,X;)]~  *E(X.Y,41) 
+  E(Y,,:X;)[E(X,X;)}7’ 

[4.1.15] 

x  E(X,X;)[E(X,X;)]-*E(X.Y,, 1) 
=  E(Y,,,)?  -  E(Y,4:X;)[E(X,X;)]~'E(X,Y,.1)- 

Notice  that  if X, includes  a  constant  term,  then  the  projection  of (aY,, , +5) 

on  X, (where  a  and  b are  deterministic  constants)  is equal  to 

Pi(aY,,.,  +  b)|X,]  =  a-P(Y,,,,|X,)  +  b. 

To  see  this,  observe  that  a-P(Y,,,|X,)  +  b is a  linear  function  of X,.  Moreover, 
| 
the  forecast  error, 

[aY,.,  +  b) as [a-P(Y,,:1X,)  +  b] =  alY +1  =  PCY, iIX,)], 

is uncorrelated  with  X,,  as  required  of a  linear  projection. 

Linear  Projection  and  Ordinary  Least  Squares  Regression 

Linear  projection  is closely  related  to ordinary  least  squares  regression.  This 

subsection  discusses  the  relationship  between  the  two  concepts. 

A linear  regression  model  relates  an  observation  on  y,,,  to  x,: 

Yr+1  =  B'X, +  uY,. 
Given  a sample  of T observations  on  y and  x, the  sample  sum  of squared  residuals 
is defined  as 

: 

| 

[4.1.16] 

Deer BP 

[4.1.17] 

The  value  of B that  minimizes  [4.1.17],  denoted  b, is the  ordinary  least  squares 
(OLS)  estimate  of B. The  formula  for b turns  out  to  be 

b =  [3 xxi] P sri}  : 

t=1 

[4.1.18] 

21f E(X,X/)  is singular,  there  exists  a nonzero  vector  ¢ such  that ¢’-E(X,X;)-¢  =  E(c'X,)?  =  0, so 
that some linear combination  ¢’X, is equal  to zero for all realizations.  For example,  if X, consists  of 
two random  variables,  the second variable must be a rescaled  version  of the first:  X,, =  ¢-X,,. One 
could simply  drop the redundant variables from such a  system and calculate the linear projection of 
Y,,, on X?,  where X? is a vector  consisting  of the nonredundant  elements of X,. This linear projection 
a*’X? can be uniquely calculated  from  [4.1.13]  with X, in [4.1.13]  replaced by X}. Any linear  com- 
bination of the original variables  a’X, satisfying  [4.1.10]  represents  this same  random variable; that is,  — 
a’X,  =  a*’X? for all values  of a consistent  with  [4.1.10]. 

4.1.  Principles of Forecasting 

75 

which  equivalently  can  be  written 

T 

i 

T 

b=  jan » xx  | jan >> xin 

[4.1.19] 

- 

Comparing  the  OLS  coefficient  estimate  b in equation [4.1.19] with  the  linear 
projection  coefficient  a  in equation  [4.1.13],  we  see  that  b is  constructed  from  the 
sample  moments  (1/T)Z7_,x,x;  and  (1/T)2/21X,Yr+1  while a.  is  censtructed  froma 
population  moments  E(X,X;)  and  E(X,Y,41).  Thus  OLS  regression  is  a  summary 
»  X7)  and  (y2,  y3;  ety  Yr+i)> 
of  the  particular  sample.  observations  (x,,  X2,  -- 
whereas  linear  projection  is  a  summary  of  the  population  characteristics  of  the 
stochastic  process  {X,,  Y,,:}7-  -=- 

Although  linear  projection  describes  population  moments  and ordinary  least 
squares  describes  sample  moments,  there  is a  formal  mathematical  sense in  which 
the two  operations  are  the same.  Appendix  4.A  to this chapter discusses  this parallel 
and  shows  how  the  formulas  for  an  OLS  regression  can  be viewed  as  a special  case 
of the  formulas  for  a  linear  projection. 

Notice  that  if the  stochastic  process  {X,,  Y,,,}  is covariance-stationary  and 
ergodic  for  second  moments,  then  the  sample  moments  will  converge  to  the  pop- 
ulation  moments  as  the  sample  size  T goes  to  infinity: 

. 

T 

(1/T) > X,X/—> E(X,X/) 

—1 

fs 

(1/T)  ¥, X,¥ia1— E(X,¥,41); 

_ 

t=1 

implying 

b— a. 

4.1.20] 

Thus  OLS  regression  of y,,,  on  x, yields  a  consistent  estimate  of the  linear 
projection  coefficient.  Note  that  this result  requires  only that the process  be ergodic 
for  second  moments.  By contrast,  structural  econometric  analysis  requires  much 
stronger  assumptions  about  the  relation  between  X and  Y.  The  difference  arises 
because  structural  analysis seeks  the effect of X on  Y. In structural  analysis,  changes 
in X are  associated  with  a  particular  structural  event  such  as  a  change  in Federal 
Reserve  policy,  and  the  objective  is to  evaluate  the  consequences  for  Y.  Where 
that  is the  objective,  it is very  important  to  consider  the  nature  of the  correlation 
between  X and Y before  relying on  OLS  estimates.  In the case  of linear  projection, 
however,  the  only concern  is forecasting,  for  which  it does  not  matter  whether  it 
is X that  causes  Y or  Y that  causes  X.  Their  observed  historical  comovements  (as 
summarized  by E(X,Y,,,))  are  all  that  is needed  for calculating  a forecast.  Result 
[4.1.20]  shows  that  ordinary  least  squares  regression  provides  a  sound  basis  for 

'  forecasting  under  very  mild  assumptions. 

| 

One  possible  violation  of these  assumptions  should  nevertheless  be  noted. 
Result  [4.1.20]  was  derived  by assuming  a covariance-stationary,  ergodic  process. 
However,  the  moments  of the  data  may  have  changed  over  time  in fundamental 
ways,  or  the  future  environment  may  be  different  from  that  in the  past.  Where 
this  is the  case,  ordinary  least  squares  may  be  undesirable,  and  better  forecasts 
can  emerge  from  careful  structural  analysis. 

| 

76  Chapter  4  | Forecasting 

Forecasting  Vectors 
The preceding  results  can  be  extended  to  forecast  an  (n  x  1) vector  ~,,,  On 

the  basis  of  a  linear  function  of an  (m  x  1) vector  X,: 
P(Y,,:1X,)  =  @’X,=¥,,  1). 

[4.1.21] 

Then  a’  would  denote  an  (nm  x  m) matrix  of projection  coefficients  satisfying 

E[(Y¥,.,  —  «’X,)X/]  =  0; 

[4.1.22] 

that  is, each  of the  n  elements  of (Y,,,  —  Pica)  is uncorrelated  with  each  of  the 
m  elements  of X,.  Accordingly,  the  jth  element  of  the  vector  Y,,,,,  gives  the 
minimum  MSE  forecast  of the  scalar  Y,,,,.  Moreover,  to  forecast  any  linear  com- 
bination  of the  elements  of Y,,,,  say,  z,,,  =  h’Y,,,,  the  minimum  MSE  forecast 
of z,,,  requires  (z,,,  —  2Z,44,)  to  be  uncorrelated  with  X,.  But  since  each  of  the 
elements  of (Y,,,  —  Y,,1,) is uncorrelated  with  X,, clearly  h’(Y,,;  —Y,,1;,)  is also 
uncorrelated  with  X,.  Thus  when  Y,, ,,, satisfies  [4.1.22],  then  h’Y,,  ,,,  is the  min- 
imum  MSE  forecast  of h’Y,,,  for  any  value  of h. 

From  [4.1.22],  the  matrix  of projection  coefficients  is given  by 

[4.1.23] 
a’  =  [E(Y,.:X/)]  -[E(X,X))]-!. 
The  matrix  generalization  of the  formula  for  the  mean  squared  error  [4.1.15]  is 

MSE(a'X,)  =  E{Y,.+1  =  a’X,] >  [Y,41  $3 a’X,]'} 

1.24 
=  BW  ¥ie1)  —  (ECW X))-EOCX))-  (BOG;  4174 

4.2.  Forecasts  Based  on  an  Infinite  Number 
of Observations 

Kiss 

| 

Forecasting  Based  on  Lagged  e's 

Consider  a process  with  an  MA(*) representation 

| 
with  ¢, white  noise  and 

(Y, —  #) =  W(L)e, 

[4.2.1] 

WL) =  > pL! 

j=0 

Yo =  1 

J, 

» lwi<~. 
é 

[4.2.2] 

Suppose  that  we  have  an  infinite  number  of observations  on  ¢ thrdugh date  ¢, {e,, 
.  .}. Say we  want 
E,-1)  €;-2»  - 
to  forecast  the  value  of Y,,,,  that  is, the  value  that  Y will  take  on  s periods  from 
now.  Note  that  [4.2.1]  implies 

-  -}, and  further  know  the values  of 

and  {y,, 2, . 

Vee 

SH FH  Or4p  t  ViGre get  tt  G18 01 F  ye 
4 
<7 
+  WatG-1  t  "°° 

[4.2.3] 

The  optimal  linear  forecast  takes  the  form 

EY, +s\€: Ey-19+  +  J =  wt  We, +  Wye rEr—1  +  Vee2br-2  t+  °°. 

[4.2.4] 

4.2.  Forecasts  Based  on  an  Infinite  Number  of Observations 

77 

That  is,  the  unknown  future  eé’s  are  set  to  their  expected  value  of zero.  The  error 
associated  with  this  forecast  is 

V4,  —  LUV adler Grnase  +) =  Sree  +  ilreo-a  +7  +  @aieeor 

[4-2-5] 
In  order  for  [4.2.4]  to  be  the  optimal  linear  forecast,  condition  [4.1.10]  re- 
quires  the  forecast  error  to  have  mean  zero  and  to  be  uncorrelated  with  €,,  €,-1; 
.  It is readily  confirmed  that  the  error  in [4.2.5]  has  these  properties,  so  [4.2.4] 
_. 
must  indeed  be the linear  projection,  as claimed.  The  mean  squared error  associated 
with  this  forecast  is 

. 

' 

E(Yia;—  ElYaslen  ev  DP=A+  H+ 
For  example,  for  an  MA(q)  process, 

+--+  +#a)o?. 

[4.2.6] 

WL)  =  1+  0,L  +  0,L?2+---+  6,14, 

the  optimal  linear  forecast  is 
|g Ae Serpe 

! 

[4.2.7] 

‘  {" +  Beet  OssSiomtite 

+  Of:5943 

m 

The  MSE  is 

for. $ mAs 
fors  =  q+i1,q+2,.... 

2:  »f 

d, « 

o? 
(1+@+  +--+  +62_,)o? 
(1+  0} +  63 +---  +  @)o? 

fors = 1 
fore=2,3,:9..9 
fors=q+1,q+2,.... 

The  MSE  increases  with  the  forecast  horizon  s  up  until  s  =  q.  If we  try  to  © 
forecast  an  MA(q)  farther  than  q periods  into  the  feture,  the  forecast  is simply  the 
unconditional  mean  of the  series  (E(Y,)  =  mw) and  the  MSE  is the  unconditional 
variance  of the  series  (Var(Y,)  =  (1  +  03 +  03 +  ---  +  63)o7). 

These  properties  also  characterize  the  MA()  case  as  the  forecast  horizon  s 
goes  to  infinity.  It is straightforward  to  establish  from  [4.2.2]  that  as  s >  ©,  the 
forecast  in  [4.2.4]  converges  in mean  square  to  uw,  the  unconditional  mean.  The 
MSE  [4.2.6]  likewise  converges  to  0? 2797,  which  is the  unconditional  varian 
rir 
of the  MA()  process  [4.2.1]. 

A  compact  lag operator  expression  for  the  forecast  in  [4.2.4]  is sometimes 

used.  Consider  taking  the  polynomial  y(L)  and  dividing  by L’: 

a =L-*+y,L'-*  +  y,L?-5  +e 

e+  y_,L°'+  w, L° 

nA rey oe z. W421?  ay 

£7 

> 

The  annihilation  operator’  (indicated  by [:],)  replaces  negative  powers  of  L by 
zero;  for  example, 

bea =  ¥, +  We4,L"  +  We a2L?  5 a 

eine, 

[4.2.8] 

Comparing  [4.2.8] with [4.2.4], the optimal forecast  could be written  in lag operator 
notation  as 

ETY,  .dé, €-;,.-.]  @  6  *  ieee 8. 

[4.2.9] 

*This discussion  of forecasting based on  the annihilation  operator  is similar to that in Sargent (1987). 

. 

78  Chapter 4  | Forecasting 

Forecasting  Based  on  Lagged  Y’s 
The  previous  forecasts  were  based  on  the  assumption  that  e,  is  observed 
directly.  In the  usual  forecasting  situation,  we  actually  have  observations  on  lagged 
Y S,  not  lagged  e’s.  Suppose  that  the  process  [4.2.1]  has  an  AR(~)  representation 
given  by 

n(L)(Y,  —  ») =  &, 

[4.2.10] 

where  n(L)  =  2j-9n,L’,  m  =  1, and  Zj7-olnJ|  <  ©.  Suppose  further  that  the  AR 
polynomial  n(L)  and  the  MA  polynomial  y(L)  are  related  by 

n(L)  =  [¥(L)}~*. 

[4.2.11] 

A  covariance-stationary  AR(p)  model  of the  form 

(l-¢,L  —-  @L*  —  --+  —  &, L’Y,  —  uw) =  €,, 

[4.2.12] 

Or,  more  compactly, 

$(L)(Y,  ‘A #) =  €,, 

clearly  satisfies  these  requirements,  with  n(L)  =  $(L)  and  y¥(L)  =  [¢(L)]~'.  An 
MA(q)  process 

ae 

Y,-p=(1+6,L+  0,L?7+---+6,L%)e, 

[4.2.13] 

or 

Y, ae  a  Sid  A(L)e, 
is also  of this form,  with  #(L)  =  @(L) and  7n(L)  =  [0(L)]~—',  provided  that  [4.2.13] 
is based  on  the  invertible  representation.  With  a  noninvertible  MA(q),  the  roots 
must  first  be flipped  as  described  in Section  3.7 before  applying  the formulas  given 
in this  section.  An  ARMA(p,  q) also  satisfies  [4.2.10]  and  [4.2.11]  with  y(L)  = 
6(L)/¢(L),  provided  that  the autoregressive  operator  $(L) satisfies  the stationarity 
condition  (roots of ¢(z)  =  0 lie outside  the unit circle) and that the moving  average 
operator  @(L) satisfies  the  invertibility  condition  (roots  of 0(z)  =  0 lie outside  the 
unit  circle). 

Where  the  restrictions  associated  with  [4.2.10]  and  [4.2.11]  are  satisfied,  ob- 
.  .}. For 

.  .} will  be  sufficient  to  construct  {e,,  €,_,,  . 

servations  on  {Y,,  Y,_,,  . 
example,  for  an  AR(1)  process  [4.2.10]  would  be 

[4.2.14] 
(1 -  oL)(Y,  —  ») =  &. 
Thus,  given  ¢ and  yw  and  observation  of  Y, and  Y,_,,  the  yalue  of  e,  can  be 

*  constructed  from 

For  an  MA(1)  process  written  in invertible  form,  [4.2.10]  would  be 

e, =  (Y, —  ») —  $(Y,-1  —  #). 

(1 +  OL)-"(Y,  —  #) =  &,. 

Given  an  infinite  number  of observations  on  Y, we  could  construct  e  from 

=  PUY  <5 

e,=(Y,  —  w) —  Y,-,  —  w) + °(Y,-2  —  H) 
ype  "a 
Under  these  conditions,  [4.2.10]  can  be substituted  into  [4.2.9]  to obtain  the _ 
Dada  Yon  d= nt  [2] warer  -  ws 

forecast  of Y,,, a8  a function  of lagged  Y’s: 

[4.2.15] 

4.2.  Forecasts  Based  on  an  Infinite  Number  of Observations 

79 

or,  using  [4.2.11]. 

. 
EL Yi acl tis  ae  ts  oe 

1  op  pokalhidde 

ee  | iz | (Hi 

sdiimeiait 

re 

[4.2.16] 

Equation  [4.2.16]  is known  as  the  Wiener-Kolmogorov  prediction  formula.  Several 
examples  of using  this  forecasting  rule  follow. 

Forecasting  an  AR(I)  Process 

For  the  covariance-stationary  AR(1)  process  [4.2.14],  we  have 

W(L)  =  11  -  dL)  =  1+  OL  +  PL?  +  PL?  +---  = [4.2.17] 

and 

Eo  =a  t+  OL!  +  gt?L2  +  ---  =  G1  —  GL). 

[4.2.18] 

Substituting  [4.2.18]  into  [4.2.16]  yields the optimal  linear  s-period-ahead  forecast 
fora  stationary  AR(1)  process: 

BLY gol  g¥  ye 

ac aaa BP  1) 

leeing 
=p  +N  —  p). 

ig  16) 

The  forecast  decays  geometrically  from  (Y,  —  «) toward  wu as  the  forecast  horizon 
s  increases.  From  [4.2.17],  the moving  average  weight  y; is given  by ¢’, so  from 
[4.2.6],  the  mean  squared  s-period-ahead  forecast  error  is 

[1  ve og ote  ¢*  ues  oct} 

abe  G6 -Y]o?. 

Notice  that  this  grows  with  s  and  asymptotically  approaches  o7/(1  —  7),  the 
unconditional  variance  of  Y. 

Forecasting  an  AR(p)  Process 

Next consider  forecasting  the  stationary  AR(p)  process  [4.2.12]. The  Wiener- 
Kolmogorov  formula  in  [4.2.16]  essentially  expresses  the  value  of (Y,,,  —  #2) in 
terms  of initial  values Y,. 
nim  a),  .  -} and  subsequent  values'of  {e,, ,, 
Ers2r  ++  €,,;} and  then  drops  the  terms  involving  future e’s, ‘An  expression  of 
this  form was  ptuyided by equation  [1.2.26],  which described thé value of a.variable 
subject  to  a pth4grder  difference  equation  in terms  of initial  conditions  and  sub. 
sequent  shocks: 

(AY 

| 

| 

fr  en  b=  fPCY,  —  p)+  A 2X4  Wt  i 

+  Fs 

e  Eryy  +  WrE ras  +  WE, 45~2  TTS  +  ee 

—  #) 

: 

[4.2.20] 

where 

| 

Wf 

80  Chapter  4  | Forecasting 

Ae 

se 

tcp 
| 

Recall that /\) denotes  the (1, 1) element  of F’,  f() denotes  the  (1, 2) element  of 
F’,  and  so  on,  where F is the  following  (p  x  p) matrix: 

d,  a)  od,  tk 
a 

ae 

$,-1  d, 
0 

0 

F= 

0 

1 

0 

0 

0 

The optimal  s-period-ahead  forecast  is thus 

O-0 

0  -~ 

] 

0 

=  =  -  +  FAY,  —  ps)  +  fry, : 

we)  +  «+s 

s) 

"  lt  ee  *  ). 

' 

[4.2.22] 

Notice  that for any forecast horizon  s the optimal  forecast  is a constant  plus a linear 
function  of {Y,,  Y,_;,  . 

, Y:-p+1}.  The associated  forecast  error  is 

- 

. 

Yies  a  Bevin =  E145  +  Ws s—1  mi Woe: +s-2 ee  Se  Bele i.  [4.2.23] 
The easiest  way to calculate  the forecast  in [4.2.22]  is through  a  simple  re- 
cursion.  This  recursion  can  be deduced  independently  from a principle  known  as 
the law of iterated projections, which will be proved formally in Section  4.5.  Suppose 
ae  Yi+1- The optimal 

|  ahs  pe — 

a* 

Cane 
* 

. 

gut) @  4 

HY, A) + Ka) 

tat 

* 

‘a 

te 

(4.2.24 

ri  Gi  Sag 29IDTIOZ  2 

Sheer 

Paresasiing  : or AR Y ; 

i0d-ahez age 

Ipp  e  that  at date 1  +  1  we  were  | 
forecas Pay. Replacing t with  ¢ +  1 in [4:2.24] 

d 

pete opine OSS:  SEE  AG-le  SRG TG  od) 
Edb  elegans aida “ 

253  wsiv  bn 
[4.2.25] 

ay  ae  ee  — 

we 

J 

= 

ena 

' 

: 

: 

i 

~~ 

G  a  =)  =s 
sean Bats 67S  me ——  o 

‘ 

for j =  1,2,...,5  where 

ae =  Y, 

for  tT  St. 

ww 

Forecasting  an  MA(1)  Process 
Next  consider  an  invertible  MA(1)  representation, 

|

Y,-p=(1+  OL)e, 
[4.2.28] 
with  |6| <  1.  Replacing  YL)  in  the  Wiener-Kolmogorov  formula [4 aA) with 
(1 +  @L) gives 

ae  =prt n+ [Let], vi —  (¥; — #).- 

1  +,.0L 

[4.2.29] 

SAT HR § 

r 

on 

To forests tan MA(1) eronkso-anis panied into the future (s =  Dex 

ate 

. 

oan 

’ 

; 

Syitie  £  deuce  2  Ss cB} 

Je 

song 

td: 

stelenieo- 

+  2hENG 

: 

om 
03 

vew 
YEW 

I2e1265 

; 
31! 

sh  inc o FNS, 

asl 
Poierbes  itt  a6i27u- 
ee  « zt  4 ee rt  £25 Paes Wo! oF 3  ‘Ory  cf  Shay asist Wi  UNDORO  % DSinrsti  a  aad ae 
i  ae 
sei 
ieeit 
ors, m +- 3 Sek  ee ae  <  6 & Siler Ot bolnew  ow  1 sisb  tg  2.30  3 

aC  dnabaeq Bis;  PX 

o  Ox 

béb-at 

ont 

acu 

ng 

z 

| 

- ee  % ae 7) = AY, = Ets re 5,  “= 

FUSS  21  3 

= _  Itis sometimes useful to write (4.2.2 ‘ode 

ite  fare, pate g 

OL, soouak oo  baa 3 dsm ot 

sing,  2  1629101  — ited a 

 
the  forecast  [4.2.16]  becomes 

7 

awa  t| 

1+  0,L  + 

tres  +  ne) 

6,L? 
"  re 

. 

. 

L 

TTL s al + 

1 

> 

y- 

[4.2.34] 

Now 

1  hOl  +  OL?  +:  >>  +.0,L4 
Li 

5 

{° +  Brash t+ Oryal?  +.  \+  OgL4**  > ‘fors  =1,2,..-,9 

fors=q+1,q+2,. 

Thus,  for horizons of s =  1, 2, . 

. 

» q, the forecast  is given  by 

Fisse =  2  + (0, +  O4,L  + O42L? +  +++ +  O,L9~*)é;, 

{4.2.35} 

sore pie tate siege egies ihe sereion 
ict 
3  *  aie  6  = y=  4) =  08,1 —  é,_2  -  eh 

; 

Wis  4 Lan  oe Mie 

Wabecgs 

[4.2.36] 

Ss  rer ¢  nt  ee  ton 
mean. 

eB ad¥'n  Ys ser os we ene ne ce 
rapencol “rere Ge - 

as Cae Ae  ’  i} 

a 

é, a= —— Come 

levuteh s#T 

Note  that  fors  =  2,3,...,  the  forecast  [4.2.39]  obeys  the  recursion 

(Yh.  z.  1) 

OF, a4  F  ). 

Thus,  beyond  one  period,  the  forecast  decays  geometrically  at  the rate  ¢ toward 
the  unconditional  mean  4.  The  one-period-ahead  forecast  (s =  1) is  given  by 

Yieuy  =»  + 

d+  0 
1  +  OL 

(Y, —  p). 

[4.2.40] 

This  can  equivalently  be  written 

(Pic =  a) = 

o(1  +  OL)  +  0(1  —  oL) 
en 

(Y,—  #) =  $(¥,—  w) +  0, 

[4.2.41] 

where 

or 

geolt aibly 

yin 

é, =  (Y, a  ) pe $(Y,-1  as #) rE  aE  ad 

en  A 

[4.2.42] 

Forecasting  an  ARMA(p,  q) Process 

Finally,  consider  forecasting  a stationary  and  invertible  ARMA(p,  q) process: 

~ 

Q—¢@L-4¢1?-—----—¢,  F)@,-  3)  Ube  AOE  se  tee 

The  natural  generalizations  of [4.2.41]  and  [4.2.42]  are 

(Lis —  ») =  $Y,  —  vw) +  &(Y,-1  —  ew) +°°: 

+  OY,  gns  —  pt) ct  Oye, + Eg  +>  +  Ob.  cass  Susie, 

with  {é,} generated  recursively  from 

é,= Y, -— Yuet 

The s-period-ahend  forecasts  would  be 

(V4 —  p) 

[4.2.44] 

[4.2.45] 

Oi  ros-t  “ar  OAV icicm  a.  oe  Es  eon  AR  —p) 
fons  #1,  2,...  9 

+  0,6,  +  0,,,€.7  +++:  +  Ob  n2-¢ 

a 

OT  ns~%  —  p)  +  PAY 45-24  a: ) + ae  7G  amit  | —  '}t) 
fors  =q+1,q+2,..., 

/ 

where 

A 

Yn =Y, 

forrst. 

Thus for a forecast  horizon s greater than the moving average  order  q, the forecasts 
follow  a  pth-order  difference  equation  governed  solely  by  the  autoregressive 
parameters. 

84  Chapter 4  | Forecasting 

4.3.  Forecasts  Based  on  a  Finite  Number 
of Observations 

The  formulas  in the  preceding  section  assumed  that  we  had  an  infinite  number  of 
past  observations  on  Y,  {Y,,  Y,_,,  . 
.  .}, and  knew  with  certainty  population  pa- 
rameters  such  as  yw,  ¢,  and  @.  This  section  continues  to  assume  that  population 
parameters  are  known  with  certainty,  but  develops  forecasts  based  on  a finite 
number  of observations  {Y,,  Y,_,,  - 

,  Y;~mei}- 

- 

. 

For  forecasting  an  AR(p)  process,  an  optimal  s-period-ahead  linear  forecast 
based  on  an  infinite  number  of observations  {Y,,  Y,_,,  . 
.  .} in  fact  makes  use  of 
only  the  p  most  recent  values  {Y,,  Y,_,,  ...,  Y,-p+:}.  For  an  MA  or  ARMA 
process,  however,  we  would  in principle  require  all  of the  historical  values  of Y in 
order  to  implement  the  formulas  of the  preceding  section. 

Approximations  to  Optimal  Forecasts 

One  approach  to  forecasting  based  on  a  finite  number  of observations  is to 
act  as  if presample e’s were  all  equal  to  zero.  The  idea  is thus  to  use  the  approx- 
imation 

a 

=E(Y,.1Y,,  VER  cpngiape: 

Game Fane Fo. 

0,2,  mii  =  Gs  -  a) 

For  example,  consider  forecasting  an  MA(q)  process.  The  recursion  [4.2.36]  can 
be  started  by setting 

[4.3.1] 

a  tesa  —  ae  —  Caw  al  =0 

[4.3.2] 

and  then  iterating  on  [4.2.36]  to  generate  é,_m41,&:—-m+2»  +--+»  &- These  calcu- 
lations  produce 

bate  =  (Hw  =  I), 

fm +2  _  Pras  a  1) =  O58.  Snr  13 

E,-m+3  J  (Y,-m+3  <" #) .* D1E,—m+2  z  02€,-m+1» 

»  €:-q+s)  are  then  substituted 
and  so  on.  The  resulting  values  for  (é,,é@,-1,  - 
directly  into  [4.2.35]  to  produce  the  forecast  [4.3.1].  For  example,  fors  =q  =  1, 
the forecast  would  be 

- 

- 

a =prt  A(Y, -  bb) -  F(Y, -v—  #) 

[4.3.3] 

-  0(Y,_2  oe)  a 

(-1)7-'O"(Y  m4  —  »), 

p+ OY, 

which  is to  be used  as  an  approximation  to  the  AR(@)  forecast, 
(4.3.4) 
mw) —  O(Y,1—w)  + OXY  2-m)—o 
| 
r  m large  and  |6| small,  this  clearly  gives  an  excellent  approximation.  For 
to  unity,  the  approximation  may  be  poorer.  Note  that  if the  moving 
{|  closer 
average operator is noninvertible,  the  forecast  [4.3.1]  is inappropriate  and  should 
8 
| 
ot  be used. . 

| 

4.3.  Forecasts  Based  on  a Finite  Number  of Observations 

85 

Exact  Finite-Sample  Forecasts 
An  alternative  approach  is to  calculate  the  exact  projection  of  Y,,,  0M  its  m 

most  recent  values.  Let 

We  thus  seek  a  linear  forecast  of the  form 

)  ere 

a")'XK,  =  af” +  afY,  +  afMY,_,t-°°  t+  gy  See 
The  coefficient  relating  Y,,,  to  Y, in  a  projection  of  Y,,,  on  the  m  most  recent 
values  of  Y is denoted  a{” in  [4.3.5].  This  will  in  general  be  different  from  the 
coefficient  relating  Y,,,  to  Y, in  a  projection  of  Y,,,  on  the  m  +  1 most  recent 
values  of  Y; the  latter  coefficient  would  be  denoted  aim*), 
If Y, is covariance-stationary,  then  E(Y,Y,_;)  =  ¥; +  p?. Setting  X, =  (1, Y,, 

[4.3.5] 

¥ ic:  .  <i  Neemat) 

(4.223)  plies 

a’”™’  =  [ae”  a”)  a”  eaters  a”) 

=[u  (1  +  Pv’)  (y+  w2)°+*  Om  +  I 

io 

1 
i  ete  Ne 

bh - 
ee 
‘tn  te  Gee  FO  Mae 

a: Soe 

Le 

[4.3.6] 

SY  Pele?  gar eh ge OF [04S 

bygab gah 

When a constant  term  is included  in  X,,  it  is  more  convenient  to  express 
variables  in deviations  from  the  mean.  Then  we  could  calculate  the  projection  of 
(Y,41  —  #) on  X,  =  iy,  ~  2)  ee” 

ee”  es  u))’: 

Yiosy =e  ay (Y, —p)+  as” (Y, 3  >  gi) +  >  -& 

[4.3.7] 

si a) 

.  1). 

For  this  definition  of X, the  coefficients  can  be calculated  directly  from  [4.1.13]  to 
be 

ay” 
ay” 

Yo 
vi 

mw 
% 

©  Ge  TE 
"Vas 

Ne 

Vesta  Soe 

eM 

my 
Y2 

Ym 

[4.3.8] 

2  We will  demonstrate  in  Section  4.5  that  the  coefficients  (af”,a{”,  ..., 
a”)  in  equations  [4.3.8]  and  [4.3.6]  are  identical.  This  is analogous  to  a familiar 
result for ordinary least squares  regression—slope  coefficients would be unchanged 
if all variables are expressed-in  deviations  from their sample means  and the constant 
term  is  dropped  from  the  regression. 

86  Chapter  4  | Forecasting 

To generate  an  s-period-ahead  forecast  Y,,,,, we  would  use 

Prose = +  al™(Y, — w) +  afl (Y,_,-  mw)  + °° 

+ al™(Y,  4  —  “), 

where 

- 
m,s 

a 

_ 

Yo 

vi 

Ni 

es 

Ym-1 

= 

Ys 

Yo 

Fat 2 

Vs+1 

[4.3 9) 

a 

spt  We 3  wr 

ny 

“se  | 

Using  expressions  such as  [4.3.8]  requires  inverting  an  (m  X  m)  matrix. 
Several algorithms can be used  to evaluate  [4.3.8]  using relatively  simple  calcula- 
in Chapter  13, which 
tions.  One approach  is based  on the Kalman filter discussed 
can  generate  exact  finite-sample  forecasts  for a broad class of processes  including 
any ARMA  specification.  A second  approach is  based on  triangular  factorization 
of the matrix  in [4.3.8]. This second  approach is developed in the next two sections. 
This  approach  will prove helpful  for the immediate  question  of calculating  finite- 
ey eee and  is y a useful geri for ar  te a  anaes of later 

om 

“7 

—- 

2 

stoves. solieite 

rizat jon Of a Dae aD Fin De Fnite  — 

Bt  Teops  2  0) Sense  .  sen> 
SPINS ciniis Iiasta  ad igut pai }s .sindsh 

; i =a ea 7. 

ai ‘lage oilt- rf : esa ‘an oA :  & > Srp pee S1q  2  0 ng 

~ 

and  is a  eS  =  ‘me rm ino 

ith the same approach. 

Sate  motives E:.  &..  ae Anis Brat 

ie 

th: 
dh  Vain:  Gucci 

js 

aie tn 
"a 

ss)  a  To 

fex i Sy Sepa  eer 

*  ee  Te 

; 

<i 

To  see  how  the  triangular  factorization  can  be  calculated,  consider 

D4, 

D1.  Q)3  p  Sh 

D1, 

Q,,  2  5  tia 
+°* 

=|  M1  O32  M33 

1, 
sy  |. 

[4.4.2] 

D1  2,2  D3  a:  Dn 

We  assume  that  2  is positive  definite,  meaning  that  x’Qx  >  0 for  any  nonzero 
(n  X  1) vector  x.  We  also  assume  that  {2 is symmetric,  so  that  1;  =  1);.. 

The  matrix  2 can  be transformed  into  a matrix  with  zero  in the  (2, 1) position 
by multiplying  the  first  row  of  2 by 1,,0;;'  and  subtracting  the  resulting  row  from 
the  second.  A  zero  can  be  put  in  the  (3,  1) position  by multiplying  the  first  row 
by 3,0;;'  and  subtracting  the  resulting  row  from  the  third.  We  proceed  in  this 
fashion  down  the  first  column.  This  set  of operations  can  be  summarized  as  pre- 
multiplying 

by the  following  matrix: 

| 

1 
-0,,95! 

00:-:-:  0 
1  0.---  0 

=o  OT  OU 

[4.4.3] 

=4,,053  0  0  So eh 

1 

This  matrix  always  exists,  provided  that  1,,  #  0.  This  is ensured  in  the  present 
case,  because  {2,, is equal toe;Me,,  wheree;  =  [1  0  O- 
-  0]. Since  1  is positive 
definite,  e;}Qe,  must  be  greater  than  zero. 

. 
is premultiplied  by E, and  postmultiplied  by E; the  result  is 

- 

When 

where 

hy, 

0 

0 

rss 

0 

0 

Riv  h,;  ae 

A>, 

H  = 

0  hy  hy;  —  hs, 

0  h,2  hy 

Ran 

E,QE;  =  H, 

[4.4.4] 

| 

[4.4.5] 

N,; 

0 

0 

= 

0 

F 

0 

. 

a 

0 

Ny  —  1,,.05'0), 

Ns.  —  05,075'N), 

Oy3  —  1,NF'A,  +++  AY  -  4,05 ',, 
M3;  —  25,07'0,, 

+++  0,  -  93,05;'0,, 

0 

Q,2  —  2,,07', 

0,3  —  A NG's  ++:  2,  -  ,,075'0,, 

__ 
We  next  proceed  in exactly  the same  way with  the second  column  of H. The 
approach  now  will  be to multiply  the second  row of H by h,,h5"  and subtract  the 
result  from  the  third  row.  Similarly,  we  multiply  the  second  row  of H by hy h=! 
and  subtract  the  result  from  the  fourth  row,  and  so  on  down  through  the  second 
88  Chapter  4 

Forecasting 

column  of  H.  These  operations  can  be  represented  as  premultiplying  H  by the 
following  matrix: 

1 
0 

0 
1 

0 
0  --- 

EF, =| 9  —Ahkp'  1  +++ 

0 
0 
OF, 

“2 

)  a 

or 

[4.4.6] 

This  matrix  always  exists  provided  that  h,,  +  0. But  h,, can  be calculated  as 
hy  =  e;He,,  where  e,  =  [0  1  0--- 
0]. Moreover,  H =  E,QEj,  where  2  is 
positive  definite  and  E, is given  by [4.4.3].  Since  E, is lower  triangular,  its deter- 
minaat  is the  product  of_terms  along  the  principal  diagonal,  which  are  all  unity. 
Fhus E, is nonsingular,  meaning  that  H  =  E,{ME; is positive  definite  and  so  h,, = 

 esHe,’ must  be strictly  positive.  Thus  the matrix  in [4.4.6] can  always  be calculated. 
ItHis premultiphed  by the matrix  in [4.4.6]  and Poet by the  trans- 

_ 

pose,  the  result is 

~  pltss, - — aiid pes 

ath ek. ae? 

- 2 a ste 

ith fo  ragga a4 

ae _— ss — sa SS 

ait abe “Asin, sins 1 
eeemneotae  ts past  lar  k33 is positive.  Proceeding  through each of che  columns 
approach,  we see that for any positive definite symmetric  matrix  1 
ricec  E,, in. a re ee  eS 

Ss ‘Positive dein and  since  E; is  nonsingular, 

If [4.4.7]  is premultiplied  by A  and  postmultiplied  by A’,  the  result  is 

Q  =  ADA’. 

[4.4.9] 

Recall  that  E,  represents  the  operation  of multiplying  the  first  row  of 2  by 
certain  numbers  and  subtracting  the  results  from  each  of the subsequent  rows. Its 
inverse  E;!  undoes  this  operation,  which  would  be  achieved  by multiplying  the 
first  row  by these  same  numbers  and  adding  the  results  to  the  subsequent  rows. 
Thus 

por 

ea® 

0 

01) Prt  i  MC  i 
BE) = | 93,97'  9  1  --: 

OF, 

0,95:  00  :-: 

1 

[4.4.10] 

as  may  be verified  directly  by multiplying  [4.4.3]  by [4.4.10]  to  obtain  the  identity 
matrix.  Similarly, 

1 

0 

0 

1 

Pes  @ 

QO 

: 

0 

E*  — 

0  h3,hz' 

1 

0  ; 

0.  :hghgh 

te"  <4 

and  so  on.  Because  of this  special  structure,  the  series  of multiplications  in [4.4.8] 
turns  out  to  be  trivial  to  carry  out: 

1 

N,,9;;} 

0 

1 

0 

0 

°  tz 

A  =  | 25,075" 

 hs2hp" 

0 

0 

0 

[4.4.11] 

OMG!  Ayghy  Keke  <- + 1 

That  is, the jth column  of A is just the jth column  of E;"’. 

We  should  emphasize  that  the  simplicity  of carrying  out  these  matrix  multi- 
plications  is due  not  just  to. the  special  structure  of the  E;'  matrices  but  also  to 
the  order  in  which  they  are  multiplied.  For  example,  A~'  = 
E,_,E,_,°--  E, 
cannot  be  calculated  simply  by using  the  jth column  of E; for  the  jth column  of 
oes 

Since  the  matrix  A in [4.4.11]  is lower  triangular  with  1s along  the  principal 

diagonal,  expression  [4.4.9]  is the  triangular  factorization  of Q. 

For  illustration,  the  triangular  factorization  2 =  ADA'  of a (2  x  2) matrix 

is 

Teed 

OM,  MO, 

,05'  1 
j  i  0 

90  Chapter  4  | Forecasting 

0  Oy»  —  1,05'N,.] 

| 

[4.4.12] 

: 7 
(0 

1 

F 

while  that  of a  (3  x  3) matrix  is 

“= *- “ 

M5,  - 7: 

x 

,, 
0 

0 

0 

0 
ai  Qe 
3,2 
hzhz'  1 
0 
0 

O 
hy 

[4.4.13] 

1  97'0,  07°; 
0 
hz'hz; 

1 

O 

hg3  —  Azghy"ho; 

0 

0 

l 

where  fz  =  (Qh.  —  N,N  75'Di2),  hs3  =  (N33  —  15,075'N,3),  and  h,,  =  hy  = 
(Q23  —  10751043). 

Uniqueness  of the  Triangular  Factorization 

We  next  establish  that  the  triangular  factorization  is unique.  Suppose  that 

2  —  A,D,A;  =  A,D.A,, 

[4.4.14] 

where  A, and  A, are  both  lower  triangular  with  1s along the  principal  diagonal  and 
D,  and  D,  are  both  diagonal  with  positive  entries  along  the  ae  diagonal. 
Then  all  the  matrices  have inverses.  Premultiplying  [4.4.14]  pe D; ‘A; ' and  post- 
multiplying  by [A3]~!  yields 

A;[A3]~!  =  Dy 'Ay AD, 

[4.4.15] 

Since  A;  is  upper  triangular  with  1s  along  the  principal  diagonal,  [A;]~!  must 
likewise  be  upper  triangular  with  1s along  the  principal  diagonal.  Since  A; is also 
of this  form,  the  left  side  of [4.4.15]  is upper  triangular  with  1s along  the principal 
diagonal.  By similar  reasoning,  the  right  side  of [4.4.15]  must  be lower  triangular. 
The  only  way  an  upper  triangular  matrix  can  equal  a  lower  triangular  matrix  is if 
all  the  off-diagonal  terms  are  zero.  Moreover,  since  the  diagonal  entries  on  the 

_  left  side  of [4.4.15]  are  all  unity,  this  matrix  must  be  the  identity  matrix: 

A‘[Aj]-! = I,. 
eile Ste  don  by A; establishes  that A;  =  A).  Premultiplying  [4.4.14]  by A~’ 
and  postmultiplying  by [A’]~!  then  yields D,  =  D,. 

| 

The  Cholesky  Factorization 

A  closely  related  factorization  of a  symmetric  positive  definite  matrix  Q is 
obtained  as  follows.  Define  D'” to be the  (n x  n) diagonal  matrix  whose  diagonal 
entries  are  the  square  roots  of the  corresponding  elements  of the  matrix  D in the 
triangular  factorization: 

Wag 
Gide 
0 

20 

0 
Way  = 0 

0  Vay * 

0 
0 
0 

pz=] 

0 

0 

0 

haat  Vdan 

Since  the  matrix  D  is  unique  and  has  strictly  positive  diagonal  entries,  the  matrix 
Dp’? exists and is  unique.  Then  the  triangular  factorization  can  be written 

Q  =  AD'2p'7A’  =  AD'?(AD"”)’ 

4.4,  Factorization  of a Positive  Definite  Symmetric Matrix 

91 

Or 

where 

QO  =  PP’, 

[4.4.16] 

P  =  AD’? 

rT  2m 
ee 
oe 

= 

a3, 

a32 

1 

Olli 

0 

0 

wa. 

0 

Vd, 

0 

0 

0 

V ds, 

0 

O 

0 

0 

0 

- 

+ 

Gnxp 

*** 

] 

0 

0 

0 

' 

Vv * il 

Any 

Q@n2 
Vay 
ay,V di, 

0 

Vado 

0 

0 

=  | @,Vd,  a2Vdnp 

Vd, 

0 

0 
0 

ee 

**: 

Ani 

Vi,  AngVd22 

An3V  433  °°  * 
Expression  [4.4.16]  is known  as  the  Cholesky factorization  of 12.  Note  that  P, like 
A,  is lower  triangular,  though  whereas  A has  1s along  the  principal  diagonal,  the 
Cholesky  factor  has  the  square  roots  of  the elements  of  D  along  the  principal 
diagonal. 

Van 

4.5.  Updating  a  Linear  Projection 

Triangular  Factorization  of a  Second-Moment  Matrix 
and  Linear  Projection 

Let  Y  =  (Y,,  Y2,...,  Y,)’  be an  (n x  1) vector  of random  variables  whose 

second-momient  matrix  is given  by 

, 

Q  =  E(YY’). 

Let  9.  =  ADA’  be  the  triangular  factorization  of 2, and  define 

: 

Y=a'Y. 

[4.5.1] 

[4.5.2] 

The  second-moment  matrix  of these  transformed  variables  is given  by 

E(YY')  =  E(A~'YY'(A‘]-!)  =  A-TE(YY')[A‘]-?. 
[4.5.3] 
Substituting  [4.5.1]  into  [4.5.3],  the  second-moment  matrix  of  Y is  seen  to  be 
diagonal:  | 

E(YY’)  =  A-'Q{A’]-!  =  A-!ADA'[A’]-!  =  D. 

[4.5.4] 

That  is, 

Jd, 
‘E(Y,Y,)  ays" 

fori  =  j 
_ 
(YY)  ‘6  fori  + j. 

[4.5.5] 

Thus  the  Y’s  form a series  of random  variables  that  are  uncorrelated  with 

_ 
one  another.*  To see  the  implication  of this,  premultiply  [4.5.2]  by A: 

AY =  Y. 
[4.5.6] 
“We  will  use  “Y, and  Y, are  uncorrelated”  to mean  “E(Y,Y,)  =  0.” The terminology  will be correct 

| 

if Y, and  Y, have  zero  means  or  if a constant  term  is included  in the  linear  projection. 
92  Chapter 4 | Forecasting 

Expression  [4.4.11]  can  be  used  to  write  out  [4.5.6]  explicitly  as 

l 

0 

0,,07' 
5,071  h32hz' 

1 

0 

0 
1 

ses 

QO 

Y, 

Y, 

.  @ 

“~~ 
oe 

Y, 
Y,  | =  | Ys  |. 

Y, 

[4.5.7] 

0,,07'  h,zhz' 

K 3k 33"  whet, 
The  first  equation  in  [4.5.7]  states  that 

1 

3 

iP 

so  the  first  elements  of the  vectors  Y and  Y represent  the  same  random  variable. 

The  second  equation  in [4.5.7]  asserts  that 

yp =  ¥33 

[4.5.8] 

or,  using  [4.5.8], 

1,,97'Y,  +  ¥, =  Y2, 

Y, =  Y,  —  0,,07'Y,  =  Y,  -  aY,, 

[4.5.9] 

where  we  have  defined  a  =  2,,0;;!.  The  fact  that  Y, is  uncorrelated  with  Y, 
implies 

E(Y,Y,)  =  E[(Y.  —  aY,)Y,]  =  0. 

[4.5.10] 

But,  recalling  [4.1.10],  the  valae  of a  that  satisfies  [4.5.10]  is defined  as  the  coef- 
ficient  of the  linear  projection  of  Y, on  Y,. Thus  the  triangular  factorization  of 2 
can  be  used  to  infer  that  the  coefficient  of a  linear  projection  of Y, on  Y, is given 
by a  =  9,,Nj;’,  confirming  the earlier  result  [4.1.13].  In general,  the  row  i, column 
1 entry  of A is 0,,0;,’,  which  is the  coefficient  from  a  linear  projection  of  Y; on 
Y;. 

Since  Y, has  the  interpretation  as  the  residual  from  a projection  of Y, on  Y,, 

from  [4.5.5]  d,, gives  the  MSE  of this  projection: 

E(¥3)  = 

=  Dy — 207'D)2. 

This  confirms  the formula  for the MSE of a linear  projection  derived  earlier  (equa- 
tion  [4.1.15]}). 

| 

The  third  equation  in [4.5.7]  states  that 

Substituting  in from  [4.5.8]  and  [4.5.9]  and  rearranging, 

0,5  'Y, +  hphz'Y,  +  ¥; =  Y3. 

[4.5.11] 
Y¥; =  Y3  —  94,N51Y,  —  hyhy(Y.  —  M,07'Y)). 
Thus Y, is  the  residual  from  subtracting a  particular  linear, combination  of Y, and 
Y, from  Y;. From  [4.5.5],  this  residual is  uncorrelated  with either  Y, or  Y,: 

E[¥;  —  93,975'Y,  —  Ayhn(Y2  -  0,,07'Y,)]Y;  “  0" \  ‘for j =  Lor  2. 
Thus  this  residual  is uncorrelated  with  either  Y, or  Y2, me  ing that  Y; has  the 
interpretation  as the residual  from  a linear projection of Y, on 1  Y, and  Y,. According 
to [4.5.11],  the linear  projection  is given by 

[4.5.12] 
P(Y3|Y2,Y,)  =  15,05'Y,  +  hyhp(Y2  —  MM7'Y)). 
The MSE of the linear  projection is the variance  of Y3, which  from  [4.5.5] is given 
by dss: 

: 

E[Y;  —  P(Y¥s|¥2,¥1)P  =  hss  —  Asahaa'has- 

[4.5.13] 

4.5.  Updating  a Linear  Projection 

93 

Expression  [4.5.12]  gives  a  convenient  formula  for  updating  a  linear  projec- 
tion.  Suppose  we  are  interested  in  forecasting  the  value  of  Y3.  Let  Y,  be  some 
initial  information  on  which  this  forecast  might  be  based.  A  forecast  of  Y, on  the 
basis  of  Y, alone  takes  the  form 

P(Y3|¥1)  =  05,07'%. 
Let  Y, represent  some  new  information  with  which  we  could  update  this  forecast. 
If we  were  asked  to  guess  the  magnitude  of this  second  variable  on  the  basis  of  Y, 
alone,  the  answer  would  be 

Equation  [4.5.12]  states  that 

P(YJY,)  =  9,,07'%4. 

P(Y3IY2,Y,)  =  P(Y3I¥1)  +  haha'[Y.  —  P(YY,)]. 

[4.5.14] 

We can  thus  optimally  update  the  initial  forecast  P(Y,|Y,)  by adding  to it a multiple 
(h3h3!')  of the  unanticipated  component  of the  new  information  [Y,  —  P(Y,|Y,)]. 
This  multiple  (43,h3')  can  also  be  interpreted  as  the  coefficient  on  Y; in a  linear 
projection  of  Y; on  Y, and  Y,. 

To understand  the nature  of the multiplier  (h,,h3'),  define  the  (n x  1) vector 

¥(1) by 

¥(1)  =  E,Y, 

[4.5.15] 

where  E, is the  matrix  given  in  [4.4.3].  Notice  that  the  second-moment  matrix  of 
Y(1)  is given  by 

E(¥(1)[¥())'}  =  EXE,YY'E;}  =  EOE}. 
But  from  [4.4.4]  this  is just the  matrix.H.  Thus  H  has  the  interpretation  as  the 
second-moment  matrix  of Y(1).  Substituting  [4.4.3]  into  [4.5.15], 

Y, 
Y, -  2,0;;'Y, 
¥(1)  =  | Y3  —  05,07'Y, 

Y,  —  OnOn"Y, 

The  first  element  of ¥(1) is thus  just  Y, itself,  while  the  ith  element  of ¥(1) for 
i =  2,3,...,n  is the  residual  from  a  projection  of  Y; on  Y,.  The  matrix  H is 
thus  the  second-moment  matrix  of the  residuals  from  projections  of each  of the 
variables  on  Yj. In particular,  hz, is the  MSE  from a projection  of Y, on  Y,: 

hy  =  E[Y,  -  P(Y.Y,)P, 
while  h3, is the  expected  product  of this  error  with  the  error  from  a projection  of 

Y; on  Y;: 

hy  =  EX[Y;  —P (Y3¥,)][¥2  7; P(Y,|Y,)}}. 
Thus  equation  [4.5.14]  states  that  a  linear  projection  can  be  updated  using  the 
following  formula: 

p (Y3|¥2,¥,)  =  h (Y3/Y;) 

+  {E[Y;  —  P(Y,IY)Y.  -  PcyY,)) 

x  {E[Y,  -  P(Y21Y,)P}-  «  [Y. —  PCY,IY,)). 

[4.5.16] 

94  Chapter 4  | Forecasting 

For  example,  suppose  that  Y, is a  constant  term,  so  that  P(Y,Y,)  is just  42,  the 
mean  of Y,, while  P(Y,/Y,)  =  u;.  Equation  [4.5.16]  then  states  that 
P(Y3|¥2,1)  =  ws  +  Cov(¥s,  ¥2)-[Var(¥2)]"'-(¥2  —  #2): 

The MSE associated  with this updated linear  projection  can  also  be calculated 
from  the  triangular  factorization.  From  [4.5.5],  the  MSE  from a linear  projection 
of Y, on  Y, and  Y, can  be calculated  from 

E[Y;  —  P(¥3|¥2,Y,)?  = E(¥3) 

=  ds; 

=  h33  —  hyyhZ'ho3. 

In general,  for i >  2, the  coefficient  on  Y, in a linear  projection  of Y; on  Y, 
and  Y, is given by the  ith element  of the  second  column  of the  matrix  A. For any 
i > j, the  coefficients  on  Y; in a  linear  projection  of Y; on  Y;, Y;_,,...-,  Y; is 
given  by the  row  i, column  j element  of A.  The  magnitude  d,, gives  the  MSE  for 
a linear  projection  of  Y; on Y;_,,  Y;-2,.--,  "1 

y 

AYE Exact Finite-Sample Forecasts for an  MA(1) 

a 
E 
$A 

«  y 

. 

' 

< 

; 

lyi  g these results, suppose  that Y, follows an MA(1) 

3  @  where 
=  Pus we. 
x. 
Wee 
eoefficient  2 in  the  grein  Seite 

— 

, 

u  + 
x 

tg 

; 

is  om SS 

a 

s  ; 
e 

d 

~"  ~  at  noise  [ 

a 

T 

ic 

: 

. 

DIOCCS' 

“aL 

4 

J 

4  ae 

- 
{  w 
i? 

2  en  <7 

A 
»  IC 
: 

tefl  oT 

A  or 

‘g S  tr 
1 

F3 
oy 

>  ~~ -  ae 
6) 
Jal 
Valu 
f  >  7  Ti  Pern 

> 

“> 

‘it 
‘oo GOAL  ; 
ith  di 

a” 

~ 

. 
iS 

-na- 

lJ  ¥ 

; 

Sea) 

r 

: Yel(%-0)  Gm  ; 
Q denote  the (n  x n) 
BORE  IO 

variance-c 
OMIcs 

$C 

BF 

= 

ee 

WPM 
24 

ppg 

0 

ve 

9 AS  ns  = 

af  aon 

[4.5.19] 

0 

; 

4 

0 

PHO  Oe 
+  Ot  ee 

1+ 

+  ORD 

To  use  the  triangular  factorization  to  calculate  exact  finite-sample  forecasts, 
recall  that  Y;, the  ith  element  of  Y =  A7'Y,  has the  interpretation  as  the  residual 
from  a  linear  projection  of  Y; on  a  constant  and  its  previous  values: 

P Sy,  oper  tei 

my. 

The  system  of equations  AY  =  Y can  be written  out  explicitly  as 

Fat 

oe. 

—— 

Y, +  Y,=  Y,-4 

O(1  40D ¥ . 
0"  4. 

no 

1 

Il  Y; — pt 

oS 

SSS  Sn gle 8  ae  I  ae ges 
‘1+  67  +  64  4+---  4  g24-)  Nes  Yi 

Y,  —  pt. 

Solving  the  last  equation  for  Y,,, 

¥.~  Beles.  ..7)  oY,  = 

NT his  =  Baya  Yar 

implying 
5 a  ee  2  ee  ae 

Yh 

[4.5.20] 

Oh  +  Gt  OF  to  +  Oro 
1  +  62  +  64  Se  927-1) 

[Yn-1  ~  E(Y,-1¥,=25  Yuan  Ce)  Y,)]. 

The  MSE  of this  forecast  is given  by d,,,:  ’ 

MSE[E(Y,¥,-15Yn-25».  » 

+Y:)) =  0? 

1+  @ +0*+--- + 9m 
1+  67+  64 4+- ++  +  9%n-D 

[4.5.21] 

It is interesting  to  note  the  behavior  of this  optimal  forecast  as  the  number 
of observations  (n) becomes  large.  First,  suppose  that  the  moving  average  repre- 
nha is  invertible  (|@| <  1). In this  case,  as  n >  ©,  the  coefficient  in [4.5.20] 
tends 

to  6: 

O[1  +  62 + OF  +--+  -  +  Grn-2) 
1+  +  O  +  «+.  + lgaend 

—  6, 

while  the  MSE  [4.5.21]  tends  to  o?, the  variance  of the  fundamental  innovation. 
Thus  the  optimal  forecast  for a finite  number  of observations  [4.5.20 
eventually 
tends  toward  the forecast  rule used for an infinite  number of Adieden [4.2.32]. 
96  Chapter 4  | Forecasting 

.5.20] 

.  Alternatively,  the  calculations  that  produced  [4.5.20]  are  equally  valid  for  a 
noninvertible  representation  with  |@| >  1.  In  this  case  the  coefficient  in  [4.5.20] 
tends  toward  @~!: 

BI  +  é2  i  64 

are  os  gr"  —  2)) 

6[1  2  ge  —DI/(1  sl  67) 

1+@+O+---  +  OD  ~ 
Ta 

(1  —  o)/(1  —  6?) 

. (sheath  ates | 

” aie  a 
—Q@-2 
a  G- 8") 
ald | 

=  O7;', 

Thus,  the  coefficient  in  [4.5.20]  tends  to  @~!  in  this  case,  which  is  the  moving 
average  coefficient  associated  with  the  invertible  representation.  The  MSE  [4.5.21] 
tends  to  0767: 

s(l  —  ey  —  6) 
o 
(1  —  6")/(1  —  6) 

a6", 

which  will  be  recognized  from  [3.7.7]  as  the  variance  of the  innovation  associated 
with  the  fundamental  representation.  © 

This  observation  explains  the  use  of  the  expression  ‘‘fundamental”  in  this 

context.  The  fundamental  innovation  e, has  the  property  that 
ee  ¥en 

Hie:  Bevvai¥ 

2 

[4.5.22] 

as  m—  «  where  —  denotes  mean  Square  convergence.  Thus  when  |6| >  1, the 
coefficient  6 in the  approximation  in [4.3.3]  should  be replaced  by 6~'.  When  this 
is done, ‘expression  [4.3.3]  will  approach  the  correct  forecast  as  m  >  ~. 

It is also  instructive  to  consider  the  borderline  case  9  =  1.  The  optimal  finite- 
sample  forecast  for an  MA(1)  process  with  @ =  1 is seen  from  [4.5.20]  to  be given by 

Ber  JY. 2, ;fiz2;-  cates  »¥s)  =pet  SG  Yn  a  EY, 211 ¥5 5s  Y @i9,8~¢3  ,Y,)], 

n=  1 

which,  after  recursive  substitution,  becomes 

|  are 

2  a8)  89  Y;) 

n-1 

+a  (Tn<i  -¥#4)i- 

n—2 
c 
n  a  Fran  on  pe  (oI  5 Me —  a) 

(Y,-2  -—  2) 

+ 

[4.5.23] 

The  MSE  of this  forecast  is given  by [4.5.21]: 

a(n  +  1)/n—  o?. 

Thus  the  variance  of the  forecast  error  again  tends  toward  that  of e,.  Hence  the 
innovation  e, is again  fundamental  for  this  case  in the  sense  of [4.5.22].  Note  the 
contrast  between  the  optimal  forecast  [4.5.23]  and  a  forecast  based  on  a naive 
application  of [4.3.3], 
P, 

# (Yao  —  B) =  (Yn-2 ~  #) +  (Yn-3 —  ») 

Mion  ar  [4.5.24] 

The approximation  [4.3.3] was  derived  under  the assumption  that the moving average 
representation  was  invertible, and the borderline ‘case @ =  1 is not  invertible.  For this 
97 

4.5.  Updating  a  Linear  Projection 

reason  [4.5.24]  does  not  converge  to  the  optimal  forecast  [4.5.23]  as  m  grows  large. 
When 6 = 1, Y, =  »  +  &  +  &-~,  and  [4.5.24] 

can be writtenas  ~ 

led  +  (€,-1  +  E,—2)  i  ee  —  E,-3)  *  bias  +  war 

(1G,  +  60)  =  M+  Enna  +  (— 1)". 
The  difference  between  this  and  Y,,  the  value  being  forecast,  is «,  —  (—1)"€p, 
which  has  MSE  20?  for  all  n.  Thus,  whereas  [4.5.23]  converges  to  the  optimal 
forecast  as  n —>  ©,  [4.5.24]  does  not. 

— 

Block  Triangular  Factorization 

Suppose  we  have  observations  on two  sets  of variables.  The  first set of var- 
iables  is collected  in an  (m,  X  1) vector  Y, and the second  set in an  (m, X  1) vector 
Y,. Their  second-moment  matrix  can  be written in partitioned  form  as 

as [Eww eed : is oh 

~ 

 LE(¥2¥;)   E(¥2Y3) 

OQ,  Qy 

where  ,, is an (m, X m,) matrix, 
matrix  2 is the transpose of the (n. X 7) matrix Q2,.. 

2, is an (nz x na) matrix, and the (my x na) 

We can  put zeros  in the  lower left (Mm, xn  block of a 

txsIR0g 
premultiplying 

2 by the peaks matrix: 

ia 

». 

__ 

~~. i ia. 

r a 

iiw  - —m  ee  j 

HD 

11diisea 

praitiw 

(fh! 

uezoigzs snob  aie 

ayer the results ode.  2 

Gi  es001g 

(Tp)  Mos  108  teeosnl  : 

As in-the earlier case,  D can  be interpreted  as  the  second-moment  matrix  of 

the  vector  Y =  A~'Y, 

Leal 

Y, 

Lovaws 

an [eh 

LY2)’ 

~2,,975'  1,3 

that is, Y, =  Y, and  Y, =  Y, — 2,,0;7)'Y,.  The  ith  element  of Y, is  given  by Y,, 
minus  a  linear combination  of  the  elements  of  Y,.  The  block-diagonality  of 
D implies  that  the  product  of  any  element  of  Y, with  any  element  of  Y,  has 
expectation  zero.  Thus  2,,12;;'  gives the  matrix  of coefficients  associated  with  the 
linear  projection  of the  vector  Y, on  the  vector  Y,, 

as claimed  in [4.1.23].  The  MSE  matrix  associated  with  this  linear  projection  is 

P(Y,JY,)  =  2,,0;'Y,, 

[4.5.27] 

E{{y,  —  P(y,Y,)|[¥.  —  Pcv,J¥,)]’}  =  E(¥2¥3) 

wor! 

,  =D, 

[4.5.28] 

=  Ny  — 

07 'Ny, 

as eet i in fs. ra 

sn a a similarly extend to a  (3  x  3) block 

%, Yaand Ye x Di x 1), and (n x 1) 

of equation  [4.4.13]: Wwo 

29!  wie st  to  2noitosy! 
ata sedis & %  a sok oe iad fae }  teaciowe  sc?  salem  ako  sv 
iF on 1 onl?  doidw  ao  esidaiisy  5d} 
Or  ook  owvie  & ‘alate teas mot  weil  s  sved  of  ive 

base 

- 

[a,.0; fer  7 q  1  I  a  na  ad  3 ol  2?  vinayv  of 

a  Lead 
ie  cal  a el 

ities  ‘sonal BVOD- SOGErey  oii  s45¢ 

0%  DSO  [4.5.29] 

=  95g  05  ya] 

ys 

3,  lo. 1,  © Ha'Hs| 

= 

4 6 

es 

- 

HH; 

3  vat nue  me  bas. , ¥ Ti 

where 

H,,  =  E{[Y;  Fn  P(Y3/Y,)I[Y  ee.  P(y,|Y,)]’}. 

Law  of Iterated  Projections 

Another  useful  result,  the  law  of  iterated  projections,  can  be  inferred  im- 
mediately  from  [4.5.30].  What  happens  if  the  projection  P(Y,|Y,,Y,)  is  itself 
projected  on  Y,?  The  Jaw  of iterated  projections  says  that  this  projection  is equal 
to  the  simple  projection  of  Y; on  Y;: 

[4.5.32] 
P{P(Y3|Y2,¥,)1¥,]  =  P(¥3IY,). 
To  verify  this  claim,  we  need  to  show  that  the  difference  between  P(Y,|Y,,Y;) 
and  P(Y,|Y,)  is uncorrelated  with  Y,.  But  from  [4.5.30],  this  difference  is given 
by 

P(Y,|Y2,¥,)  2,  P(Y3IY,)  *  H3,Hz'[Y2  tT?  P(Y.lY,)], 

which  indeed  is  uncorrelated  with  Y,  by  the  definition  of  the  linear  projection 
P(Y,1Y,). 

4.6.  Optimal  Forecasts  for Gaussian  Processes 

The  forecasting  rules  developed  in this chapter  are  optimal within  the class of linear 
functions  of the  variables  on  which  the  forecast  is based.  For  Gaussian  processes, 
we  can  make  the  stronger  claim  that  as  long as  a  constant  term  is included  among 
the  variables  on  which  the  forecast is  based,  the  optimal  unrestricted  forecast  turns 
out  to  have a linear  form  and  thus is  given  by the  linear  projection. 

To  verify  this,  let  Y, be  an  (mn, X 1) vector  with  mean  p,,  and  Y, an  (n,  X  1) 

vector  with  mean  ja,,  where  the  variance-covariance  matrix  is given  by 

et —  pi)(¥,  —  w)'  EY,  -  w.)(¥2  -  act z  be _ 
E(Y,  —  #2)(¥,  —  #1)’  E(¥,  —  w.)(¥,  —-  H2)' 
_  If ¥Y, and  Y, are  Gaussian,  then  the  joint  probability  density  is 

M,,  2) 

f ¥1.¥(Y 1»  Y2) 

. 

1 

2,  OQ» 
(Qaryors* nr  2,  2» 

~1/2 

[4.6.1] 

xX  exp; 

of ,) [(y:  — #1)’  (¥2  — B)’]  oy 3  “ ”  all} 

=~ 

’ 

a 

' 

Qa, 

il 

a 

12 

as 

yi 

By 

1 
—- 

-1 

The  inverse  of 

is readily  found  by inverting  [4.5.26]: 

Q-!  =  [ADA’-! 

ie [A’]-'D-'A~! 

es ry ie aes 

0 

0 

I,,, 

0 

(2,  “Ss  0,05'0,,)7! 

[4.6.2 

«|  In 

\ 
=-2,,0j;'  I, 

Likewise, the determinant  of  2 can  be found  by taking the determinant  of [4.5.26]: 
|Q|  =  [Al -  [Dj -  [A’. 

100  Chapter 4  | Forecasting 

But A is a lower triangular matrix.  Its determinant is therefore  given by the product 
~ — ite the  principal  diagonal,  all of which  are  unity.  Hence  |A| = 1 and 

os 2,2  a  fn 
Q,,  Ny 

0 

0  2»  -  2,0;'2,, 

[4.6.3] 

Substituting  [4.6.2]  and  [4.6.3]  into  [4.6.1],  the  joint  density  can  be  written 

=  (Oy  -  (A,  -  2,,07'D2l- 

fyyvo(¥  Y2) 

1 

. 

a  (2m)  *nv2 |Q,,|- 1? -  |Q,.  —  2,,0710,,|-'2 

a 

x  exo 3 [Wr > H,)' (Y2  —  p2)’]  es  L 

“oa 

- nef =I 

5 

0 

J tL,  4 i -  |} . 

E 

@  (x - ~  9,05'M,.)"'  :  wae  i nt 
1 

mend 

4). 

LY2  —  Pe 

= 

——- -|Q, - MuAh al 

[4.6.4] 
aparorg  oxizewel)  2302  , soa  SF! 
eT 

ees 

eK RMSA Yo ome  Cb 
bs = mii  » a te stan  $i  est0igKs  Niiose  ett 
PRA  sate ic art Sitti secatie PAA 

Rtas tendinitis 

The  result  of this  division  is 

fvaiv,  (Yal¥1)  oe  eC 

‘- fy  Y2) 

rau’ 
(2m)?  |H] ~ 1/2 exp| -} (y,  —  m)’H~"(y,  -  | 

where 

In  other  words, 

Y,|/Y,  ~  M(m,  H) 

H  =  ,,  -—  2,,0;'0,,. 

[4.6.6] 

[4.6.7] 

Sf m(( +  2,07"  -  #1),  [Ax  -  0,,0'0,). 

We  saw  in  Section  4.1  that the  optimal  unrestricted  forecast  is given  by the 

conditional  expectation.  For  a Gaussian  process,  the optimal  forecast  is thus 

~  E(Y.|Y,)  =  we +  2,07'(1  —  wy). 

On  the  other  hand,  for any.distribution,  the  linear  projection-of  the  vector  Y, on 
a  vector  Y, and  a  constant  term  is given  by 

E(YY,)  =  pw.  +  2,05y,  —  wm). 
Hence,  for a Gaussian  process,  the  linear  projection  gives the  unrestricted  optimal 
forecast. 

4.7.  Sums  of ARMA  Processes 

This  section  explores  the  nature  of  series  that  result  from  adding  two  different 
ARMA  processes  together,  beginning  with  an  instructive  example. 

Sum  of an  MA(1)  Process  Plus  White  Noise 

Suppose  that  a series  X, follows  a zero-mean  MA(1)  process: 

X,  =  u,  +  du,_;, 

[4.7.1] 

where  u, is white  noise: 

E(u,u,-;)  = 

*  for j =  0 
0 
otherwise. 

The  autocovariances  of X, are  thus 

(1 +  &)o2  = forj  =  0 

E(X,X,_))  =  4 603 

0 

forj  =  +1 
otherwise. 

Let  v, indicate  a separate  white  noise  series: 

o  = forj  =  0 
E(v,¥,-;)  =  {¢  otherwise 

102 

Chapter 4  | Forecasting 

[4.7.2] 

[4.7.3] 

Suppose,  furthermore,  that  v  and  u  are  uncorrelated  at  all leads  and  lags 

E(u,v,_;)  =  0 

for  all j, 

implying 

[4.7.4] 
Let an  observed  series  Y, represent  the sum  of the  MA(\)  and  the  white  noise 

E(X,v,_;)  =  0 

for  all j. 

process: 

Ye, 
et 

+ 

ti 

[4.7.5] 

The  question  now  posed  is, What  are  the  time  series  properties  of  Y? 

Clearly,  Y, has mean  zero,  and its autocovariances  can  be deduced  from  [4.7.2] 

sagan [4.7.4]: 

E(Y,Y,_;)  =  E(X,  +  v,(X,-;  +  vs-)) 
=  E(X,X,_;)  + E(v,v,-;) 
rt weit o? 
ma FORA sericea  at  Bins:  si?  tiie  bs 
Shida sy, aie od oat 
Thus, the sum  X, + y, is covariance-stationary,  and its r a 
as are those for an MA(1).  We might n 
one lag, 
beyond 
there ome a zero-mean MA(1)  representation  for Y, 

pad basing alle 22. 

 forj =  0 
for ie “ oe 

vise 

: 

[4.7.6] 

| 

3 

- 

§ 

i.  Meee  mnsstns oat o i fote) 2:9» ae 
bee  aoe a: Poona hrirealar Pg ata a bi 

Se: 

ye a 

For  given  values  of 5, 02, and  a2, two  values  of @ that  satisfy  [4.7.11]  can  be found 
from  the  quadratic  formula: 

»  —  + 8) ieee)  = Ve  ere 

+  VI  + 8) + 

(olan) — 40° 

§? 

2/g2)) 

26 

eee 

If a2 were  equal  to  zero,  the  quadratic  equation  in  [4.7.11]  would  just  be 

60?  —  (1  +  6)0  +  6  =  5(@  —  6)(@  —  6")  =  0, 

[4.7.13] 

whose  solutions  are  6 =  5 and  6 =  5~',  the  moving  average  parameter  for X, from 
the  invertible  and  noninvertible  representations,  respectively.  Figure  4.1  graphs 
equations  [4.7.11]  and  [4.7.13]  as  functions  of 6 assuming  positive  autocorrelation 
for X, (5 >  0).  For  @ >  0 and  a? >  0, equation  [4.7.11]  is everywhere  lower  than 
[4.7.13]  by the  amount  (02/02)6,  implying  that  [4.7.11]  has  two  real  solutions  for 
6, an  invertible  solution  6*  satisfying 

0 <  |a*| <  [4], 

[4.7.14] 

and  a  noninvertible  solution  6* characterized  by 

1 <  |6-"|  <  |6*|. 

Taking  the  values  associated  with  the  invertible  representation  (@*,  a*7),  let 
us  consider  whether  [4.7.7]  could  indeed  characterize  the  data  {Y,}  generated  by 
[4.7.5].  This  would  require 

(1  +  6*L)e,  =  (1 +  SL)u,  +  v,, 

[4.7.15]  | 

or 

e, 

(1  +  O*L)-'[(1  +  SL)u,  +  v,} 
(u,  —  0*u,_,  +  0*2u,_>  —  0*3u,_3  +  -  °°) 

+  S(u,_,  —  0*u,_,  +  0*7u,_,  —  0*3u,_4  +  °  °°) 
+  (vy,  —  8*v,_  y+.  077,22.  —  0°7¥,23  +  +  2+). 

[4.7.16] 

The  series  e, defined  in  [4.7.16]  is a  distributed  lag on  past  values  of u  and  v,  so 
it might  seem  to  possess  a  rich  autocorrelation  structure.  In fact,  it turns  out  to  be 

[4.7.13] 

[4.7.11] 

FIGURE  4.1  Graphs  of equations  [4.7.13]  and  [4.7.11]. 

104  Chapter  4  | Forecasting 

white  noise!  To  see  this,  note  from  [4.7.6]  that  the  autocovariance-generating 
function  of  Y can  be  written 

8y(z)  =  (1  +  dz)o2(1  +  5z~')  +  o?, 

[4.7.17] 

so  that  the  autocovariance-generating  function  of  «,  =  (1  +  6*L)~'Y,  is 

8.(z)  = 

(1  +  6z)o2(1  +  6z~')  +  o? 
(i  +  6*z)(1  +  6*z~1) 

[4.7.18] 

But  6*  and  o*?  were  chosen  so  as  to  make  the  autocovariance-generating  function 
of (1  +  @*L)e,,  namely, 

identical  to  the  right  side  of [4.7.17].  Thus,  [4.7.18]  is simply  equal  to 

(1  +  6*z)o**(1  +  6*z~'), 

&.(z)  =  o*?, 

a  white  noise  series. 

To  summarize,  adding  an  MA(1)  process  to  a  white  noise  series  with  which 
it is uncorrelated  at all leads  and  lags produces  a new  MA(1)  process  characterized 
by [4.7.7]. 

Note  that  the  series  e,  in  [4.7.16]  could  not  be  forecast  as  a  linear  function 
of lagged  «  or  of lagged  Y.  Clearly,  ¢  could  be  forecast,  however,  on  the  basis  of 
lagged  u  or  lagged  v.  The  histories  {u,} and  {v,} contain  more  information  than  {e,} 
or  {Y,}.  The  optimal  forecast  of  Y,,,  on  the  basis  of {Y,,  Y,_;,  . 

.  .} would  be 

EX ial ¥,  Moni 

0" 

&, 

with  associated  mean  squared  error  o*?.  By contrast,  the  optimal  linear  forecast 
of  Y,,,  on  the  basis  of {u,,  u,_1,  . 

-  -} would  be 

,  Vj,  V;-1,  - 

. 

- 

ECY, «st,  Uy)  - 

+  +»  Vey  Ve-1-  -  -) tz du, 

with  associated  mean  squared  error  02  +  o%.  Recalling  from  [4.7.14]  that  |@*| < 
|6|, it appears  from  [4.7:9]  that  (0*?)o0*?  <  5202,  meaning  from  [4.7.8]  that  o?  > 
a2 +  o2. In other  words,  past values  of Y contain  less  information  than  past values 
of u  and  v. 

This  example  can  be  useful  for  thinking  about  the  consequences  of differing 
information  sets.  One  can  always  make a sensible  forecast  on  the  basis  of what 
one  knows,  {Y,,  Y,_1,  . 
.  .}, though  usually  there  is other  information  that  could 
have  helped  more.  An  important  feature  of such  settings  is that  even  though  e,, 
u,,  and  v, are  all  white  noise,  there  are  complicated  correlations  between  these 
white  noise  series. 

Another  point  worth  noting  is that  all  that  can  be  estimated  on  the  basis  of 
{Y,} are  the two  parameters  6* and o*?, whereas  the true  ‘‘structural”  model  [4.7.5] 
has three  parameters  (5, 02, and  02). Thus  the parameters  of the  structural  model 
are  unidentified  in the  sense  in which  econometricians  use  this  term—there  exists 
a  family  of alternative  configurations  of  5, 02,  and  a? with  |6| <  1 that  would 
produce  the  identical  vatue  for the  likelihood  function  of the  observed  data  {Y,}. 
The processes  that were  added  together for this example  both  had mean  zero. 
Adding constant  terms  to the processes  will not change the results  in any interesting 
way—if X, is an MA(1) process  with mean  jy and if v, is white  noise plus a constant 
u,, then  X,  +  v, will  be an  MA(1)  process  with  mean  given by wx  +  s,.  Thus, 
nothing is lost by restricting  the subsequent  discussion to sums of zero-mean  processes. 

4.7.  Sums  of ARMA  Processes 

105 

Adding  Two  Moving  Average  Processes 
Suppose  next  that  X, is a  zero-mean  MA(q)  process: 

X,  =  (1  +  6b  +  6  L?  +  +++  +  6,,.L%)u,  =  5(L)u,, 

with 

Eton)  =  {6°  hers 

—— 

2 

Let  W, be  a  zero-mean  MA(qz2)  process: 

W,  =  (eee  Ky L?  +  +++  +  Kg,L®)v,  =  K(L)v,, 

with 

E(v.v,_,) 

= 

for  j =  0 
. 
(1-1)  ¥  otherwise. 
+ 

Thus,  X has  autocovariances  yk, yX,. 
++ 
autocovariances  y’,  YI", 
W are  uncorrelated  with  each  other  at  all  leads  and  lags: 
for  all j; 

E(X,W,-_;)  =  9 

+  V9, Of the  form  of [3.3.12]  while  W has 
>  yW of the  same  basic  structure.  Assume  that X and 

) 

and  suppose  we  observe 

: 

ws 

Y;=  X,  +  W,: 

Define  g to  be  the  larger  of q,  OF  92: 

q =  max{q;,  42}. 

Then  the jth autocovariance  of Y is given  by 

E(Y,Y,-;)  =  E(%,  +  W,(X,-;  +  We-)) 

E(X,X,-;)  +  E(W.W.-;) 

ayn 2  forj  =  0,  #1,  +2,...,  +@ 
otherwise. 

0 

Thus  the  autocovariances  are  zero  beyond  q lags,  suggesting  that  Y, might  be 
represented  as  an  MA(q)  process. 
What  more  would  we  need  to  show  to  be  fully  convinced  that  Y, is indeed 
an  MA(q)  process?  This  question  can  be  posed  in  terms  of autocovariance-gen- 
erating  functions.  Since 

yf =  yf +  YP 

it follows  that 

S vel = 

jz-= 

DS yiei +  D tel 

jz-= 

j=-= 

But these  are  just the definitions  of the respective  autocovariance-generating  func- 
tions, 

[4.7.19] 
8y(z)  =  &x(z)  +  Bw(z): 
Equation  [4.7.19]  is a quite  general  result—if one  adds  together  two  covariance- 
stationary  processes  that  are  uncorrelated  with each other  at all leads  and lags, the 

106  Chapter  4  | Forecasting 

autocovariance-generating  function  of the  sum  is the  sum  of the  autocovariance- 
generating  functions  of the  individual  series. 

If Y, is to  be expressed  as  an  MA(q)  process, 

Y,=  (1  +  OL  +  01?  +--+  +  O,L%e,  =  L)e, 

with 

E(e,¢,-))  = 

oe” 

0 

§6forj  =  0 

otherwise, 

then  its autocovariance-generating  function  would  be 

8y(z)  =  0(z)6(z~*)o?. 

The question  is thus whether  there  always  exist  values  of (0,, 8,,..  . 
that  [4.7.19]  is satisfied: 

,  8,, 07) such 

[4.7.20] 
A(z) A(z~")o?  =  8(z)8(z~!)ot  +  K«(z)K(z~*)o%. 
It turns  out  that  there  do.  Thus,  the conjecture  turns  out  to  be correct  that if two 
moving average  processes  that  are  uncorrelated  with each  other  at  all leads  and 
lags are  added  together,  the result  is  a new  moving  sine 2 ‘gre whose  order 
two series: 
te the larger of the order of the original 

“arany 
MA(q;)  +  MA(q,) = MA(max{q;,  92}). 
A proof of this assertion, along with  a constructive  algorithm  for achieving the  — 
factorization  in [4.7.20],  will  be provided in Chapter 13. 

) 

Adding Two Autoregressive Processes 
_  Suppose now  that X, and  W, are two AR(1) processes: 
of 

4  on thereat  tutocbness” 
on 
ABS  ay 3 
om,  wil  be  expr 
. 
*  a =  pL)W,  =  ye 
where  u, and  v, are “on aos ated with v, for all ¢ ar 

tL Wjat tk  re  =  (ie  j 

me 

pops! 

: 

h

e

t

of, 122} 

ae ; 

| 

eee mre’ f PLU) 
2 2 

e | 

: 

a 

SE 
eties ‘of pases? e =  sing aptiow 

i 

os 

2  Ti  im 

4  as: s ¢ f it tH os © 

3 

. 

rons  ia: By oe er) 

Gye 

: 

nt  Ma eS -— x the: x ot 

 
and  similarly,  [4.7.23]  could  be  multiplied  by (1  —  mL): 

(1  —  wL)(1  —  pL)W,  =  (1  —  wL)v,. 

[4.7.25] 

Adding  [4.7.24]  to  [4.7.25]  produces 

(1  —  pL)(1  —  wL)(X,  +  W,)  =  (1  —  pL)u,  +  (1  —  aL)y,. 
From  [4.7.21],  the  right  side  of  [4.7.26]  has  an  MA(1)  representation.  Thus,  we 
could  write 

[4.7.26] 

(1  —  o,L  -  ¢,L’)Y,  =  (1  +  OL)e,, 

where 

and 

In  other  words, 

(1  —  $L  —  ¢L7)  =  (1  —  pL)(1  —  aL) 

(1  +  6L)e,  =  (1  —  pL)u,  +  (1  —  zL)v,. 

AR(1)  +  AR(1)  =  ARMA(2,  1). 

[4.7.27] 

In  general,  adding  an  AR(p,). process 

mL)X,  =  u,, 

to  an  AR(p,)  process  with  which  it is uncorrelated  at  all  leads  and  lags, 

produces  an  ARMA(p,  +  p2,  max{p,,  p2}) process, 

A(L)W,  =  v,, 

where 

and 

P(L)Y,  =  AL)e,, 

$(L)  =  m(L)p(L) 

O(L)e,  =  p(L)u,+  m(L)y,. 

4.8.  Wold’s  Decomposition  and  the  Box-Jenkins 
Modeling  Philosophy 

Wold’s  Decomposition 

All  of  the  covariance-stationary  processes  considered  in  Chapter  3 can  be 

written  in the  form 

Y,=p+  2, WE, — js 

[4.8.1] 

where  e,  is the  white  noise  error  one  would  make  in forecasting  Y, as  a  linear 
function  of lagged  Y. and  where  2j_ 947 <  ©  with 

=  1. 

One  might  think  that  we were  able  to  write  all these  processes  in the  form 
of  [4.8.1]  because  the  discussion  was  restricted  to  a convenient  class  of models. 
However,  the  following  result  establishes  that  the  representation  [4.8.1]  is in fact 
fundamental  for  any  covariance-stationary  time  series. 

108  Chapter  4  | Forecasting 

Proposition  4.1; 
process  Y, can  be  represented  in  the form 

(Wold’s  decomposition).  Any  zero-mean  covariance-stationary 

Rite  > wees  +  tt, 

j=0 

[4.8.2] 

where  Yo =  1 and  So?  <  ~.  The  term  «,  is  white  noise  and  represents  the  error 
made  in  forecasting  Y, on  the  basis  of a  linear  function  of lagged  Y: 

e.™  ¥,  ~(  2A i Vi;  2»), 
[4.8.3] 
The  value  of «,  is  uncorrelated  with  e,_ j  for  any  j,  though  x,  can  be  predicted 
arbitrarily  well from a linear  function  of past  values  of Y: 

K,  =  E(KlY,_1,  Re  a  .  sp 

The  term  x, is called  the  linearly  deterministic  component  of Y,, while  2-0 WEr-; 
is called  the  linearly  indeterministic  component.  If x, =  0, then  the  process  is called 
purely  linearly  indeterministic. 

This  proposition  was  first  proved  by Wold  (1938).°  The  proposition  relies  on 
stable  second  moments  of Y but  makes  no  use  of higher  moments.  It thus  describes 
only  optimal  linear  forecasts  of  Y. 

Finding  the  Wold  representation  in principle  requires  fitting  an  infinite  num- 
ber  of parameters  (y,,  y,  . 
.  .) to  the  data.  With  a  finite  number  of observations 
on(Y;,  Y2,...  ,  Y7), this will never  be possible.  As a practical  matter,  we  therefore 
need  to  make  some  additional  assumptions  about  the  nature  of (Y,,  %,  ...).  A 
typical  assumption  in Chapter  3 was  that  w(L) can  be expressed  as  the  ratio  of two 
finite-order  polynomials: 

m2 
Ap  =  PO a 
j=0 

CL) 

1  +  6,L  +  GFAGae tt  Oh 
yn  i  Gel? 
shimdrlami  dil? 

ne ee  [4.8.4] 

Another  approach,  based  on  the  presumed  “‘smoothness”  of the  population  spec- 
trum,  will  be  explored  in  Chapter  6. 

The  Box-Jenkins  Modeling  Philosophy 

Many  forecasters  are  persuaded  of the  benefits  of parsimony,  or  using as  few 
parameters  as  possible.  Box  and  Jenkins  (1976)  have  been  influential  advocates  of 
this view.  They noted  that  in practice,  analysts  end  up replacing  the true  operators 
6(L) and  $(L) with  estimates  6(L) and  #(L) based  on  the data.  The  more  param- 
eters  to  estimate,  the  more  room  there  is to  go wrong. 

Although  complicated  models  can  track  the  data  very  well  over  the  historical 
period  for which  parameters  are  estimated,  they often  perform  poorly  when  used  for 
out-of-sample  forecasting.  For  example,  the  1960s  saw  the  development  of a  number 
of large macroeconometric  models  purporting  to describe  the economy  using hundreds 
of macroeconomic  variables  and equations.  Part of the disillusionment  with such efforts 
was  the  discovery  that  univariate  ARMA  models  with  small  values  of p or  q often 
produced  better  forecasts  than  the  big models  (see for example  Nelson,  1972).’  As 
we  shall  see  in later  chapters,  large  size  alone  was  hardly  the  only liability  of these 
large-scale  macroeconometric  models.  Even  so,  the claim  that simpler models  provide 
more  robust  forecasts  has  a great  many  believers  across  disciplines. 

°See Sargent (1987, pp.  286-90)  for a nice  sketch  of the  intuition  behind  this  result. 
7For  more recent pessimistic  evidence  about  current  large-scale  models,  see  Ashley  (1988). 

4.8.  Wold’s  Decomposition  and the Box-Jenkins  Modeling  Philosophy 

109 

The  approach  to  forecasting  advocated  by  Box  and  Jenkins  can  be  broken 

down  into  four  steps: 

(1)  Transform  the  data,  if  necessary,  so  that  the  assumption  of  covariance- 

stationarity  is a  reasonable  one. 

(2)  Make  an  initial  guess  of small  values  for p and  g for  an  ARMA(p,  q) model 

that  might  describe  the  transformed  series. 
(3)  Estimate  the  parameters  in $(L)  and  @(L). 
(4)  Perform  diagnostic  analysis  to  confirm  that  the  model  is  indeed  consistent 

with  the  observed  features  of the  data. 

The  first  step,  selecting  a  suitable  transformation  of the  data,  is discussed  in 
Chapter  15.  For  now  we  merely  remark  that  for  economic  series  that  grow  over 
time,  many  researchers  use  the  change  in the  natural  logarithm  of the  raw  data. 
For  example,  if X, is the  level  of real  GNP  in year  ¢, then 

Y, =  log X,  —  log X,_, 

[4.8.5] 

might  be  the  variable  that  an  ARMA  model  purports  to  describe. 

The  third  and  fourth  steps,  estimation  and diagnostic  testing,  will be discussed 
in Chapters  5 and  14.  Analysis  of seasonal  dynamics  can  also  be an  important  part 
of step  2 of the  procedure;  this  is briefly  discussed  in Section  6.4.  The  remainder 
of this  section  is devoted  to  an  exposition  of the  second  step  in  the  Box-Jenkins 
procedure  on  nonseasonal  data,  namely,  selecting  candidate  values  for p and  g.® 

Sample  Autocorrelations 

An  important  part  of this  selection  procedure  is to form  an  estimate  6, of the 

population  autocorrelation  p;.  Recall  that  p; was  defined  as 

where 

pj =  ¥;/Yo 

y,  =  E(Y,  —  »)(Y,-;  —  o). 

A  natural  estimate  of  the  population  autocorrelation  p; is provided  by the 

corresponding  sample  moments: 

where 

Pj  -  4;!Vo 

r 
2 (=  WO-7-  I) 

1 
‘i 

forj  =  0,1,2,.-.,T-1  [4.8.6] 

1  Fi 

i 

[4.8.7] 

Note  that  even  though  only  T  —  j observations  are  used  to  construct  4,, the 
denominator in  [4.8.6] is  T rather  than  T —  j. Thus,  for large j, expression  (4. 8.6] 
shrinks  the estimates  toward  zero,  as  indeed  the population  autocovariances  go to 
zero  as  j —  ©,  assuming  covariance-stationarity.  Also,  the  full  sample  of obser- 
vations  is used to  construct  y. 

“Box  and Jenkins  refer  to this step as “identification”  of the appropriate  model.  We avoid  Box and 

Jenkins’s  terminology,  because  “identification”  has a quite different  meaning  for econometricians. 

110 

Chapter  4  | Forecasting 

Recall  that  if the  data  really  follow  an  MA(q)  process,  then  p, will  be  zero 
for j >  q.  By contrast,  if the  data  follow  an  AR(p)  process,  then  p, will  gradually 
decay  toward  zero  as  a  mixture  of  exponentials  or  damped  sinusoids.  One  guide 
for  distinguishing  between  MA  and  AR  representations,  then,  would  be  the  decay 
Properties  of p,.  Often,  we  are  interested  in  a  quick  assessment  of whether  p,  =  0 
forj=q  +  1,q+2,....  Ifthe  data  were  really  generated  by a Gaussian  MA(q) 
process,  then  the  variance  of the  estimate  A, could  be  approximated  by? 

var(p) =  541 +23 oi}  forj=q+i,qg+2,.... 

l 

[4.8.8] 

Thus,  in particular,  if we  suspect  that  the  data  were  generated  by Gaussian  white 
noise,  then  6; for  any j #  0 should  lie  between  +2/\/T  about  95%  of the  time. 

In general,  if there  is autocorrelation  in the process  that  generated  the original 
data  {Y,},  then  the  estimate  §, will  be  correlated  with  6, for  i #  j.!° Thus  patterns 
in the  estimated  p; may  represent  sampling  error  rather  than  patterns  in the  true  pj). 

Partial  Autocorrelation 

Another  useful  measure  is the  partial  autocorrelation.  The  mth  population 
partial  autocorrelation  (denoted  a”)  is defined  as  the  last  coefficient  in a  linear 
projection  of  Y on  its m  most  recent  values  (equation  [4.3.7]): 

ae =  a” (Y, oa Lt) +  ag”(Y,  5  *  Ht) vo 

ee  OPO  cari  “  }2). 

We  saw  in equation  [4.3.8]  that  the  vector  a”  can  be  calculated  from 

ay” 

af”) 

Yo 

"1 

Tie 

Yo 

Font 

-1 

Se,  ee 

a | 

Y2 

a”) 

Ym-1 

Ym-2 

Sit 

Yo 

Ym 

Recall  that  if the  data  were  really generated  by an  AR(p)  process,  only the p most 
recent  values  of  Y would  be  useful  for  forecasting.  In  this  case,  the  projection 
coefficients  on  Y’s  more  than  p periods  in the  past  are equal  to  zero: 

a™=0 

form  =  p+  1,p  +2,.... 

By contrast,  if the  data  really  were  generated  by an  MA(q)  process  with  q =  1, 
then  the  partial  autocorrelation  a”)  asymptotically  approaches  zero  instead  of 
cutting  off abruptly. 

A natural  estimate  of the mth  partial  autocorrelation  is the last coefficient  in 

an  OLS  regression  of y on  a constant  and  its m  most  recent  values: 

Yio1  =  E+  ayy,  +  ay” y, 1  eae 

gg 

+  é, 

where  é, denotes  the  OLS  regression  residual.  If the  data  were  really generated  by 
an  AR(p)  process,  then  the  sample  estimate  (4{”) would  have  a variance  around  . 
the  true  value  (0) that  could  be approximated  by" 

Var(a”)  =1/T 

form=p+i1,p+2,.... 

_ 

*See Box and Jenkins  (1976, p. 35). 
1A gain, see Box and Jenkins (1976, p. 35). 
"Box  and Jenkins  (1976,  p. 65). 

4.8.  Wold’s  Decomposition  and the Box-Jenkins  Modeling  Philosophy 

111 

Moreover,  if  the  data  were  really  generated  by  an  AR(p)  process,  then  @\’’  and 
a) would  be  asymptotically  independent  for  /, / >  P- 

Example  4.1 
We  illustrate  the  Box-Jenkins  approach  with  seasonally  adjusted  quarterly  data 
on  U.S.  real  GNP  from  1947  through  1988.  The  raw  data  (x,)  were  converted 
to  log  changes  (y,)  as  in  [4.8.5].  Panel  (a)  of  Figure  4.2  plots  the sample 
autocorrelations  of y  (6, for j =  0,  1,...  ,  20),  while  panel  (b)  displays  the 
m  =  0,  1,  ....  20).  Ninety-five 
sample  partial  autocorrelations  (a  for 
percent  confidence  bands  (+2/\/T)  are  plotted  on  both  panels;  for  panel  (a), 
these  are  appropriate  under  the  null  hypothesis  that  the  data  are  really  white 
noise,  whereas  for  panel  (b)  these  are  appropriate  if the  data  are  really  gen- 
erated  by an  AR(p)  process  for  p less  than  m. 

oe: 

O 

~t 

fe) 

10 
Lag  (j) 

20 

(a)  Sample  autocorrelations 

a: 

laces  os 

i  i Hee | et 

piflecp sea! sds Bi Roiselsnoootie  leieg Mis cdi Jo rhemitenieraten  A | 

=1.2 

fe) 

10 
Log  (m) 

20 

(b)  Sample  partial  autocorrelations 

FIGURE  4.2  Sample  autocorrelations  and  partial  autocorrelations  for  U.S.  quar- 
terly real  GNP  growth,  1947:II  to  1988:IV.  Ninety-five  percent  confidence  intervals 
are  plotted  as  + 2/\/T. 

112 

Chapter  4  | Forecasting 

The  first  two  autocorrelations  appear  nonzero,  suggesting  that  q  =  2 
would  be  needed  to  describe  these  data  as  coming  from  a  moving  average 
process.  On  the  other  hand,  the  pattern  of autocorrelations  appears  consistent 
with  the  simple  geometric  decay  of an  AR(1)  process, 

=  ¢d/ 

with  @ =  0.4.  The  partial  autocorrelation  could  also  be  viewed  as  dying  out 
after  one  lag,  also  consistent  with  the  AR(1)  hypothesis.  Thus,  one’s  initial 
guess  for  a parsimonious  model  might  be  that  GNP  growth  follows  an  AR(1) 
process,  with  MA(2)  as  another  possibility  to  be  considered. 

APPENDIX  4.A.  Parallel  Between  OLS  Regression 
and Linear  Projection 

This  appendix  discusses  the sian) between  ordinary  least  squares  regression  aye ingas 
projection. This parallel is developed by introducing  an artificial  random  variable  specifically 
arma  a  el  ie  moments identical to the sample moments  of a particular 
pata  sats t  in some particular sample  on  which  we  intend  to perform  OLS we  have 
sample. 
values  for the explanatory  vector,  denoted x,,  x.,..  . ,  x.  Consider 
an artificial sania  sin random  variable  § that can take  on  only one  of these particular 
— each  with  probability  (1/7): 

P{é  =  x,} = WT 
PE =  it =  V/T 

Ps  P= 0) = WT: 

$ 2  8 

Ss =e 

o 4° 33 — zs 23 

ge . 

5g 

— od 2 5. 

Vis = 

ay = tf 
am ow <8  a = 

; 

5 fl as 

a  7 

Ji Uz 

‘ve  €  Ly  *  & 

co 

Wad 

> 

’ 

;  AS:  thee formas | . Re. i) 

to =  ed =  ype a. 

a 
i  fe  i 
Ahi 

OvVEtta  | mE 2X ry  jaotcs..  Ge  prow 

that  minimizes  [4.A.3]  can  be  found  from  substituting  the  expressions  for the  population 
moments  of the  artificial  random  variables  (equations  [4.A.1]  and  [4.A.2])  into  the  formula 
for  a  linear  projection  (equation  [4.1.13]): 

1Z 
=  (e06¢)1-'6@0)  =  [2S xx] [23 x0} 

oe: 

a 

Thus  the  formula  for  the  OLS  estimate  b in  [4.1.18]  can  be  obtained  as  a  special  case  of 
the  formula  for  the  linear  projection  coefficient  «  in [4.1.13]. 

-  Because  linear  projections  and  OLS  regressions  share  the  same  mathematical  struc- 
ture,  statements  about  one  have a parallel  in  the  other.  This  can  be  a  useful  device  for 
remembering  results  or  confirming  algebra.  For  example,  the  statement  about  population 
moments, 

has  the  sample  analog 

E(Y?)  =  Var(Y)  +  [E(Y))?, 

1s 
22,3) =  gee  2  OY 

1g 

[4.A.4] 

[4.4.5] 

with y =  (1/7)2.,)y,. 

As  a  second  example,  suppose  that  we  estimate  a  series  of  n  OLS  regressions,  with 
y,,  the  dependent  variable  for  the  ith  regression  and  x,  a  (k  x  1) vector  of  explanatory 
»  Yu)’  and  write  the  regression 
variables  common  to  each  regression.  Let  y,  =  ()1,, Ya,  - 
model  as 

- 

- 

for  II’  an  (n  xX  k) matrix  of regression  coefficients.  Then  the  sample  variance-covariance 
matrix  of the  OLS  residuals  can  be  inferred  from  [4.1.24]: 

y,  =  Il’x,  +  u, 

ae 
T a uu,  =  72 va: | =e  +2 yai|[23 xx: E > x3:|,  [4.A.6] 

“T1  = 

ee 

iz 

ps 

where  a, =  y,  —  Il'x, and  the  ith  row  of I’ is given  by 

{Lim ] BE] 

r=1 

APPENDIX  4.B.  Triangular Factorization 
of the  Covariance  Matrix for an  MA(1)  Process 

mate eee establishes  that the triangular  factorization  of Q in [4.5.17]  is given by [4.5.18] 
and  [4.5.19]. 

The  magnitude  o? is simply  a  constant  term  that  will  end  up  multiplying  every  term 
in  the  D  matrix.  Recognizing  this,  we  can  initially  solve  the  factorization  assuming  that 
o”  =  1, and  then  multiply  the  resulting  D matrix  by a? to  obtain  the  result  for  the general 
case.  The  (1,  1) element  of D (ignoring  the  factor  a) is given  by the  (1, 1) element  of 2: 
d,,  =  (1  +  67).  To  put a zero  in the  (2,  1) position  of 2, we  multiply  the  first  row  of N 
by @/(1  +  6?)  and  subtract  the  result  from  the  second;  hence,  a,,  =  @/(1  +  67).  This 
operation  changes  the  (2, 2) element  of 2 to 

ei 

Ne ED  id 

6? 

bey  =F 

1  +  O+ e 
POPE  ee 

To put a zero  in the (3, 2) element  of 2, the second  row of the new  matrix  must  be multiplied 
by @/d,,  and  then  subtracted  from  the  third  row;  hence, 

(1 +  6?) 

sth  of ts  to m  betes  i 

114 

Chapter  4  | Forecasting 

This  changes  the  (3, 3) element  to 

=  (1  +  @)  - 

e(1  +  6°) 

}  1  og 

=e...  — 

oo 

_  (1  +  @  +  6)  +  (1  +  A  +  04)  —  OL  +  62) 
ie  Greil  a eee 
» 
t+ + 
WA. 

Die  Ghee  gt 

In general,  for the ith row, 

d,  = 

1+  @+  O*+--- + 6 
+  QE 

1+  P+  8  +  OS 

To put a zero  in the  (i + 1, é) position,  multiply  by 

4,.,,  =  9d,  = 

Ai+  e+ 

+--+ 

+  Gury 

1+  P+  O+--- +6 

and subtract  from  the  (i +  1)th row,  producing 

Ginsiss  al (1 a  0°)  = 

Ol  +  2+  Ott --- + Oe HY 
1+  @+  +--+ + 6 

en  et  Ot 
. 

BE  oe  th oe  ale  ee 

1+  2+  6+ ---  +  6 

es 

; : 
: 

lt  +  OF  @  450  +  ge] 
1+  0+  6+--- + oF 

ED  te Et Me  aes, 5 Soe 
1+  @+  6+  ---  + OF  ° 

ae 
’ 

+e 

r 

DS  DAs 

— 

hettes 
ae  ©  Scitleie 

P 

npg  es  neipie  of  Which  catimation.  w 

R  - 

al  : 

es 

Z  fi 

7 

‘ 

1 

»  fais  ta 

5  oO.  ' 

tf  at 

= 

' 

— 

ql 

4.6. 
Generalize  Exercise  4.5  to deduce  that  if one  adds  together  an  AR(p)  process  with 
an  MA(q)  process  and  if these  two  processes  are  uncorrelated  with  each  other  at  all  leads 
and  lags,  then  the  result  is an  ARMA(p,  p  +  q) process. 

hase 

Chapter  4 References 

Ashley,  Richard.  1988.  “On  the  Relative  Worth  of  Recent  Macroeconomic’ Forecasts.” 
International  Journal  of Forecasting  4:363-76. 
Box,  George  E.  P.,  and  Gwilym  M.  Jenkins.  1976.  Time  Series  Analysis:  Forecasting  and 
Control, rev.  ed.  San Francisco:  Holden-Day. 
Nelson,  Charles R. 1972.  “The  Prediction  Performance  of the F.R.B.—M.1.T. -PENN Model 
of the  U.S.  Economy.”’  American  Economic  Review  62:902-17. 
Sargent,  Thomas  J.  1987.  Macroeconomic  Theory,  2d ed.  Boston:  Academic  Press. 
Wold,  Herman.  1938  (2d  ed.  1954).  A  Study  in the  Analysis  of Stationary  Time  ir 
Uppsala,  Sweden:  Almgvist  and  Wiksell. 

i  Ve 

. 

e. 

= 
- 

hoon gt 

ity 

; 

7 a  Ny 

ae 

oie 

>  i  yas. Se 

—  de 

Lye 

ery 

re  ffs  wor 
co 

- 
P-  =o  A 

my 

- 
mh 

giis@eesty 
3 
rt 

.wesy  A  PS  9)  saul MOIS  ri  ut DMs 

; 

- 

3 

- 

do 

=  th ie :  estes 

* te  2.  aA  ea 
: 

: 

A 

»  ta  : 

ee  pres  ce  ae Pee  A  niipcupaaaitc 

et, 

ve  ; 

Ea): 

4  ~  4  +  ~% 

= 

-5 

e  foe  =  i 

‘4 

BG 

\ 

4 

be 

pe 

& 

; 

oS 

2 

a 

4  $e 

P< 

1.2 

[2  Sys pa  nae 

aera aketen 

f yeegs 
— 

ae, 

Hg  + its = 9  “S 
+) See 

+  its 

=> 

ca 

a 

F< 
a ae  2 

+ 

* 

EP 

+ 

.  = 

4 

x and the  ith cow of 11" is given by 

5 

Maximum  Likelihood 
Estimation 

5.1.  Introduction 

Consider  an  ARMA  model  of the  form 

| 

Y, =  £ 

oY +  1 boxe a  thet  te oO  My  EO  VDE  fo, 

[5.1.1] 

with e, white noise: 

+  261-2 +t  t  O48,-q, 
a 

| 

eet. 

E  1& 

GOO  Fore  =  F 
IAIOI  ORT 
(ee)  f  otherwise. 
on 
The previous chapters  assumed  that the population  parameters  (c, 6,  .  ..  .  dp 
@,, . .  .  , 9,, 07) were  known  and showed how population  moments  such  as E(Y,Y,_;) 
ee E(Y,.,|¥,,  ¥;-1,  -  -  -) could be calculated as functions  of these 
arameters.  This eboney explores how to catisnate: the: alse: of me d,, 

SA 

pe 

fas> 

= 

IG 

P 

\ 

i 

, 

rvation 

Y. 

5.2.  The  Likelihood  Function  for a  Gaussian 
AR(J1)  Process  -- 

Evaluating  the  Likelihood  Function 

A  Gaussian  AR(1)  process  takes  the  form 

Y,=c+  OY,_,  +  €, 

[5.2.1] 

with  e,  ~  i.i.d.  M(O,  a”).  For  this  case,  the  vector  of population  parameters  to  be 
estimated  consists  of  @ =  (c, ¢, a7)’. 

Consider  the probability  distribution  of Y,, the first  observation  in the  sample. 

From  equations  [3.4.3]  and  [3.4.4]  this  is  a random  variable  with  mean 

and  variance 

E(Y,;)  =  uw  =  cl  —  ) 

EY,  ~  2)  =  OE  —  *)- 
Since  {e,}*.  _.  is  Gaussian,  Y,  is  also  Gaussian.  Hence,  the  density  of  the  first 
observation  takes  the  form 

fy, nis  0)  =  fy,Ons  c,  p,  a7) 

Z 

1 
V20r  Vo2l(1  —  2) 

_  —{y,  —  [c(1  —  ¢)]}* 

[5.2.2] 

207/(1  —  7) 

: 

Next  consider  the distribution  of the second  observation  Y, conditional  on  observing 
Y,  =  y,.  From  [5.2.1], 

Y,  =ct  oY,  I  E>. 

[5.2.3] 

Conditioning  on  Y,  =  y,  means  treating  the  random  variable  Y, as  if it were  the 
deterministic  constant  y,.  For  this  case,  [5.2.3]  gives  Y, as  the  constant  (c +  @y,) 
plus  the  M(0,  a) variable  ¢,.  Hence, 

meaning 

(YY,  =  y:)  ~  N((c  +  dy),  27), 

fray, (valyi;  8)  =  agin]  OF) 

1 

my 

ae 

ae 

[5.2.4] 

The  joint density  of observations  1 and 2 is then  just the  product  of [5.2.4]  and  [5.2.2]: 

fy,.v,Q2  yi;  8)  =  fray  aly 9)-fy,.O3  6). 

Similarly,  the  distribution  of the  third  observation  conditional  on  the  first  two  is 

fray..y,(Val¥2»  Vis  0) 

V2708  cxp| U3 = du] 

1 

<< 

a 

od, 

2 

from  which 

fyy.vy.¥,  V3 Yo,  Yi;  8)  =  fray.  (al¥2. Yis  ®)-fy,.v,  2. y;  9). 

In  general,  the  values  of  Y,,  Y,...,  Y,_,  matter  for  Y, only  through  the 
value  of Y,_,,  and  the  density  of observation  ¢ conditional  on  the  preceding  ¢  —  1 
observations  is given  by 

Pp  yoy%pepe¥  Vedr=15  Vite 

«>  33i.  O) 
=  fry,  Onlyr-15  9) 

99 

" fy rao 

salliggge 

[5.2.5] 

~  Vino  exp  20? 

| 

118 

Chapter  5 | Maximum  Likelihood  Estimation 

The  joint  density  of the  first  ¢ observations  is then 

vv 7  bs ¥,  Yer  Ve -  a.  cx  Mar  @) 

[5.2.6] 

=  fryay,  dys  OP  rc) 

(0-18  Fr -2)  rr  2  0). 

The  likelihood  of the  complete  sample  can  thus  be  calculated  as 

Frive.....¥¥m  Yr-1--->¥3  ®)  =  fy,  8)  TT fray,  Onlye-a @). 

[5.2.7] 

, a 

The  log likelihood  function  (denoted  £(@))  can  be found  by taking  logs of [5.2.7]: 

£(8)  =  log fy,(1;  8)  +  yi log fyjy,_,(vdy.-13  9). 

[5.2.8] 

tT 

Clearly,  the  value  of @ that  maximizes  [5.2.8]  is identical  to  the  value  that 
maximizes  [5.2.7].  However,  Section  5.8  presents  a  number  of useful  results  that 
can  be  calculated  as  a  by-product  of  the  maximization  if one  always  poses  the 
problem  as  maximization  of  the  log likelihood  function  [5.2.8]  rather  than  the 
likelihood. function [5.2.7]. 

- Substituting [5.2.2] and  [5.2.5]  into [5.2.8],  the  log likelihood  for  a  sample 

, 

oi size  T from ‘ Gaussian  AR(1) process  is seen  to  be 
£(0)  = 4 log(2m)  —  4 loglo(1 — $7)] 
2  {  24a) 

eater 

‘, 

meReeth 

ee dds pam  dak (T= 12} tog2my  529 
ter 

= nr} eae  ~ 3 are at 

,-— ¢ -—  oy,-1) 

1318543  OOF 

G2  oi 

fi 8 

stg 

Drs 

f 

rahe! 

e: 

its) 

: 

ae 

e! eitive 
@ notionu! Tes at  ohana ante. S Selene  by A  tl 

ils | fob the Likelihood Function  — 

Tuncti 

ott eS = 

where 

E(Y,  -  py 
E(Y.  —  p)(Y,  -—  pw) 

EY,  ~—  uY,  —  wie’ 

E(Y,  -  w)? 

tt  EAA  u)(Vr  —  Hh) 
‘i,  BO  as — 9) 

E(Yy,  —  #)(¥,  —  ow)  EY,  -  B%2  ~  »)  °° 

E(Y;  —  mw) 

[5.2. 12] 
The eats of this  matrix  correspond  to  autocovariances  of  Y.  Recall  that  the 
jth autocovariance  for  an  AR(1)  process  is given  by 

“Fe 
Hence,  [5.2.12]  can be  written  as 

7A  Saad u)(Y,- “j —  p)  =  o' h/(1  —  $7). 

“[5.2.13} 

[5.2.14] 
: 

| 
oi  16 ee 
B 

ee, 

23291 
to  eee  ee 
ronda 
rAec  zi dini 

green  oT?  ee  moment 
(21.2) 

sae  [oR et chapel 

oT 

iiss 
ta 
where rt ft 
Bene ca  By Ms ee 
ih 

Insitashi 
to 

OBC 

F 

1 

eT  Oe 
ba 

:  ¢7 Ap  ae  (sSigei,? 

sa single draw fr Pratl  . 
Viewing the  observed sa 
the sample ikelihood  c could be wi down  immed ately  from t 
multivariate Gaussian density: 

Sek 

. 

rae 
— wy'-Ky — wh [5.2. 16 
x 
.  treats  HySating  th sc  Tandam,  varzabic-  ¥ ,  ast tx were  the 
Fat tbeis TRE, 9.2 as ves.  ¥, as the constant “nr +  $y) 

stance 
as 

=, 

pl-Hy 

implying  from  [5.2.14]  that 

a)  =  o-7L'L. 

[5.2.20] 

Substituting  [5.2.20]  into  [5.2.17]  results  in 

L£(8)  =  (— T/2) log(2m)  +  slogla~*L'L|  —  (y ~  p)'o~*L'L(y  —  w). 

[5.2.21] 

Define  the  (T  x  1) vector  ¥ to  be 

y=L(y  -  p) 

hog’ 
-—¢ 

Osss.0 
0 
1 

“ 

. 

Teel 

0 
OA. 

0 

0 
0 

0 

yi  ~  wb 
psvanret 

y3  —~  bo 

V  ae  *  (y,  -  M) 
(2  —  B)  -  Oy  - 7) 

} 

=  es: +  M) = “yo sai ae 

ESI  a 

Ae 

cio  5 Wt bbe Hs 

ne He ra om ~e  4:  fy ms sar br 1  =p).  eietaiaieds 
Substituting  uw =  cl =  4), “this becomes 
BIUTRISISO 

Ib  E. 

. 

i 

ae 

Be 

VIR  bn -  alt  -  ON) 

eo  | 

ok 

te 

vin 
; 

a 

>  Suigv  orit  516951  3 
ieyisede  PEN?  St  110 

“ia ae pir bers 

‘ iad e a2  ‘ 
Heys (as!  Gaussian  variable  The 
EF  shor wach of whose clement: 

‘ 

. 

: 

. 

| 

sk 

Expression  [5.2.17]  requires  inverting  a (7  x  7) matrix,  whereas  [5.2.9]  does 
not.  Thus,  expression  [5.2.9]  is clearly  to  be  preferred  for  computations.  It avoids 
inverting  a  (JT  Xx  7) matrix  by writing  Y, as  the  sum  of a  forecast  (c  +  Y,-  ;) and 
a  forecast  error  (¢,).  The  forecast  error  is independent  from  previous  observations 
by construction,  so  the  log  of  its  density  is  simply  added  to  the  log  likelihood  of 
the  preceding  observations.  This  approach  is  known  as a prediction-error  decom- 
position  of the  likelihood  function. 

Exact  Maximum  Likelihood  Estimates  for the  Gaussian 
AR(J)  Process 

The  MLE 6 is  the  value  for  which  [5.2.9]  is  maximized.  In  principle,  this 
requires  differentiating  [5.2.9]  and  setting  the  result  equal  to  zero.  In  practice, 
when  an  attempt  is  made  to  carry  this  out,  the  result  is  a  system  of  nonlinear 
,  yz)  for  which  there  is no  simple  solution  for  @ in 
equations  in @ and  (y,, y2,  . 
terms  of (y,, y2,  - 
,  Yr).  Maximization  of  [5.2.9]  thus  requires  iterative  or  nu- 
merical  procedures  described  in  Section  5.7. 

. 

. 

- 

. 

Conditional  Maximum  Likelihood  Estimates 

An  alternative  to  numerical  maximization  of the  exact  likelihood  function  is 
to  regard  the  value  of y,  as  deterministic  and  maximize  the  likelihood  conditioned 
on  the  first  observation, 

Svr¥ rei  .¥l¥,Or  Yr-as  - 

+ 

>  Yalyis  8)  =  IT fray,.,.Ordyr-15  9), 

[5.2.26] 

Yi 

the  objective  then  being  to  maximize 

log fyn¥r-\..%IN  Or PP 19) + 

= 

YalY13 6) 

=  -[(T  —  1)/2]  log(27)  —  [(T —  1)/2} log(o?) 

[5.2.27] 

o  > (y,  a 

Sa  y;-1)° 

t=2 

207 

of 

Maximization  of [5.2.27]  with  respect  to c and  ¢ is equivalent  to minimization 

: 
2% - 

| 

: 

>  byw, 

re 

which  is achieved  by an  ordinary  least  squares  (OLS)  regression  of y, on  a constant 
and  its own  lagged  value.  The  conditional  maximum  likelihood  estimates  of c and 
¢ are  therefore  given  by 

bed  toagit chee Fad 

Zy,-,  27} 

Lyd 

d 

where  2 denotes  summation  over ¢ =  2,3,...,  T. 

The  conditional  maximum  likelihood  estimate  of the  innovation  variance  is 
found  by differentiating  [5.2.27]  with  respect  to  a? and  setting  the  result  equal  to 
zero: 

. 

=f al  pomdihty  ewitodmegtd  a, 

29?  +3] 

oewene 

adicrs ve 

122  Chapter 5  | Maximum  Likelihood  Estimation 

or 

4?  =  5 } ~£-—  @,  |. 

(=2 

T  - 

] 

In other  words,  the  conditional  MLE  is the  average  squared  residual  from  the  OLS 
regression  [5.2.28]. 

In contrast  to  exact  maximum  likelihood  estimates,  the  conditional  maximum 
likelihood  estimates  are  thus  trivial  to  compute.  Moreover,  if the  sample  size  T is 
sufficiently  large,  the  first  observation  makes  a  negligible  contribution  to  the  total 
likelihood.  The  exact  MLE  and  conditional  MLE  turn  out  to  have  the  same  large- 
sample  distribution,  provided  that  |¢| <  1. And  when  || >  1, the  conditional  MLE 
continues  to  provide  consistent  estimates,  whereas  maximization  of  [5.2.9]  does 
not.  This  is  because  [5.2.9]  is  derived  from  [5.2.2],  which  does  not  accurately 
describe  the  density  of  Y; when  |¢| >  1.  For  these  reasons,  in  most  applications 
the  parameters  of an  autoregression  are  estimated  by OLS  (conditional  maximum 
likelihood)  rather  than  exact  maximum  likelihood. 

5.3.  The  Likelihood  Function  for a  Gaussian 
AR(p)  Process 
This  section  discusses  a  Gaussian  AR(p)  process, 

Her  a  6.  G3  to  Fee  tee 

[5.3.1] 

with  ¢,  ~  i.i.d,  N(O,  a7).  In  this  case,  the  vector  of population  parameters  to  be 
$,  97)’. 
estimated  is ®  =  (c, d,  2,  ..., 

Evaluating  the  Likelihood  Function 

A  combination  of the  two  methods  described  for  the  AR(1)  case  is used  to 
calculate  the  likelihood  function  for  a  sample  of size  T for  an  AR(p)  process.  The 
, Yp) are  collected  in a (p  x  1) vector 
first p observations  in the  sample  (y,, y2,  . 
y,,  which  is viewed  as  the  realization  of a  p-dimensional  Gaussian  variable.  The 
mean  of this  vector  is 4,,  which  denotes  a  (p  x  1) vector  each  of whose  elements 
is given  by 

. 

- 

[5.3.2] 
w=  cKl—  $  -  &  -**:  —  4). 
Let  o?V, denote  the  (p  x  p) variance-covariance  matrix  of (Y,,  Y2,  .-..  Y,): 

E(Y,  -  py 
E(Y,  -  u)(Y,  -  pH) 

E(Y,  —  w)(¥.-  4)  --:  EY,  —  w)(Y,  -  ») 
VECYs  —  py,  —  B) 
++  > 

E(Y,  —  p)? 
' 

o’7V,  = 

E(Y,  >  BY,  -  By.  EO,  =  PO  =  BO 

E(Y,  —  ») 

[5.3.3] 
For  example,  for a  first-order  autoregression  (p  =  1), V, is the  scalar  I/(1  —  o?). 
For  a  general  pth-order  autoregression, 

Yo 

v1 

2 

v1 

Yo 

78 

Nn 

Tl  as 

5" 

Wwp~i 

Yi  ee  TS on & 

a’V,  = 

Yp-1 

Vp -2 

Yp -3 

aie 

Yo 

5.3.  The  Likelihood  Function  for a  Gaussian  AR(p)  Process 

123 

where  y,,  the jth autocovariance  for  an  AR(p)  process,  can  be calculated  using the 
methods  in  Chapter  3.  The  density  of  the  first  p  observations  is then  that  of  a 
N(p,,  O° *V_,) variable: 

ad 

fy,.y,  rey  yp  Vp  ate 

By oo  0) 

=pl2|an 

Ul 

=  2Y  — 

(27)  p  lo  ve 5 ake exp] sity, nag  B,) V, (Y,  ,]  [5.3.4] 
2  hl  ld  oo  Ee  exp| sala, —  Bp)'Vp (Yo —  Bp)  |» 

Ve 

J 

io 

< 

1 

 § 

/ 

= 

-- 

’ 

? 

where  use  has  been made of result  [A.4.8]. 

For  the  remaining  observations  in  the  sample,  (¥p,41  Yp+2>  -  ..  Yn) the 
prediction-error  decomposition can be used.  Conditional  on  the first we we? obser- 
vations, the th gPservation | is ‘Gaussian with mean 

ARID  ep 

FO) 

Dye  py yy  at vo  th Deny  | 

and  variance  ot Only the P most recent  Succ  rations matter for this ancetbiitiol: 
dens  aiipiaiesitiaeaaa 
Hence, for t > p, . 

aac 

DEE 

SE 

; 

fay, et  jeer  te  Me 6) 

2 
: : a  € 2 
Ye aie Ne a 5  Yy=ps 8)  ©  a 

=  Fepy  ¥e 

wi  Gh  ci Say  orc jog  io  1Gteey 
The  likelihood function for the fomicns ig  en a peace: @ .  Yeti 
zi  ois iss 

of}  S26, >  Zh 

.b.ii 

isi 

OW 

3 

— 

2 

ie,  sy fie nel  teh re HioeeePasr  se yi  @) 

; 

rte 

, 

> 

o 

. ; objective 

of bee 2 2289 ( 

fick ii Ope Rg A oh g ning 35] 
mat para bedinszeb  2  ye ows od ae baton  at 

f 

test 

visedg Raia} 

16,  p.  70) showed  that 

Too  | 

prti-j 

vi(p)  =  , i  :  S  $ibes- | forlsisjsp,  [5.3.7] 

° 

=e+i-—f 

where  ¢) =  —1.  Values  of  v/(p)  for  i >  j can  be  inferred  from  the  fact  that  V,' 
is symmetric  (v"(p)  =  v(p)).  For  example,  for  an  AR(1)  process,  V7!  is a  eater 
whose  value is  found  by taking  i =  j =  p  =  1: 

0 

1 

7  > DP  —  > 6,  | =  ($5  —  $7)  =  (1  —  $%). 

k=0 

k=1 

Thus  o?V,  =  o7/(1  —  #7),  which  indeed  reproduces  the  formula  for  the  variance 
of an  AR(1)  process.  For p  =  2, equation  [5.3.7]  implies 

ou  | (1  —  $3)  re 
(1  -  3)  J’ 
s 

-(¢, + did.) 

from  which one readily calculates 
Prete  (oF 
MO. 
Wi 

BF 
Weiz|are  ik  lompyiee ity  =  >). 

ahi 

"Fj 

ot. 

. 

rea 

= 
fd, on oy  -  #3 Bis 

25 

=e 

ae 

= ll 

ay’ oft  30  ane iat902  wy: 10 SILER  2S-  faotewroog 

arts:  : » 

bluow  aust 2ii  ig  4  bre 
AOTIS,Ow 

: 

eo.  B2)'V> Wy, —  Bo)  os 

Jfor-#)] 
cals  moizes) NCR  iae  Oe: A >  w+ +! eh > —— *) lon 
Jt 
eeso0rg  sti  aoys.  .2us} 
e-n08  @ 
tie  ant? Wedd) Sollds 
}  2 cir xem 
oximixe 
er ge meee * tie ak >  t= a  ti 

mat Oy ep) insscies  oe  vn 

(l=)  -& 

rh 

rh 

te 

7 

= — 

loc 

a 

ln snags i {yet gchar  ounenmmtaea 3.9.0) 

=  iyo 

Ae  Reet 

hy Res  3K)  Pee  oe eS E — 

oe  a  - 

= 

— 

; 

; 

The  values  of  c,  ¢,,  @2,---;  ,  that  maadunize  [5.3.9]  are  the  same  as  those  that 

minimize 

1 

>  (y,  =  ¢r  Piy,—  |  =  $2 y,-2  ee 

ee  $,1-p)- 

[5.3.10] 

t=prl 

Thus,  the  conditional  maximum  likelihood  estimates  of  these  parameters  can  be 
obtained  from  an  OLS  regression  of y, on  a constant  and p of its own  lagged  values. 
The  conditional  maximum  likelihood  estimate  of  a?  turns  out  to  be  the  average 
squared  residual  from  this  regression: 

Be  FS  yy  RR  ter  bys -1 

a  bry, -2  pe, 

Six  bpYi-p)- 

The  exact  maximum  likelihood  estimates  and  the  conditional  maximum  likelihood 
estimates  again  have  the  same  large-sample  distribution. 

Maximum  Likelihood  Estimation  for Non-Gaussian  Time 
Series 

We  noted  in  Chapter  4 that  an  OLS  regression  of  a  variable  on  a  constant 
and p of its  lags  would  yield  a  consistent  estimate  of the  coefficients  of the  linear 
projection, 

E(Y\Y,-1.  ¥, 235  Sa 

ae  | ae, 

provided  that  the  process  is ergodic  for second  moments.  This  OLS  regression  also 
maximizes  the  Gaussian  conditional  log likelihood  [5.3.9].  Thus,  even  if the  process 
is  non-Gaussian,  if we  mistakenly  form  a  Gaussian  log  likelihood  function  and 
maximize  it,  the  resulting  estimates  (é, d,, do, MAY  é,) will  provide  consistent 
estimates  of the  population  parameters  in  [5.3.1]. 

An  estimate  that  maximizes  a  misspecified  likelihood  function  (for example, 
an  MLE  calculated  under  the  assumption  of a Gaussian  process  when  the  true  data 
are  non-Gaussian)  is known  as  a  quasi-maximum  likelihood  estimate.  Sometimes, 
as  turns  out  to  be  the  case  here,  quasi-maximum  likelihood  estimation  provides 
consistent  estimates  of the  population  parameters  of interest.  However,  standard 
errors  for  the  estimated  coefficients  that  are  calculated  under  the  Gaussianity 
assumption  need  not  be  correct  if the  true  data  are  non-Gaussian.” 

Alternatively,  if the  raw  data  are  non-Gaussian,  sometimes  a  simple  trans- 
formation  such  as  taking  logs  will  produce  a  Gaussian  time  series.  For  a  positive 
random  variable  Y,,  Box  and  Cox  (1964)  proposed  the  general  class  of transfor- 
mations 

yo  no  for  A  #  0 

r= 

log  Y, 

forA  =  0. 

Ore  approach  is to pick a particular  value  of A and maximize  the likelihood  function 
for  Y;"’ under  the  assumption  that  Y) is  a Gaussian  ARMA  process.  The  value 
of A that  is associated  with  the  highest  value  of the  maximized  likelihood  is taken 
as  the  best  transformation.  However,  Nelson  and  Granger  (1979)  reported  dis- 
couraging  results  from  this  method  in practice. 

*These  points  were  first  raised  by White  (1982)  and  are  discussed  further  in Sections  5.8  and  14.4. 

126  Chapter  5  | Maximum  Likelihood  Estimation 

Li  and  McLeod  (1988)  and  Janacek  and  Swift  (1990)  described  approaches 
to  maximum  likelihood  estimation  for  some  non-Gaussian  ARMA  models.  Martin 
(1981)  discussed  robust  time  series  estimation  for  contaminated  data. 

5.4.  The  Likelihood  Function  for a  Gaussian 
MA(J/)  Process 

Conditional  Likelihood  Function 

Calculation  of the  likelihood  function  for  an  autoregression  turned  out  to  be 
much  simpler  if we  conditioned  on  initial  values  for the  Y’s.  Similarly,  calculation 
of the likelihood  function  for  a moving  average  process  is simpler  if we  condition 
on  initial  values  for  the  e's. 

Consider the Gaussian  MA(1) process 

oe 

YY, =e  +  €  +  68,_, 

[5.4.1] 

with es .  ~  iid, NO. o°) 
be estimated. If 

' oa! &,- 1 aes known  with certainty,  then 

0, 0)’ denote  the population  parameters  to 

Yile,—1 ~ No +  66, D; a) 

GFR ibe  had  i} 

sgh) LEE te = Tae 
Suppose that we knew fi for certain that €) = 0. OP 

4 Ribas S 

oat  -bebrsozi  Sb Je  ii +  ea}  0 ~  N( 

ti ptt 

at 

4 

i 

: 
sts 

: 
Bil 

: 

, 

;  pier 162001  ie Ss Be  oii} Mi ee Boy  Tir S458  | 

2H  eT  Geen  Tiitiiate 

SP 

7 

give 

i  n  IDS servati O  uy 

; 

) 

: 

e  ¢  ati ) 
PP!  ME 

2% 

— =  wes “fa * soiseas doqwreain 
ShUbibsib ISR  ars a  her Bessy a ey Dae  nae 

Nn.  PES  Yt 

, JD.F.L5  a 

ISIS 

7]  7  by 

SILI 

2  + 

“fro 

‘72s 

or 

q 
J 

f 

| 

; 

. 

; 

; 

- 
ii 

ean 

; 

eA 

ephice whet  an mi &: a ” 

Le 

ioe,  a  era 

7407) 

a  $y 
- 

.. 

en Tae 

ers 

The  sample  likelihood  would  then  be  the  product  of  these  individual  densities: 

fener,  ic¥ 

lee  Ye Yr  sine  /¥l€y  =  0;  0) 

=  fyi  ol Diléo  =  0:0)  TL fry  ye  seen 

neynol  Yet  Vente 

> 

+  9¥us  6g  mets): 

The  conditional  log likelihood  is 

£(0)  =  lopifg 2: li  Yileg=O0¥T  Yr-11  +  +>  yiléo  =  0; 8) 

Te 

[5.4.5] 

>  -5 log(2m)  a  = log(o*) ie  sity 

For  a  particular  numerical  value  of 8,  we  thus  calculate  the  sequence  of  e’s 
implied  by the  data  from  [5.4.3].  The  conditional  log likelihood  [5.4.5]  is then  a 
function  of the  sum  of squares  of these  e’s.  Although  it is simple  to  program  this 
iteration  by computer,  the  lég likelihood  is a  fairly  complicated  nonlinear  function 
of »  and  @, so  that  an  analytical  expression  for  the  maximum  likelihood  estimates 
of uw  and  @ is not  readily  calculated.  Hence,  even  the  conditional  maximum  like- 
lihood  estimates  for  an  MA(1)  process  must  be  found  by numerical  optimization. 
Iteration  on  [5.4.3]  from  an  arbitrary  starting  value  of €  will  result  in 

£,.  =  i  JL  =  OY,—1  ie HL) se 6°(y,-2  we)  See 

+  Rady OO  yy  Bye  Le  eo. 

If |6| is substantially  less  than  unity,  the  effect  of imposing  ey  =  0 will  quickly  die 
out  and  the  conditional  likelihood  [5.4.4]  will  give  a  good  approximation  to  the 
unconditional  likelihood  for  a  reasonably  large  sample  size.  By contrast,  if |6| >  1, 
the  consequences  of  imposing  €)  =  0 accumulate  over  time.  The  conditional  ap- 
proach  is not  reasonable  in such a case.  If numerical  optimization  of [5.4.5]  results 
in a  value  of @ that  exceeds  | in absolute  value,  the  results  must  be discarded.  The 
numerical  optimization  should  be  attempted  again  with  the  reciprocal  of 6 used  as 
a  Starting  value  for  the  numerical  search  procedure. 

Exact  Likelihood  Function 

Two  convenient  algorithms  are  available  for  calculating  the  exact  likelihood 
function  for  a Gaussian  MA(1)  process.  One  approach  is to  use  the  Kalman  filter 
discussed  in  Chapter  13.  A  second  approach  uses  the  triangular  factorization  of 
the  variance-covariance  matrix.  The  second  approach  is described  here. 

As  in Section  5.2,  the  observations  on  y can  be collected  in a (T x  1) vector 
y=  (yi,  Yo...  ..  Yr)’  with  mean  pp  =  (p,  pw,  ...,  a)’  and  (T  x  T) variance- 
covariance  matrix 

| 

f="  ECY  (=  pi) 
|The  variance-covariance  matrix  for  T consecutive  draws from  an  MA(1)  process  is 

(1+ 6) 
Pro 
0 

8 

oP 
eT  4-Ug2)  5 mgr) 

ee 
gainer 
(1  +  67)  <:-; 

8 
9 
0 

Q  =  oc’ 

0 

The  likelihood  function  is then 

a 

ee  ae ey 

fly:  8)  =  (27)>  ™|Q|>'?  exp{=Hy  -  p)'QA>"y  -  p)). 

[5.4.6] 

128 

Chapter 5  | Maximum  Likelihood  Estimation 

6 

0 

A  prediction-error  decomposition  of the  likelihood  is provided  from  the  tri- 

angular  factorization  of 2, 

©  =  ADA’, 
[5.4.7] 
where A is the  lower  triangular  matrix  given  in [4.5.18]  and  D  is  the diagonal  matrix 
in n  [4. 5.19].  Substituting  [5.4.7]  into  [5.4.6]  gives 

fly;  ®) = (27)~  7 |ADA'|>  1? 

x  exp[—3(y  —  m)'[A‘]'D-'A-'(y  —  w)]. 

[5.4.8] 

But  A  is a  lower  triangular  matrix  with  Is  along  the  principal  diagonal.  Hence, 
|A] =  1 and 

Further  defining 

|ADA'|  =  |A|-|D)-|A’|  =  |Dj. 

y=A'(y  —  p). 

(5.4.9] 

the  likelihood  [5.4.8]  can  be  written 

flys  8)  =  (27)- ™ [Dl -'?  exp[ —4y’D>  'Y]. 

[5.4.10] 

Notice  that  [5.4.9] implies 

Ay  ¥  = 
The  first row. of this system states  that  Y¥i= y,  —  #,  while  the  th  row  implies  that 
1  +  02  +  OF tee 
‘ 
W=WE  a  ee  oe  ro Sev 

+  QH-M) 

| 
72) 

Sin 

The  vector  y can thus  be calculated  by iterating  on 15. 4.11] for  r =  oe  OF. 
Starting from y,  = y,  —  m.  The variable  y, has the aeePerene  as  the residual 
+  Yr  while the th 
from a linear  projection  of y, on  a constant  and  y, _,,  y,->,--- 
diagonal  element  of D gives  the  MSE  of this  linear  praicchane 
fe  ete? ef.  SS go 

- 
ey  Re 
E(Y;) 

ee SE  a 
1+  @  +  6 
+--+  >  prOtahe  ous  i  ied 7] 

agonal,  its determinant is the product of the terms  along the Principat 
yanse ior  @ 

5 ol  1  348  1s  a 

.ca8 be wPanton  35 i= 7 fies bool  BNA —— a 
roe Se  laa 

] 
* 
oy  & 
of Dik obtained  b 

ia  ssc 

le  ml 

ain 

>< 

7 

: 

. 

ie Bee: KE +yty 

“hep  f  at  el  Ler a 

= 

3% 

, 

| 

At  & 

3  ye  ee  ae 

“1C 

a 

7 

: 

: 

> 

+. 

aoe 

Mth  ade: 

— 

iy  esate ot 

4. 

‘  ~  ‘ 

‘ 

7 

- 

5.5.  The  Likelihood  Function  for a  Gaussian 
MA(q)  Process 

nee: 

Conditional  Likelihood  Function 

For  the  MA(q)  process, 

Vp  ste  +.  &,  trRiikey  F  Oe,  T° 

re  eee, 

[5.5.1] 

a  simple  approach  is to  condition  on  the  penispere  that  the  first  q values  for  e€ 
were  all  zero: 

fg  F  fea = 

=  fas  =  0. 

[5.5.2] 

From these starting  values  we  can  iterate  on 

E,  = y,  =  tp  Gj e,_ 9  eRe  Spe  Sas  0£,-4 

[5.5.3] 

fort  =  1,2,..., 
The  conditional  log likelihood  is then 

T.  Let  €9  denote  the  (q  x  1) vector (€, €_,,  ieee Pecica? 

LO), lebdis ya ee ae + yileo = 0; 6) 

5.5.4 
sites 
F  vog(2: nm) — 3 boat?) - >= 
a 
“=  ee = aN 46} 2 + it ad  wie ee 6h  Hie puts Vi bo ¥ 1OI9y ‘od 

‘ian  +r  a 

ei 
= 

pte 

eee 

; 

in ; x 

OSE  sis fg HS 14 isoml  6 ito 
18 i  gat  és Kaas 8 o Ans a  Tariggeib 
ksinf pit  tO  AE  Set 
288 
a 

= 

155.4] is wi -  efi 

‘if all 

(642 + 0  ih  seaty 

P  mt 

ft  |  ots  es qpoarded. The 

BE on  Path: °oe 

captor a 

The  row  i, column j element  of Q is given  by y,_),  where  y, is the  Ath  autocovan- 
ance  of an  MA(q)  process: 

a  ba ee  Peet 
, 

0 

+  68) 

fork Ost TY 
for  k >  q, 

5.5.7] 
where  6 =  1. Again,  the  exact  likelihood  function  [5.5.5]  can  be  evaluated  using 
either  the  Kalman  filter  of Chapter  13 or  the  triangular  factorization  of 2, 

©  =  ADA’, 

[5.5.8] 

where A  is  the  lower  triangular  matrix  given  by [4.4.11]  and  D  is  the  diagonal 
matrix  given  by [4.4.7]. Note  that  the  band  structure  of 2 in [5.5.6]  makes  A and 
D simple  to calculate.  After  the  first (q  +  1) rows,  all  the  subsequent  entries  in 
the first  column  of  Q  are  already  zero,  so  no  multiple  of  the  first  row  need  be 
added to make  these zero.  Hence, a,  =  0 for i >  q +  1.  Similarly,  beyond  the 
first  (q +  2) rows  of the second  column,  no  multiple  of the  second  row  need  be 
added to make these entries zero,  meaning that a, = 0 fori  > q  +  2. Thus  A  is 
abae | 

band  matrix 

ular 

with a, = Ofori >q +  j: 
0 

base: 

Of 

950  2@den. 

1 

-ie,; 

ma  1 

2  ae 

=v 

= 

> 

y 

43,  4432 

hs 

ct  Oc  10  8], 

ss  ia  tag 

ca 

’  } 

af  2a,  x®  2  0  , 

¢ 
at 
. 

~ 

“ee  « 

0 

0 

0 

é 

0  i 
* 

1 

4 

_ 

= 

. 
.  o- 

eae. 
° 

SHE  boadritiss: 

os 

" 

2 
sadasensbin 
. 

a 

& 

. 

* 

© 

“5 

Ag+  Bax t2  4g +13  = a 

fT 

. 

0  :  gfe eA  Aq +23 os 
— 
cid  ‘7 
’  ee  = 
OS  fax 

eS 

sigel  SS  yon  Ss 

0 

0  z  0 

ca 

. 

Nn 
Lae 9% men Yr soe : ey  “a ~ 

ere  or 
aaa 

[o.%. 

Dy 

lias 

. 

A  aie  pe 

5.6.  The  Likelihood  Function  for a  Gaussian 
ARMA(p,  q) Process 

Conditional  Likelihood  Function 

A  Gaussian  ARMA(p,  q) process  takes  the  form 

f  Cot  HY, -  1  24 2Y,_2  :  eS 

ee 

uy E, 

[5.6.1] 

i  O,&,_,  4  O32, _>  :  ga 

i  9,€1-4> 

where  e,  ~  i.i.d.  N(0,  a).  The  goal  is to  estimate  the  vector  of population  param- 
. 
= 
eters  O  =  (C, is  Oren 

Des 0),  Or,  , 

>  Oo; On) 

ee 

The  approximation  to  the  likelihood  function  for  an  autoregression  condi- 
tioned  on  initial  values  of  the  y’s.  The  approximation  to  the  likelihood  function 
A common 
for  a  moving  average  process  conditioned  on  initial  values  of  the  e’s. 
approximation  to  the  likelihood  function  for  an  ARMA(p,  q) process  conditions 
on  both  y’s and  e's.  ° 

Taking  initial  values  for yy = (yo. Y-1,-  - 

-  »Y-p+i)'  and  €9 =  (€,  €_),.--, 
oo as  given,  the  sequence  {¢,,  E2,-  ++,  €7}  can  be  calculated  from  {y,,  yo, 

.  Y7}  by iterating  on 

Mage  ©  PY, -1  ba  $2y,-2  Si  —  3.25 

[5.6.2] 

fot  fick  Pek  T.  The  conditional  log likelihood  is then 

es  Oe,  EF  2, _2  a 

2  See  06:25 

£(8)  =  log fy,.y;  at  ¥I¥y.ey('T>  Vrs  ss  yil¥o.  Ey;  9) 

{2 

45  log(27 

Ee 
: 
5 log(a*) 

Se 
2  957 

eas 

7 

[5.6.3] 

. 

One  option  is to  set  initial  y’s and  e’s  equal  to  their  expected  values.  That 
Is,  set  y, =  cl  —  6,  —  @  —  **+- = ¢,) fors =  0/1, 0. 4, =p  4  diand set 
-€,  =  Ofors  =  0,  —1,...,  —q  +  1, and  then  proceed  with  the  iteration  in [5.6.2] 
for?  =  1.2, 4 ii sft  Alternatively,  Box  and  Jenkins  (1976,  p. 211)  recommended 
setting  €’s  to  zero  but  y’s equal  to  their  actual  values.  Thus,  iteration  on  [5.6.2] 
is 
started  at  date  ¢ =  p  +  1 with  y,,  y., 
,  y, set  to  the  observed  values  and 

... 

EF 

Eyay  =  SOI 

Bega pe A. 

Then  the  conditional  likelihood  calculated  is 

log  f(y'7,  =o  *  oN yin Irs AT  Ep  FsOe *  Pisa  Ep-q+l  >a  0) 

T  -p 

=- 

? 

log(27) 
” 

— 

T-p 

2 

log(a?) 

&? 
—_ 
ao")  t=p+1  207 

— 

As in  the case  for the  moving  average  processes,  these  approximations  should 

be  mee only  if all  values  of z satisfying 

1  +  0,2  +  0,27  +  He +  Oxt  =  0 

lie  outside  the  unit  circle. 

Alternative  Algorithms 
The simplest  approach  to calculating the exact  likelihood  function  for a Gaus- 
sian  ARMA  process  is to  use  the  Kalman  filter described in Chapter  13.  For more 
132  Chapter 5 | Maximum  Likelihood Estimation 

details  on  exact  and  approximate  maximum  likelihood  estimation  of ARMA  models, 
see  Galbraith  and  Galbraith  (1974),  Box  and  Jenkins  (1976,  Chapter  6),  Hannan 
and  Rissanen  (1982),  and  Koreisha  and  Pukkila  (1989). 

5.7.  Numerical  Optimization 
Previous  sections  of  this  chapter  have  shown  how  to  calculate  the  log  likelihood 
function 

: 

£(0)  =  log fy,.y,_,..  ¥(¥r  Yr-i  + 

+ 

- 

yi;  9) 

[5.7.1] 

for  various  specifications  of  the  process  thought  to  have  generated  the  observed 
data  y,.  Yo,-.-,  yr.  Given  the  observed  data,  the  formulas  given  could  be  used 
to  calculate  the  value  of £(6)  for  any  given  numerical  value  of 0. 

This  section  discusses  how  to  find  the  value  of @ that  maximizes  £(@)  given 
no  more  knowledge  than  this  ability  to calculate  the  value  of £(@)  for any  particular 
value  of @.  The  general  approach  is to  write  a  procedure  that  enables  a  computer 
to  calculate  the  numerical  value  of £(6)  for  any  particular  numerical  values  for  ® 
and  the  observed  data  y,,  y>,  . 
,  yr.  We  can  think  of this  procedure  as  a  ‘‘black 
box”  that  enables  us  to  guess  some  value  of @ and  see  what  the  resulting  value  of 
L(8)  would  be: 

. 

. 

Input 

Procedure 

Output 

values  of 

Vir  Y2a0--+-5Yr 
and  6 

calculates 
(0) 

|__|  value  of 

(0) 

The  idea  will  be  to  make a series  of different  guesses  for  @, compare  the  value of 
¥(®)  for  each  guess,  and  try  to  infer  from  these  values  for  £(@)  the  value  @ for 
which  £(@)  is largest.  Such  methods  are  described  as  numerical  maximization. 

Grid  Search  - 

The  simplest  approach  to  numerical  maximization  is known  as  the grid search 
method.  To  illustrate  this  approach,  suppose  we  have  data  generated  by an  AR(1) 
process,  for which  the  log likelihood  was  seen  to  be  given by [5.2.9].  To  keep  the 
example  very  simple,  it is assumed  to  be  known  that  the  mean  of the  process  is 
-  zero  (c =  0) and  that  the  innovations  have  unit  variance  (o*  =  1). Thus  the  only 
unknown  parameter  is the  autoregressive  coefficient  @, and  [5.2.9]  simplifies  to 
baw 
—5 log(2m)  +  5 log(1  - 4%) 
42 
| 
I 
1< 
—  5(1  —  Pi  -  5D  Cy  -  y-1). 

£($)  = 

squsihs 

3 

: 

Z 

2 r=2 

Suppose  that  the  observed  sample  consists  of the  following  T =  5 observations: 
yy  =  -1.2 

y,=  -04 

ys  =  0.0. 

y,=02 

y,=08 

If we  make  an  arbitrary  guess  as-to  the  value  of ¢, say,  @ =  0.0,  and  plug  this 
guess  into expression  [5.7.2],  we  calculate  that  £(¢)  =  —5.73  at  @ =  0.0. Trying 
another guess (¢ =  0.1), we calculate  £(¢)  =  —5.71  ato  =  0.1—the log likelihood 
=  0.0.  Continuing in this  fashion,  we  could  calculate 
is higher at  @ =  0.1  than  at 
the  value  of £(¢)  for  every  value  of @ between  —0.9  and  +0.9  in increments  of 

5.7.  Numerical  Optimization 

133 

0.1.  The  results  are  reported  in  Figure  5.1.  It appears  from  these  calculations  that 
the  log likelihood  function  £(¢)  is nicely  behaved  with  a  unique  maximum  at  some 
value  of  @ between  0.1  and  0.3.  We  could  then  focus  on  this  subregion  of  the 
parameter  space  and  evaluate  £()  at  a  finer  grid,  calculating  the  value  of £(¢) 
for  all  values  of  ¢ between  0.1  and  0.3  in  increments  of  0.02.  Proceeding  in this 
fashion,  it should  be possible  to get arbitrarily  close  to  the  value  of ¢ that  maximizes 
L(d)  by making  the  grid  finer  and  finer. 

: 

Note  that  this  procedure  does  not  find  the  exact  MLE  ¢,  but  instead  ap- 
proximates  it with  any  accuracy  desired.  In  general,  this  will  be  the  case  with  any 
numerical  maximization  algorithm.  To  use  these  algorithms  we  therefore  have  to 
specify  a convergence  criterion,  or  some  way  of deciding  when  we  are  close enough 
to  the  true  maximum.  For  example,  suppose  we  want  an  estimate  ¢ that  differs 
from  the  true  MLE  by no  more  than  +0.0001.  Then  we  would  continue  refining 
the  grid  until  the  increments  are  in  steps  of 0.0001,  and  the  best  estimate  among 
the  elements  of that  grid  would  be  the  numerical  MLE  of  ¢. 

For  the  simple  AR(1)  example  in  Figure  5.1,  the  log  likelihood  function  is 
unimodal—there  is a  unique  value  ®  for  which  0£(8)/0@  =  0.  For  a  general 
numerical  maximization  problem,  this  need  not  be  the  case.  For  example,  suppose 
that  we  are  interested  in estimating  a scalar  parameter 6 for which  the log likelihood 
function  is as  displayed  in  Figure  5.2.  The  value  6  =  —0.6  is a  local  maximum, 
meaning  that  the  likelihood  function  is  higher  there  than  for  any  other  @ in  a 
neighborhood  around  6  =  —0.6.  However,  the  global  maximum  occurs  around 
6  =  0.2.  The  grid  search  method  should  work  well  for  a  unimodal  likelihood  as 
long  as  £(8)  is continuous.  When  there  are  multiple  local  maxima,  the  grid  must 
be  sufficiently  fine  to  reveal  all  of the  local  “hills”  on  the  likelihood  surface. 

_Steepest  Ascent 

Grid  search  can  be  a  very  good  method  when  there  is  a  single  unknown 
parameter  to  estimate.  However,  it quickly  becomes  intractable  when  the  number 
of elements  of 8 becomes  large.  An  alternative  numerical  method  that  often  suc- 

-8 
£($) 

FIGURE  5.1  Log likelihood  for an  AR(1)  process  for various  guesses  of d. - 
134  Chapter  5 | Maximum  Likelihood  Estimation 

| 

wae 

7 

_ 
7 

; 

the parameter 
viendied a  d wish to  come >  up with a better  estimate 
”. —- that we are  cmafchs  to choose 6 so  that the  squared  distance 

© 

and 3 7 some fseltas number k: 
per 

Lj  OI 

)  D9 (0) 

edema 

alue to  choose for 0° > w would then bet the cosa  tits 

(o"  0)'(0"  0 ye  ato  t HHGY  SH  a 

t 

< 

ro! 

1p 

Satisied,  ch  as  the 

Using  this  notation,  expression  [5.7.4]  can  be  written  as 

6°)  —  @  =  [1/(2A)]  -  g(0"). 

[5.7.5] 

Expression  [5.7.5]  asserts  that  if we  are  allowed  to  change  ® by only a fixed 
amount,  the  biggest  increase  in  the  log likelihood  function  will  be  achieved  if the 
change  in  @ (the  magnitude  6°)  —  0)  is chosen  to  be  a constant  1/(2A)  times  the 
gradient  vector  g(@”).  If we  are  contemplating  a  very  small  step  (so that  k is near 
zero),  the  value  g(@"!”)  will  approach  g(@).  In  other  words,  the  gradient  vector 
g(0)  gives the  direction  in which  the  log likelihood  function  increases  most  steeply 
from  6. 

For  illustration,  suppose  that  a  =  2 and  let  the  log likelihood  be 

$(0)  =  —1.563  —  263. 

[5.7.6] 

We  can  easily  see  analytically  for  this  example  that  the  MLE  is  given  by  ®  = 
(0, 0)’.  Let  us  nevertheless  use  this example  to illustrate  how  the  method  of steepest 
ascent  works.  The  elements  of the  gradient  vector  are 

Sp  73%  SO = 46, 

[5.7.7] 

Suppose  that  the  initial  guess  is  0  =  (—1,  1)’.  Then 

9L0)) 

00,  | e-em 

a) 
00,  |e-em 

Ete 

An  increase  in  6,  would  increase  the  likelihood,  while  an  increase  in  6, would 
decrease  the  likelihood.  The  gradient  vector  evaluated  at  @©  is 

BOM  =  is ‘| 

3 

so that the optimal  step @ —  @ should  be proportional  to (3, —  4)’. For example, 
with  k  =  1 we  would  choose 

iC  “v=  8 
o>  =  9)  =  —% 
that  is,  the  new  guesses  would  be  6{’)  =  —0.4  and  0{")  =  0.2.  To  increase  the 
likelihood  by the  greatest  amount,  we  want  to increase  6, and  decrease  6, relative 
to  their  values  at  the  initial  guess  9.  Since  a one-unit  change  in  6, has  a  bigger 
effect  on  £(8)  than  would  a  one-unit  change  in  6,, the  change  in  @, is larger  in 
absolute  value  than  the  change  in 0,. 

Let  us  now  return  to the black  box  perspective,  where  the  only capability  we 
have  is to  calculate  the  value  of £(@)  for  a  specified  numerical  value  of @.  We 
might  start  with  an  arbitrary  initial  guess  for the value  of @, denoted  @  Suppose 
we  then  calculate  the  value  of the  gradient  vector  at  6: 

g(6)  =  on 

@=@() 

[5.7.8] 

This  gradient  could  in  principle  be  calculated  analytically,  by differentiating  the 
general  expression  for  £(@)  with  respect  to  @ and  writing  a computer  procedure 
to calculate  each  element  of g(@) given  the  data  and  a numerical  value  for @. For 
example,  expression  [5.7.7] could  be used  to calculate  g(@) for any particular value 
of @. Alternatively,  if it is too  hard  to differentiate  £(@) analytically, we can  always 

136  Chapter  5 | Maximum  Likelihood  Estimation 

get  a  numerical  approximation  to  the  gradient  by seeing  how  £(@)  changes  for  a 
small  change  in  each  element  of  8.  In  particular,  the  ith  element  of g(@’)  might 
be  approximated  by 

! 

g(0)  =  Ai e(ay”, OP  is  2  ODO  4,00).  08). 

OM) 

i+2? 

- 

(0) 

LAM, 

g(0) 

OP... 

(0 

81%, 81,  1,  O92, 

0 

0 

i+2? 

.

OM). 

/ 

[5.7.9] 

where  A  represents  some  arbitrarily  chosen  small  scalar  such  as  A  =  107°.  By 
numerically  calculating  the  value  of  £(@)  at  6  and  at  a  different  values  of  @ 
corresponding  to  small  changes  in  each  of  the  individual  elements  of  6),  an  es- 
timate  of the  full  vector  g(@)  can  be  uncovered. 

Result  [5.7.5]  suggests  that  we  should  change  the  value  of @ in  the  direction 

of the  gradient,  choosing 

@)  —  9  =  5-g(@() 

for some  positive  scalar  s.  A suitable  choice  for s could  be  found  by an  adaptation 
of the  grid search  method.  For  example,  we  might  calculate  the  value  of £{0  + 
s-g(0)}  for  s  =  %&, 4, 4, 4, 1, 2, 4, 8, and  16 and  choose  as  the  new  estimate  0“ 
the  value  of  0  +  s-g(@)  for  which  £(@)  is largest.  Smaller  or  larger  values  of 
s could  also  be  explored  if  the: maximum  appears  to  be  at  one  of the  extremes.  If 
none  of the  values  of s improves  the  likelihood,  then  a very  small  value  for s such 
as  the  value  A  =  10~°  used  to  approximate  the  derivative  should  be  tried. 

We  can  then  repeat  the  process,  taking  0“)  =  @  +  s-g(@)  as  the  starting 
point,  evaluating  the  gradient  at  the  new  location  g(@“"),  and  generating  a  new 
estimate  8)  according  to 

for  the  best  choice  of s.  The  process  is iterated,  calculating 

Qin +  =  Qo  +  s:g(0°) 

form  =  0, 1,2,  ..  .  until  some  convergence  criterion  is satisfied,  such  as  that  the 
gradient  vector  g(@°”)  is within  some  specified  tolerance  of  zero,  the  distance 
between  0°"*")  and  @‘”  is less  than  some  specified  threshold,  or  the  change  be- 
tween  £(0°"*")  and  £(0”)  is smaller  than  some  desired  amount. 

Figure  5.3  illustrates  the  method  of steepest  ascent  when  @ contains  a  =  2 
elements.  The  figure  displays  contour  lines  for  the  log likelihood  £(8);  along  a 
given  contour,  the  log likelihood  £(@)  is constant.  If the  iteration  is started  at  the 
initial  guess  0,  the  gradient  g(@)  describes  the  direction  of steepest  ascent. 
Finding  the  optimal  step  in  that  direction  produces  the  new  estimate  0“.  The 
gradient  at  that  point  g(@)  then  determines  a  new  search  direction  on  which  a 
new  estimate  8)  is based,  until  the  top of the  hill  is reached. 
Figure  5.3  also  illustrates  a  multivariate  generalization  of the  problem  with 
multiple  local  maxima  seen  earlier  in Figure  5.2.  The  procedure  should  converge 
to  a local  maximum,  which  in this  case  is different  from  the  global  maximum  06°. 
In Figure  5.3,  it appears  that  if 6*  were  used  to  begin  the  iteration  in place  of 
6,  the procedure  would  converge  to  the  true  global  maximum  8°.  In practice, 
the  only way  to  ensure  that  a  global  maximum  is found  is to  begin  the  iteration  . 
from  a  number  of different  starting  values  for  8  and  to  continue  the  sequence 
from  each  starting  value  until  the  top of the hill  associated  with  that  starting  value 
is discovered. 

5.7.  Numerical  Optimization 

137 

 
FIGURE  5.3 

Likelihood  contours  and  maximization  by steepest  ascent. 

6; 

Newton-Raphson 

One  drawback  to  the  steepest-ascent  method  is that  it may  require  a  very 
large number  of iterations  to close  in on  the local  maximum.  An alternative  method 
known  as  Newton-Raphson  often  converges  more  quickly  provided  that  (1) second 
derivatives  of the  log likelihood  function  £(@)  exist  and  (2) the  function  £(@)  is 
concave,  meaning  that  —1  times  the  matrix  of second  derivatives  is everywhere 
positive  definite. 

Suppose  that  @ is an  (a x  1) vector  of parameters  to be estimated.  Let g(@) 

denote  the  gradient  vector  of the  log likelihood  function  at  0: 

a£(8) 

qo)  =  —+ 

: 

te -  08  @=@ 

and  let  H(@)  denote  —1  times  the  matrix  of second  derivatives  of the  log like- 
lihood  function: 

9)  =  —-— 

0°£(8)} 

- _  00 00 ’  0-00 

Consider  approximating  £(6) with  a second-order  Taylor  series  around  @®: 
£(0)  =  £0)  +  [g(0)]'[6  —  6]  —  316 —  0) H)6  —  6).  [5.7.10] 
The  idea  behind  the  Newton-Raphson  method  is to  choose  @ so  as  to  maximize 
[5.7.10].  Setting  the  derivative  of [5.7.10]  with  respect  to  @ equal  to  zero  results 
in 

é 

g(6)  —  H(60)[6  —  6)  =  0. 

[5.7.11] 

138  Chapter  5 | Maximum  Likelihood  Estimation 

_ 

Let  6  denote  an  initial  guess  as  to  the  value  of  @.  One  can  calculate  the 
derivative  of  the  log  likelihood  at  that  initial  guess  (g(@))  either  analytically,  as 
in  [5.7.7],  or  numerically,  as  in  [5.7.9].  One  can  also  use  analytical  or  numerical 
methods  to  calculate  the  negative  of the  matrix  of second  derivatives  at  the  initial 
guess  (H(@)).  Expression  [5.7.11]  suggests  that  an  improved  estimate  of @ (de- 
noted  @‘'))  would  satisfy 

2(0)  =  H(0) [oo  =  9} 

or 

(5.7.12] 
0  —  9  =  [H(0)]-12(6). 
One  could  next  calculate  the  gradient  and  Hessian  at  0“)  and  use  these  to  find  a 
new  estimate  6) and  continue  iterating  in this fashion.  The  mth  step in the  iteration 
updates  the  estimate  of @ by using  the  formula 

oe" +)  =  Of”)  +  [H(O™))]~'2(0(). 

[5.7.13] 

If the  log likelihood  function  happens  to  be a perfect  quadratic  function,  then 

[5.7.10]  holds  exactly  and  {5.7.12]  will  generate  the  exact  MLE  in a single  step: 

0°”  =  Ouve- 
If the  quadratic  approximation  is reasonably  good,  Newton-Raphson  should  con- 
verge  to  the  local  maximum  more  quickly  than  the  steepest-ascent  method.  How- 
ever,  if  the  likelihood  function  is  not  concave,  Newton-Raphson  behaves  quite 
poorly.  Thus,  steepest  ascent  is often  slower  to  converge  but  sometimes  proves  to 
be  more  robust  compared  with  Newton-Raphson. 

Since  [5.7.10]  is  usually  only  an  approximation  to  the  true  log  likelihood 
function,  the  iteration  on  [5.7.13]  is often  modified  as  follows.  Expression  [5.7.13] 
is taken  to  suggest  the  search  direction.  The  value  of the  log  likelihood  function 
at  several  points  in that  direction  is then  calculated,  and  the  best  value  determines 
the  length  of the  step.  This  strategy  calls  for  replacing  [5.7.13]  by 

[5.7.14] 
er")  =  O™  +  s[H(O™)]-  120), 
where  s  is  a  scalar  controlling  the  step  length.  One  calculates  6°"*!)  and  the 
associated  value  for  the  log likelihood  £(0*!)  for various  values  of s in [5.7.14] 
and  chooses  as  the  estimate  0°”"*"  the  value  that  produces  the  biggest  value  for 
the  log likelihood. 

Davidon-Fletcher-Powell 

If  @ contains a unknown  parameters,  then  the  symmetric  matrix  H(@)  has 
a(a +  1)/2 separate  elements.  Calculating  all these  elements  can  be extremely  time- 
consuming  if a  is large.  An  alternative  approach  reasons  as  follows.  The  matrix  of 
second  derivatives  (—H(@))  corresponds  to  the  first  derivatives  of  the  gradient 
vector  (g(@)),  which  tell  us  how  g(®)  changes  as  @ changes.  We  get  some  inde- 
pendent  information  about  this  by comparing  g(6“)  —  2(6)  with  @)  —  9. 
This  is not  enough  information  by itself  to  estimate  H(@),  but  it is information  that 
could  be used  to update  an  initial  guess  about  the value  of H(@).  Thus,  rather  than 
evaluate  H(@)  directly  at  each  iteration,  the  idea  will  be  to  start  with  an  initial 
guess about  H(@) and update  the guess solely on the basis of how much  g(6) changes 
between  iterations,  given  the  magnitude  of the  change  in  8.  Such  methods  are 
sometimes  described  as  modified  Newton-Raphson. 

One  of the  most popular  modified  Newton-Raphson  methods  was  proposed 
by Davidon  (1959)  and  Fletcher  and  Powell  (1963).  Since  it is H~!  rather  than  H 

5.7.  Numerical  Optimization 

139 

itself  that  appears  in  the  updating  formula  [5.7.14],  the  Davidon-Fletcher-Powell 
algorithm  updates  an  estimate  of H~'  at  each  step  on  the  basis  of the  size  of the 
change  in  g(@)  relative  to  the  change  in 8.  Specifically,  let  0°””  denote  an  estimate 
of @ that  has  been  calculated  at  the  mth  iteration,  and  let  A“”’  denote  an  estimate 
of [H(0‘””)]~'.  The  new  estimate  0°"*"  is given  by 

gi")  =  OC)  +  sArmg(Qir) 

[5.7.15] 

for  s  the  positive  scalar  that  maximizes  LO’  +  sA’™g(6'”)}.  Once  6"*”  and 
the  gradient  at  @’"*")  have  been  calculated,  a  new  estimate  A‘”"*!)  is found  from 

At" +1)  —  A™  al 

A’  (Ag”  +  (Ag  +  D)'A™ 

(Ag(” +  )‘AC™(Ag™  +  1)) 

(Ae  *  )(Ae'"*  Uys 

 (Ag"*)'(A0%*») 

[5.7.16] 

where 

A@’*  1)  =  Qi"  +  ee  r 

Agi"*})  ==  g(6"*")  =  g(0”). 

In  what  sense  should  A‘’"*")  as  calculated  from  [5.7.16]  be  regarded  as  an 
estimate  of  the  inverse  of  H(@°"*!))?  Consider  first  the  case  when  @ is a  scalar 
(a  =  1). Then  [5.7.16]  simplifies  to 

Amt)  =  Aim)  —  a  AA Bris:  otesiat 

(Ag +  1))2(A(™) 

= 

nd aco) nn: ae 
(Ag” +  (Ae +  )) 

Agim+) 

oti  Agit+) 

setohoF  es 

Agin+) 

~ 

~  Agim"  , 

In  this  case, 

[Aen  =1  ane 

(m+  1) 

Agin+)’ 

which  is the  natural  discrete  approximation  to 

CT  me  —arelg 

re 

F) 
-  = 

@=eimr!) 

@=gimr!). 

More  generally  (for a  >  1), an  estimate  of the  derivative  of g(-)  should  be. 

related  to  the  observed  change  in g(-) according  to 

That  is, 

or 

g(0"+))  =  pam)  +  cat 

dg 

[eem+)  —  gem]. 

@=e'7"*!) 

(0°)  =  gO™)  —  HO  [a+  —  Gem] 

Agi"  +!)  =  —  (H(@(™*)))-!  Agi" +"), 

| 

Hence  an  estimate  A’”'*")  of [H(6°"*")]~!  should  satisfy 
—  Agime  ny 

Amr)  Agim)  = 

ry 

| 

TNCs 
[5.7.17] 

140  Chapter  5  | Maximum  Likelihood  Estimation 

Postmultiplication  of [5.7.16]  by Ag’  *")  confirms  that  [5.7.17]  is indeed  satisfied 
by the  Davidon-Fletcher-Powell  estimate  A("*  !): 

Alm+))  Agi"*)  =  aim  Ag+!) 

ij A’(Ag™  +  (Ag  +  AM  Agim +  ')) 

_  Bots  ))(40*  P)'(Aagi"*  ») 

At)  Ag(”*  1)  Alm)  Ag’”*  ')  —  AQlred 

=   Agins!). 

Thus,  calculation  of [5.7.16]  produces  an  estimate  of [H(@‘”*  !))]~!  that  is consistent 
with  the  magnitude  of  the  observed  change  between  g(0°"*!)  and  g(@’”)  given 
the  size  of the  change  between  0(*!)  and  0”. 

The  following  proposition  (proved  in Appendix  5.A  at the end  of the chapter) 

establishes  some  further  useful  properties  of the  updating  formula  [5.7.16]. 

Proposition 
ZL:  R* —  R'  has continuous  first derivatives  denoted 

(Fletcher 

Powell 

5.1: 

and 

(1963)). 

Consider 

£(®), 

where 

g(0'")  =  £(0) 

(ax  1) 

00 

@=e'") 

Suppose  that  some  element  of g(@°”)  is nonzero,  and  let A’”  be a positive  definite 
symmetric  (a  X  a) matrix.  Then  the following  hold. 

(a)  There  exists  a  scalar  s >  0 such  that  £(0°"*")  >  £(0”)  for 

Ol")  =  O—™  +  sA™g(Q(™)., 
(6)  If s in  [5.7.18]  is chosen  so  as to  maximize  £(0"*"),  then  the first-order 

[5.7.18] 

conditions  for an  interior  maximum  imply  that 

3 

[g(e°"*  ?)}'[aer+)  =  eo”)  =  0. 

[5.7.19] 

(c)  Provided  that  [5.7.19]  holds  and  that  some  element  of  g(0°"*")  — 
g(@°”)  is nonzero,  then  A\"*")  described  by [5.7.16]  is a positive  definite 
symmetric  matrix. 

, 

Result  (a)  establishes  that  as  long  as  we  are  not  already  at  an  optimum 
(g(0°”)  #  0), there  exists  a  step  in the  direction  suggested  by the  algorithm  that 
will  increase  the  likelihood  further,  provided  that  A’”) is a positive  definite  matrix. 
Result  (c) establishes  that  provided  that the  iteration  is begun  with  A  a positive 
definite  matrix,  then  the  sequence  of  matrices  {A‘”}*"_,  should  all  be  positive 
definite,  meaning  that  each  step  of the  iteration  should  increase  the  likelihood 
function.  A standard  procedure  is to  start  the  iteration  with  A®  =  L,, the  (a  x  a) 
‘identity  matrix. 

| 

| 

If the  function  £(@)  is exactly  quadratic,  so  that 

£(0)  =  £(0)  +  g'[o  —  6)  —  3[6  —  6)  H[@  —  0), 
with  H positive  definite,  then  Fletcher  and  Powell  (1963)  showed  that iteration  on 
[5.7.15]  and  [5.7.16]  will  converge  to  the  true global  maximum  in a steps: 

0°  =  Oy¢  =  0  + Ho's; 
and  the  weighting  matrix  will  converge  to  the  inverse  of  —1  times  the  matrix  of 
second  derivatives: 

| 

AM  =  Ho, 

5.7.  Numerical  Optimization 

141 

More  generally,  if  £(@)  is  well  approximated  by  a  quadratic  function,  then  the 
Davidon-Fletcher-Powell  search  procedure  should  approach  the  global  maximum 
more  quickly  than  the  steepest-ascent  method, 

a”  =  Dicom 
for  large  N,  while  A’™  should  converge  to  the  negative  of  the  matrix  of  second 
derivatives  of  the  log likelihood  function: 

A™)  an.  Ee 

00  00"  @ =  Ome 

) 

-1 

[5.7.20] 

In  practice,  however,  the  approximation  in  [5.7.20]  can  be  somewhat  poor,  and  it 
is  better  to  evaluate  the  matrix  of second  derivatives  numerically  for  purposes  of 
calculating  standard  errors,  as  discussed  in  Section  5.8. 

If the  function  £(8)  is not  globally  concave  or  if the  starting  value  @©  is far 
from  the true  maximum,  the  Davidon-Fletcher-Powell  procedure  can  do very  badly. 
If problems  are  encountered,  it often  helps  to  try  a  different  starting  value  6,  to 
rescale  the  data  or  parameters  so  that  the  elements  of @ are  in  comparable  units, 
or  to  rescale  the  initial  matrix  A©’—for  example,  by setting 

A  =  (1  x  10-4)I,. 

Other  Numerical  Optimization  Methods 
A  variety  of other  modified  Newton-Raphson  methods  are  available,  which 
use  alternative  techniques  for  updating  an  estimate  of H(@‘”)  or  its  inverse.  Two 
of the  more  popular  methods  are  those  of Broyden  (1965,  1967)  and Berndt,  Hall, 
Hall,  and  Hausman  (1974).  Surveys  of these  and  a  variety  of other  approaches  are 
provided  by Judge,  Griffiths,  Hill,  and  Lee  (1980,  pp.  719-72)  and  Quandt  (1983). 
Obviously,  these  same  methods  can  be used  to  minimize  a function  Q(@) with 
respect  to.6,  We  simply  multiply  the  objective  function  by  —1  and  then  maximize 
the  function  —  Q(@). 

5.8.  Statistical  Inference  with  Maximum  Likelihood 
Estimation 

The  previous  section  discussed  ways  to  find  the  maximum  likelihood  estimate  6 
given  only  the  numerical  ability  to  evaluate  the  log likelihood  function  £(@).  This 
section  summarizes  general  approaches  that  can  be used  to test  a hypothesis  about 
6.  The  section  merely  summarizes  a  number  of useful  results  without  providing 
any  proofs.  We  will  return  to  these  issues  in more  depth  in Chapter  14, where  the 
Statistical  foundation  behind  many  of these  clainis  will  be  developed. 

Before  detailing  these  results,  however,  it is worth  calling  attention  to  two 
of the  key  assumptions  behind  the  formulas  presented  in  this  section.  First,  it is 
assumed  that  the  observed  data  are  strictly  stationary.  Second,  it is assumed  that 
neither  the  estimate  @ nor  the  true  value  @ falls  on  a  boundary  of the  allowable 
parameter  space.  For  example,  suppose  that  the  first  element  of @ is  a parameter 
corresponding  to  the  probability  of a  particular  event,  which  must  be  between  0 
and  1.  If the  event  did  not  occur  in the  sample,  the  maximum  likelihood  estimate 
of the  probability  might  be zero.  This  is an  example  where  the estimate 6 falls on 
the  boundary  of the  allowable  parameter  space,  in which  case  the  formulas  pre- 
sented  in this  section  will  not  be  valid. 

142 

Chapter  5 | Maximum  Likelihood  Estimation 

Asymptotic  Standard  Errors  for Maximum  Likelihood 
Estimates 

If the sample  size  T is sufficiently  large,  it often  turns  out  that  the distribution 
of the  maximum  likelihood  estimate  6 can  be  well  approximated  by the  following 
distribution: 

6 ~  N(O),  T-'9-'), 

[5.8.1] 

where  8, denotes  the  true  parameter  vector.  The  matrix  ¥ is known  as  the  infor- 
mation  matrix  and  can  be  estimated  in either  of two  ways. 

The  second-derivative  estimate  of the  information  matrix  is 

$.,  =. -  ~~ 4 

[5.8.2] 

Here  £(8)  denotes  the  log likelihood: 

£(8)  = 3 log frye, (Vd, 15 8): 

of 

and Y, denotes the  history of observations on y obtained  through  date t. The matrix 
second derivatives of the log likelihood is often calculated numerically. Substi- 
ants [5.8.2] st  1], the  terms involving the sample size rire  ene 

e  matrix of 6 can be approximated by 

wits  & 
yiziiG2 

WO  &  Sti 

; 
ertitod a2£(0)|  fare i 

+  ni  sie = 

T3297 

joe  #e  Bc 

 FO- 616 - 5 0)! -{- 00 00" e=6 

; 

12253  oi 583) 

Ss eesnaxe  sd  nso 6% 
tan A second estimate of the information pemsant int La sated 8 the outer- 

A. -t  Pte  -odtt  to  iataienGo.y 

baiointeorny ai  deoncliogm®  bons 
tobe  i 

eo 

aay 
e 

al 

approximate  95%  confidence  interval  for  0, is given  by 

0+2Vj= 

Note  that  unless  the  off-diagonal  elements  of 

are  zero,  in general  one  needs 
to  calculate  all  the  elements  of  the  matrix  ¥ and  invert  this  full  matrix  in order  to 
obtain  a  standard  error  for  any  given  parameter. 

Which  estimate  of  the  information  matrix,  $,, or  $op,  is it better  to  use  in 
practice?  Expression  [5.8.1]  is  only  an  approximation  to  the  true  distribution  of 
6, and  $,, and  $op are  in  turn  only. approximations  to  the  true  value  of ¥.  The 
theory  that  justifies  these  approximations  does  not  give any  clear  guidance  to  which 
is  better  to  use,  and  typically,  researchers  rely  on  whichever  estimate  of  the  in- 
formation  matrix  is  easiest  to  calculate.  If the  two  estimates  differ  a  great  deal, 
this  may  mean  that  the  model  is misspecified.  White  (1982)  developed  a  general 
test  of model  specification  based  on  this  idea.  One  option  for constructing  standard 
errors  when  the  two  estimates  differ  significantly  is  to  use  the  ‘‘quasi-maximum 
likelihood”  standard  errors  discussed  at  the  end  of this  section. 

Likelihood  Ratio  Test 

Another  popular  approach  to  testing  hypotheses  about  parameters  that  are 
estimated  by maximum  likelihood  is the  likelihood  ratio  test.  Suppose  a  null  hy- 
pothesis  implies  a  set  of  m  different  restrictions  on  the  value  of  the  (a  x  1) 
parameter  vector  @.  First,  we  maximize  the  likelihood  function  ignoring  these 
restrictions  to  obtain  the  unrestricted  maximum  likelihood  estimate  6. Next,  we 
find  an  estimate  6 that  makes  the  likelihood  as  large  as  possible  while  still  satisfy- 
ing  all  the  restrictions.  In  practice,  this  is  usually  achieved  by  defining  a  new 
[(a —  m)  X  1] vector  A  in  terms  of which  all of the  elements  of @ can  be expressed 
when  the  restrictions  are  satisfied.  For  example,  if the  restriction  is that  the  last 
a  —  m  elements  of @.  Let 
m  elements  of @ are  zero,  then  A consists  of  the  first 
£(6) denote  the  value  of the  log likelihood  function  at  the  unrestricted  estimate, 
and  let  £(6)  denote  the  value  of  the  log  likelihood  function  at  the  restricted 
estimate.  ‘Clearly  £(6)  >  £(6),  and  it often  proves  to  be  the  case  that 

2[£(0)  —  £(6)]  ~  x2(m). 

[5.8.5] 

For  example,  suppose  that  a  =  2 and  we  are  interested  in  testing  the  hy- 
pothesis  that  6,  =  6,  +  1.  Under  this  null  hypothesis  the  vector  (6,,  65)’  can  be 
written  as  (A,  A  +  1)’,  where  A  =  @,.  Suppose  that  the  log likelihood is  given  by 
expression  [5.7.6]. One  can  find  the  restricted  MLE  by replacing  @, by @,  +  1 and 
maximizing  the  resulting  expression  with  respect  to  @,: 

The  first-order  condition  for maximization  of £(6,)  is 

L(0,)  = 

—  1.56?  a  2(8,  +  1): 

—36,  —  4(0,  +  1) =  0, 

or  0,  = 

—%4.  The  restricted  MLE  is thus  6 =  (—3,  3)’, and  the  maximum  value 

attained  for  the  log likelihood  while  satisfying  the  restriction  is 

£(6)  = (-3)(-3)?  -  (3G)? 

~{(3  +  4)(2+7+7)K4  +  3} 

=  -$. 

The  unrestricted  MLE  is 6 = 0, at  which  £(6)  =  0.  Hence,  [5.8.5]  would  be 

2[£(6)  —  £(6)}  =  ¥ =  1.71. 

144  Chapter 5  | Maximum  Likelihood  Estimation 

The  test  here  involves  a  single  restriction,  so  m  = 
1.  From  Table  B.2  in  Appendix 
B,  the  probability  that  a  y?(1)  variable  exceeds  3.84  is 0.05.  Since  1.71  <  3.84.  we 
accept  the  null  hypothesis  that  6,  =  @,  +  1 at  the  5%  significance  level. 

Lagrange  Multiplier  Test 
In order  to  use  the  standard  errors  from  [5.8.2]  or  [5.8.4]  to  test  a  hypothesis 
about  @, we  need  only to  find  the  unrestricted  ML £ 6. In order  to  use  the  likelihood 
ratio  test  [5.8.5],  it  is  necessary  to  find  both  the  unrestricted  MLE  @ and  the  re- 
stricted  MLE  @.  The  Lagrange  multiplier  test  provides  a  third  principle  with  which 
to  test  a  null  hypothesis  that  requires  only  the  restricted  MLE  6. This  test  is  useful 
when  it is easier  to  calculate  the  restricted  estimate  @ than  the  unrestricted  estimate 
0. 

Let  @ be  an  (a  x  1) vector  of parameters,  and  let  @ be  an  estimate  of @ that 
maximizes  the  log likelihood  subject  to  a  set  of m  restrictions  on  @.  Let  fOyly,-1, 
Yr-2,  - 
-  ; @) be  the  conditional  density  of  the  mh  observation,  and  let  h(0,  Y,) 
denote  the  (a  x  1) vector  of  derivatives  of  the  log  of  this  conditional  density 
evaluated  at  the  restricted  estimate  0: 

- 

2 

d lo 

i 

ee 

6=80 

The  Lagrange  multiplier  test  of the  null  hypothesis  that  the  restrictions  are  true  is 
given  by the  following  statistic: 

T 

; 

T 

: a  b h(6, | ap h(6, ¥)| 

[5.8.6] 

1=1 

r=1 

If the  null  hypothesis  is  true,  then  for  large  T this  should  approximately  have  a 
x?(m)  distribution.  The  information  matrix  ¥ can  again  be  estimated  as  in  [5.8.2] 
or  [5.8.4]  with  6 replaced  by 6. 

Quasi-Maximum  Likelihood  Standard  Errors 

It was  mentioned  earlier  in this  section  that  if the  data  were  really  generated 
from  the  assumed  density  and  the  sample  size  is sufficiently  large,  the  second- 
derivative  estimate  $,, and  the  outer-product  estimate  $op of the  information 
matrix  should  be  reasonably  close  to  each  other.  However,  maximum  likelihood 
estimation  may  still  be  a  reasonable  way  to  estimate  parameters  even  if the  data 
were  not  generated  by the  assumed  density.  For example,  we  noted  in Section  5.2 
that  the  conditional  MLE  for  a Gaussian  AR(1)  process  is obtained  from  an  OLS 
regression  of y,on y,_ ,.  This OLS regression  is often  a very  sensible  way to estimate 
parameters  of an  AR(1)  process  even  if the  true  innovations  ¢, are  not  i.i.d.  Gaus- 
sian.  Although  maximum  likelihood  may  be  yielding  a  reasonable  estimate  of 8, 
when the innovations  are not  i.i.d.  Gaussian,  the standard  errors  proposed  in [5.8.2] 
or  [5.8.4]  may  no  longer  be valid.  An  approximate  variance-covariance  matrix  for 
6 that  is sometimes  valid  even  if the  probability  density  is misspecified  is given  by 
[5.8.7] 
E(6 -  8,)(6  —  0)! =  T-49ip$5b9  20}. 
This variance-covariance  matrix  was proposed by White  (1982),  who  described  this 
approach as quasi-maximum  likelihood  estimation. 

5.8.  Statistical  Inference  with  Maximum  Likelihood  Estimation 

145 

5.9.  Inequality  Constraints 

A  Common  Pitfall  with  Numerical  Maximization 
Suppose  we  were  to  apply  one  of  the  methods  discussed  in  Section  5.7  such 
as  steepest  ascent  to  the  AR(1)  likelihood  [5.7.2].  We  start  with  an  arbitrary  initial 
guess,  say,  @ =  0.1.  We  calculate  the  gradient  at  this  point,  and  find  that  it  is 
positive.  The  computer  is  then  programmed  to  try  to  improve  this  estimate  by 
evaluating  the  log  likelihood  at  points  described  by  #')  =  ©  +  s-g(6)  for 
various  values  of s,  seeing  what  works  best.  But  if the  computer  were  to  try a value 
for  s  such  that  6)  =  @©  +  s-g(¢)  =  1.1,  calculation  of [5.7.2]  would  involve 
finding  the  log of (1 —  1.17)  =  —0.21.  Attempting  to  calculate  the  log of a negative 
number  would  typically  be  a  fatal  execution  error,  causing  the  search  procedure 
to  crash. 

Often  such  problems  can  be  avoided  by  using  modified  Newton-Raphson 
procedures,  provided  that  the  initial  estimate  6)  is chosen  wisely  and  provided 
that  the  initial  search  area  is  kept  fairly  small.  The  latter  might  be  accomplished 
by setting  the  initial  weighting  matrix  A©  in  [5.7.15]  and  [5.7.16]  equal  to  a  small 
multiple  of  the  identity  matrix,  such  as  A®  =  (1  x  10~‘)-L,.  In  later  iterations, 
the  algorithm  should  use  the  shape  of the  likelihood  function  in the  vicinity  of the 
maximum  to  keep  the  search  conservative.  However,  if the  true  MLE  is close  to 
one  of the  boundaries  (for  example, if d,,,-  =  0.998  in  the  AR(1)  example),  it 
will  be  virtually  impossible  to  keep  a  numerical  algorithm  from  exploring  what 
happens  when ¢ is greater  than  unity,  which  would  induce a fatal  crash. 

Solving  the  Problem  by Reparameterizing  the  Likelihood 
Function 

One  simple  way  to  ensure  that  a  numerical  search  always  stays  within  certain 
specified  boundaries  is to  reparameterize  the  likelihood  function  in  terms  of  an 
(a x  1) vector  A for  which  @  =  g(A),  where  the  function  g:  R* —  R? incorporates 
the  desired  restrictions.  The  scheme  is then  as  follows: 

: 

Input 

Procedure 

Output 

y  nae  = 

_,| 

Id 

egies  sa 

and  A 

set  ®  =  g(A); 
calculate  £(0) 

value  of » 
L(g(a)) 

| 

For  example,  to  ensure  that  ¢ is always  between  +1,  we  could  take 

bd  =  g(A)  =  1+) 

A 

[5.9.1] 

The goal  is  to  find  the  value  of  A  that  produces  the  biggest  value  for the  log 
likelihood.  We start  with  an  initial  guess  such  as  A =  3. The  procedure  to evaluate 
the  log likelihood  function  first  calculates 

o =  3(1  +  3) =  0.75 

and  then  finds  the  value  for  the  log likelihood  associated  with  this  value  of 6 from 
[5.7.2]. No matter  what  value  for A the  computer  guesses,  the value  of ¢ in [5.9.1] 
will  always  be less  than  1 in absolute  value  and  the likelihood  function  will be well 
146  Chapter  5 | Maximum  Likelihood  Estimation 

defined.  Once we  have  found  the  value  of A that  maximizes  the  likelihood function, 
the  maximum  likelihood  estimate  of @, is then  given  by 

‘ 
° =  Tar 

A 

This  technique  of reparameterizing  the  likelihood  function  so  that  estimates 
always satisfy  any  necessary  constraints  is often  very  easy  to  implement.  However, 
one  note  of caution  should  be  mentioned.  If a  standard  error  is calculated  from 
the  matrix  of second  derivatives  of the  log likelihood  as  in  [5.8.3],  this  represents 
the  standard  error  of A, not  the  standard  error  of ¢. To  obtain  a  standard  error 
for  ¢, the  best  approach  is first  to  parameterize  the  likelihood  function in  terms 
of A to  find  the  MLE,  and  then  to  reparameterize  in  terms  of ¢ to  calculate  the 
matrix  of second  derivatives  evaluated  at  ¢ to  get  the  final  standard  error  for  ¢. 
Alternatively,  one  can  calculate  an approximation  to the standard  error  for ¢ from 
the standard  error  for  A, based  on  the formula  for  a  Wald  test  of  a  nonlinear 
emer Sees 8 Fans, 14: 

eatin for a ais  iicg Matrix: 

Testriction one needs to impose is that a variance parameter 
oaraiane ‘An obvious  way  to  achieve  this is  to parameterize  the likelihood 
in terms of A which represents +1 times the standard yarn, The seosptare. (6 to 
evaluate the eet then a by squaring this cigar A:  eebut 

sic 

aye 

devi  rhe eet, itis calculated as 

and  if 
Coe}. 
end {5 A  3h impiies 

the  standard 
fiterc 

i: 

. 

=  Vie 

1.2  naltieoget  to Yor! 
— eR  Henn, 30  in es  clase  Osi} ee  val ta) 

wl 

See,  1 

on 
ppt  =v. ntolows t feast nl Sebo  sdisiaind dae 
bed)  tapes 
pe - -  may 

ahoenat  = 

=  it  Bp 

Mad | | ale Ih  Sainte  - 

Parameterizations  for  Probabilities 
Sometimes  some  of  the  unknown  parameters  are  probabilities  p,,  p2,  -- 

- 

; 

Px  Which  must  satisfy  the  restrictions 

0sp,=1 

oo  he  o>  ee  4 

Pr. tiPon tee bit  Pe 

In  this  case,  one  approach  is  to  parameterize  the  probabilities  in  terms  of A,,  Ao, 
wT,  Aor  Where 

' 

Pi 

PK 

Mz/Ch ot AZ  +  AZ  +--+  +  Any) 
(1  +  AZ  +  AZ  +  +++  +  A&_1). 

Te  ee  Oy aS me 

More  General  Inequality  Constraints 
For  more  complicated  inequality  constraints  that  do  not  admit  a  simple  re- 
parameterization,  an  approach  that sometimes  works  is to put a branching  statement 
in the  procedure  to  evaluate  the  log likelihood  function.  The  procedure  first  checks 
whether  the  constraint  is satisfied.  If it is, then  the  likelihood  function  is evaluated 
in  the  usual  way.  If it is not,  then  the  procedure  returns  a  large  negative  number 
in place  of the  value  of the  log likelihood  function.  Sometimes  such  an  approach 
will  allow  an  MLE  satisfying  the  specified  conditions  to  be  found  with  simple 
numerical  search  procedures. 

If these  measures  prove  inadequate,  more  complicated  algorithms  are  avail- 
able.  Judge,  Griffiths,  Hill,  and  Lee  (1980,  pp.  747-49)  described  some  of  the 
possible  approaches. 

APPENDIX  5.A.  Proofs  of Chapter  5 Propositions 

@  Proof  of Proposition  5.1. 

(a)  By Taylor’s  theorem, 

L(+)  =  £10)  +  [g(0°”)]'[O"*  —  OO)  +  R,(O™,  Of *).  — [5.A.1] 

Substituting  [5.7.18]  into  [5.A.1], 

LO" *)  —  £10)  =  [g(0°)]'sA™g(O™)  +  R,(O™,  OO" +). 

[S.A.2] 

Since  A  is positive  definite  and  since  g(@°”)  # 0, expression  [5.A.2]  establishes  that 

LO" *)  —  £100)  =  sx(O™)  +  R,(O™,  O"*), 

where  «(6“”)  >  0.  Moreover,  s~'-R,(0°”,  0°"*")  —  0 as  s  >  0.  Hence,  there  exists  ans 
such  that  £(0°"*)  —  £(0'”)  >  0, as  claimed. 
(b)  Direct  differentiation  reveals 

LON")  _  0£ 00, |  £90, 
00,  ds 

0,  as 

as 

[2( 

Qi" +  , 

FY Vasa 
J  rs 

BL 08, 
0,  as 

[5.4.3] 

=  [(g(0"*))'A™g(0™), 
with  the  last  line  following  from  [5.7.18].  The  first-order  conditions  set  [S.A.3]  equal  to 
zero,  which  implies 

0 =  [g(0"*™)]'sA™gO™)  =  [g@™*)]'[O™"  —  Om], 

with  the  last  line  again  following  from  [5.7.18].  This  establishes  the  claim  in [5.7.19]. 

148 

Chapter 5  | Maximum  Likelihood  Estimation 

(c)  Let  y be  any  (a  x  1) nonzero  vector.  The  task  is to  show  that  ees  Cy"  (). 

Observe  from  [5.7.16]  that 

[5.A.4] 

ehh 
y A‘  ‘  Vy 

y 

— 

; 
‘Aw(Agir +  (Ag  +  by  Aimle 
A™  e.  yA  Neg.  ASB"  2 A 

(Ag  *  ?)‘A’™(Ag"*») 

y 

y 

Since  A’”  is positive  definite,  there  exists  a  nonsingular  matrix  P such  that 

_  VO" )(AO™"'y 

(Ag'”*  )'(A@'"  +)  . 

Define 

A™  =  PP. 

y*  =  P'y 

x*  =  rag". 

Then  [5.A.4]  can  be  written  as 

: 

. 

¥ 

y Ary  =  y’PP’y 

F- y'PP’(Ag’"*)(Ag'”*  ')’PP’y 

(Ag'"* )'PP'(Ag”  +  ») 

_  ¥'(A0e"*  (AO  P)'y 

(Ag +  »y'(Ae + Al) 

[5.A.5] 

yy 
=  yeiye DMV) yBoer= 
(gery (een)  ~ 
repr 
ni 

My A0er* 

Recalling  equation  [4.A.6],  the first two terms in the last line of 
of — residuals  from  an  OLS  regression  of y* on  x* Te  ee be 

x" "x" 

[5.A.5] 

os ee has ap perfect fit, or if y* = Bx* or old 
— expression (5 ‘A 6] ‘would er sed 
‘if 

eee en lite ase, the in ali 5A ans 

yeG-nabiedt  :o3zt 
[y'Aoe""  3 cash) A  eet  2. 2, Seer 

nie 

—$—$—$—$—$—$—$—$—$$— $$ 

I  —_=KRe—ee 

Chapter  5 Exercises 

Show  that  the  value  of [5.4.16]  at  @  =  6, 0?  =  &?  is identical  to  its  value  at  @  =  @~', 

5.1. 
a?  =  67a". 

Verify  that  expression  [5.7.12]  calculates  the  maximum  of [5.7.6]  in a  single  step  from 

5.2. 
the  initial  estimate  @  =  (-—1,  1)’. 
5.3. 
(a) 

Let  (y,,¥2,..-,»¥z)  be  a sample  of size  T drawn  from  an  i.i.d.  N(u,  77) distribution. 
Show  that  the  maximum  likelihood  estimates  are  given  by 

T 

a=  72, Y. 

7 

fa,  T-*o  fy.  x  A)’. 

= 1 

(b) 

Show  that  the  matrix  $,, in  [5.8.2]  is 

hy  62 
2P  ke Onin 

0 
Aan)  1) 

(c) 

Show  that  for  this  example  result  [5.8.1]  suggests 

oe  N 

eS 
o| 

mM 
o7/T 
“\o2V  | 0 

0 
26/T{)" 

Chapter  5 References 

Anderson,  Brian  D.  O.,  and  John  B.  Moore.  1979.  Optimal  Filtering.  Englewood  Cliffs, 
N.J.:  Prentice-Hall. 
Berndt,  E. K., B. H.  Hall,  R.  E. Hall,  and J.  A. Hausman.  1974.  “Estimation  and  Inference 
in Nonlinear  Structural  Models."’  Annals  of Economic  and  Social  Measurement  3:653-65. 
Box,  George  E.  P.;  and  D.  R.  Cox.  1964.  “‘An  Analysis  of Transformations.”  Journal  of 
the  Royal Statistical  Society  Series  B, 26:211-S2. 

and  Gwilym  M.  Jenkins.  1976.  Time  Series  Analysis:  Forecasting  and  Control,  rev. 

ed.  San  Francisco:  Holden-Day. 
Broyden,  C. G.  1965.  “‘A  Class  of Methods  for Solving  Nonlinear  Simultaneous  Equations.” 
Mathematics  of Computation  19:577-93. 

, 

.  1967.  ““Quasi-Newton  Methods  and  Their  Application.to  Function  Minimization.” 

| 

| 

; 

Mathematics  of Computation  21:368-81. 
Chiang,  Alpha  C.  1974.  Fundamental  Methods  of Mathematical  Economics,  2d  ed.  New 
York:  McGraw-Hill. 
_Davidon,  W.  C.  1959.  ‘Variable  Metric  Method  of Minimization.”  A.E.C.  Research  and 
Development  Report  ANL-5990  (rev.). 
Fletcher,  R.,  and  M.  J.  D.  Powell.  1963.  “A  Rapidly  Convergent  Descent  Method  for 
Minimization.””  Computer  Journal  6:163-68. 
Galbraith,  R.  F., and  J. I. Galbraith.  1974.  ‘‘On  the Inverses  of Some  Patterned  Matrices 
Arising  in the Theory  of Stationary  Time  Series.”  Journal  of Applied  Probability  11:63-71. 
Hannan,  E.,  and J. Rissanen.  1982.  “Recursive  Estimation  of Mixed  Autoregressive —Mov- 
ing Average  Order.”  Biometrika  69:81-94. 
Janacek,  G. J., and  A.  L. Swift.  1990.  “A Class  of Models  for Non-Normal  Time  Series.” 
Journal  of Time  Series  Analysis  11:19-31. 
Judge,  George  G., William  E.  Griffiths,  R.  Carter  Hill,  and  Tsoung-Chao  Lee.  1980.  The 
Theory  and  Practice  of Econometrics.  New  York:  Wiley. 
Koreisha,  Sergio,  and  Tarmo  Pukkila.  1989.  “Fast  Linear  Estimation.  Methods  for  Vector 
Autoregressive  Moving-Average  Models.”’  Journal  of Time  Series  Analysis  10:325-39. 
Li, W.  K., and A.  I. McLeod.  1988.  “ARMA  Modelling  with  Non-Gaussian  Innovations.” 
Journal  of Time  Series  Analysis  9:155-68. 
Martin,  R.  D.  1981.  “Robust  Methods  for Time  Series,”  in D.  F.  Findley,  ed.,  Applied 
Time  Series,  Vol.  Il.  New  York:  Academic  Press. 

150  Chapter 5 | Maximum  Likelihood Estimation 

Nelson,  Harold  L.,  and  C.  W.  J.  Granger.  1979.  “Experience  with  Using  the  Box-Cox 

tion When  Forecasting  Economic  Time  Series.”’  Journal  of Econometrics  10:57 - 

69. 
Quandt,  Richard  E.  1983.  “Computational  Problems  and  Methods,”  in Zvi  Griliches  and 
Michael  D. Intriligator,  eds., Handbook  of Econometrics,  Vol.  1. Amsterdam:  North-Holland. 
White,  Halbert.  1982.  ‘Maximum  Likelihood  Estimation  of Misspecified  Models.”  Econ- 
ometrica  50:1 -25. 

op S43 tol nonKinge S7gsi  £  daus to 28  L5H) 

tO}  941 
oe senna SDH  ss ai  Pntiallh 16 q ce Y abswisd 
sibonsg to ws boityiow $6 at esctee adi eadimesd  bagteni raigeds 2 
QemaypT?  raleoiica' t Biohsb wo sed fused  ond (ii)}203 ono! ods te an 
|  Rive thas  fos +  RtrRUME TONSA  ERAN OA  4 ie it 4] implie 

Ani  see  ond  a  - 

aoe se  Siii 

as 

, 2 a ininies 

iy  ome 

Slih  terasinys  ingnodini  wort siitrnaatst: as  orf  tire  pei Ba 

a  Bass MONSepet,  26  nwond  ai anil  ¥ to  yorvedod  or?  xt  af 

+13 

eo 

re  sashise ee  es  oe Padiw  aye  .nteytens 
“2 omit & od esd eine | vienoisleaonshavos 
aes. mFS  plang  e hae “Tose  pao 

Saar ge ree gainer bce erent 

yee. 3 

ai 

arn  erestteirseoi Meares  et =  pes  seh ne 

: 

. 

= 
~ 

gq 

oct 

: 

(- 

me 

2 

= 

a 

——S. 

fs  ~ 

= 

eee 

Spectral  Analysis 

Up to  this  point  in  the  book,  the  value  of a  variable  Y, at  date  1 has  typically  been 
described  in  terms  of a  sequence  of innovations  {e,}7_  _.  in  models  of  the  form 

Y,  =e  i  bs YE, _;- 

j= 

The  focus  has  been  on  the  implications  of such  a  representation  for  the  covariance 
between  Y, and  Y,at distinct  dates  t and 7.  This  is known  as  analyzing  the  properties 
of {Y,}*=  _.  in  the  time  domain. 

This  chapter  instead  describes  the  value  of  Y, as  a weighted  sum  of periodic 
functions  of the  form  cos(wt)  and  sin(wt),  where  w  denotes  a  particular  frequency: 

y  i=  ok  fe a(w)-cos(wt)  dw  +  [" 5(w)-sin(wt)  dw. 

The  goal  will  be  to  determine  how  important  cycles  of different  frequencies  are  in 
accounting  for  the  behavior  of  Y.  This  is known  as  frequency-domain  or  spectral 
analysis.  As  we  will  see,  the  two  kinds  of analysis  are  not  mutually  exclusive.  Any 
covariance-stationary  process  has  both  a  time-domain  representation  and a fre- 
quency-domain  representation,  and  any  feature  of the  data  that  can  be  described 
by one  representation  can  equally  well  be  described  by the  other  representation. 
For  some  features,  the  time-domain  description  may  be  simpler,  while  for  other 
features  the  frequency-domain  description  may  be  simpler. 

Section  6.1  describes  the properties  of the population  spectrum  and  introduces 
the  spectral  representation  theorem,  which  can  be  viewed  as  a  frequency-domain 
version  of Wold’s  theorem.  Section  6.2  introduces  the  sample  analog  of the  pop- 
ulation  spectrum  and  uses,an  OLS  regression  framework  to  motivate  the  spectral 
representation  theorem  and  to  explain  the  sense  in  which  the  spectrum  identifies 
the  contributions  to  the  variance  of the observed  data  of periodic  components  with 
different  cycles.  Section  6.3  discusses  strategies  for estimating  the  population  spec- 
trum.  Section  6.4 provides  an  example  of applying  spectral  techniques  and discusses 
some  of the  ways  they can  be used  in practice.  More  detailed  discussions  of spectral 
analysis  are  provided  by Anderson  (1971),  Bloomfield  (1976),  and  Fuller  (1976). 

6.1.  The  Population  Spectrum 

The  Population  Spectrum  and  Its  Properties 

Let  {Y,}/.  ..  be  a  covariance-stationary  process  with  mean  E(Y,)  =  mu  and, 

jth autocovariance 

152 

E(Y,  -  u)(Y,-;  =  ~)  =  4;. 

Assuming  that these  autocovariances  are  absolutely  summable,  the autocoyariance- 
generating  function  is given  by 

8y(z)  =  Pee 

(6.1.1] 

where  z denotes  a complex  scalar.  If (6.1. gl is divided  by 27 and  evaluated  at some 
z  represented  by z  =  e~  for  i =  \/—1  and  w a 
real  scalar,  the  result is  called 
the  population  spectrum  of  Y: 

as 
Sy(w)  =  5, By (e"™)  $e  es me, 

lex 

1 

[6.1.2] 

Note  that  the  spectrum  is a function  of w:  given  any  particular  value  of w  and  a 
sequence  of autocovariances  {y,}* _.,  we  could  in principle  calculate  the  value  of 

j=z-= 

Sy(w). 

De  Moivre’s  theorem  allows  us  to  write  e~ ‘”!  as 

Substituting [6.1.3] into [6.1.2], ,  it appears that  the spectrum  can  equivalently  be 

e~  =  cos(w/)  —  i-sin(w/). 

{6.1.3} 

(Hisytasi  7 

~ sy(w) = 2 yfeos(wj) ~ i-sin(wj)). 
whe oe 
Note that for a iRiears ant  Aehie o  he yj =  y-;.  Hence,  (6. i, ai sl 

"Seta  4 

Fc  = r0[cos(0) ~  Fsin(0)]  | neat 

etsy  “66 1.5] 

- Af3 2 res iio «) chub; eae i-sin(—j)] ¢. 

| 
use  of ‘the fo ‘ol om  1B results fom genome! 

ot  i  36 

: 

BF We to he Bague use  ai  Tl ane 

eof or(0} for rn  ana 2} or i616). The ¢ 

now te  we 

‘apijoay mel 

Tl — 

p  r% 

a 

: 

i 

=, 
Pore 

ah 

6) aes: ;  (6) sa 

Th 

eek  pr 

sie 

aecuaigs 

epee. Mpg  end Bo  asic mas 

aS 

a  aie  ee 

>  ie 

aa 

We 

: 

; 

ar 

: 

; 

. 

:  a  ~  “  -  d 

> 

Calculating  the  Population  Spectrum  for Various  Processes 
Let  Y, follow  an  MA()  process: 

scans 

Y,="  +  ML), 

[6.1.7] 

where 

WL) =  2, yj L’ 

2, wl < 

o?  = fort  = 
E (ce,) =  {° alee 

Recall front rureision  [3.6.8] that the sce  atlas function for Y 
is given  by 

gr(z)  = “ev(2W(2-). 
Hence, from  [6.1.2],  the population spectrum for an  MA(~) process i is given by 
(6.1.8) 
eulets fora Ane or Process, (z) = Land ae 5  ean  eon 

 Sy(w) = (2a) -o7y(e-“u(e". 

| 

cage e 

vm  eS bh ieponateny 

102  —  [6.1.9], 

: 

i= 6, + Bea 

EBS seed; “¢O)e0 nbacietuipenin 
a>  i 

1  Es # 

rg 

rcecrral 
z |  bgucive,  Any 
msipe and a Be. 

When  ¢ > 0, the denominator  is monotonically increasing in w over (0, 7], meaning 
that  sy(w)  is  monotonically  decreasing.  When  @ <  0, the  spectrum  sy(w)  is a 
monotonically  increasing  function  of w. 

In general,  for an  ARMA(p,  q) process 

Y=e+  Oil,-1.  +  GYi2 

+ 

+  $,Y,-p  +  €,  +  O€,-4 

+  0,£,_>  tree  +  ea 

the population  spectrum  is given  by 

Sy(w)  = 

a  (1  +  Oe"  +  Ge~™  + 
+  +  Oe~  em) 
an (i —  Gye3®  -  doer.  ~—  +++  -  oe-*) 
‘  (I  +  Oe"  +  Gee +  ---  +  Oc) 

> 

(6.1.14] 

Gieca  Sd Gul toss 
If the moving  average  and  autoregressive  polynomials  are  factored  as  follows: 

GE!) 

1  +  02  +  027 + -+-  +  0,24 = (1  —  mz)(1  —  mz)°-- (1 -  9,2) 
-  (1 —  A,2), 
De ie 

=  Oe?  =  (hi  Ayz)(1  —  Agz)  + 

get  —  «+ 

- 

then the spectral  density in [6.1.14] can be written 
tadt  Y ic.  ssnsiiey 
_wborsg  OF  hatadnis  sd 

bina 

ort  10  copiacsds 

ete 

5 

ta 

= 

 'ase 

a 

ie 

iT  aT 

fF 
ded 
Seimot  = 

od 

OR EN  age  gp 

u 

% 

| 

a 
4 

*. 

| 

Interpreting  the  Population  Spectrum 

The  following  result  obtains  as  a  special  case  of  Proposition  6.1  by  setting 
(0): 

k  = 

f Sy (w)  dw  =  %- 

[6.1.17] 

In other  words,  the  area  under  the  population  spectrum  between  +  7  gives  Yo,  the 
variance  of  Y,. 

More  generally—since  sy(w)  is nonnegative —if we  were  to  calculate 

w) 

| Sy(w)  dw 
“4 

a. 

for  any  w,  between  0 and  7,  the  result  would be a  positive  number  that  we  could 
interpret  as  the  portion  of the  variance  of  Y, that  is associated  with  frequencies  w 
that  are  less  than  w,  in absolute  value.  Recalling  that  sy(w)  is symmetric,  the  claim 
is that 

2 -{" sited  des 

[6.1.18] 

represents  the  portion  of  the  variance  of  Y that  could  be  attributed  to  periodic 
random  components  with  frequency  less  than  or  equal  to  ,. 

What  does  it mean  to attribute  a certain  portion  of the  variance  of  Y to cycles 
with  frequency  less  than  or  equal  to  w,?  To  explore  this  question,  let  us  consider 
the  following  rather  special  stochastic  process.  Suppose  that  the  value  of  Y at  date 
t is determined  by 

M 

Y, =  > [a;cos(w,t)  +  §,sin(w,t)]. 

: 

j= 1 

[6.1.19] 

Here,  a, and 6; are  zero-mean  random  variables,  meaning  that  E(Y,)  =  0 for  all 
t.  The  sequences  {a},  and  {6,}™,  are  serially  uncorrelated  and  mutually  uncor- 
related: 

. 

ao? 
forj=k 
yp i  {*  a +k 

tor  y=  & 
oe 
Pee  te  for j #  k 

E(a;6,)  =  0 

for  all j and  k. 

The  variance  of  Y, is then 

E(Y?)'= 

Soke: 

Eoresen +  £(65) sino) 

! | cosu)  +  sino 

[6.1.20] 

with the  last  line  following  from  equation  [A.1.12].  Thus,  for  this  process,  the 
portion  of the  variance  of  Y that  is due  to  cycles  of frequency  w, is given  by @?. 

156  Chapter  6  | Spectral  Analysis 

If the  frequencies  are  ordered  0 <  w,  <  w,  <-**  <  wy  <  7,  the  portion  of  the 
variance of  Y  that is  due to  cycles  of frequency  less  than  or  equal  to  w, is given  by 

The kth  autocovariance  of  Y is 

E(Y,Y,_,)  =  af { E(a?)-cos(w,t)-cos[w,(t  —  k)] 

“3 

[6.1.21] 

=  2% {cos(w,t)-cos[w,(t  —  k)] 
ne sin(w,t)-sin{w,(¢  —  k))}. 

Recall  the  trigonometric  identity? 

cos(A  —  B)  = cos(A)-cos(B)  +  sin(A):sin(B). 

[6.1.22] 

For A =  wf and  B  = 

w(t —  k), we  have  A  —  B  =  wk, so  that  [6.1.21]  becomes 

M 

E(Y,Y,_,)  =  > o?-cos(w;k). 
j=) 

[6.1.23] 

Since  the  mean  and  the  autocovariances  of Y are  not  functions  of time,  the  process 
described  by  [6.1.19]  is covariance-stationary,  although  [6.1.23]  implies  that  the 
sequence  of autocovariances  {y,}¢-  is not  absolutely  summable. 

We  were  able  to  attribute  a  certain  portion  of the  variance  of Y, to  cycles  of 
less  than  a given  frequency  for the  process  in [6.1.19]  because  that  is a rather  special 
covariance-stationary  process.  However,  there  is  a  general  result  known  as  the 
spectral  representation  theorem  which  says  that  any  covariance-stationary  process 
Y, can  be expressed  in terms  of a generalization  of [6.1.19].  For any fixed  frequency 
w  in  [0,  7], we  define  random  variables  a(w)  and  6(w)  and  propose  to  write  a 
stationary  process  with  absolutely  summable  autocovariances  in the  form 

Y,=prt  [ [a(w)-cos(wt)  +  5(w)-sin(wt)]  dw. 

The  random  processes  represented  by a(-) and  6(-) have  zero  mean  and  the  further 
properties  that  for  any  frequencies  0 <  w,  <  w,  <  w;  <  w,  <  7,  the  variable 
J%? 5(w)  dw  is 
S2:  a(w)  dw  is  uncorrelated  with  f%: a(w)  dw  and  the  variable 
uncorrelated  with  f%: 5(w)  dw,  while  for  any  0 <  w,  <  w#,  <  mand  0 <  a;  < 
w,  <  7,  the  variable  [%? a(w)  dw  is uncorrelated  with  f%s 5(w)  dw.  For  such  a 
process,  one  can  calculate  the  portion  of the  variance  of Y, that  is due  to  cycles 
with frequency  less than  or equal to some  specified  value  w, through a generalization 
of the  procedure  used  to  analyze  [6.1.19].  Moreover,  this  magnitude  turns  out  to 
be given  by the  expression  in [6.1.18]. 

| 

We  shall  not  attempt  a  proof  of the  spectral  representation  Secret  here; 
for  details  the  reader  is referred  to  Cramér  and  Leadbetter  (1967,  pp.  128-38). 
Instead,  the  next  section  provides  a formal  derivation  of a finite-sample  version  of 
these  results,  showing  the  sense  in  which  the  sample  analog  of [6.1.18]  gives  the 
portion  of the sample  variance  of an  observed  series  that  can  be attributed  to cycles 
with frequencies  less  than  or  equal  to a. 

See,  for example,  Thomas  (1972.  p.  176). 

6.1.  The  Population  Spectriim 

157 

6.2.  The  Sample  Periodogram 

For  a  covariance-stationary  process  Y, with  absolutely  summable  autocovariances, 
we  have  defined  the  value  of the  population  spectrum  at  frequency  w  to  be 

1 
sy(w)  =  Im eve 

| 

[6.2.1] 

where 

y =  EY,  —  ply,.3>  @) 

and  w«  =  E(Y,).  Note  that  the  population  spectrum  is  expressed  in  terms  of 
{y;}79,  which  represents  population  second  moments. 

Given  an  observed  sample  of T observations  denoted  y,, y2,.  . 

. 

,  yr,  we  can 

calculate  up  to  T  —  1 sample  autocovariances  from  the  formulas 

T 

oe  dee 0-5 
# 

poe 

t=s+ 

= 

; 

ee  ee  eet 
for) = —1)<2;  Linesd  +  A, 

; 

where  y is the  sample  mean: 

L(3 

PATAD)  gi. 

[6.2.2] 

[6.2.3] 

For any given  w we  can  then  construct  the sample  analog of [6.2.1],  which  is known 
as  the  sample  periodogram: 

44d)  BA  oy ye": 

27  j=  —T+1 

! 

[6.2.4] 

As  in [6.1.6],  the  sample  periodogram  can  equivalently  be expressed  as 

1  F 

T-1  r 

§,(w)  =  ak +  22 7; cosa) | 

[6.2.5] 

The  same  calculations  that  led  to [6.1.17]  can  be used  to show  that  the  area  under 
the  periodogram  is the  sample  variance  of y: 

j= 

} §,(w) dw  =  Fp. 

Like  the  population  spectrum,  the  sample  periodogram  is  symmetric  around 
w  =  0), so  that  we  could  equivalently  write 

yo =  2{ §,(w)  dw. 

There  also  turns  out  to  be  a sample  analog  to  the  spectral  representation 
theorem,  which  we  now  develop.  In particular,  we  will  see  that  given  any  T ob- 
servations  on  a process  (y,, 3,  . 
.  Ws, 
and  coefficients  ji, @,, @2,...  ,  &y,  1, 
,  bay Such  that  the  value  for y at 
date  ¢ can  be expressed  as 
Mae 

, yp), there  exist  frequencies  w,,  w:,.  . 

5;,  . 

. 

. 

. 

. 

. 

| 

| 

y=  a +  p> {é;cos[w,(¢  —  1)] +  §;sinfw,(r  =  1))}. 

[6.2.6] 

158  Chapter 6  | Spectral Analysis 

where  the  variable  @;-cos{w;(¢  ~ 
1)}'is orthogonal in  the  sample  to  a,;cos[w,(t  ~  1)] 
for  j; #  k, the  variable §;sin{w,(¢  —  1)) is  orthogonal  to  §,-sinfw,(¢  —  1)] for j +  k. 
and  the  variable  4, -cos[w, (¢ —  1)] is orthogonal  to  5,-sin{w,(¢  —  1)] for  all j and  k. 
The  sample  variance  of y is  T~'S7_,(y,  —  ¥)?,  and  the  portion  of this  variance 
that  can  be  attributed  to  cycles  with  frequency  w, can  be inferred  from  the  sample 
periodogram  $§,(w,). 

We  will  develop  this  claim  for  the  case  when  the  sample  size  T is an  odd 
number.  In  this  case  y,  will  be  expressed  in  terms  of  periodic  functions  with 
M =(T  -  1)/2  different  frequencies  in  [6.2.6].  The  frequencies  w,,  w,,...  ,  wy 
are  specified  as  follows: 

w,  =  2n/T 

w  =  4n/T 

wy  =  2Mn/T. 

{6.2.7] 

Thus,  the highest  frequency  considered  | is 

/ 

' 

er -  In. 

tej 

mF 

. 

Sy 

=:  Gia  : 

7 

Mig ey ected ae  ra of the value of, on constant and on  the various 

y=  nt S tapcosfay(e  - Y) + 4 sinloe —1 Dp + a 

This. can ne viewed as  a tina  regression model of the form 

ii 

yp us Bix, % Uy,  Tat  7  sels 

to  .  &  ey  (6. 2.8] 

patie’ amv a par Rk; NZ Yeas  eonaideas  rf <p wit 

- 

Sitch oii = hi Ds 

id Brey 

1 

a  ans “Sig pee pe all 

" 

re 

ing i 

<  pr” Weed  7 

+ 

% 

i. 

ato 

« 

De 

oe 

“ 

a 

+  my 

q 

7 

¥ 

ra 

’ 

- 

a 

‘ 

edi, 

e 

'. 

1)/2.  Let 
Proposition  6.2: 
w,  =  2nj/T  forj  =  1,2,...,M,  and  let  x,  be  the  (T  x  1) vector  in  [6.2.9].  Then 

Let  T  denote  an  odd  integer  and  le  M  =  (T  — 

T 

SX 
(=I 

aes 

: 
0 

/ 

“  | 

(T/2)17_, 

[6.2.11] 

Furthermore,  let  {y,, y>,  - 

- 

- 

»  Yr} be  any  T numbers.  Then  the following  are  true: 

(a) 

The  value of y,  can  be  expresséd  as 

B=  en  +  > {a;-cos[w(t  — 

1)]  +  §;-sin[w(t  — 

1)]}, 

with  « =  y (the sample  mean  from  [6.2.3])  and 

&, =  (2/T)  > y,cos[w(t  —  1)] 

forijcigdias..,  M- 

[6.2.12] 

5; =  (2/T)  > y,sin[w(t  —  1)]  eet  2:  MS 

162338 

(b)  The  sample  variance  of y, can  be  expressed  as 

(1/T)  z (y,  —  y)?  =  (1/2)  2 (a?  +  87), 

[6.2.14] 

and  the portion  of the sample  variance  of y that  can  be attributed  to  cycles  of 
frequency  w, is given  by 3(a?  +  5?). 

(c)  The  portion  of the  sample  variance  of y  that  can  be  attributed  to  cycles  of 

frequency  w, can  equivalently  be expressed  as 

(1/2)(a?  +  6?) =  (4m/T)-S,(w,), 

[6.2.15] 

where  $,(w,)  is the sample  periodogram  at frequency  w;. 

Result  [6.2.11]  establishes  that  2/_,x,x;  is a  diagonal  matrix,  meaning  that 
the  explanatory  variables  contained  in x, are  mutually  orthogonal.  The  proposition 
,  Yr) with  T odd  can  be  written 
asserts  that  any  observed  time  series  (y,, y2,  . 
as  a  constant  plus a  weighted  sum  of (7  —  1) periodic  functions  with  (TJ  —  1)/2 
different  frequencies;  a  related  result  can  also  be  developed  when T is an  even 
integer.  Hence,  the  proposition  gives  a  finite-sample  analog  of  the  spectral  rep- 
resentation  theorem.  The  proposition  further  shows  that  the  sample  periodogram 
captures  the  portion  of  the  sample  variance  of y that  can  be  attributed  to  cycles 
of different  frequencies. 

. 

. 

Note  that  the  frequencies  w, in terms  of which  the  variance  of y is explained 
all lie  in [0, a7]. Why aren’t  negative  frequencies  w <  0 employed  as  well?  Suppose 
that  the  data  were  actually  generated  by a  special  case  of the  process  in [6.1.19], 

Y, =  a-cos(—  wt)  +  d-sin(  -  wf), 

[6.2.16] 

where  —w  <  0 represents  some  particular  negative  frequency  and  where  a  and  6 
are  zero-mean  random  variables.  Since cos( —  wt) = cos(wf) and sin(— wrt) =  —sin(wt), 
the  process  [6.2.16]  can  equivalently  be written 

Y, =  a-cos(wt)  —  d:sin(wf). 
[6.2.17] 
Thus  there  is no  way  of using observed  data  on  y to  decide  whether  the  data  are 
generated  by a cycle  with  frequency  —  w  as  in [6.2.16]  or  by a cycle  with frequency 

160  Chapter  6  | Spectral Analysis 

cos((m/2)t} 

— 

cos({ (31/2) ¢] 

FIGURE  6.1 

Aliasing:  plots of cos{(z/2)1]  and cos[(37/2)r]  as  functions  of ¢. 

+w  as  in (6. 2 ine seri a  matter ate convention that we c  choose re only 
on  positive frequencies. 

Why  is  »  =  7a the  largest  frequency  considered?  Suppose  the Male were  » 
generated  from a periodic  eee  ay with  foes w >  wm,  say, w =  3/2  for 
illustration: 

— 

i  lauipe  1  nif seat- cos(3m/2)¢)  + 6 -sin{(377/2)t]. 

[6.2.18] 

© yosoupont.  rehsoitg  ens  Yo 

roperties of the sine an cosine function ml that ib? sh is sme 
nouudi 

em 

Siu 

Y, =a: sae voi: + 3. ate eee  MAAI2! “(62 19) 

i < id is spanish  i tse,  &  Matis: 4M Bot isiug og  of} meee Ty 

Note  that  in  a particular  finite  sample,  the  lowest  frequency  used  to  account 
for  variation  in y is #,  =  27/T,  which  corresponds  to  a  period  of  T.  Ifa  cycle takes 
longer  than  T periods  to  repeat  itself,  there  is not  much  that  one  could  infer  about 
it if one  has  only  7 observations  available. 

Result  (c) of Proposition  6.2  indicates  that  the  portion  of the  sample  variance 
of y that  can  be  attributed  to  cycles  of frequency  «,  is proportional  to the  sample 
periodogram  evaluated  at  w,,  with  47/T  the  constant  of proportionality.  Thus, the 
proposition  develops  the  formal  basis  for  the  claim  that  the  sample  periodogram 
reflects  the  portion  of the  sample  variance  of y that  can  be  attributed  to  cycles  of 
various  frequencies. 

. 

Why  is the  constant  of proportionality  in  [6.2.15]  equal  to  47/T?  The  pop- 
ulation  spectrum  sy(w)  could  be  evaluated  at  any  w  in  the  continuous  set  of points 
between  0 and  7.  In this  respect  it is much  like  a  probability  density  f(x),  where 
X is a  continuous  random  variable.  Although  we  might  loosely  think  of the  value 
of fy(x)  as  the  “probability”  that  X  =  x,  it is more  accurate  to  say  that  the  integral 
*2 f(x)  dx  represents  the  probability  that  X takes  on  a  value  between  x,  and  x). 
As x,  —  xX,  becomes  smaller,  the  probability  that  X will  be observed  to  lie  between 
x,  and  x,  becomes  smaller,  and  the  probability  that  X would  take  on  precisely  the 
value  x  is effectively  equal  to  zero.  In just  the  same  way,  although  we  can  loosely 
think  of the  value  of sy(w)  as  the  contribution  that  cycles  with  frequency  w  make 
to  the  variance  of  Y, it is more  accurate  to  say  that  the  integral 

| sve) j=  [ si,  Pree 

represents  the  contribution  that  cycles  of frequency  less  than  or  equal  to  w,  make 
to  the variance  of Y, and  that  f%2s)(w)  dw represents  the  contribution  that  cycles 
with  frequencies  between  w,  and  w,  make  to  the  variance  of  Y.  Assuming  that 
Sy(w)  is continuous,  the  contribution  that  a  cycle  of  any  particular  frequency  w 
makes  is technically  zero. 

Although  the  population  spectrum  s,(w)  is defined  at  any  o in (0, z], the 
representation  in [6.2.6] attributes  all of the sample variance  of y to the particular 
frequencies  w,,  W>,...  ,  Wy.  Any  variation  in  Y that  is in  reality  due  to  cycles 
with  frequencies  other  than  these  M particular  values  is attributed  by [6.2.6]  to 
one  of  these  M  frequencies.  If  we  are  thinking  of  the  regression  in  [6.2.6] 
as  telling  us  something  about  the  population  spectrum,  we  should  interpret 
3(@?  +  6?) not  as  the  portion  of  the  variance  of  Y that  is  due  to  cycles  with 
frequency  exactly  equal  to  w,, but  rather  as  the  portion  of the  variance  of Y that 
is due  to  cycles  with  frequency  near  w,.  Thus  [6.2.15]  is not  an  estimate  of the 
height  of the  population  spectrum,  but  an  estimate  of the  area  under  the  pop- 
ulation  spectrum. 

r934Q 

. 

This  is illustrated  in  Figure  6.2.  Suppose  we  thought  of 4(@?  +  6?) as  an 
estimate  of the  portion  of the  variance  of Y that  is due  to  cycles  with  frequency 
between  w,_,  and  w,, that  is, an  estimate  of 2 times  the  area  under s)(w)  between 
w;_,  and  w;.  Since  w;,  =  27j/T,  the  difference  w;  —  w,  , is equal  to 2m/T.  If §,(@,) 
is an  estimate  of sy(w,),  then  the  area  under  s,(w)  between  w,_,  and  w, could  be 
approximately  estimated  by the  area  of a  rectangle  with  width  27/T  and  height 
5,(w,).  The  area  of such  a rectangle  is (27/T):§,(w,).  Since  (a?  +  5?) is an estimate 
of  2 times  the  area  under  s,(w)  between  w;-,  and  w,,  we  have  $(a?  +  8?)  = 
(47/T)-8,(w;),  as  claimed  in equation  [6.2.15]. 

Proposition  6,2  also  provides  a convenient  formula  for calculating  the  value  of 
the  sample  periodogram  at  frequency  w  =  2aj/T  for  j =  1, 2,4...  0(T  =  A), 

162 

Chapter 6  | Spectral Analysis 

$,(0) 

JRSS ae BS 2) 

: 

; 
> 

; 

oy od ie) 5 ee 
: 

at  2 

ym 
: 

e-< 
i 
: 
eae  ace 

§ 

<<  Pa  2 

yA 
T 
*  6 Ee  RO  es 
* 
DGOR  BS  emer 
Asie  | 
= 
c  Om  a Ve 
A  fon} (x  : 
| ‘ 

ee  a 

a 

‘Zz 

% 

yes teed  snes  dhe =e and the  portion of the 

nes of y attributable to cycles of different, frequencies. 

jG  &  vi  fu)  } ct. Sitie 

be, 

: 

ub)  Pa  yf  bf mie  % 

? 

,  ~ ila oer 7. agit Bes  ad ‘33  Bai pau uu =)  bie itt: era  sho da  ] 

eas siya \(a7 

ts) 
2B od aj  iedila  2t  (we 
- 

6) 
bepeidua  vistemixoigas 
te.  yeep 
Bye  EE Lodt ©. ae 
nade 
ils re) re tau FL  ual  SY  bas  70.5 
‘der i 16 Soules Sui 

+ 

ut 

serious  limitations.  Suppose  that 

Y,  a  > we,  ‘ 

. 

j/=0 

where  {w;}* , is absolutely  summable  and  where  {e,}7  _,.  is an  i.1.d.  sequence  with 
E(e,)  =  Oand  E(e?)  =  o?.  Let  sy(w)  be  the  population  spectrum  defined  in [6.1.2], 
and  suppose  that  s,(w)  >  0 for  all  w.  Let §,(w)  be  the  sample  periodogram  defined 
in  [6.2.4].  Fuller  (1976,  p.  280)  showed  that  for  w  #  0  and a sufficiently  large 
sample  size  T, twice  the  ratio  of the  sample  periodogram  to  the  population  spectrum 
has  approximately  the  following  distribution: 

CSA)  oc  y?(2). 
Sy(w) 

[6.3.1] 

Moreover,  if  A  #  w,  the  quantity 

“SA oo 
Sy(A) 
also  has  an  approximate  y2(2)  distribution,  with  the  variable  in  [6.3.1]  approxi- 
mately  independent  of that  in  [6.3.2]. 

bees 

Since  a  y2(2)  variable  has  a  mean  of 2, result  [6.3.1]  suggests  that 

2°§,(w) 
} 

=2 

e| Sy(w)  | 

or  since  5;(w)  is  a population  magnitude  rather  than  a  random  variable, 

E[S,(w)]  =  sy(w). 
Thus,  if the  sample  size  is sufficiently  large,  the  sample  periodogram  affords  an 
approximately  unbiased  estimate  of the  population  spectrum. 

Note  from  Table  B.2  that  95%  of the  time,  a x7(2)  variable  will  fall  between 
0.05  and  7.4.  Thus,  from  [6.3.1],  5,(w)  is unlikely  to  be as  small  as  0.025  times  the 
true  value  of sy(w),  and  §,(w)  is unlikely  to  be  any  larger  than  3.7  times  as  big as 
Sy(w).  Given  such  a  large  confidence  interval,  we  would  have  to  say  that  §,(w)  is 
not  an  altogether  satisfactory  estimate  of sy(w). 

Another  feature  of  result  [6.3.1]  is that  the  estimate  $,(w)  is not  getting  any 
more  accurate  as  the  sample  size  T increases.  Typically,  one  expects  an  econometric 
estimate  to get better  and  better  as the sample size grows.  For example,  the variance 
for  the  sample  autocorrelation  coefficient  6, given  in  [4.8.8]  goes  to  zero  as  T  >  ~, 
so  that  given  a sufficiently  large  sample,  we  would  be  able  to  infer  the  true 
value  of p, with  virtual  certainty.  The  estimate  §,(w)  defined  in  [6.2.4]  does  not 
have  this  property,  because  we  have  tried  to  estimate  as  many  parameters  (Yo, 7, 

-  + Yr-1)  as  we  had  observations  (y,, y2,  ..-,  Yr). 

Parametric  Estimates  of the  Population  Spectrum 

Suppose  we  believe  that the  data  could  be represented  with  an  ARMA(p,  q) 

model, 

“ 

YS  OLS  GY,  PQ a FOS  48,  +  Oe,  [6.3.3] 

+5  O5€,.5  AN 

OSs 

oe  O46 ra 

where  e, is white  noise  with  variance  a2,  Then.an  excellent  approach  to estimating 
the  population  spectrum  is first  to  estimate  the  parameters  wu,  ,,...,  dp, 9, 

164  Chapter  6  | Spectral Analysis 

,  8, and  a? by maximum  likelihood  as  described  in the  previous  chapter.  The 
maximum  likelihood  estimates  (¢,,.  .  .., 6, 6,,.  .  . , 6,,  6?) could  then  be plugged 
into  a  formula  such  as  [6.1.14]  to  estimate  the  outaten spectrum  s,(w)  at  any 
peanciey w.  If the  same is  correctly  specified,  the  maximum  likelihood  estimates 
iabis  con dgtbjior.  6,, *) will  get  closer  and  closer  to  the  true  values  as  the 
sample  s size  grows,  hence, the  resulting  estimate  of the  population  spectrum  should 
have  this  same  property. 

Even  if the  model  is incorrectly  specified,  if the  autocovariances  of the  true 
process  are  reasonably  close  to  those  for  an  ARMA(p,  q) specification,  this  pro- 
cedure  should  provide  a  useful  estimate  of the  population  spectrum. 

Nonparametric  Estimates  of the Population  Spectrum 
The  assumption  in [6.3.3]  is that  Y, can  be reasonably  approximated  by an 
ARMA(p,  q) process  with p and  q small.  An  alternative  assumption  is that  sy(w) 
will  be  close  to  sy(A)  when w is close  to A. This  assumption  forms  the  basis  for 
nother  class  of estimates  of the evi epectruan known  as  semerstectric or 

| estimates. 

* 

RD  (AG 

if ants bt. tn to Sy A) when  w is diene to A, ‘his suggests  that  tas might 
average of the values  of §,(A) for values of A in a 
a, w, where the weights depend  on  the distance  between  w and 

(w) denote such an estimate  of sy(w) and  let  w;  =  27j/T.  The 1  cag : 

: 

in to take 

Ad a  yA 1209 ny  Ke  + 

oe  ; 

:  “fhe sh)  = =  tu 

| 

16.6.8} 

bile 

sti Tete eae 

oh an 

rot  ees  a¥  ei Si 

Pano wen! (a, em ) iw em 

._~ 

i:  44 

wh) 

Rie: 
ae  ee 

yi 
n how, many different eae ys ts 
as useful for estimating sy(w). The kernel (0.  mr). 
eae each Srequency;i is marae given. The kernel weights  sum 
= js.  asymptotically  und:  ay  Ay pine  She >  asaaieneet  ‘otha genet 

i 

feed 

.  iy 

63.4 

¢ 

th 

at  different  freaucn  ic 

Sots sa \  ip He watt aps ca  fie Did as depent: 

esice  so  opulaten if specu a  sthosizesigae 

oe  pee cee ee 

and  the  estimator  [6.3.4]  becomes 

s 
$y (w;)  =  1D 3  Basa  §,(@) 4m): 

|h+1—-|mil. 

~~ 

(6.3.6] 

For  example,  for  h  =  2, this  is 

$y (@;)  ra  $$, (@;_>)  +  $5, (w;_ 1) 7  5S, (w;)  ai §§,(w;+1)  7  3S, (@;,2). 
Recall  from  [6.3.1]  and  [6.3.2]  that  the  estimates  §,(w) and $,(A) are  approximately 
independent  in  large  samples  for  w  #  A.  Because  the  kernel  estimate  averages 
over  a  number  of different  frequencies,  it should  give  a  much  better  estimate  than 
does  the  periodogram. 

Averaging  §,(w)  over  different  frequencies  can  equivalently  be  represented 
as  multiplying  the  jth  sample  autocovariance  ¥, for  j >  0 in  the  formula  for  the 
sample  periodogram  [6.2.5]  by a  weight  x;*. For  example,  consider  an  estimate  of 
the  spectrum  at  frequency  w  that  is  obtained  by  taking  a  simple  average  of  the 
value  of §,(A)  for  A between  w  —  vandw  +  »: 

§y(w)  =  (2v)-?  | * 

wtv 

bya)  ah 

[6.3.7] 

Substituting  [6.2.5]  into  [6.3.7],  such  an  estimate  could  equivalently  be  expressed 
as 

; 

wtv 

T-1 

‘ 

w-v 

j=l 

=  (Avm)"Qu)ig  + Qua)" S Hayy [sina]. 
=  2m)-Vig + (2vm)- S 5(1/))4sin{(w  + »)j] —  sin{(w —  »)j). 

1 

6.3.8] 

Using  the  trigonometric  identity® 

sir(A  +  B)—  sin(A  —  B) =  2-cos(A)-sin(B), 

[6.3.9] 

expression  [6.3.8]  can  be  written 

$y(w)  =  (2m)  "FJ +  (2v7)-?  D> 9,(1/j)-[2-cos(wj)-sin(vj)] 

T-1 

ik 
=  (2m)-? {ie 1ay  ee | jcoscwi 

dasa 

j=l 

vj 

Notice  that  expression  [6.3.10]  is of the  following  form: 

ae 

$y(w)  =  (27) -! {f +2  Py kK; ios). 

i 

where 

.  _ 
[sina] 
kK}  =  [seen] 

[6.3.10] 

[6.3.11] 

: 

[6.3.12] 

The sample periodogram  can  be regarded as  a special  case  of [6.3.11]  when  Ky =  1. 
Expression [6.3.12] cannot  exceed  1 in absolute  value,  and so the estimate  [6.3.11] 
essentially  downweights  4, relative  to the  sample  periodogram. 

*See,  for example,  Thomas  (1972,  pp.  174-75). 

166  Chapter 6  | Spectral Analysis 

. 

Recall  that  sin(j)  =  0 for  any  integer  jy. Hence,  if  v  =  a,  then  «x;  =  0  for 

all j and  [6.3.11]  becomes 

§y(w)  =  (27) ~ '¥. 
(6.3.13] 
In  this  case,  all  autocovariances  other  than  Yo Would  be  shrunk  to  zero.  When  v  = 
m,  the  estimate  [6.3.7]  is an  unweighted  average  of §,(A)  over  all  possible  values 
of A, and  the  resulting  estimate  would  be the  flat  spectrum  for a  white  noise  process. 
Specification  of a  kernel  function  «(w,,,,,  w,) in  [6.3.4]  can  equivalently  be 
described  in  terms  of  a  weighting  sequence  {«*}”—{  in  [6.3.11].  Because  they  are 
just  two  different  representations  for  the  same  idea,  the  weight  kK; is also  sometimes 
called  a  kernel.  Smaller  values  of  x*  impose  more  smoothness  on  the  spectrum. 
Smoothing  schemes  may  be  chosen  either  because  they  provide  a  convenient  speci- 
fication  for  x(w;,,,,  @;) or  because  they  provide  a  convenient  specification  for  «;. 

One  popular  estimate  of the  spectrum  employs  the  modified  Bartlett  kernel, 

which  is given  by 

K*  = 

q+1 

forj=1,2,...,q 

0 

forj > q. 

[6.3.14] 

The  Bartlett  estimate  of the  spectrum  is thus 

$)(w)  =  (27)7!  {fe +  2> {1 —jl(q  +  1}4costa)} 

[6.3.15] 

Autocovariances  y, for j >  q are  treated  as  if they  were  zero,  or  as  if Y, followed 
an  MA(q)  process.  For j =  q,  the  estimated  autocovariances  7, are  shrunk  toward 
zero,  with  the  shrinkage  greater  the  larger  the  value  of j. 

How  is one  to  choose  the  bandwidth  parameter  h in [6.3.6]  or  q in  [6.3.15]? 
The  periodogram  itself  is asymptotically  unbiased  but  has  a  large  variance.  If one 
constructs  an  estimate  based  on  averaging  the periodogram  at different  frequencies, 
this reduces  the variance  but introduces  some  bias.  The  severity  of the bias depends 
on  the  steepness  of the  population  spectrum  and  the  size  of the  bandwidth.  One 
practical  guide  is to  plot  an  estimate  of the  spectrum  using  several  different  band- 
widths  and  rely on subjective  judgment  to choose  the  bandwidth  that  produces  the 
most  plausible  estimate. 

6.4.  Uses  of Spectral  Analysis 

We  illustrate  some  of  the  uses  of  spectral  analysis  with  data  on  manufacturing 
production  in the  United  States.  The  data  are  plotted  in Figure  6.3.  The  series  is 
the  Federal  Reserve  Board’s  seasonally  unadjusted  monthly  index  from  January 
1947  to  November  1989.  Economic  recessions  in  1949,  1954,  1958,  1960,  1970, 
1974,  1980,  and  1982  appear  as  roughly  year-long  episodes  of falling  production. 
There  are  also  strong  seasonal  patterns  in  this  series;  for  example,  production 
almost  always  declines  in July and  recovers  in August. 

The sample  periodogram  for  the  raw  data  is plotted  in  Figure  6.4,  which 
‘displays §,(w,) as  a function  of j where w,  =  27j/T.  The  contribution  to the sample 
variance  of the  lowest-frequency  components  (j near  zero)  is several  orders  of 
magnitude  larger  than  the  contributions  of economic  recessions  or  the  seasonal 
factors.  This  is due  to  the  clear  upward  trend  of the  series  in Figure  6.3.  Let  y, 

6.4.  Uses  of Spectral  Analysis 

167 

: BERT  3 
eel y 

yf: 

pe  a 

Gr 

GE 

ok.  4]  at  Gk  $8).  eae 

“% 

— Valueofj 

FIGURE  6 6s Estimate of the spectrum for monthly | growth rate of industrial 
production,  or spectrum of 100 times  the first difference of the log of the  series in 
Fegnte 8.3. 

y,  = = 

S:sin(wt), 

sche Shigthd te hase ‘ee that  w  is  so small  that  even  at  dz late t at of 
eee  Aik ie ite thn, 2 pL cain  poesia 
are  by far the most important determin 

5  tha the t trend 

nponents 

— 

The  second  peak  in  Figure  6.5  occurs  at j =  44  and  corresponds  to  a  period 
of  513/44  =  11.7  months.  This  is  natural  to  view  as  a  12-month  cycle  associated 
with  seasonal  effects.  The  four  subsequent  peaks  correspond  to  cycles  with  periods 
of  6,  4,  3,  and  2.4  months,  respectively,  and  again  seem  likely  to  be  picking  up 
seasonal  and  calendar  effects. 

Since  manufacturing  typically  falls  temporarily  in  July,  the  growth  rate  is 
negative  in  July  and  positive  in  August.  This  induces  negative  first-order  serial 
correlation  to  the  series  in [6.4.1]  and  a  variety  of calendar  patterns  for x, that  may 
account  for  the  high-frequency  peaks  in  Figure  6.5.  An  alternative  strategy  for 
detrending  would  use  year-to-year  growth  rates,  or  the  percentage  change  between 
y,  and  its  value  for  the  corresponding  month  in  the  previous  year: 

w,  =  100-[log(y,)  —  log(y,-12)]. 

[6.4.2] 
The  estimate  of the  sample  spectrum  for  this  series  is plotted  in  Figure  6.6. 
When  the  data  are  detrended  in this  way,  virtually  all  the  variance  that  remains  is 
attributed  to  components  associated  with  the  business  cycle  frequencies. 

Filters 

Apart  from  the  scale  parameter,  the  monthly  growth  rate  x,  in  [6.4.1]  is 

obtained  from  log(y,)  by applying  the  filter 

[6.4.3] 
x, =  (1 —  L)log(y,), 
where L is the  lag operator.  To  discuss  such  transformations  in general  terms,  let 
Y, be  any  covariance-stationary  series  with  absolutely  summable  autocovariances. 

5 

. 

| 

21 

4\ 

61 

~8l 

12) 

101 
Value of j 

141 

161 

i8t 

=8201 

221 

FIGURE 6.6  Estimate  of the  spectrum  for  year-to-year  growth  rate  of monthly 
industrial  production,  or  spectrum  of 100 times  the seasonal  difference  of the log 
of the  series  in Figure  6.3. 

170  Chapter 6  | Spectral Analysis 

Denote  the  autocovariance-generating  function  of  Y  by gy(z),  and  denote  the 
population  spectrum  of Y by s,(w).  Recall  that 

Sy(w)  =  (277) ~'gy(e~”). 

[6.4.4] 

Suppose  we  transform  Y according  to 

where 

and 

X, =  h(L)Y,, 

A(L)  =  > AL’ 

j=-= 

j 2 thy aaa 

from equation [3.6.17] that  the autocovariance-generating  function  of X can 
be calculated from the autocovariance-generating function  of Y using  the  formula 
[6.4.5] 
8x(z)  = A(z)h(2~ gy (2). 
The population spectrum of xis thus Rating:  off 
dia 
en 
[6.4.6] 
Sx(w) = (27) ~"gx(e-*) = (Qn) inle-")h(e)gy(e-M). 
fro} 
Substituting  (6. 4.4] into [6.4.6] reveals that the Boa tide  of li is related 
teabesneep lags apsstnn, of Y according to 
i  age  al  ache  - Sx(w) sale “yne)sya). 
Nott  ..«0  3G  rit i33q 
Operating  on a series — ni the filter A(L) has the fet of mtipyng the 
_  spectrum by the function  h(e~“)h(e). 

| 
oswrciay 

Gs  it teed 

>  s9xlons  | 

[64 7 

S80QC 

(tf 

2 

:  fr the Percocet operator in . ‘3, ithe fier i is WL) = Li  L and  the 

This  function  is  equal  to  zero  when  12w  =  0, 27,  47,  67,  82,  107,  or  1277;  that 
is,  it  is  zero  at  frequencies  w  =  O,  27/12,  47/12,  67/12,  87/12,  107/12,  and  7. 
Thus,  seasonally  differencing  not  only  eliminates  the  low-frequency  (w  =  0) com- 
ponents  of a  stationary  process,  but  further  eliminates  any  contribution  from  cycles 
with  periods  of  12,  6,  4,  3, 2.4,  or  2 months. 

Composite  Stochastic  Processes 

Let  X,  be  covariance-stationary  with  absolutely  summable  autocovariances, 
autocovariance-generating  function  g y(z),  and  population  spectrum  sy(w).  Let  W, 
be  a  different  covariance-stationary  series  with  absolutely  summable  autocovari- 
ances,  autocovariance-generating  function  gy(z),  and  population  spectrum  Sw(), 
where  X, is  uncorrelated  with  W, for  all  ¢ and  r.  Suppose  we  observe  the  sum  of 
these  two  processes, 

Y,  =  X,  +  W,. 

Recall  from  [4.7.19]  that  the  autocovariance-generating  function  of the  sum  is the 
sum  of the  autocovariance-generating  functions: 

, 

8y(Z)  =  8x(z)  +  8w(z). 

It follows  from  [6.1.2]  that  the  spectrum  of the  sum  is the  sum  of the  spectra: 

Sy(@)  =  Sy(w)  +  Sw(@). 

[6.4.9] 

For  example,  if a  white  noise  series  W, with  variance  a? is added  to  a series  X, and 
if X, is uncorrelated  with  W, for  all  ¢ and  7,  the  effect  is to  shift  the  population 
spectrum  everywhere  up  by the  constant  o?/(27).  More  generally,  if  X has  a peak 
in  its  spectrum  at  frequency  w,  and  if W  has  a  peak  in  its  spectrum  at  w,,  then 
typically  the  sum  X  +  W will  have  peaks  at  both  w,  and  a). 

As  another  example,  suppose  that 

Y,  at. 

= h,X,_;  +  €,, 

jar 

where  X, is covariance-stationary  with  absolutely  summable  autocovariances  and 
spectrum  sy(w).  Suppose  that  the  sequence  {h,}*_ _..  is absolutely  summable  and 
that  e, is  a white  noise  process  with  variance  a? where e is uncorrelated  with  X  at 
all  leads  and  lags.  It follows  from  [6.4.7]  that  the  random  variable  2% _.h;X,_; 
has  spectrum  h(e~‘)h(e'”)sy(w),  and  so,  from  [6.4.9],  the  spectrum  of Y is 

sy(w)  =  h(e~™)h(e)sy(w)  +  02/(2n). 

APPENDIX  6.A.  Proofs  of Chapter  6 Propositions 

@  Proof  of Proposition  6.1.  Notice  that 

| Sy(w)e  dw  =  |  > ye.  “e™ dw 

,  = 

27  7  je  =m 

7 

=i  Sf 
< 

2t ;j- ~o 

vT 

eiotk -/)  dw 

=  5 Dim] fooslutk  ~  j)) +  ésinfutk  ~  j))) do 

: 

[6.4.1] 

172 

Chapter  6  | Spectral  Analysis 

Consider  the  integral  in (6.A.1].  For k  =  j, this would  be 

. {cos[@(k  ~  j)]  +  isin[w(k  —  j)]} dw  =  | "  {cos(0)  +  ivsin(O)}  dw 

3  [’ os 

[6.A.2] 

For k # j, the  integral  in [6.4.1]  would  be 

[7 teostate  —  j)] +  ésinfek  ~  ))) du 

e. are DJ  a.  salett ; Die e 

[6.4.3] 

| 

=  (k  —  j)-Xsin[w(k  —  j)]  ~  sin[—a(k  —  j)] 
~  beos[a(k  —  j)]  +  bcos[ -a(k —  j)]}- 

But the difference  between  the frequencies  m(k  —  j) and ~2(k  —  j) is  2m(k  —  j), which 
teger  multiple of 27. Since  the sine and cosine functions  are  periodic,  the  magnitude 
. 3) this sum en. scent be term for j = ki in  the  sum  in Si A. 1} is nonzero,  and 

: 

seen t 

Tare  { 

¥, selade* dw = 5° re trou +  ae ie  = 

itt  ah 

‘ 

3 

q 
= 

: 

\ 

4 
., 
ot: 
eee 

~ 

5  a  yp ee  - 

as claimed  in  [6.1. 15)  : 

“<- a 
To  derive  (6. a: io) notice  that since atte is symmetric around nd w = 0, eis 
oid  gaitusineive  bes 

__tetls sa2  pw ,{2.A..6}  Gini  tiueay 

SF 

Fae 

« 

. 

~~ 

(T.A.8}  ai  T=  Vognittee  ~ 
tal 
Sy  a Hk —  WCTenshiiges <  RR 

or  sania  eee  ee = tu Lael dw  +  ["s swe dw 

fe, 

+ 

at 

[ 

—  .  on 

Sf  i  ai SE aed  ee   sy(w)eme dw 

at  a  Ss 

a 

EJ 

(i 

; 

: 

ty 

a  [AO] nF R  Jo aoitinited of) 

Yesollot tet  > >  if 03 

eh 

Then  the  expression  to  be  evaluated  in [6.4.4]  can  be  written 

» exp[i(2ms/T)(t  —  1)] =  > zy 

rel 

anv 

[6.4.6] 

We  now  show  that  for  any  N, 

N 

= 

=  ae 

N 

’ 

[6.4.7] 

provided  that  z  #  1, which  is the  case  whenever  0 <  |s| <  7.  Expression  [6.A.7]  can  be 
verified  by induction.  Clearly,  it holds  for  N =  1, for  then 

he  ts 

=1 

Green shige ans. A. utk holds for N, we  see eatiat 

N 
> ze-)  =  70  =  |. 

ae te 

itt 

A  : 

ff 
Be 
py zed  = > ow -  zn 

2 

AT 

a  a  ‘1 
Dittitetitrscens 
‘cnagg  bettie 

BaP  TONNOR 

a 

CI.  ass 

‘4 
“firtt—este  deo  ern  des 

SARIN  seecsprecsrnrey  china  a0 ae mSitosk 

329 
he  ee 

yay 
2 
aor 

-Sfonesuper 

Be 

cio. 

f 

Bb 

Bt  SM  eat  le  zotesnsy  oi  vine  fei  sa  fLA 

c 

at  tee 

fon 

JA 

nu 

ni  46.  fn  aMhi2i 

ok  4 FS) “4 .  ~~ 
‘ 

dl et  ‘ 

> 

pe {; 3} mi 

ts 

t i id 

. 

we  must  show  that 

7 

2D cosla(¢- 

=  0 

for  =  1,2...  ,M 

(6.A.11] 

and 

t 
> sinfo,(¢-  1}  =  0 

 forj  =  1,2,...,M 

(6.A.12] 

for w, the  frequencies  specified  in  [6.2.7].  But  [6.A.4]  establishes  that 

Tis  > exp[i(27j/T)(t  —  1)] 

=  ¥ cosf(2mj/T)(  -  1)) +  eS sin(2ny/TV(t  -  1)) 

9 
_ 
: 

ey @  2.2...  «<.  M. For [6.A.13] to equal zero,  both  the real and the imaginary  component 
phe  zero.  Since *; =  2nj/T, results (6. A.11]  and  [6.A. 12] follow immediately from 
6.A.13 

Result [6.A. 4] can  also  be used to calculate  the  other diane of oe Xx! To  see 

7  obom, note  that 
= 

fu  at  Igxo  +  C1 

—  \).eijqxs}  K  . 

sle* + cM} = Heol) + ssn + cos(6) ~ sim) 

gg ag) 

Ee  ROG hes 
a  a 

SCiinchlqxst  Z  — - 
ia -  Vets igs  = 
e 

Bes. 
* lave > ys -wy  = eau)  4 + ¥: 9a Lae ‘- sin(@)) 

@eyineb  sat  zstelqima 

bar 

7 

we Ao 8 home  Gaye ehh pepe diol.  3835.0  neilizogosd ke 15, 

48.5.3]  at  agizeoipe;  of} 

Similarly,  elements  of 2/.  |x,x/  corresponding  to  cross  products  of the sine terms  can 

be  found  from 

T 
S sinfw,(¢  -  1)}sinfo(e  -  1) 

| 

P 

: 
~ 

=  -7 2 {exp[iw,(¢  —  1)] —  exp[  -iw,(t  —  1)]} 

x  {exp[iw,(t  —1)]  —  exp[—iw,(t  — 

1)]} 

i  -1 D texpli(2m/TV(j  +  kME~  V)]  ~  expli2m/T\(k  = je  ~  1) 
—  expli(2a/T)(j  —  k(t  -  1]  +  expli(2a/T)(-j  —  kr  —  YP} 

| 

i  ie  for j =  k 
otherwise. 
mt) 

Finally,  elements  of 2/7. ,x,x;  corresponding  to  cross aie of the  sine  and  cosine 

SS  AREF WADA  ai  nes  snk  “ore  twttps  ds ELA.  2 
gov!  ylotssbomean  Wolo}  ist. A, i Al bas  et  A.  ayer 
2 cos|, es -  1]: stole - Y 

aeS  = 

“g 

wo  some 

.o1ss 

ioum 

feups 
cts 

ae 

, 
| 

Tr  $10  Si  sisiuaia>  oO; 

bose  3G  o#lf  nad 
= 2 {explia,(¢  —  1)]  + eral ~fo( :  Dp 

otk 

oe 

&  {explia,(t  = 1)] -  expl —  ie, aa et) 

1b  A  3}  ues; 
tet?  ston 

arose 

yet 

‘ 

ie clans  ts  (As 
"Seung he ae 

See 

yet 

=  iy fexp[i(2a/T)(j  + we ) + exon -/e-))  - 
5  + 
We a tt aad  saveriteting 
“expli(2m/T)(j j-  or ~  y] t- cxsinTX—j - Ke  yy. 
S
e
of (6.2.10;  &  .  sal 
ot ae ed 

teat 

a  &),  eo 

7 

 
Since  there  are  as  many  explanatory  variables  as  observations  and  since  the  explanatory 
— are  linearly independent,  the OLS  residuals i, are  all zero.  Hence. 2. 6. A.19] implies 

$= [Soail[$ eu} 

[Sao] 

[6.4.20] 

But  [6.A.17]  allows  us  to  write 

S x,y,  “|i 

a  | 

0 

(7/2)-1,_, 

:.  J 

[6.4.21] 

Substituting  [6.4.21]  and [6.2.11]  into  (6.A.20]  establishes  that 
0’ 

T 
Dy? sa v  ase  ek  ¥ 

i  es 

0’ 

T 

BS 

[T 

0’ 

i  i:  (T/2)-1,- Ha 

=  T-f?  +  (T?2) > (a7  + 

so that 

ee 

i 

7 

eee  - | 

(1/7) 5 yr = a +  (12) ) > (a?  +  8). 

; 
¥)  RS 

0 

bat 

[6.22 

et 2 my 

Finally, observe from [4.A.5] and the "hae dat i = y that 

“eG 

= 

a 

antl 

WT)  Sy = # = OS 0.- 

q 

sowing 6821  be wien (l  ~ Dhgxse 

am Zo. - yy  nd, (a3 + 

i 

Hid  L 

ates  ne = - 

‘a  a red 

ia  <™ 

heehacesyer  o>  ee  ‘pie ream 

~~ 

; 

’ 1  ies sas Ce 

¢ 

;  rae  a 

F 

> 

e, 

= 

i 

en 

+ 

«i 

4 

—< 

a 

} 

Similarly, 

& -  i}, =  (2/T)  3 (y,  —  y)-exp[—-iw (7 -  1) 

(6.A.27] 

Substituting  (6.A.26]  and  [6.4.27]  into [6.A.23]  produces 

a? +  8? = airy S (y,  —  y)-expliw(t  —  »)| 

x  > Or,  ~  Wyexpl - ile  -  vi} 

=  (4/7) 

wT 

67 

YD On — IM ~  Vexplio(t  -  7] 

: 

T 

T-1 

| 

‘ 

~~ 

=  airy  S (3 =  yy  +  2 , “a Wei  =  y)-exp[ —  iw] 

s 

F 

. 

+ > 0. —  YO  —  Yrexplia}) 

nal 

YY 

tHe or ki pe =  —  . 

= 

: 

etl?  Ge 

+ yo. ~ I). mee aodinad S 

[6.4.28] 
i,  reat?  Jost: otf mw 42.5, rant siieate  Heer t 

ad 

: 

+ She ae iu 

ra  he 

winch  a  2 rere for 

a 

}  Cramér,  Harald,  and  M.  R.  Leadbetter.  1967.  Stationary  and  Related Stochastic  Processes. 
New  York:  Wiley. 
Fuller,  Wayne  A.  1976.  /ntroduction  to Statistical  Time  Series.  New  York:  Wiley. 
Thomas, George B., Jr. 1972.  Calculus and Analytic Geometry, alternate  ed. Reading, Mass. : 
Addison-Wesley. 

-. 

| 

foustenos  of  bseu  nocd  esd  (.: 

. 

() 

2hoitsvise 

1S)9mMs1eq  2a  oiqensxs  104  .z73!ome  taq  WOSiUG oq  to  1olger 
aged 

idgic  299074  iSJPp  as. 301. mS wat 

sved 

> 

9 

ag tae AF bh  ra 

iat 

i  Move  ga 8 tts Ott Hoyerhite 
ss Hod ons  |  sale sire Stree ia 

aa 
- 

| 

y Bahan pei sai aft ae 

aqnise ditt va sieloaeal tient en 

7 

Asymptotic  Distribution 
Theory 

,  Y7)  has  been  used  to  construct 
Suppose  a  sample  of  T observations  (y,, y2,  - 
6, an  estimate  of the  vector  of population  parameters.  For  example,  the  parameter 
¢,,  9)’  for  an  AR(p)  process  might  have  been 
vector  @  =  (c,  ¢),  $2,  .-., 
estimated  from  an  OLS  regression  of y, on  lagged y’s.  We  would  like  to  know  how 
far  this  estimate  @ is likely  to be from  the  true  value  @ and  how  to test  a  hypothesis 
about  the  true  value  based  on  the  observed  sample  of y’s. 

- 

- 

Much  of the  distribution  theory  used  to  answer  these  questions  is asymptotic: 
that  is,  it  describes  the  properties  of  estimators  as  the  sample  size  (T)  goes  to 
infinity.  This  chapter  develops  the  basic  asymptotic  results  that  will  be  used  in 
subsequent  chapters.  The  first  section  summarizes  the  key  tools  of  asymptotic 
analysis  and  presents  limit  theorems  for  the sample  mean  of a  sequence  of i.i.d. 
random  variables.  Section  7.2  develops  limit  theorems  for serially  dependent  var- 
iables  with  time-varying  marginal  distributions. 

7.1.  Review  of Asymptotic  Distribution  Theory 

Limits  of Deterministic  Sequences 

Let {c;}7_,  denote  a sequence  of deterministic  numbers.  The  sequence  is said 
to converge  to c if for any  e  >  0, there  exists  an  N such  that  |c; —  c| <  e whenever 
T =  N; in other  words,  cz will be as  close  as  desired  to c so  long as  Ti is sufficiently 
large.  This is  indicated  as 

lim cr  =  ¢, 

[7.1.1]  - 

or,  equivalently, 

Cr  C. 

For  example,  cr  =  1/T denotes  the  sequence  {1, }, 4, . 

.  .}, for which 

lim  cy = 0. 
Tox 

| 

A sequence  of deterministic  (m x  n) matrices  {C;}7_,  converges  to C if each 

element  of C; converges  to  the corresponding element  of C. 

Convergence  in  Probability 

Consider  a  sequence  of  scalar  random  variables,  {X ,}7_,.  The  sequence  is 
said  to  converge  in probability  to  c  if for  every  «  >  0 and  every  5 >  0) there  exists 
a  value  N  such  that,  for  all  T =  N, 

P(X;  -—  cl >  8} <e. 

[7.1.2] 

In words,  if we  go far enough  along  in the  sequence,  the  probability  that  X ;  differs 
from  c  by more  than  6 can  be  made  arbitrarily  small  for  any  6. 

When  [7.1.2]  is satisfied,  the  number c is called  the probability  limit,  or  plim, 

of the  sequence  {X;}.  This  is indicated  as 

or,  equivalently, 

plim  X;  =  c, 

Xr =>  €, 

Recall  that  if {c;}7-_,  is a  deterministic  sequence  converging  to  c,  then  there 
exists  an  N such  that  |c; —  c| <  6 for  all 
T=  N.  Then  P{|c;  —  c| >  6}  =  0 for 
all  T =  N.  Thus,  if a  deterministic  sequence  converges  to  c,  then  we  could  also 
say  that  c; pc. 

| 

A  sequence  of  (m  xX  n)  matrices  of  random  variables  {X 7} converges  in 
probability  to the  (m  x  n) matrix  C if each  element  of X; converges  in probability 
to the corresponding  element  of C. 

More  generally,  if {X;} and  {Y7} are  sequences  of (m  x  n) matrices,  we  will 

use  the  notation 

<>  ¥- 

to  indicate  that  the  difference  between  the  two  sequences  converges  in probability 
to  zero: 

| 

X,;-Y;—  0. 

An  example  of a  sequence  of random  variables  of interest  is the  following. 

Suppose  we  have  a  sample  of T observations  on  a  random  variable  BARS 
Y,}.  Consider  the  sample  mean, 

teres 

Y, =  (1/T)  > ee 

[7.1.3] 

as  an estimator  of the  population  mean, 

fr  =  Y;. 
We  append  the  subscript  T to  this  estimator  to  emphasize  that  it describes  the 
mean  of  a  sample  of size  T.  The  primary  focus  will  be  on  the behavior  of this 
estimator  as  T grows  large.  Thus, we  will be  interested  in the  properties  of the 
sequence  {fir}7=1- 
?  When  the  plim of a sequence  of estimators  (such  as  {ji7}7~,)  is  equal  to  the 
true  population  parameter  (in this  case,  2), the  estimator  is said  to  be consistent. 
If an estimator  is consistent,  then  there  exists  a sufficiently  large sample  such  that 
we  can  be assured  with  very  high probability  that  the  estimate  will  be within  any 
desired  tolerance  band  around  the  true  value. 

= 

| 

7.1.  Review  of Asymptotic  Distribution  Theory 

181 

The  following  result  is quite  helpful  in finding  plims, a proof of this  and  some 
of  the  other  propositions  of  this  chapter  are  provided  in  Appendix  7.A  at  thaend 
of  the  chapter. 

Let {Xr}  denote  a sequence  of (n  x  1) random  vectors  with  plim 
Proposition  7.1: 
c,  and  let  g(c)  be  a  vector-valued  function,  g: R”"  —  R”,  where  g(-)  is  continuous 
at‘c  and  does  not  depend  on  T.  Then  g(X 7) — g(c). 

The  basic  idea  behind  this  proposition  is that,  since  g(-)  is continuous,  g(X7) 
will  be  close  to  g(c) provided  that  X ; is  close  to  c.  By choosing  a  sufficiently  large 
value  of  T,  the  probability  that  X;  is close  to  ¢  (and  thus  that  g(X7)  is close  to 
g(c))  can  be  brought  as  near  to  unity  as  desired. 

Note  that  g(X ,) ng  = the  value  of  X ; but  cannot  depend  on  the  index 

T itself.  Thus,  g(X7,  T)  = 

2 is not  a  function  covered  by Proposition  7.1. 

Example  Vs | 
16 Xue  —*  C,  Bid  hoe  ee  then  (X,7  +  X27)  +  (c,  +  ¢).  This  follows 
immediately,  since  g(Xi7,  X27)  =  (Xir  +  X27)  is  a  continuous  function  of 

(X, T>  X>7). 

; 

Example  7.2 
Let  {X,7}  denote  a  sequence  of (nm  X  n) random  matrices  with  X.- =e C,, a 
nonsingular  matrix.  Let  X57  denote a  sequence  of  (m  xX  1). random  vectors 
with X,~—> ¢>. Then  [X,7]~ 1X, — [C,]~'c.  To see  this, note  that the elements 
of  the  matrix  [X,7]~!  are  continuous  functions  of  the  elements  of  X,7 at 
X,7  =  Cy, since  [C,]~!  exists.  Thus,  [X,7]~'—>  [C,]-!.  Similarly,  the elements 
of [X,7]~!X,7  are  sums  of products  of elements  of [X,7]~'  with  those  of X,7. 
Since  each  sum  is again  a  continuous  function  of X,7 and  X,7, 

plim  [X,7]~'X27 = [plim  X,7]~'  plim  X,7 = [C,]~'ep. 

Proposition  7.1  also  holds  if some  of  the  elements  of  X; are  deterministic 
with  conventional  limits  as  in expression  [7.1.1].  Specifically, let  X7  =  (Xiz,  Cor), 
where  X,7 is  a stochastic  (mn,  x  1) vector  and  c,,;is  a deterministic  (nm,  X  1) vector. 
If plim  X,;  =  ¢,  and  lim;  ¢,7  =  ¢2,  then  g(X,7,  7) LA g(c,  C2).  (See  Exer- 
cise  7.1.) 

Example  7.3 
Consider  an  alternative  estimator  of the  mean  given  by Y¥ =  [1/(T  —  1)} x 
~7_,Y,.  This  can  be  written  as  c,;Y;7,  where  c,7 = [TAT ~  1)] and  Y; = 
(1/T)>7_,Y,.  Under  general  conditions  detailed  in  Section  7.2,  the >_sample 
mean  is a consistent  estimator  of the  population mean,  implying  that  Y--> pe. 
It is also  easy  to  verify  that  cir  >  1.  Since G1rYri is  a  continuous  function  of 
cr and Y,, it follows  that  ey Yr  lw = pw. Thus, Y%, like Y;, isa  consistent 
estimator  of w. 

Convergence  in Mean  Square  and  Chebyshev’s  Inequality 
A stronger condition  than convergence  in probability  i iS mean  square convergence. 

The  random  sentieten {X;7} is said  to converge  in mean  mers to c, indicated  as 

182 

Chapter  7  | Asymptotic  Distribution  Theory 

X;+>  ¢, 

if for  every  e  >  0 there  exists  a  value  N such  that,  for  all  T =  N, 

Another  useful  result  is the  following. 

E(X,;  e..  c)?  <  ¢. 

[7.1.4] 

Proposition  7.2:  (Generalized  Chebyshev’s  inequality).  Let X be a random  variable 
with  E(|X|) finite for some  r  >  0.  Then,  for any  5 >  0 and  any  value  of c, 

P{|X  —-  cl >5s 

E\X  —-  c|’ 
= 

(7.1.5] 

oe 

An  implication  of Chebyshev’ s  inequality is  that  if X;  > c,  then  X r—c.  To 
see  this,  note  that if X; *  c,  then  for  any  e  >  O and  6 > 0 there  exists  an  N such 
that  E(X;  —  c)? <  8’e  for  all  T =  N.  This  would  ensure  that 

: 
: 

E(X;  —  c)? 

2 

ahaa 

for all  T =  N.  From  Chebyshev’s  inequality,  this  also  implies 

BFF  EET  HOS GD) 

bige 

e:  . Pilea  whan WS <e 

for al T= Nor that Xy-% 6 

Law of Large Numbers ae Independent 
Taw) . 
Distributed Variables _ 
I¢ 
ind 

Tt 

Mh oP al 
pee 

_Let  us now consider  the  behavior  of the pines mean Yr: =.  (TELA, 
where {Y¥,} i is i.i.d.  with mean  yp. and  variance ee For this case,  Y,has “eerie 

- 4 = ary  3 Var(¥,) = 02/7. 

mea eee aoe pe 

ise 
tag a aac  ee 
the  ta apheptang Pune ag es 

- panies  re  As  = RE  i  ial 

FIGURE  7.1 

Density  of the  sample  mean  for  a  sample  of size  T. 

at  any  value  x  at  which  F,(-)  is continuous.  Then  X  is said  to  converge  in  distri- 
bution  (or in  law)  to  X,  denoted 

[eee  « 

When  F(x)  is of  a common  form,  such  as  the  cumulative  distribution  function  for 
a  N(u,  7?) variable,  we  will  equivalently  write 

; 

X7> N(u, 07). 

The  definitions  are  unchanged  if the  scalar  X;  is replaced  with  an  (nm  x  1) 
vector  X;.  A  simple  way  to  verify  convergence  in distribution  of a  vector  is the 
+  +  A,X,,7)  converges  in distribution 
following.’  If the scalar  (A,X ,7  +  A2X27  +  * 
to  (A,X,  +  A,X,  +  ++:  +  A,X,,)  for  any  real  values  of (A,, Az,  .  ..  ,  A,,),  then 
the  vector  X7;  =  (X,7,  X27,  ...,  Xnr)’  converges  in  distribution  to  the  vector 
Fs 

ee  Fe 

ee 

Pee 

+ 

The  following  results  are  useful  in determining  limiting  distributions.‘ 

Proposition  7.3: 
(a)  Let  {Y,}  be a  sequence  of om x  1) random  vectors  with  Yr + Y.  Sup 
that {X 7} is a sequence  of (n x  1) random  vectors such  that (X;  —  Yr) 
Then  X; —  Y; that  is,  X; and  Y; have  the same  limiting distribution. 
(b)  Let  {X7} be a sequence  of random  (n x  1) vectors  with  X;—> ¢, and let {Y;7} 
be a sequence  of random  (n  x  1) vectors  with  Y;—>  Y.  Then  the sequence 
constructed  from  the sum  {X;  + Y7} converges  in distribution  toc  +  Y and 
the  sequence  constructed  from  the  product  {X7Y7}  converges  in  distribution 
toc’Y. 

0. 

(c)  Let{X7}  be a sequence  of random  (n x  1) vectors  with  X; =» X, and let g(X), 
g: R”  —  R” be a continuous function  (not dependent on  T). Then the — 
of random  variables  {g(X 7)} converges  in distribution  to 8(X). 

>This  is known  as  the  Cramér- Wold theorem.  See  Rao  (1973,  p.  123). 
“See  Rao  (1973,.pp.  122-24). 

184  Chapter  7  | Asymptotic  Distribution  Theory 

FIGURE  7.2  Density  of V7(Y;  —  1). 

Example  7.4 
Suppose  that  X;-> c and  teks Y, where  Y ~  N(y,  a7). Then,  by Proposition 
7.3(b),  the  sequence  X,Y, has  the  same  limiting  probability  law  as  that  of c 
times  a N(u,  a7) variable.  In other  words,  X,Y,  —  N(cu,  c?o7). 
| 
Example  7.5 
Generalizing  the  previous  result,  let  {X}  be-a  sequence  of random  (m  x  n) 
matrices  and  {Y,}  a  sequence  of random  (n  x  1) vectors  with  Ket C and 
Y;—  Y,  with  Y  ~  N(p,  0).  Then the  limiting  distribution  of X,Y;  is the 
same  as  that  of CY;  that  is,  X;¥; > N(Cp,  CNC’). 
Example7.6 
Suppose  that  X; — N(0,  1). Then  Proposition  7.3(c)  implies  that  the  square 
of  X;  asymptotically  behaves  as  the  square  of  a  M(0,  1) variable:  X} 
x7(1). 

| 

Central  Limit  Theorem 

We have  seen  that the sample  mean  Y; for an  i.i.d.  sequence  has a degenerate 
probability  density  as  T —  ©,  collapsing  toward  a  point  mass  at  yu as  the  sample 
size grows.  For  statistical  inference  we  would  like  to  describe  the  distribution  of 
Y; in more  detail.  For  this  purpose,  note  that  the random  variable  V7T(Y7  —  ) 
has mean  zero  and  variance  given  by (7)? Var(Y7)  =  o? for all  T, and  thus,  in 
contrast  to Y;, the  random  variable  VT(Yr7  —  ») might  be expected  to converge 
to  a  nondegenerate  random  variable  as  T goes  to  infinity. 

The  central  limit  theorem  is the  result  that,  as  T increases,  the  sequence 
VT(Yr  —  m) converges  in distribution  to  a Gaussian  random  variable.  The  most 
familiar,  albeit  restrictive,  version  of the  central  limit  theorem  establishes  that  if 
Y, is i.i.d.  with  mean  yw and  variance  o?, then? 
: 

7.1.6} 
VT(¥7  -  ») > N(0, 0”). 
Result  [7.1.6]  also  holds  under  much  more  general  conditions,  some  of which  are 
explored  in the next  section. 

Figure 7.2 graphs an example of the density of VT(Yr—-  u) for three different 

‘See,  for example,  White  (1984,  pp.  108-9). 

7.1.  Review  of Asymptotic  Distribution  Theory 

185 

values  of  T.  Each  of these  densities  has  mean  zero  and  variance  a*.  As  T becomes 
large,  the  density  converges  to  that  of  a  N(0,  a’)  variable. 

A  final  useful  result  is the  following. 

Let  {X,}  be  a  sequence.of  random  (n  x  1)  vectors  such  that 
Proposition  7.4: 
VT(Xr  -  ¢) a X,  and  let  g:  R"”  —  R”™  have  continuous  first  derivatives  with  G 
denoting  the  (m  xX  n) matrix  of derivatives  evaluated  at  c: 

G= 

Be 
Ox’ |. 

=c 

Then  VT(g(X7)  —  g(c)]  > GX. 

Example  7.7 
Let  {Y,,  Y>,...,  Yr} be  ani.i.d.  sample  of size  7 drawn  from a distribution 
with  mean  »  #  0 and  variance  a”.  Consider  the  distribution  of the  reciprocal 
of  the  sample  mean,  S;  =  1/Y;,  where  > be (1/T)Z7_,Y,.  We  know  from 
the  central  limit  theorem  that  \/7(Y;  —  4) —  Y, where  Y ~  N(0,  a).  Also, 
g(y)  =  1/y  is continuous  at  y =  pw.  Let  G  =  (dg/dy)|,_,  =  (—1/u?).  Then 

VT[Sr  —  (1/p)]  eh G-Y;  in other  words,  VT[S;  —  (1/)]  —  N(0,  o?/p4). 

i. 

7.2.  Limit  Theorems for Serially  Dependent  Observations 

The  previous section  stated  the  law  of large  numbers  and  central  limit  theorem  for 
independent  and  identically  distributed  random  variables  with  finite  second  mo- 
ments.  This  section  develops  analogous  results  for  heterogeneously  distributed 
variables  with  various  forms  of serial  dependence.  We  first  develop  a  law  of large 
numbers  for  a  general  covariance-stationary  process. 

Law  of Large  Numbers  for a  Covariance-Stationary  Process 

Let  (Y,,  Y2, 
stationary  process  with 

...,  Yy)  represent  a  sample  of size  T from  a  covariance- 

. 
‘for alle 
E(Y,)  = 
E(Y,  —  w)(Y,-;  -  w) =  y  ~ foralle 

Consider  the  properties  of the  sample  mean, 

x lyi| <  =. 
Zz 

) 

Y; =  (1/7)  x Y.. 

T 

i 

Ee 

Taking expectations  of [7.2.4]  reveals  that  the  sample  mean  provides  an  unbiased 
estimate  of the  population  mean, 

186  Chapter  7  | Asymptotic  Distribution  Theory 

E(Y;)  =  p, 

[7.2.1] 
[7.2.2] 

[7.2.3] 

while  the  variance  of the  sample  mean  is 

E(¥;  -  pw) 

r 
Shs ean 3 (Y=  »| 

2 

=  (1/T*)EXI(Y,  -  #)  +  (¥2  -  uw)  +--+:  +  (¥7  —  w)] 
x  [Y=  w)  +  (Y.-H)  +--+  +  (Yr - w))} 

=  (WT)EQY,  -  wl,  -  w) +  (¥2.  -  4)  ++  >>  +  (Yr  - 
+  (¥7 - pw) 

+(¥,  —  w)((%,  ~  4)  +  (Y2-  we) +  °° 

+  (Ys  —  wl,  — #)  +  (%  -  )  +  ‘sab  (¥, — pl 

*+  (Yr - wl,  -  w»)  +  (%  -  wv)  +--+:  +  (¥7 - wh 

=  (VT)  [y+  n+  2+  3  +7  °°  +  Yr-) 

i 
+[mn+nt+rwtn  + 

ee,  +  ¥r-2)  | 
+  Yr 

Fees  Hl  Hi Ar-2.¥ ary s—t  z + Yo}. 

ee  ee  BP  TT  aT ly  ee  eee 

+  i -  2)¥2 +  °T —  3)y, + °°  *  + 277-3} 

3  neo  2iluess  seedT 

or TIVIR  BASIRO:  S  fare 

- 

rome =  U0 (er = inte) +0 - Tn)  | 
pereurene: 

‘tage  red . a 

. 

Ayinviw; 

Now 

> y  —  TE(Y;  -  wy 

j=  -%. 

=  Kyo  t  2y  +  2y  + 2y3  + 

-  +} 

—  {yo  +  [((T -  1)/T]}:2y  + ia ~  2)/T}: 272 
+  ((T -  3)/T]-2y, + +> 

+  [1/T]-2y7-sl 

<  (1/T)-2|y|  +  (2/T)-2}y2]  +  (3/T)-2lys|  +  °° 
+  (q/T):2|¥q|  +  2l¥q+1)  +  Ava)  A.  + 
< (1/T)-2|y;|  +  (2/T)-2] 
+  (q/T)-2lyq  +  €/2. 

yo] +  ery 2lysl  ase 

: 

Moreover, for this  given  q, we can  find  an  N such that 

~  (UT)-Ay|  +  (2/T)-2yal  + (B/T)-2lyl  + «+ +  (G/T)-2lyQl  < €/2 

for all T =  N, ensuring  that 

|  Dy, -  EY; - wy]  <e, 

fk 

Lag 3  ; 

* 

.  as was  to  be shown. 

= 

These  results can be sumnidtized a8 Yollows  a 

r  eiiade eae  Pe  ett  pS  us J  aif 

*)  ne 

ee 

be goa 

soa proces th  amen  by 

7 

A 

- wed crs ris yp yt ; 

Gales  second  mo- 

deseo. analogous  results  ior  Beier  deter distributed 

. 

2 

= 

*  1 

7  aa 

: 

Thus,  result  (b) could  equivalently  be described  as  the  autocovariance-generating 
function  evaluated  at  z  =  |, 

z 

2%, =  8y(I). 

4=  -  2 

Or  as  27  times  the  spectrum  at  frequency  w  =  0, 

>} y,  =  27s,(0), 

jzrure 

the  last  result  coming  from  the  fact  that  e”  =  |. For  example,  consider  the  MA(~) 
process 

Y=  m+  Dbe, =a +  W(Lye, 

e3 

with E(¢,e,)  =  o7 if ¢ =  7 and,0  otherwise  and  with 2/_,,|y,|  < x.  Recall  that  its 
autocovariance-generating function is given  by 

| 

Br(z) = V(z)o7p(z7').. 

Evaluating  this at z =  1, 

i“  a7 

= 

2m =  WoW)  = 

{4  44 

ms  i  Fy 

[Lt  Wt  yet  Yate  Pe 

(7.2.8). 

ry 5 

- oo Difference pation ot Beli 
Some very useful limit theorems  pertain to penaneniadio oF sarang pide ES 
a sequence  of random  scalars  with  E(Y,) = 0 for all r.° Let , 
{Y,}7_,  denote 
ir a orm: = available at date  r, where  this information  includes current and 
alt 

ote Sic we  might have 

Dabeunad 

ne 

i 

ote 

t 

Pasig 

| 

| 
secingg  8! red : » ax  | 

ike ee: ss  ad: c ei  hag 

a 

. 

m  Vi 

riers Nore . 

Tite Fy Fs 

; 

Beams  at 

7: 

ied, 
ey  eater 

42,  is pre 

= 

SN 

sie  oka 

— = 7 = 

7 
. 
7 
. 

Note  that  condition  [7.2.10]  is stronger  than  the  condition  that  Y, is serially 
uncorrelated.  A  serially  uncorrelated  sequence  cannot  be  forecast  on  the  basis  of 
a  linear  function  of its  past  values.  No  function  of past  values,  linearor  nonlinear, 
can  forecast  a  martingale  difference  sequence.  While  stronger  than  absence  of 
serial  correlation,  the  martingale  difference  condition  is weaker  than  independence, 
since  it does  not  rule  out  the  possibility  that  higher  moments  such  as  E(Y?|Y,_,, 
Y,_2,..-.,  Y,)  might  depend  on  past  Y’s. 

Example  7.8 
If e,  ~  i.i.d.  N(O,  a7),  then  Y,  =  e,e,_,  is a  martingale  difference  sequence 
but  not  serially  independent. 

L'-Mixingales 

A more  general  class  of processes  known  as  L'-mixingales  was  introduced  by 
Andrews  (1988).  Consider  a  sequence  of random  variables  {Y,}*,  with  E(Y,)  = 
O fort  =  1,2,....  Let  2, denote  information  available  at time  ¢, as  before,  where 
Q, includes  current  and  lagged  values  of Y.  Suppose  that  we  can  find  sequences  of 
nonnegative  deterministic  constants  {c,}=,  and  {€,,}* 
> such  that  lim,,..  €,,  =  0 
and 

E 

E(Y,|Q,_m) 

scé, 

[7.2.11] 

for all  t =  1 and  all m  =  0. Then  {Y;} is said  to foHow  an  L'-mixingale  with  respect 
to  {0,}. 

Thus,  a zero-mean  process  for which  the m-period-ahead  forecast  E(Y,|Q,_,.) 
_converges  (in  absolute  expected  value)  to  the  unconditional  mean  of zero  is de- 
scribed  as  an  L'-mixingale. 

Example  7.9 
Let  {Y,}  be  a  martingale  difference  sequence.  Let  c,  =  E|Y,|,  and  choose 
&  =  1 and  &,  =  0 for  m  =  1, 2,  ....  Then  [7.2.11]  is satisfied  for  2,  = 
{Y,,  Y,-1,.-..,  ¥,},  so  that  {Y,}  could  be  described  as  an  L'-mixingale 
sequence. 

Example  7.10 
Let  ¥, =  Zj-0  €,-;,  where  L7_olp;|  <  oo and  {e,} is  a martingale  difference 
sequence with  Ele,| < M for all t for some  M < ~.  Then  {Y,} is an  L'-miningvale 
with  respect  to  N, =  {e€,, €,_,,  . 

.  .}. To  see  this,  notice  that 

£| ECM  ems tiom- 

.  ) 7  E|3 WE, -; 

j=m 

= ES ive.) 

Since  {,}7, is absolutely  summable  and  E|e,_;|  <M,  we  can  interchange  the 
order  of expectation  and  summation: 

mizil 

e  > Me. 7  2 \v)|"Ele,_;|  =  D \W;|-M. 

m 

=m 

j=m 

Then  [7.2.11]  is  satisfied  with  c,  =  M  and  é,  =  Lj=mlW;|.  Moreover, 
lim,,.x  &,  =  0, because  of absolute  summability  of {W,}* 9. Hence,  {Y,} is an 
L'-mixingale. 

/190  Chapter  7 | Asymptotic  Distribution  Theory 

Law  of Large  Numbers  For  L'-Mixingales 

Andrews  (1988)  derived  the  following  law  of  large  numbers  for  L'-mixingales.* 

Proposition  7.6: 
and  (b)  there  exists  a  choice  for {c,}  such  that 

Let  {Y,}  be  an  L'-mixingale.  If (a)  {Y,}  is  uniformly  integrable 

then  (1/T)Z7_,Y,  >  0. 

ia 

lim  (1/T)>  c,  <  x, 
i= 1 
T--+= 

To  apply  this  result,  we  need  to  verify  that  a sequence  is uniformly  integrable. 
A  sequence  {Y,}  is said  to  be  uniformly  integrable  if for  every  «  >  0 there  exists  a 
number  c  >  O such  that 

for  all  ¢, where  5,,)-.,  =  1 if |Y,|  =  c  and  0 otherwise.  The  following  proposition 
gives  sufficient  conditions  for  uniform  integrability. 

Proposition  7.7: 
(a)  Suppose  there  exist  an  r  >  1  and  an  M'  <  ©  such  that 
E(\Y)  <  M’  for all t.  Then  {Y,} is uniformly  integrable.  (b) Suppose  there  exist  an 
r  >  1 and  an  M'  <~« 
such  that  E(|X,’)  <  M' for allt.  If Y,  =  27. _.h,X,_;  with 
yj _~  lh <  =,  then  {Y,}  is uniformly  integrable. 

Condition  (a) requires  us  to  find  a moment  higher  than  the  first  that  exists. 
Typically,  we  would  use  r  =  2.  However,  even  if a  variable  has  infinite  variance,  it 
can  still  be  uniformly  integrable  as  long  as  E|Y J’ exists  for  some  r between  1 and  2. 

Example  7.11 
Let  Y; be  the  sample  mean  from  a  martingale  difference  sequence,  Y;  = 
(1/T)=7,  Y, with  E|Y,|’  <  M’  for  some  r >  1 and  M'  <  ~.,  Note  that  this  also 
implies  that  there  exists  an  M  <  = such  that  E|Y,|  <  M.  From  Proposition 
7.7(a),  {Y,} is uniformly  integrable.  Moreover,  from  Example  7.9,  {Y,} can  be 
M<®, 
viewed  as  an  L'-mixingale  with  c,  =  M.  Thus,  lim;_..  (1/T)2/_,c,  = 
and  so,  from  Proposition  7.6,  Y;—> 0. 

Example  7.12 
Let  Y,  =  D7-o%e,-;,  where  Z7_oly|  <  ©  and  {e,} is a  martingale  difference 
sequence  with  Ele)’  < M'  <  ~  for some  r >  1 and  some  M’  <  .  Then,  from 
Proposition  7.7(b), {Y,} is uniformly  integrable.  Moreover,  from  Example  7.10, 
{Y,} is an  L'-mixingale  with  c,  =  M,  where  M represents  the  largest  value  of 
Ele, for any  t.  Then  lim;_.,  (1/T7)=7,c,  =  M  <  ~,  establishing  again  that 

, amd 0. 

Proposition  7.6 can  also  be applied  to  a double-indexed  array  {Y, 7}; that  is, 
each  sample  size  T can  be  associated  with  a  different  sequence  {Y,7,  Y27,-.  ., 
Y,7}.  The  array  is said  to  be  an  L'-mixingale  with  respect  to  an  information  set 
11,7 that  includes  {Y, 7, Y2,7,  .--  ,  Yr.r}  if there  exist nonnegative  constants  ,, 
and c, 7 such that  lim,,..  €,  =  0 and 

E\E( Y,.719,-m,7)I  =  C1.7&m 

Paisribr vephacts part (b) of the proposition with the weaker condition lim,_.  (1/T) 27, c, <  =. 

See Royden (1968,  p. 36) on  the  relation  between  “lim”  and  “fim.” 

7.2.  Limit  Theorems for Serially  Dependent  Observations 

191 

T=  1, and¢  =  1, 2, 
for  all 
with  lim;_..  (V/T)EF.,<,  7  <  &,  then  (TYEE.  Y,7,—  0. 

m  =  0, 

ee A if ae is uniformly  integrable 

Example  7.13 
Let  {e,}7_,  be  a  martingale  difference  sequence  with  Ele,’  <  M’  for  some 
r>  land  M'  <~,  and  define  Y,  ; =  (t/T)e,.  Then  the  array  {Y, 7} is  a uniformly 
integrable  L'- <mixingaile  with  c,;  =  M,  where  M  denotes  the  maximal  value 
for  Ele,|,  &  =  1, and  &, = 0 for m >  0.  Hence,  (1/T)27.,(/T)e, > 9. 

Consistent  Estimation  of Second  Moments 

Next consider  the  conditions  under  which 

T 

(1/T)  > Y,Y,-4  > E(Y,Y,_+) 

t=1 

(for  notational  simplicity,  we  assume  here  that  the  sample  consists  of  T  +  k 
observations  on  Y).  Suppose  that  Y, =  Tow,e,_;,  where  Tol]  <  ~  and  {e,} is 
an  i.i.d.  sequence  with  Ele,|’  <  2  for some  r  >  2.  Note  that  the population  second 
moment  can  be  written? 

EY ,Y, 4)  E( ya a  pe  (S dea.) 

v=0 

u=0 

aa 
>> 

ee 

W,.E(e,.  uer—k-  2. 

UWE, u=t—k-  ) 

[7.2.13] 

Define  X,,  to be  the  following  random  variable: 

7  a  bf) an  os  ECT  Fx) 

& x, taeer-ulr-n-«]  J  t3: > tate  Bles-uts-r-»)) 

u=0  v= 

a=0  p= 

x 

x 

py  Dy wh, [e,_ w=, k-\+  ECE  yy  24  LJ}: 
u=0 4 

Consider  a forecast  of X,, on  the  basis  of 0,_,,  =  {€,_».  Srinays: 

[Ore  Pk  - 

E(X,4|Q,-m)  =  > i. Muadoler- wer  k=  ~  E(€,_.€,.2~v)]. 

’Notice that 

Jo Pe Wadsl  =  ps Ww >, Wd <  * 

and  Ele,  ,€,.4_,|  <  *,  permitting  us  to move  the expectation 
the  last  line  of [7.2.13]. 

‘asin 

operator  inside  the 

iii 

192  Chapter 7  Asymptotic  Distribution  Theory 

summation  signs  in 

signs 

i 

ion 

The  expected  absolute  value  of this  forecast  is bounded  by 
E| E(X, .|0, ~| -  E| > > Aad ry werk 

~~" Ce;  u€r-k  Dy 

“aemyv=em 

lA 

e( > 2 Mutbel le —ubr—a-»  wie  E(6,-u6r-4-l) 

lA 

>  > ladbeM 

u“=mvem- 

for  some  M  <  ~.  Define 

=  >  Dll = Dial  > lve 

a  =niv=m 

“=m 

v=  Om 

Since  {W,}7 is  absolutely  summable,  lim,,_..  27 <mlW.|  =  0 and  lim,,.,  &,  =  0. 
It follows  that  X,,  is an  L'-mixingale  with  respect  to  0, with  coefficient  c,  =  M. 
Moreover,  X,,  is uniformly  integrable,  from  a simple adaptation  of the  argument 
in ene y id ee ratage 7.5):  Hence, 

arty 3 myx ity S ny 2g) 

from  which 

EY ¥ x > 0. 
: 
Ae 

(VT) pA VY, Res E(WY,. a 

(72. M4 

| 
| 

<t is chan tfor pus otdades from,{7. 3s 14] that the jth sain SAboavdrekdd 
for a sample  fee T aves a ba alti estimate of the population wa  ite 

ee, ree i 

ae Fo)  Eth ~  Wa  We  ra a 

I  A  ah  hei 

7A Praahs saith ari wt noi 

bh  tet  aft)  dere she Bh  sicment «f o{¢) g,. BP 
nae BO} 5  a *%y ait ahr sate 2 

RY  We 

Eel  inion  ie elas, 

~  in 

me  eat 

Let  {Vibe  be  an  n- ditties  vector  martingale  difference  se- 
Proposition  7.9: 
quence  with  ¥; =  (1/T)=/_,¥,.  Suppose  that  (a) E(¥,Y;)  = 21,, a positive  definite 
matrix  with  (1/T)>/_,Q,  > ‘a, a positive  definite  matrix;  (b)  E( YY YnV in)  <  * 
for all  tand  all i, j, 1, and  m  (including i =  j is  |.=  m),  where, Y,, is the ith element 
of the  vector  Y,; and  (c) (1/T)2/.,Y,¥;. £,'Q.  Then  VT Y;>  N(O, 2). 

Again, Proposition  7.9 holds for arrays {Y,_;}/_ , satisfying the stated conditions. 
To apply  Proposition  7.9, we  will  often  need  to  assume  that  a certain  process 

has  finite  fourth  moments.  The  following  result  can  be  useful  for  this  purpose. 

= 
Proposition  7.10: 
ba  <  x.  Let  Y, =  Di oh;X,_;,  where  X7olh|  <  *.  Then  Y, is a  strictly  stationary 
stochastic process  with  E|Y,Y,Y,Y,|  <  * isi all t, s,  u,  and  v. 

Let  X, be a strictly stationary  stochastic  process  with  E(X}) 

, 

ill! lei 

; 

oagean 

tiie 
|  Exanipie mpeg div 0.01 
"Ett;  — Ort, Reighly  yamgias  tet plgewsy P pailiastigMinenaichs 
sequence  and  where roots  of  (1 —  ¢,z  -  peas  +++  =  6,2?) = O lie outside 
the  unit  circle.  We  saw  in Chapter  3 that  Y, can be written  as 27_o¥;e,_; with 
Zrolwl  <  =.  Pepa 7.10  states phat if ¢, has finite  fourth aoa = 
so )  does: Y.  . 

- 

aimee 

»| 
| 

- 

<4 
Wor S 

ep Sion ple 7. 15 

at 
pia Y, ae Bot a ee = and nd iid 

> 

: 
be-7t0)  THOR: 

orzy 

i 

hid im nea A. 

5(Y% y<  3 vy E  xa nF  i 1 

:  | ‘ 

it 

cae oY “. pis mee es  (TAD, 

> 

x, 

i 2 SCT =f 

ey TT 

It further  follows  from  result  [7.2.14]  that 

$ 

(1/7)  >) o?¥}_,  > 0? E(Y?). 

Thus,  {7.2.18}  implies 

t=1 

" 

(/T)  > X?4  @?-E(Y?), 

t=) 

- 

as  chaumed  in [7.2.16]. 

Central  Limit  Theorem  for Stationary  Stochastic  Processes 

We  now  present  a central  limit  theorem  for  a  serially  correlated  sequence. 
Recall  from  Proposition  7.5  that  the  sample  mean  has  asymptotic  variance  given 
by (1/T)2/_  .y,.  Thus,  we  would  expect  the  central  limit  theorem  to  take  the 
form  \/T\ (7, —  p) >  NO, J). -x¥,).  The  next  proposition  gives  a  result  of this 
type. 

n 7.11: (Anderson, 1971, p. 29) ‘Let 

iets an 

saiedh  Wi wink te A  er at, Xt} 
= 
veleys-  nent oat  aids snap ay  Y,  = 1.  GE  jr =  eg  ee  eS iitsah)  *  <>  a. 

PIRES,  2.  Pierce  2x20. 58  od  ws  vkisctasehh  +  Jeeta  ete  pce 
where {e,} is m  sequence oft d. — warriables with  E(e?) <2 and &;-sl6) < 
«.  Then 

Siz  HpOw  -  te  beso rt  a 
RI 

0:8  nebticonss 
e.s  € 

ens  :shieioD 

3 

be  “Te, - 5 NO. Sih, 

jou-* 

Ps 

ae  Reise 

for  all  T  =  N.  Since  [7.A.2]  was  a  oecessary  condition  for  ik, (Xr)  —  8, fe) to be  greater 
than  6,  it follows  that  the  probability  shat  |e (X,)  ~  g,()|  is greater  than  6 is  less  than  ¢. 
which  was  to  be  shown. 

@ 

| 

7 

~ 

@  Proof  of Proposition  7.2. 
denote  its  complement  (all  x  such  that  | 

Let  § denote  the  set  of all x  such  that  |x —  c| >  6, and  let  $ 

x  —  c| <  6).  Then,  for  fy(x)  the  density  of x, 

EIX -  cl  =  | le ~  cbs)  de 

[le -  ebfatay de +  | be ~  fe) ax 

\V  [ile -  tPee ax 

=  [ Bf ¢(x) dx 

so that.° 

tee 

| 

= 8'P{|X°=  cl, >6}, 
bow 
weqoig 
E|X —  cl =  8 PIX ~  | >  8}, 

3 

. 

| 

;  i.  ARS  } 

a 

=  claimed.  a 
Consider  any real (m  x  1) vector  A, and  form  the  function 
@  Proof  of Proposition  7.4. 
h:  R" >  R!  defined  by h(x)  =  A‘g(x).  noting  that A(-)  is differentiable.  The  mean-value 
theorem  states  that  for a differentiable  function  A(-),  there  exists  an  (m  x  1) vector ¢; 
Ihetviden" 

and  esuch  tha@’  anu. 

ection:  suche 

Ce  eee  sae 
94.8 

YM.  Ho  LSP  iw 

en 

ie 

set 

2 

Y  tye  ;  by 

“-  >. 

mess  Lies  Se 

a  ag 

Thiet) 

26  (Ay Ree PR  B

A

Sd, 

> 

5  Sh.  = 

x,  Oy  Exampic  7  i4,  Hence 

 
@  Proof  of Proposition  7.7. 
Part  (a)  is established  as  in  Andrews  (1988,  p.  463)  using 
Holder's  inequality  (see,  for  example,  White,  1984,  p.  30),  which  states  that  for  r  >  1,  if 
E(|Y¥!]  <  = and  E{|Wi"  | <  x,  then 

EVYW|  =  (EUV  x {ELIY) 

 ye 

This  implies  that 

E(IY,-8yy)2<))  =  {E[[Y,|"]}'”  x  {E[(8 yy.) 20)"  Ae,  bee 

{7.A.4] 

Since  6),.;..;  is either  0 or  1, it follows  that 

and so 

(Spyjac)  v=  Bi ype) 

El(Sirj20))""  | =  ElSyy,-<)  =  a L-fy.(y,)  dy,  +  PXY,|  >  c} < 

E\Y, BM  nas 

where  the last  result  follows  from  Chebyshev's  inequality.  Substituting  [7.A 5] into  [7.4.4]. 

EY Buren) 

$AEUY AT” {A i} 

y,  (e-twer 

[7.4.6 

Recall  that E[|Y,|"]  < Pad  Celt f. ns that nine also  exists  an  M  < =  such  that 
ély| < M for all ¢. Hence, — 

ee 

BUY  pie) <(M’) i re (mies zsligae: ses 

OT 

Etat  = 

This expression can be made  as  small as desired  by choosing  ¢ sufficiently  large.  Thus 
condition  [7.2.12] holds, tee that {Y,] is  uniformly  integrable. 

¥. establish  (b), notice 

A  ewes  5 

Tt  StF 

+7  QHD) = > a h,X wv  Fee 
tak” | 
hise  .  Pt 

ef > IHX, | ae er |
: }<M and — pai S  c it bitstiows that te ih foie is  bounded. 
ing 
Oo  aur: inside the 

sur 

t 

: 

 
7.8  is satisfied.  (a)  E(Y;)  =  A’ 

QA  =  a? »  0, by positive  definiteness  of 12,.  Likewsse. 

(1/T)  Dy o?  =  N(1/T)  s QA—>  ANNA  =  07,  ™ 

tat 

with  a?  >  0, by positive  definiteness  of 2. (b) E(Y#)  is a  finite  sum  of terms  of the  form 
ANA AAmECVi YY, Yn)  and  so  is  bounded  for  all  1  by condition  (b)  of Proposition  7.9; 
hence,  ¥. satisfies  condition  (b) of  Proposition  7.8  for  r  =  4,  (c) Define  S,; = (1/T)  x 
>, Y¥? andS,;=(1/T)Z7,Y,Y;,  noticing  that  $;  =  A’S,A.  Since  S; is  a continuous  function 
of  Sp. we  know  that  plim rs =  AOA  =  o?,  where  C  is  given  as  the  plim  of  Sr. 
Thus  Y, satisfies  conditions  (a) through  (c) of Proposition  7.8,  and  so  VT Y,-— N(O,  7°), 
or  VT Y, 5 d'Y,  where  Y  ~  (0, 2).  Since  this is  true  for  any  A, this  confirms  the  claim 
that VT Y,; > M0,  2).  & 

a Proof  of Proposition  7.10.  Let Y  =  X,X,  sae | W=  Rates Then  Holder's inequality 
implies  that for r > 1. 

‘3 

EX, X,X,X,| = UR RA A As 

ehiete 

For  r ey 25 dies means 

A, XR  feeedi  nk 5  naan X,)}'2 = maE(X,X,)?,  E(X, Xb 
A second  application  of Holder's  inequality  with Y =  X}.and W = X}revealsthat 

E(X,X,)"  =  EUXGX2)  =  (E(X2))"  x (E(Xzy 

ae  for r  =  2, this implies  from the strict stationarity of {X,} that 

OO  ee  =  ee 
The  ak aaa 

cipeaisicni : 

tects  BO  IE  NE at “sbi  od nes - A oie ate 
lietaet 4 re 1X); is stiety stationary vith h finite fourth h me ment al Fis DL  ya 
ORE igs ering 

2 

j — ne a See ae a ‘3 * hes 

7.240 

Let  Y,  =  O8Y..,  +  #, with E(e,¢.)  =  1 fort  =  rand  zero  otherwise. 
(a)  Calculate  lim,*,  T-Var(¥,). 
(b)  How  large  a  sample  would  we  need  in  order  to  have  95%  confidence  that  Y, 

differed  from  the  true  value  zero  by no  more  than  0.1? 
7.3. 

Does  a  martingale  difference  sequence  have  to  be  covariance-stationary? 
Let  Y, = 37 we,  ,,  where  Yj-olv,|  <  *  and  {e,} is  a martingale  difference  sequence 

7.4. 
with  E(e?)  =  a. Ts Y, covariance-stationary? 
75. 
ov  Ele, 18, .,  JJ,  where*e,  is  an  i.i.d.  se- 
quence  with Eje|’  <  M”  for  some  r  >  2 ae M" <  ~ with  =% »  \b,| <  ~.  Show  that  X,,  is 
uniformly  integrable. 
7.6. 

Derive  result  [7.2.15]. 

Bz  52s  i bedle, 6,24 

Define XG, 

7.7. 

Let  Y, follow  an  ARMA(p,  q) process, 

(1  —  $L  -  Ll?  -  ---  -  o L*V(Y,  —  pw) =  (1  +  OL  +  OL?  +  ---  +  6,L4)e,, 
with  roots of (1 —  $,z  —  @,z°7-:::  =  @, 2°) = OQand  (1'+  0,2  +-0,27  +  >  +. +  0,29) 
outside the unit circle.  Suppose  e, has mean  zero  and  is independent  of ¢, for  1  # 7 
= 0 
with E Ee) = o* and E(e?) <= for all. Prove the  following: 

| 

-  teers (a)  Gn S Yop  )  AAT  ee 
yrotansiixe  od  Asha  : ftir RGD esigute  Site  =  pe 63 

HOH! 

dampen 

$76 
29.  20  RRL PE. resi  vismb 1 363}  Sti  gly  Gh 

vat 
a abi  im rene ble patie. ene ee 6  type stom 

zoids: 

ainmen 

egos 

“os >eab 2 bide” 2o}eu  pal  59!  box is rosied  we a of 

Anderson,  T. W. 1971. The Statistical  Unabyst of Time Series.  New  York: Wiley. 
Andrews,  Donald  W. K.  1988.  Sani of Ee Numbers for Pd sa  i Non-Identically j 
Jistributed  Kandom 
Sg 
tone. | 7 rota  Pb 
heory. 

Variables. 
Hoel,  Paul G.,  sidney C 

Boston:.  Houg * 

Coos 30 sad. 4 faint 

yet 

a  nn Samer 

!  Analy: 

Linear  Regression  Models 

We  have  seen  that  one  convenient  way  to  estimate  the  parameters  of  an  auto- 
regression  is with  ordinary  least  squares  regression,  an  estimation  technique  that 
is also  useful  for  a  number  of  other  models.  This  chapter  reviews  the  properties 
of linear  regression.  Section  8.1  analyzes  the simplest  case,  in which  the explanatory 
variables  are  nonrandom  and  the  disturbances  are  i.i.d.  Gaussian.  Section  8.2 
develops  analogous  results  for  ordinary  least  squares  estimation  of more  general 
models  such  as  autoregressions  and  regressions  in which  the  disturbances  are  non- 
Gaussian,  heteroskedastic,  or  autocorrelated.  Linear  regression  models  can  also 
be  estimated  by generalized  least  squares,  which  is described  in Section  8.3. 

8.1.  Review  of Ordinary  Least  Squares  with  Deterministic 
Regressors  and  i.i.d.  Gaussian  Disturbances 

Suppose  that  a  scalar  y, is related  to  a  (kK  X  1) vector  x, and  a  disturbance  term  u, 
according  to  the  regression  model 

y,  =  XB  +  u,. 

: 

[8.1.1] 

This  relation  could  be  used  to  describe  either  the  random  variables  or  their  real- 
ization.  In  discussing  regression  models,  it proves  cumbersome  to  distinguish  no- 
tationally  between  random  variables  and  their  realization,  and  standard  practice 
is to  use  small  letters  for  either. 

This  section  reviews  estimation  and  hypothesis  tests  about  B under  the  as- 
sumptions  that  x, is deterministic  and  u, is i.i.d.  Gaussian.  The  next  sections  discuss 
regression  under  more  general  assumptions.  First,  however,  we  summarize  the 
mechanics  of linear  regression  and  present  some  formulas  that  hold  oes of 
Statistical  assumptions. 

The  Algebra  of Linear  Regression 

Given  an  observed  sample (Yi, Nz: 

« 
« 
Yr), the niagaa least squares  (OLS) 
estimate  of  B (denoted  b)  is the  value of  B that  minimizes  the  residual  sum  of 
squares  (RSS): 

, 

RSS  =  2 (y,  —  x/B)?. 

[8.1.2] 

200 

We  saw  in Appendix  4.A  to  Chapter  4 that  the  OLS  estimate  is given  by 

=  > aun}  I> ay  «it 

(8.1.3] 

t=] 

assuming  that  the  (k  x  k)  matrix  S7,(x,x/)  is  nonsingular.  The  OLS  sample 
residual  for  observation  ¢ is 

Often  the  model  in [8.1.1]  is written  in  matrix  notation  as 

8.  yr  &, Dd, 

=  Xp  +  O. 

{8.1.4] 

[8.1.5] 

where 

y  = 

(Tx) 

X} 
X> 

So 
|: 

(Txk) 

uy 

Uz 

u  =]. 

(7*1}) 

tf 

3  ee  ft 

ii ar  4c  : 

i 

& 

aes 

} 

-_ 

oO  HU Then thee > Ugh dio,  ot 1  ale ; 

4 

é  Fa 

the OLS einai 1 can be writen a5 1g  te oRad stig beens 

; 

}  25°65: 

¥; 

j 

+ be 

Ra  SB 

+ 

43 SYS - 

Him  29%:  Ps 

pe! 

b = 

erty),  at  BE) 

{ 335s j eSwis om fl | eS 
Sorted E 

ere:  } 

T 

2 

+ 

% 

>  hi 

[8.1.6] 

isp  econ 

> 

larly, the vector of OLS  Resmi Zesl@ucis 8 1. : can be written  as __ 

hae’  PS  5titriare. 

=y-xX (% 34 ) 

and  population  residuals  can  be  found  by substituting  [8.1.5]  into  [8.1.7]: 

[8.1.11] 
The  difference  between  the  OLS  estimate  b and  the  true  population  parameter 

@  =  Mx(XB  +  u)  =  Myu. 

B is  found  by substituting  [8.1.5]  into  [8.1.6]: 

=  (X'X)~'X’[X6  +  uj]  ="B  +  (X’X)~  'X’w. 

(8.1.12] 

The  fit  of  an  OLS  regression  is sometimes  described  in  terms  of  the  sample 
multiple  correlation  coefficient,  or  R?.  The  uncentered  R*  (denoted  R72)  is defined 
as  the  sum  of  squares  of  the  fitted  values  (x;b)  of  the  regression  as  a  fraction  of 
the  sum  of squares  of y: 

R2 = 

> xx 
1 ___  -.  24 _ ya  [8.1.13] 
i 

y’X(X’X)-!X’y 
y'y 

b’X’Xb 
y'y 

| 

If the  only  explanatory  variable  in the  regression  were  a  constant  term  (x,  = 
1), then  the  fitted  value  for  each  observation  would  just be  the  sample  mean  y and 
the  sum  of squares  of the  fitted  values  would  be  Ty.  This  sum  of squares  is often 
compared  with  the  sum  of squares  when a vector  of variables  x, is included:  in  the 
regression.  The  centered  R?  (denoted  R2)  is defined  as 

as 

y'X(X'X)"'X'y  —  Ty? 
yy.  TP 

1.14 
[8.1.14] 

Most  regression  software  packages  report  the  centered  R?  rather  than  the  uncen- 
tered  R?.  If the  regression  includes  a  constant  term,  then  R2 must be between  zero 
and  unity.  However,  if the  regression  does  not  include  a  constant  term,  then  R2 
can  be  negative. 

The  Classical  Regression  Assumptions 

Statistical  inference  requires  assumptions  about  the  properties  of the  explan- 
atory  variables  x, and  the  population  residuals  u,.  The  simplest  case  to  analyze  is 
the  following. 

Assumption  8.1: 
(a) x, is a  vector  of deterministic  variables  (for example,  x, might 
include  a  constant  term  and  deterministic  functions  of t);  (b)  u, is i.i.d.  with  mean 
0 and  variance  a;  (c) u, is Gaussian. 

To  highlight  the  role  of  euch  of these  assumptions,  we  first  note  the  impli- 
cations  of  Assumption  8.1(a)  and  (b) alone  and  then  comment  on  the added im- 
plications  that  follow  from  (c). 

Properties  of the  Estimated  OLS  Coefficient  Vector 
Under  Assumption  8.1(a)  and  (b) 

In vector  form,  Assumption  8.1(b) could  be written  E(u)  =  0 and  E(uu')  = 

c+. 

Taking  expectations  of [8.1.12]  and  using these  conditions  establishes  that  b 

is unbiased, 

E(b)  =  B +  (X’X)-'X'[E(u)]  =  B, 

[8.1.15] 

202 

Chapter  8  | Linear  Regression  Models 

with  variance-covariance  matrix  given  by 

E{(b  —  B)(b  —  B)']  =  E[(X'X)~*X'uu'X(X’X) ~"] 

=  (X'X)~'X’[E(uu’)|X(X'X) =! 

[8.1.16] 

o?(X'X)~X'X(X'X)~ 

=  o@°(X’X)7?. 

The  OLS  coefficient  estimate  b is unbiased  and  is a linear  function  of y.  The 
Gauss-Markov  theorem  states  that  the variance-covariance  matrix  of any alternative 
estimator  of B, if that  estimator  is also  unbiased  and a linear  function  of y,  differs 
from  the  variance-covariance  matrix  of b by a  positive  semidefinite  matrix.’  This 
means  that  an  inference  based  on  b about  any  linear  combination  of the  elements 
of B will  have  a  smaller  variance  than  the  corresponding  inference  based  on  any 
alternative  linear unbiased  estimator.  The  Gauss-Markov  theorem  thus  establishes 
the optimality  of the OLS  estimate  within  a certain  limited  class. 

Properties  of the Estimated. Coefficient Vector 
Un  rr Assumption  8.1(a) Through  (c) 

_  _ When vis Gaussian, [8.1 val sae that bi is Gaussian. Hence, the preceding 
“results imply — 

'  7A r be shown that  slsrei  iption  8. 1(a) through  (c), no  unbiased 
is more efficient than the OLS estimator b.? Eee: with Gaussian 

MERON ClesizscD 

ci  we  asd 

inebaoqobai ‘= -  "[) te eoxsepe  to 

with  an  eigenvalue  equal  to  zero.  Also  from  [8.1.8],  Mxv  =  V  for  any  vector  v 
that  is orthogonal  to  the  columns  of  X  (that  is,  any  vector  v  such  that  X'v  =  0); 
(T  -  k) such  vectors  that  are  linearly  independent  can  be  found,  associated  with 
(T —  k) eigenvalues  equal  to  unity.  Thus,  A contains  k zeros  and  (T —  k) 1s along 
its  principal  diagonal.  Notice  from  [8.1.20}  that 

u'Myu  =  u’PAP'u 

=  (P’u)'A(P’u) 

[8.1.22] 

=  w'Aw 
=  wed,  +  w3A,  +  °°  + 

WAZ, 

w  =  P'’u. 

where 

Furthermore, 

E(ww’)  =  E(P’uu'P)  =  P’E(uu’)P  =  o7P'P  =  o7I 7. 

Thus,  the  elements  of w  are  uncorrelated,  with  mean  zero  and  variance  a~.  Since 
k of the  A’s  are  zero  and  the  remaining  JT  —  k are  unity,  [8.1.22]  becomes 

u/Myu  =  w? +  w3 ++  +>  +  WH. 

(8.1.23] 

Furthermore,  each  w? has  expectation  a, so  that 

E(u'Mxu)  =  (T —  k)o’, 

and  from  [8.1.19],  s? gives  an  unbiased  estimate  of a: 

E(s?)  =  o”. 

Properties  of Estimated  Residual  Variance  Under 
Assumption  8.1(a)  Through  (c) 

When  u, is Gaussian,  w, is also  Gaussian  and  expression  [8.1.23]  is the  sum 

of squares  of (T —  k) independent  N(0,  a) variables.  Thus, 

RSS/o?  =  u'Myu/o?  ~  y>(T  —  k). 

[8.1.24] 

Again,  it is possible  to  show  that  under  Assumption  8.1(a)  through  (c), no 

other  unbiased  estimator  of a” has  a  smaller  variance  than  does  s*.4 

Notice  also  from  [8.1.11]  and  [8.1.12]  that  b and  @ are  uncorrelated: 
E[a(b  —  B)']  =  E[M,wu’X(X'X)-"]  =  o?M,X(X’X)-!  =  0. 

[8.1.25] 

Under  Assumption  8.1(a)  through  (c), both  b and  a are  Gaussian,  so that  absence 
of correlation  implies  that  b and  @ are  independent.  This  means  that  b and  s? are 
independent. 

t Tests  About. B Under  Assumption  8.1(a)  Through  (c) 

Suppose  that  we  wish  to  test  the  null  hypothesis  that  6,, the  ith  element  of 
B, is equal to some  particular  value  B?. The  OLS t statistic  for  testing  this  null 
hypothesis  is given  by 
Ye 

_ 

(6, -  B9)  | 6,  - 
t=  al  =  Ml 19 
s(€") 

oy, 

BP 

[8.1.26] 

‘See  Rao (1973,  p. 319). 

204  Chapter 8  | Linear  Regression  Models 

where  é“ denotes  the  row  i, column  i element  of (X’X)~!  and  &,  =  V/sé"  is  the 
Standard  error  of the  OLS  estimate  of the  ith coefficient.  The  magnitude  in [8.1.26] 
has an exact  ¢ distribution  with  T  —  k degrees  of  freedom  so  long  as  x,  is deter- 
ministic  and  u, is  i.i.d.  Gaussian.  To verify  this  claim,  note  from  [8.1.17]  that  under 
the  null  hypothesis,  b; ~  N(8°,  o£),  meaning  that  (b,  —  B°)/\/o2é"  ~  N(0,  1). 
Thus,  if [8.1.26]  is written  as 

_ 

(0;  —  BIN  o7E" 
t=  ——_—_— 

V s7/o? 

: 

the  numerator  is N(0,  1) while  from  [8.1.24]  the  denominator  is  the  square  root 
of a x* (T —  k) variable  divided  by its degrees  of freedom.  Recalling  [8.1.25],  the 
numerator  and  denominator  are  independent,  confirming  the  exact  ¢ distribution 
claimed  for  [8.1.26]. 

F Tests  About  B Under  Assumption  8.1(a)  Through  (c) 

More  generally,  suppose  we  want a joint test  of m  different  linear  restrictions 

about  B, as  represented  by 

Hy:  RB  =  r. 

[8.1.27] 

Here R is  a known  (m  x  k) matrix  representing  the  particular  linear  combinations 
of B about  which  we  entertain  hypotheses  and  r  is a  known  (m x  1) vector  of the 
values  that we  believe  these  linear  combinations  take  on.  For example,  to represent 
the  simple  hypothesis  8, =  ? used  previously,  we  would  have  m  =  1, Ra 
(1  x  k) 
vector  with  unity  in the  ith  position  and  zeros  elsewhere,  and r the  scalar  B?.  As 
a second  example,  consider  a  regression  with  k  =  4 explanatory  variables  and  the 
joint  hypothesis  that  8B,  +  B,  =  1 and  B;  =  fy.  In this  case,  m  =  2 and 

fit  0.51 

6 
:  b 01  a  5  | 

1 

= 

= 

, 

Notice  from  [8.1.17]  that  under  Ho, 

Rb  ~  Mr,  o2R(X'X)~'R’). 

A  Wald  test  of Hy is based  on  the  following  result. 

4% 
Ree 

[8.1.29] 

Proposition  8.1: 
Then  z'Q.—'z  ~  x(n). 

Consider  an  (n  x  1) vector  z  ~  N(0,  92)  with  Q  nonsingular. 

For  the  scalar  case  (n =  1), observe  that  if  z ~  N(0, a”), then  (z/c)  ~  N(Q,  1) 

and  z2/a?  ~  y?(1),  as  asserted  by the  proposition. 

To  verify  Proposition  8.1  for  the  vector  case,  since  1  is symmetric,  there 
exists  a  matrix  P, as  in  [8.1.20]  and  [8.1.21],  such  that  © =  PAP’  and  P'P  =  I, 
with  A containing  the  eigenvalues  of 2.  Since  0 is positive  definite,  the  diagonal 
.  elements  of A are  positive.  Then 

z'Q-'z  = 2'(PAP’)—'z 

iP | AM Px 
=  (P~1z]'A-"P "2 

wA-'w 

> w/d,, 
i=) 

[8.1.30] 

8.1.  Review  of Ordinary  Least  Squares 

205 

where  w  =  P~'z.  Notice  that  w  is  Gaussian  with  mean  zero  and  variance 

E(ww’)  =  E(P~'22z'[P’]-")  =  P-'n[P’}-*  =  P-'PAP’[P’]-*=  A. 

Thus  [8.1.30]  is the  sum  of squares  of n independent  Normal variables, each  divided 
by its  variance  A,.  It accordingly  has  a  y2(n)  distribution,  as  claimed. 
Applying  Proposition  8.1  directly  to  [8.1.29],  under  Ho, 

(Rb  —  r)'[o2R(X’X)~'R‘]- (Rb —  r) ~  x2(m). 

(8.1.31] 

Replacing a? with  the  estimate  s? and  dividing  by the  number of restrictions  gives 
the  Wald  form  of the  OLS F test  of a  linear  hypothesis: 

F =  (Rb  —  r)'[s?R(X'X)~'R’]~'(Rb  —  r)/m. 

[8.1.32] 

Note  that  [8.1.32]  can  be  written 

F 

_  (Rb  —  r)'[o?R(X’X)“'R'] 
. 

[RSS(T  —  k)|/o? 

“(Rb _—  r)/m 

The  numerator  is a  x7(m)  variable  divided  by  its  degrees  of  freedom,  while  the 
denominator  is  a  x7(T  —  k) variable  divided  by its  degrees  of  freedom.  Again, 
since  b and  @ are  independent,  the  numerator  and  denominator  are  independent 
of each  other.  Hence,  [8:1.32]  has  an  exact  F(m,  T  —  k) distribution  under  Hy 
when  x, is nonstochastic  and  4, is i.i.d.  Gaussian. 

Notice  that  the  ¢ test  of the  simple  hypothesis  8; =  B? is a  special  case  of the 

general  formula  [8.1.32],  for  which 

(8.1.33] 
F =  (6;  -  B%)[s*é"]- (6; —  82). 
This  is the  square  of the  ¢ statistic  in [8.1.26].  Since  an  F(1,  T —  k) variable  is just 
the  square  of a  «(T —  k) variable,  the  identical  answer  results  from  (1) calculating 
[8.1.26]  and  using ¢ tables  to  find  the  probability  of so  large  an  absolute  value  for 
a  «(T  —  k) variable,  or  (2)  calculating  [8.1.33]  and  using  F tables  to  find  the 
probability  of so  large  a  value  for  an  F(1,  T  —  k) variable. 

A  Convenient  Alternative  Expression  for the  F Test 
It  is often  straightforward  to  estimate  the  model  in  [8.1.1]  subject  to  the 
restrictions  in  [8.1.27].  For  example,  to  impose  a  constraint  B,  =  B° on  the  first 
element  of  B,  we  could  just  do  an  ordinary  least  squares  regression  of  y,  — 
By% i; OU  23.  X3,,  0 
34)  The  resulting  estimates  b3,  bj,  ....,  bf  minimize 
Zio  [(Y,  =  Bin)  —  bFx>,  ~  b¥x5,  — 
++  —  bE x,,]? with  respect  to  by, be, ..., 
z and  thus  minimize  the  residual  sum  of squares  [8.1.2]  subject  to  the  constraint 
that  B,  =  B). Alternatively,  to  impose  the  constraint  in [8.1.28],  we  could  regress 
¥,  ~  Xz,  0n  (x1,  —  X2,)  and  (x3,  +  Xa): 

2 

Vr  —  Xu  =  By(Xy  —  Xz)  +  Bs(Xa  +  Xq)  +  U,. 

The  OLS  estimates  b¥ and  b} minimize 
- 
| 
2 (O -  *u)  —  OF (ty  -  x2)  =  OF Gry  +  xa)P 
oy 

T 

[8.1.34] 

2  2 ly +  Ofty  =  CL —  bf )xa,  —  b3zy,  =  O3x4)" 

 * 

and  thus  minimize  [8.1.2] subject  to [8.1.28]. 

Whenever  the constraints  in [8.1.27]  can  be  imposed  through  a simple  OLS 
regression  on  transformed  variables,  there  is an  easy way to calculate  the F statistic 
206  Chapter 8  | Linear  Regression  Models 

(8.1. 32} just  by comparing  the  residual  sum  of  squares  for  the  constrained  and 
unconstrained  regressions.  The  following  result is  established in  Appendix  8.A 
at 
the  end  of this  chapter. 

Proposition  8.2: 
be the residual  sum  of squares  a from  using  this  estimate: 

Let  b denote  the unconstrained  OLS  estimate [8.1.6]  and let RSS, 

RSS, =  5 (y,  —  x/b)?. 

(8.1.35] 

Let  b*  denote  the constrained  OLS  estimate  and  RSS,  the  residual  sum  of squares 
from  the  constrained  OLS  estimation: 

RSSo  =  > (y,  —  x/b*)?. 

[8.1.36] 

Then  the Wald form of ues OLS F test of a linear hypothesis  {8.1.32} can  equivalently 
be calculated  as 

ae 
RSS,  —  RSS,)/m 
RSSAT  —  b 

[8.1.37] 

Expressions  8.1.37}  and  [8.1.32]  will  generate  exactly  the  same a 

spillies of whether  the null hypothesis and  the model  are  valid or not. 

© 

For example,  suppose  the  sample size  is T = 50 observations and the null 
othe:  is is Bs =  8, =  0 in an  OLS regression with  k =  4 explanatory  variables. 
egress  y, ON X4,,  Xz, X3,,  X4, and call the  residual  sum of squares  from  this 
RSS,.  Next, regress  y, on just x,, and x, and call the residual  sum of 

n 

om 

thi is restricted regression Leip? 

|  sy  ae 
is greater  than 3.20  (the 5%  critical  value  for  an  FQ, 46) random  variable),  then 
the null  st § i should  be rejected. 

Sirigtiril 

a. Sass sw  3 

nary  ;  east  ‘5  ‘y  U4 f)  az 

b7)  (14 

or  MO 

Many  of  the  results  for  deterministic  regressors  continue  to  apply  for  this 
case.  For  example,  taking  expectations  of [8.1.12]  and  exploiting  the  independence 
assumption, 

E(b)  =  B +  {E[(X'X)~'X']HE(u)}  =  B, 

(8.2.1) 

so  that  the  OLS  coefficient  remains  unbiased. 

The  distribution  of  test  statistics  for  this  case  can  be  found  by  a  two-step 
procedure.  The  first  step  evaluates  the  distribution  conditional  on  X;  that  is,  it 
treats  X as  deterministic  just as  in the  earlier  analysis.  The  second  step  multiplies 
by the density  of X and  integrates  over  X to find  the true  unconditional  distribution. 
For  example,  [8.1.17]  implies  that 

b|X ~  M(B,  o7(X’X)~'). 
If this  density  is multiplied  by the  density  of X  and  integrated  over  X,  the  result 
is  no  longer  a  Gaussian  distribution;  thus,  b is non-Gaussian  under  Assumption 
8.2.  On  the  other  hand,  [8.1.24]  implies  that 

[8.2.2] 

RSS|X  ~  o?-y°(T  —  k). 

But  this  density  is the  same  for  all X. Thus,  when  we  multiply  the  density  of RSS|X 
by the  density  of  X  and  integrate,  we  will  get  exactly  the  same  density.  Hence, 
[8.1.24]  continues  to  give  the  correct  unconditional  distribution  for  Assumption 
8.2. 

The  same  is true  for  the ¢ and F statistics  in [8.1.26]  and  [8.1.32].  Conditional 
on  X, (5;  —  B?)/[o(€")’7]  ~  N(0,  1) and  s/o is the  square  root  of an  independent 
[1/(T  —  k)]-x?(T  —  k) variable.  Hence,  conditional  on  X, the  statistic  in [8.1.26] 
has  a t(T  —  k) distribution.  Since  this  is true  for any  X, when  we  multiply  by the 
density  of X and  integrate  over  X we  obtain  the  same  distribution. 

Case  3.  Error  Term  t.i.d.  Non-Gaussian  and  Independent 
of Explanatory  Variables 

Next  consider  the  following  specification. 

Assumption  8.3: 
(a)  x, stochastic  and  independent  of u, for all  t,  s;  (b)  u,  non- 
Gaussian  but i.i.d.  with  mean  zero,  variance  a”,  and  E(u*)  =  p,  <  ~;  (c) E(x,x’) 
=  Q,, 4 positive  definite  matrix  with  (1/T)Z7_,Q,  >  Q, a positive  definite  matrix; 
(d) E(xiXjX1%m)  <  ©  for all i, j, l, m,  and t; (e) (1/T)Z7,(x,x;)  > Q. 

Since  result  [8.2.1]  required  only the  independence  assumption,  b continues 
to  be  unbiased  in  this  case.  However,  for  hypothesis  tests,  the  small-sample  dis- 
tributions  of s?  and  the  ¢ and F statistics  are  no  longer  the  same  as  when  the 
population  residuals  are  Gaussian.  To  justify  the  usual  OLS  inference  rules,  we 
have  to  appeal  to  asymptotic  results,  for  which  purpose  Assumption  8.3  includes 
conditions  (c) through  (¢).  To  understand  these  conditions,  note  that  if x, is co- 
variance-stationary,  then  E(x,x;)  does  not  depend  on  t. Then  Q, =  Q for all ¢ and 
condition  (e) simply  requires  that  x, be ergodic  for second  moments.  Assumption 
8.3  also  allows  more  general  processes  in that  E(x,x;)  might  be different  for dif- 
ferent  ¢, so  long as  the  limit  of (1/T)7_,  E(x,x/)  can  be consistently  estimated  by 
27.4 (x,x;). 
(1/T) 

208  Chapter 8  | Linear  Regression  Models 

(
4

-

1

‘
w
)
y

y
o
e
x
a

(
4
-
2
)

p
a
s

(
y
¥

—

‘
w
)
q
y
w
e
x
o

—
—
 (
y
¥

—

[
)
s

W
e
X
D

(
m
)
,
X

<
—
 4
q
u
u

(
T
O
N

4
7

I
U
S
U
D
I
S

J

S
u
S
H
D
I
s

1

- 

¥ 

h
a
l

a

=

n
e

.

S

n
s

©
;
)

.

. 

=) 

: 

e 

) 

* 

p
r
e
s
.

c
a
l
l
 ’

>

o

s

n
e

v
e

X
’
X
Z
-
L
1

‘
(
4
z
'
2
'
8
]

i

a

j

3

<

‘
n

j
o

A
q

}

j
u
a
y

&

:
4

|

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
To  describe  the  asymptotic  results,  we  denote  the  OLS  estimator  [8.1.3]  by 
b, to  emphasize  that  it is based  on  a sample  of size  T. Our  interest  is in the  behavior 
of  b; as  T becomes  large.  We  first  establish  that  the  OLS  coefficient  estimator  1s 
consistent  under  Assumption  8.3,  that  is, that  by —> B. 

Note  that  [8.1.12]  implies 

v0  [Zeal]  [30] 

=  Ge ) xxi] wn > x 

23 

Consider  the  first  term  in [8.2.3].  Assumption  8.3(e)  and  Proposition  7.1  imply 
that 

ce  Rae 
Jar eS xx  * Q-?. 

t=1 

(8.2.4] 

Considering  next  the  second  term  in  [8.2.3],  notice  that  x,u,  is a  martingale  dif- 
ference  sequence  with  variance-covariance  matrix  given  by 

which  is finite.  Thus,  from  Example  7.11, 

E(x,uU,X; U,) a. {E(x,x;)}-o7, 

jun 5 x + 0. 

| 

[8.2.5] 

Applying  Example  7.2  to  [8.2.3] through  [8.2.5], 

br —  B>Q*0  =  0, 

verifying  that  the  OLS  estimator  is consistent. 

Next  turn  to  the  asymptotic  distribution  of b.  Notice  from  [8.2.3]  that 

VT(b;  —  B)  =  jar Baxi] lawn) Su  [8:2.6] 

We  saw  in [8.2.4]  that  the  first  term  converges  in probability  to  Q~'.  The  second 
term  is \/T times  the  sample  mean  of x,u,,  where  x,u,  is a  martingale  difference 
sequence  with  variance  o?-E(x,x;)  =  o7Q, and  (1/7)=%,07Q,  >  o7Q.  Notice  that 
under  Assumption  8.3  we  can  apply  Proposition  7.9: 

T 

law) p> Xu + N(0,  02Q). 

[8.2.7] 

Combining  [8.2.6],  [8.2.4], and [8.2.7],  we  see  as  in Example  7.5  that 

VT(br  —  B)  > N(0,  [Q-!-(07Q)-Q-"})  =  NO,  02Q-').  [8.2.8] 

In other  words,  we  can  act  as  if 

bz =  N(B,  0?Q-/T), 
[8.2.9] 
where  the symbol  ~  means  “is approximately  distributed.”  Recalling  Assumption 
8.3(e),  in large  samples  Q should  be close  to  (1/7)27_,x,x;.  Thus  Q-/T should 
be  close  to  [2/_,x,x;]~'  =  (X;X7)~!  for  X; the  same  (T x  k) matrix  that  was 
represented  in [8.1.5]  simply  by X (again,  the subscript  T is added  at this point to 

210  Chapter 8  | Linear  Regression  Models 

emphasize  that  the  dimensions  of this  matrix  depend  on  7). Thus,  [8.2.9]  can  be 
‘approximated  by 

b, ~  N(B,  o7(X7X7)~'). 

This,  of course,  is the  same  result  obtained  in  [8.1.17],  which  assumed  Gaussian 
disturbances.  With  non-Gaussian  disturbances  the  distribution  is  not  exact,  but 
provides  an  increasingly  good  approximation  as  the  sample  size  grows. 

Next,  consider  consistency  of the  variance  estimate  s}.  Notice  that  the  pop- 

ulation  residual  sum  of squares  can  be  written 

(yr  —  X7B)'(yr  —  X7B) 

=  (yr  —  Xrby  +  Xrbr  —  X7B)'(yr  —  Xrbr  +  Xzby  —  X7H) 
=  (yr  —  Xrbr)'(yr  —  Xrbr)  +  (Xzb;  —  X7B)'(X7b;  —  X-7), 

[8.2.10] 

where  cross-product  terms  have vanished,  since. 

(yr  —  Xrbz)’X7(b;  —  B) =  9, 

by the OLS  orthogonality  condition 8. 1. nh Buia tt 2. 1 2: 

(1/T)(yr  —  X7B)'(yr  —  X7B) 
aS8] 

Ss  wT; 4 CxMS Xb) + UM  - py XIX ~  B), 

or 

lari?  sorte 

rept:  finieb  satin 

t  2nrt  =z 

ied} 

352  07 

(/T)(yr ~  XbV'or - wauy 

- 

wor  FR a BAY 

. 

=  DANA = tal BY Oh R rl T)(b; —  B).  [8.2.11] 
Now) (/7)(uia,) = “WTyst  2 , meee {u2} is ani. id. sciceitaaas mean  o?. 
Thus, by the law  of large numbers, 
Pius an  asymptotic  inte  anu 2: RotaegerD trot!  ae  d  bie 

ee 

: 

| 

Soe see ee  ee, See 

2}  as #  vmrotudin  sseceneranet 

| 

>  a  ea 

-  .  s  rs  nioee 6 ye ue 

a  =  eo aie te aa 

a 

‘ 

© 

——-, 

*% 

ting 

© 

i 

To  find  the  asymptotic  distribution  of s7, consider  first  V1(67  —  7).  From 

[8.2.11],  this  equals 

V1(6}  -  0?)  =  (UVT)(upuz)  —  VTo? 

—  VT(b;  —  B)'(X7X7/T)(br  —  B). 

(8.2.15) 

But 

(Q/VT)(uyu;)  —  VTo?  =  (1/VT)  D> (u2  —  0°), 

T 

where  {u2  —  o?}  is  a  sequence  of  i.i.d.  variables  with  mean  zero  and  variance 
E(u?  —  o?)?  =  E(u*)  —  207E(u2)  +  o*  =  pw,  —  o*.  Hence,  by the  central  limit 
theorem, 

(/V/T)(uu;)  —  VTo2 > NO,  (Hs  —  0°). 
For  the  last  term  in  [8.2.15],  we  have  V7T(b;  —  B) >  N(O,  o7Q~*),  (X7X7/T) 
, Q, and  (b;  —  B)  > 9.  Hence, 

(8.2.16] 

L 

’ 

VT (br  —  B)'(X7X7/T)(by  —  B) > 0. 

Putting  [8.2.16]  and  [8.2.17]  into  [8.2.15],  we  conclude 

VT(6%  —  02) > N(O,  (u4  -  07%). 

[8.2.17] 

[8.2.18] 

To  see  that  s% has  this  same  limiting  distribution,  notice  that 

VI(s%  ~  0?)  —  VI(64  —  07)  =  VITA  —  67  —  67} 

=  [KVI(T  —  b)]6%-. 

But  lim...  [((kV7)/(T  —  k)]  =  0, establishing  that 

VT(s3.  —  0?)  —  VT(6%  —  0?) > 0-0? 

= 

and  hence,  from  Proposition  7.3(a), 

VT(s3  —  02) > N(0,  (us  —  0°). 

[8.2.19] 

Notice  that  if we  are  relying  on  asymptotic  justifications  for  test  statistics, 
theory  offers  us  no  guidance  for  choosing  between  s* and  G?  as  estimates  of 7, 
since  they  have  the  same  limiting  distribution. 

: 

Next  consider  the  asymptotic  distribution  of  the  OLS ¢ test  of the  null  hy- 

pothesis  B; = 

B°, 

1  =  Wir  =  BY)  _  VT(bir  —  BF) 

[8.2.20] 

where  4 denotes  the  row  i, column  i element  of (X;X,)~'.  We  have  seen  that 
VT(b,  —  B°) >  N(O,  oq‘),  where  qi denotes  the  row  i, column  i element  of 
Q~-'.  Similarly,  Té/ is the  row  i, column  i element  of (X7X7/T)~'  and  converges 
in  probability  to  g“.  Also,  s; > o.  Hence,  the f¢ statistic  [8.2.20]  has  a  limiting 
distribution  that  is the  same  as  a N(0,  o7q")  variable  divided  by \/o7q";  that  is, 
[8.2.21] 
tr—> N(O,  1). 
Now,  under  the  more  restrictive  conditions  of Assumption  8.2,  we  saw  that 
t; would  have a f  distribution  with  (T —  k) degrees  of freedom.  Recall  that  a t 
variable  with  N degrees  of freedom  has  the  distribution  of the  ratio  of a N(0,  1) 
variable  to  the  square  root  of (1/N)  times  an  independent  y?(N)  variable.  But  a 
x’(N)  variable  in turn  is the  sum  of N squares  of independent  N(0,  1) variables. 
212  Chapter 8  | Linear  Regression  Models 

Thus,  letting  Z denote  a  N(0,  1) variable,  a  ¢ variable  with  N degrees  of freedom 
has  the  same  distribution  as 

| NT  NN eee 

z= 
{(Z?  +  Z3-4--  >  +  ZR)/N}?’ 

N 

By the  law  of large  numbers, 

L 

(Zi  +  Z3  +  ---  +  Z2\/N—  E(Z?)  =  1, 
and  so  ty —  N(0,  1).  Hence,  the  critical  value  for  a  ¢ variable  with  N  degrees  of 
freedom  will  be  arbitrarily  close  to  that  for  a  N(0,  1) variable  as  N becomes  large. 
Even  though  the  statistic  calculated  in  [8.2.20]  does  not  have  an  exact  t(T  —  k) 
distribution  under  Assumption  8.3,  if we  treat  it as  if it did,  then  we  will  not  be 
far  wgpng  if our  sample  is sufficiently  large. 

The  same  is true  of [8.1.32],  the  F test  of m  different  restrictions: 

Fr;  =  (Rb;  —  r)'[s7R(X7X7)~'R']~ (Rb;  —  r)/m 

=  V1I(Rb;  —  r)’'[s?R(X7X,/T)~'R’}]-!  V/T(Rb;  —  r)/m.  enn 

Here  s3-—>  o?, XX,/T  > Q, and,  under  the  null  hypothesis, 

VT(Rb;  —  r) =  [RVT(b;  —  B)] 
+ N(O,  o?RQ-'R’). 

Hence,  under  the  null  hypothesis, 

m:F, > [RVT(br  ~  B)]'[77RQ-'R’}-"[RVT(br  —  B)].- 
This  is a quadratic  function  of a Normal  vector  of the type described  by Proposition 
8.1,  from  which 

m:-F,—> x*(m). 

Thus  an  asymptotic  inference  can  be  based  on  the  approximation 

(Rb;  —  r)'[s?R(X7;X7)~'R’]~'(Rb;  —  r) ~  x?(m). 

[8.2.23] 

This  is known  as  the  Wald form  of the  OLS  ,” test. 

As  in the  case  of the  ¢ and  limiting  Normal  distributions,  viewing  [8.2.23]  as 
x?(m)  and  viewing  [8.2.22]  as  F(m,  T  —  k) asymptotically  amount  to  the  same 
test.  Recall  that  an  F(m,  N) variable  is a ratio  of a y?(m)  variable  to  an  indepen- 
dent  x7(N)  variable,  each  divided  by its  degrees  of freedom.  Thus,  if Z; denotes 
a N(O,  1) variable  and  X a y?(m)  variable, 

ines 
mN 

X/m 
(23  +  Z2 +--+  +  Z2/N 

For  the  denominator, 

} 

(Z2  +  Z3 +--+  +  Z2)/N—>  E(Z?)  =  1,, 

implying 

Fru >, Xim. 

Hence,  comparing  [8.2.23]  with  a x2(m) critical  value  or  comparing  [8.2.22]  with 
an  F(m,  T —  k) critical  value  will  result  in the  identical  test  for  sufficiently  large 
T (see Exercise  8.2). 

For  a given  sample  of size  7, the  small-sample  distribution  (the  ¢ or  F dis- 
tribution)  implies wider confidence  intervals  than the large-sample  distribution  (the 

notes 

, 

2 

8.2.  Ordinary  Least  Squares  Under  More  General  Conditions 

213 

Normal  or  y? distribution).  Even  when  the  justification  for  using  the  ¢ or  F distri- 
bution  is only  asymptotic,  many  researchers  prefer  to  use  the  f or  F tables  rather 
than  the  Normal  or  x? tables  on  the  grounds  that  the  former  are  more  conservative 
and  may  represent  a  better  approximation  to  the  true  small- sample  distribution. 
If we  are  relying  only  on  the  asymptotic  distribution,  the  Wald  test  statistic 
(8.2.23]  can  be  generalized  to  allow a test  of a  nonlinear  set  of restrictions  on  B. 
Consider  a  null  hypothesis  consisting  of  m  separate  nonlinear  restrictions  of  the 
form  g(B) = 0 where  g:  R‘ >  R”  and  g(-)  has  continuous  first  derivatives.  Result 
[8.2.8]  and  Proposition  7.4  imply  that 

VT[g(br)  —  8(Bo)]  > | 

; B=Bo 

where  z  ~  N(0,  0*Q7')  and 

98 
ap’ 

B=Bo 

denotes  the  (m  x  k) matrix  of derivatives  of g(-) with  respect  to  B, evaluated  at 
the  true  value  B,.  Under  the  null  hypothesis  that  g(B))  =  0,  it  follows  from 
Proposition  8.1  that 

| 

“(VT etbn) te 

3Q- | 2 

ii {V/T-g(b7)} > x2(m). 

B= mi  op 

B=Bo 

Recall  that  Q is the  lim of  (1/T)(X7;X7).  Since  dg/dB’  is continuous  and  since 
b;—>  Bo, it follows  from  Proposition  7.1  that 

OB'|\g-0, 

op 

B= J 

Hence a set  of m  nonlinear  restrictions  about  B of the  form  g(B)  =  0 can  be tested 
with  the  statistic 

{g(bz)}'  ES 3B, 

\s 7(X7X7)~ | ah 

|} {g(b;)}  > x2(m). 

Note  that  the  Wald  test  for linear  restrictions  [8.2.23]  can  be obtained  as  a a  ee 
case  of this  more  general  formula  by setting  g(B) = RB  —  r. 

One disadvantage  of the Wald  test  for nonlinear  restrictions  is that the answer 
one  obtains  can  be  different  depending  on  how  the  restrictions  g(B) = 0 are 
parameterized.  For example,  the hypotheses  8, =  B, and B,/B,  =  1 are  equivalent, 
and  asymptotically  a  Wald  test  based  on  either  parameterization  should  give  the 
same  answer.  However,  in  a  particular  finite  sample  the  answers  could  be  quite 
different.  In effect,  the  nonlinear  Wald  test  approximates  the  restriction  g(b;)  = 
0 by the  linear  restriction 

2(Bo)  +  E  Jo. —  By)  =  0. 
B=Bo 

Some  care  must  be  taken  to  ensure  that  this  linearization is redsotiable over  the 
range  of plausible  values  for  B. See  Gregory  and  Veall  (1985),  Lafontaine  and 
White  (1986),  and  Phillips  and  Park  (1988) for further  discussion. 
| 
214  Chapter 8 | Linear  Regression  Models 

Case  4.  Estimating  Parameters  for an  Autoregression 
Consider  now  estimation  of the  parameters  of a pth-order  autoregression  by 

v 

LS. 

Assumption  8.4: 

The  regression  model  is 

Y=  C+  OiWi-1  +  O2y-2  te  +  O,Ji-p  +  €,, 

[8.2.24] 

-  $2")  =  0 outside  the  unit  circle  and 
with  roots  of (1  —  $,z  —  @)z27  -—--- 
with  {e,} an  i.i.d.  sequence  with  mean  zero,  variance  a7,  and finite fourth  moment 
Ka: 

An  autoregression  has  the  form  of  the  standard  regression  model  y,  = 
»  »  Yep)  and  u,  =  e,.  Note,  however,  that 
x/B  +  u, with  x;  =  -(1, y,-1,  Yi-2,  - 
an  autoregression  cannot  satisfy  condition  (a) of Assumption  8.2  or  8.3.  Even 
though  u, is independent  of x, under  Assumption  8.4,  it will  not  be  the  case  that 
u, is independent  of x,,,.  Without  this  independence,  none  of the  small-sample 
—— for case  1 applies.  Specifically,  even  if ¢, is Gaussian,  the  OLS  coefficient 
es a  biased  estimate  of  B for  an  autoregression,  and  the  standard f¢ and F 

- 

s  can  only be justified asymptotically. 

However,  the iat  ak, results for case  4 are  the same  as fos case 3 and are 
e same way. To adapt the earlier  notation, suppose  that 
rived  in | 
sample consists  3 + p observations  on y,, numbered  (y_p41, ¥—p+2)-  ++  > 

Yor Y ria } 

ar yn O ‘estimation will thus use observations 1 ee T. Then, as 

On HEHE 10-127 vier = -[on 3 *, i] [own > iF (8.2. ee 

katt in [8.2.25] is 

tio SIRE  * DAY  cay oe  t;  Sy ya  1HisSs 
rsh  . cS °-  bee vd  nevis  er eid  teris  € esteeet >  ox wae  oW 
* 
aneinittsos 

Etih  -  Gx}  =  (%'X) 

=e, 

His  iO  fi 

~  Hy 

iW 

; 

| - i agp aren 

| 

= -e ro 

| eee) 
T—isX°. 

he 

where 

] 
ae 
QulHe 

qv 

nr  ew 

M 

>  oe 
ee  a 
gk  et 

u 

oe  ype 

[8.2.27] 

Bg  le  it  Ips  te?  5  te Yorba 

For  the  second  term  in  [8.2.25],  observe  that  x,u,  is a  martingale  difference 

sequence  with  positive  definite  variance-covariance  matrix  given  by 

E(x,u,uU,X;  ) a  E(u?)-E(x,x;)  =  a7Q. 

Using  an  argument  similar  to  that  in  Example  7.15,  it can  be  shown  that 

Juv S xu + N(O,  o7Q) 

[8.2.28] 

(see  Exercise  8.3).  Substituting  [8.2.26]  and  [8.2.28]  into  [8.2.25], 

VT(b;  —  B) > N(0,  0?Q7?). 

[8.2.29] 

It  is straightforward  to  verify  further  that  bz and  s? are  consistent  for  this 
case.  From  [8.2.26],  the  asymptotic  variance-covariance  matrix  of  \/7(b;  —  B) 
can  be  estimated  consistently  by s3(X7X-7/T)~—',  meaning  that  standard  t and  F 
statistics  that  treat  b; as  if it were  N(B, s}(X7X-,)~*)  will yield asymptotically  valid 
tests  of hypotheses  about  the  coefficients  of an  autoregression. 

, 

As a special  case  of [8.2.29],  consider  OLS  estimation  of a  first-order  auto- 

regression, 

Me  =  PY,-1  a5 E;, 

with  |¢| <  1. Then Q is the scalar  E(y?_,)  =  yo,  the variance  of an  AR(1) process. 
We  saw  in  Chapter  3 that  this  is given  by o7/(1  —  7).  Hence,  for  @ the  OLS 
coefficient, 

result [8.2.29] implies  that 

VT(br  -  ¢) > N(0,  o?-[o71  ~  $))-")  =  NO,  1 —  42). 

[8.2.30] 

If more  precise results  than  the asymptotic  approximation  in equation  [8.2.29] 
are  desired,  the exact  small-sample  distribution  of br can  be calculated  in either 
of two  ways.  If the errors  in the autoregression  [8.2.24]  are  N(0,  0), then  for any 
specified  numerical  value  for  ¢,, @2,...  ,  ¢, and  c the  exact  small-sample  distri- 
bution  can  be calculated  using numerical  routines  developed  by Imhof  (1961);  for 
illustrations  of this  method,  see  Evans  and  Savin  (1981)  and  Flavin  (1983).  An 
alternative  is to approximate  the  small-sample  distribution  by Monte  Carlo  meth- 
ods.  Here  the  idea  is to  use  a computer  to  generate  pseudo-random  variables  61, 
+s  Oxy each distributed  N(0, 0?) from numerical  algorithms such as that described 
in  Kinderman  and  Ramage  (1976).  For  fixed  starting  values  y_,,,,...,  y,,  the 
216  Chapter 8  | Linear  Regression Models 

values  for  y,,  yo, ...,  yr  can  then  be  calculated  by iterating  on  [8.2.24].°  One 
then  estimates  the  parameters  of [8.2.24]  with  an  OLS  regression  on  this  artificial 
sample.  A  new  sample  is generated  for  which  a  new  OLS  regression  is estimated. 
By performing,  say,  10,000  such  regressions,  an  estimate  of the  exact  small-sample 
distribution  of  the  OLS  estimates  can  be  obtained. 

For the  case  of a  first-order  autoregression,  it is known  from  such  calculations 
that  @; is downward-biased  in small  samples,  with  the  bias  becoming  more  severe 
as  @ approaches  unity.  For  example,  for  a  sample  of  size  T =  25  generated  by 
[8.2.24]  with  p =  1,  c  =  0, and  ¢ =  1, the  estimate  ¢, based  on  OLS  estimation 
of [8.2.24]  (with  a  constant  term  included)  will  be  less  than  the  true  value  of 1 in 
95%  of the  samples,  and  will  even  fall  below  0.6  in  10%  of the  samples.’ 

Case  5.  Errors  Gaussian  with  Known  Variance-Covariance 
Matrix 

Next  consider  the  following  case. 

Assumption  8.5: 
u  is N(O0,  a*V);  (c) V is a  known  positive  definite  matrix. 

(a) x, stochastic;  (b) conditional  on  the full matrix  X, the  vector 

When  the  errors  for  different  dates  have  different  variances  but  are  uncor- 
related  with  each  other  (that  is, V is diagonal),  then  the  errors  are  said  to  exhibit 
heteroskedasticity.  For  V  nondiagonal,  the  errors  are  said  to  be  autocorrelated. 
Writing  the  variance-covariance  matrix  as  the  product  of  some  scalar  a”  and  a 
matrix  V is a  convention  that  will  help  simplify  the  algebra  and  interpretation  for 
some  examples  of  heteroskedasticity  and  autocorrelation.  Note  again  that  As- 
sumption  8.5(b)  could  not  hold  for  an  autoregression,  since  conditional  on  x,,;  = 
(1, Ys Ye-15  - 

»  Ye-p+i1)'  and  x,,  the  value  of u, is known  with  certainty. 

- 

- 

Recall  from  [8.1.12]  that 

(b —  B)  =  (X’'X)!X’u. 

Taking  expectations  conditional  on  X, 

E[(b  —  B)|X]  =  (X'X)~'X'-E(u)  =  0, 

and  by the  law  of iterated  expectations, 

E(b —  B) =  Ex{El(b  ~  B)|X]}  =  0. 

Hence,  the  OLS  coefficient  estimate  is unbiased. 
The  variance  of b conditional  on  X is 

E{((b  ~  B)(b  ~  B)'IX}  =  E(X'X)'X'ww'XXX) I  ig 9 31 

A 
Thus,  conditional  on  X, 

=  o7(X'X)~'X/VX(X’X)7?. 

b|x ~  v(s. o%(X'X)'X'VX(KX)-*), 

6Alternatively,  one  can  generate  the  initial  values  for y with  a  draw  from  the  appropriate  uncon- 
+  Yo)’ 
ditional  distribution.  Specifically,  generate  a  (p  x  1) vector  v ~  N(O,  I,) and  set  (y_,.1,  - 
=  pl  +  P-v,  where w =  c/(1  -  ,  —  $,  —  ***  —  $,),  1 denotes a (p x 1) vector of 1s, and P is 
the Cholesky  factor such that P-P’ =  I for f the (p x  p) matrix  whose columns  stacked  in a  (p? x  1) 
vector  comprise the first column  of the matrix o*(I,2  —  (F @ F)]~',  where F is the  (p  x  p) matrix 
defined  in equation  [1.2.3]  in Chapter  1. 

- 

- 

7These  values  can be inferred  from  Table  B.5. 

8.2.  Ordinary  Least  Squares  Under  More  General  Conditions 

217 

Unless  V  =  Iz,  this  is not  the  same  variance  matrix  as  in  [8.1.17],  so  that  the  OLS 
t statistic  [8.1.26]  does  not  have  the  interpretation  as  a  Gaussian  variable  divided 
by  an  estimate  of  its  standard  deviation.  Thus  [8.1.26]  will  not  have  a  «(T  —  k) 
distribution  in  small  samples,  nor  will  it even  asymptotically  be  N(0,  1).  A  valid 
test  of  the  hypothesis  that  B,  =  B? for  case  5 would  be  based  not  on  [8.1.26]  but 
rather  on 

i 

(b,  —  Bi) 
sVdii 

8.2.32 
| 

where  d,, denotes  the  row  i,  column  i element  of  (X’X)~'X’VX(X'X)~".  This 
statistic  will  be  asymptotically  N(0,  1). 

Although  one  could  form  an  inference  based  on  [8.2.32],  in this  case  in which 
V is known,  a  superior  estimator  and  test  procedure  are  described  in  Section  8.3. 
First,  however,  we  consider  a  more  general  case  in which  V is of unknown  form. 

Case  6.  Errors  Serially  Uncorrelated  but  with  General 
Heteroskedasticity 
It may  be  possible  to  design  asymptotically  valid  tests  even  in  the  presence 
of heteroskedasticity  of  a completely  unknown  form.  This  point  was  first  observed 
by  Eicker  (1967)  and  White  (1980)  and  extended  to  time  series  regressions  by 
Hansen  (1982)  and  Nicholls  and  Pagan  (1983). 

(a) x, stochastic,  including perhaps  lagged  values  of y;  (b)  x,u,  is 
Assumption  8.6: 
a  martingale  difference  sequence;  (c) E(u?x,x;)  =  Q,, a positive  definite  matrix,  with 
(1/T) =7_,Q,  converging  to  the positive  definite  matrix  Q and  (1/T)=7_  ,u?x,x;  +0; 
(d) E(u8xXjXnXm)  <  © for all i, j, l, m,  and t; (e) plims of (1/T)2/_,u,x.%,x;  and 
(1/T) 
D7 xX X,X;, exist  and  are  finite for all  i and j and  (1/T)2/_,x,x,  + Q,  a 
nonsingular  matrix. 

Assumption  8.6(b)  requires  u, to  be  uncorrelated  with  its own  lagged  values 
and  with  current  and  lagged  values  of x.  Although  the  errors  are  presumed  to  be 
serially  uncorrelated,  Assumption  8.6(c)  allows  a  broad  class  of conditional  het- 
eroskedasticity  for  the  errors.  As  an  example  of such  heteroskedasticity,  consider 
a  regression  with  a  single  i.i.d.  explanatory  variable  x,  with  E(x?)  = 
yw,  and 
E(x?)  =  m4.  Suppose  that  the  variance  of  the  residual  for  date  ¢  is  given  by 
E(u?|x,)  =  a  +  bx?. Then  E(u2x?)  =  E,[E(u2|x,)-x?]  =  E,[(a  +  bx?)-x?]  =  ap, 
+  by,  Thus,  1,  =  au,  +  bu,  =  ©  for  all  t.  By  the  law  of  large  numbers, 
(1/T)27_,u?x?  will  converge  to  the  population  moment  2.  Assumption  8.6(c)  al- 
lows  more  general  conditional  heteroskedasticity  in that  E(u?x?)  might  be  a  func- 
tion of t, provided  that  the  time  average  of (u?x?)  converges.  Assumption  8.6(d) 
and  (e€) impose  bounds  on  higher  moments  of x  and u. 

Consistency  of b is established  using  the  same  arguments  as  in case  3.  The 

asymptotic  variance  is found  from  writing 

V7(b;  -  B) =  ur > xxi] |aivr) > x) 

Assumption  8.6(e)  ensures  that 

T 

=f 

Ga > x  4Q" 

t=) 

218  Chapter 8  | Linear  Regression  Models 

for some  nonsingular  matrix  Q. Similarly,  x,u,  satisfies  the  conditions  of Proposition 
7.9,  from  which 

wv > x, + N(0,  2). 

The  asymptotic  distribution  of the  OLS  estimate  is thus  given  by 

VT(b;  —  B) > NO,  Q-'2Q-?). 

[8.2.33] 

White’s  proposal  was  to  estimate  the  asymptotic  variance  matrix  consistently 
by substituting  Q;  =  (1/T)=7,x,x/  and  Q, =  (1/T)=7  ,A2x,x’  into  [8.2.33],  where 
a, denotes  the  OLS  residual  [8.1.4].  The  following  result  is established  in Appendix 
8.A  to  this  chapter. 

Proposition  8.3: 
With  heteroskedasticity  of unknown  form  satisfying  Assumption 
8.6,  the  asymptotic  variance-covariance  matrix  of the  OLS  coefficient  vector  can  be 
consistently  estimated  by 

07'2,07!  > Q-12Q"!. 

(8.2.34] 

Recalling  [8.2.33],  the  OLS  estimate  b; can  be  treated  as  if 

where 

Vr-=  Q7'0707! 

b; ~  M(B,  V,/T) 

xpxy7y| (VT) p> din:  [OxeXe/T)™ 

fe 

[8.2.35] 

Ty. 

T:(X7X7)~ i» tnx (X7X,)~ z 

The  square  root  of  the  row  i, column  i element  of  VT  is  known  as  a 
heteroskedasticity-consistent  standard  error  for  the  OLS  estimate  b;.  We  can,  of 
course,  also  use  (V,/T)  to  test  a joint  hypothesis  of the form  RB  =  r, where R is 
an  (m  x  k) matrix  summarizing  m  separate  hypotheses  about  B. Specifically, 
(Rb;  —  r)'[R(V;/T)R']- 

(Rb;  —  r) 

[8.2.36] 

has  the  same  asymptotic  distribution  as 

[V7T(Rb;  —  r)]'(RQ-'2Q-'R')-'[V7T(Rb;  —  ¥)], 
which,  from  [8.2.33],  is  a quadratic  form  of an  asymptotically  Normal  (m  x  1) 
vector  \/7(Rb;  —  r) with  weighting  matrix the  inverse  of its variance-covariance 
matrix,  (RQ~'!9Q-'R’).  Hence,  [8.2.36]  has an  asymptotic  x? distribution  with  m 
degrees  of freedom. 
It is also possible to develop an estimate of the asymptotic variance-covariance 
matrix  of b; that  is robust  with respect to  both  heteroskedasticity  and  autocorre- 
lation: 
(V,/T) 

: 

=  ox.)  3 A? x,Xx; 

T. 

+  yf [3 ad q rn 5]  nine  +  kahit)  [ORE 

8.2.  Ordinary  Least Squares  Under  More  General  Conditions 

219 

Here  q  is  a  parameter  representing  the  number  of  autocorrelations  used  to  ap- 
proximate  the  dynamics  for  u,.  The  square  root  of the  row  /, column  i element  of 
(V ,/T) is known  as  the  Newey-West  (1987) heteroskedasticity-  and autocorrelation- 
consistent  standard  error  for  the  OLS  estimator.  The  basis  for  this  expression  and 
alternative  ways  to  calculate  heteroskedasticity-  and  autocorrelation-consistent 
standard  errors  will  be  discussed  in Chapter  10. 

8.3.  Generalized  Least  Squares 
The  previous  section  evaluated  OLS  estimation  under  a  variety  of  assumptions, 
including  E(uu’)  #  oI.  Although  OLS  can  be  used  in this  last  case,  generalized 
least  squares  (GLS)  is usually  preferred. 

GLS  with  Known  Covariance  Matrix 

Let  us  reconsider  data  generated  according  to  Assumption  8.5,  under  which 
u|X ~  N(0, o?V) with  V a known  (T x  T) matrix.  Since  V is symmetric  and positive 
definite,  there  exists  a nonsingular  (TJ  x  T) matrix  L such  that® 

VOI  =U. 

[8.3.1] 

Imagine  transforming  the  population  residuals  u  by L: 

u  =Lu. 

(Tx  1) 

This  would  generate  a new  set  of residuals  G with  mean  0 and  variance  conditional 
on  X given  by 

E(aa'|X)  =  L-E(uu'|X)L’  =  Lo? VL’. 

But  V  =  [V~']~'  =  [L’L]~',  meaning 

E(aa'|X)  =  o?L[L'L]-'L'  =  o2I,. 

[8.3.2] 
We can  thus take  the matrix  equation  that characterizes  the basic  regression  model, 
y=  XP  +u, 

; 

and  premultiply  both  sides  by L: 

to  produce  a  new  regression  model 

Ly  =  LXB  +  Lu, 

where 

y =  XB +  a, 

[8.3.3] 

j=Ly 

X=LX 
[8.3.4] 
with  |X ~  N(0, 071).  Hence,  the transformed  model  [8.3.3] satisfies  Assumption 
8.2,  meaning  that  all  the  results  for  that  case  apply  to  [8.3.3].  Specifically,  the 
estimator 

aG=Lu 

b =  (X’X)~'X’y  =  (X’L’LX)-'X’'L’Ly  =  (X'V-!X)-'X'V-ly 

[8.3.5] 

“We  know  that  there  exists  a nonsingular  matrix  P such  that  V =  PP’  and  so  V-'  =  Pen, 

Take  L  =  P~'  to deduce  [8.3.1]. 

220  Chapter 8  | Linear  Regression  Models 

is Gaussian  with  mean  B and  variance  o2(K'X)~!  =  o?(X'V~!X)~'  conditional 
on X and  is the  minimum-variance  unbiased  estimator  conditional  on  X.  The  es- 
timator  [8.3.5] is known  as the generalized least squares  (GLS) estimator.  Similarly, 

=  [IMT  —  k))  & (9,  ~  %:bY 

[8.3.6] 

has an  exact  [a?/(T  —  k)]-x?(T  —K)  distribution  under  Assumption  8.5,  while 
(Rb —  r)'[s2R(X’V-!X)-'R’]- (Rb —  r)/m 

has an  exact  F(m,  T —  k) distribution  under  the  null  hypothesis  RB  = 

We  now  discuss  several  examples  to make  these  ideas concrete. 

Heteroskedasticity 

A simple case to analyze  is one for which the  variance  of u, is presumed  to 
= to the silt of one  of the explanatory variables  for that equation, 
say 

zis 

i Po 

aiee 

eee 

yt  of  mo 3a de  |  seyhj  leh  ae 

fh cay 

2% Bee  - 
Seed.  RBS  Ae 
Le 
Rese!’ Aon  *)  (A 

"x 

ae 

(a 

Yo) 

Jie. 

tH 

Sheet & i Fi 

oth Se oi  iv  <3  tiv} 

Fa  a 62 om  a 3  >  +4 

” 

*- 

+ 

+, 

Aa. Raa 

Notice  from  expression  [5.2.18]  that  the  matrix 

54  ery  ee  ee  eee 

eer 

1s 

=A 
0 

8 
Le. 
—p~l 

0 
0 

O 
O 

[8.3.9] 

0 

GO:  -*? 

&y” 

1 

satisfies  [8.3.1].  The  GLS  estimates  are  found  from  an  OLS  regression  of y =  Ly 
on  X =  LX;  that is,  regress  y,V/1  —  p? on  x,V/1  —  p? and  y,  —  py;-1  ON  X&,  — 
px,_, 

for t = 2, 3, 

ren: 

GLS  and  Maximum  Likelihood  Estimation 
Assumption  8.5  asserts  that  y|[K  ~  M(XB,  o”V).  Hence,  the  log of the  like- 

lihood  of y conditioned  on X is given  by 

(—T/2)  log(27)  —  (1/2) loglo?V|  —  (1/2)(y  —  XB)'(o7V)~'(y  —  XB). 

Notice  that  [8.3.1]  can  be  used  to  write  the  iast  term  in  [8.3.10]  as 

—(1/2)(y  —  XB)'(o7V)~"(y  —  XB) 

7 

—[1/(207)|(y  —  XB)'(L'L)(y  —  XB) 
-[1/(202)|(Ly  - LXB)'(Ly  —  LXB) 
—[1/2e*)|(¥  —  XB)'(¥  —  XB). 
Similarly,  the  middle  term  in  [8.3.10]  can  be  written  as  in  [5.2.24]: 

[8.3.10] 

(8.3.11] 

[8.3.12] 
—(1/2)  loglo?V|  =  —(T/2)  log(a?)  +  log|det(L)|, 
where  |det(L)|  denotes  the  absolute  value  of  the  determinant  of  L.  Substituting 
{8.3.11]  and  [8.3.12]  into  [8.3.10],  the  conditional  log likelihood  can  be  written  as 

—(T/2)  log(27)  —  (T/2)  log(a?)  +  log|det(L)| 

[8.3.13] 
Thus,  the  log likelihood is  maximized  with  respect  to  B by an  OLS  regression  of 
y on  X,° meaning  that  the  GLS  estimate  [8.3.5]  is also  the  maximum  likelihood 
estimate  under  Assumption  8.5. 

—  [1/(2e’)\(¥  —  XB)'(y  —  XB). 

The  GLS  estimate b is still  likely  to  be reasonable  even  if the  vesidionls u  are 
non-Gaussian.  Specifically,  the residuals  of the  transformed  regression  [8.3.3]  have 
mean  0 and  variance  oI,  and  so  this  regression  satisfies  the  conditions  of  the 
Gauss-Markov  theorem—even  if the  residuals  are  non-Gaussian,  b will  have min- 
imum  variance  (conditional  on  X) among  the  class  of all  unbiased  estimators  that 
are  linear  functions  of  y.  Hence,  maximization  of  [8.3.13],  or  quasi-maximum 
likelihood  estimation,  may offer a useful  estimating  principle  even  for non-Gaussian 
u. 

GLS When  the  Variance  Matrix  of Residuals  Must 
Be Estimated from  the  Data 
Up to this point we  have  been  assuming  that  the elements  of V are  known  a 
priori.  More  commonly, V is posited  to  be of a particular  form  V(®),  where @ is a 

*This  assumes  that  the  parameters  of L do not  involve  B, as  is implied  by Assumption  8.5. 

222  Chapter 8  | Linear  Regression  Models 

vector  of parameters  that  must  be estimated  from  the  data.  For  example,  with  first- 
order serial  correlation  of  residuals  as  in  [8.3.7],  V  is the  matrix  in  [8.3.8]  and  @ 
‘Is  the  scalar  p.  As  a  second  example,  we  might  postulate  that  the  variance  of 
observation  ¢ depends  on  the  explanatory  variables  according  to 

in which  case  @  =  (a,,  a>)’. 

Our  task  is then  to  estimate  6 and 8 jointly  from  the  data.  One  approach  is 
to  use  as  estimates  the  values  of  @  and  B that  maximize  [8.3.13].  Since  one  can 
always  form  [8.3.13]  and  maximize  it numerically,  this  approach  has  the  appeal  of 
offering  a  single  rule  to  follow  whenever  E(uu’|X)  is not  of the  simple  form  o71,. 
However,  other,  simpler  estimators  can  also  have  desirable  properties. 

It often  turns  out  to  be  the  case  that 

V1(X7{V7(67)]-!X7)~  (X LV 7(67)] ~y7) 

-  V17(X4[Vr(8o)]~  X17) ~(X7[V7(80)]}“*y7), 

where  V;(@,)  denotes  the  true  variance  of errors  and  6; is any  consistent  estimate 
of 8.  Moreover,  a  consistent  estimate  of  @ can  often  be  obtained  from  a  simple 
analysis  of OLS  residuals.  Thus,  an  estimate  coming  from  a  few  simple  OLS  and 
GLS  regressions  can  have  the  same  asymptotic  distribution  as  the  maximum  like- 
lihood  estimator.  Since  regressions  are  much  easier  to  implement  than  numerical 
maximization,  the  simpler  estimates  are  often  used. 

Estimation  with  First-Order  Autocorrelation  of Regression 
Residuals  and  No  Lagged  Endogenous  Variables 

We  illustrate  these  issues  by considering  a  regression  whose  residuals  follow 
the AR(1)  process  [8.3.7].  For  now  we  maintain  the  assumption  that  u|X has  mean 
zero  and  variance  o7V(p),  noting  that  this  rules  out  lagged  endogenous  variables; 
that  is,  we  assume  that  x,  is  uncorrelated  with  u,_,.  The  following  subsection 
comments  on  the  importance  of  this  assumption.  Recalling  that  the  determinant 
of a lower triangular matrix  is just the product of the terms  on the principal diagonal, 
we  see  from  [8.3.9]  that  det(L)  =  V/1  —  p?. Thus,  the  log likelihood  [8.3.13]  for 
this  case  is 

—(T/2)  log(27)  —  (T/2)  log(a?)  +  (1/2)  log(1  —p?) 
—  [C1  —  p*)/(207)](y.  —  xi B)? 

T 

[8.3.14] 

~  [1/(207)]  2, iy,  =  %,8).-  AY¥%1.-  x/_1B))’. 

One  approach,  then,  is to  maximize  [8.3.14]  numerically  with  respect  to  B, p,  and 
a2.  The  reader  may  recognize  [8.3.14]  as  the  exact  log likelihood  function  for  an 
AR(1)  process  (equation  [5:2.9])  with  (y,  —  ) replaced  by (y,  —  x,B). 

Just  as  in the  AR(1)  case,  simpler  estimates  (with  the  same  asymptotic  dis- 
tribution)  are  obtained  if we  condition  on the first observation,  seeking to maximize 
—[(T —  1)/2] log(2m)  ~  [(T —  1)/2} log(o) 

: 
=  [1/(207)] >» [Or _  x/B)  ‘<: p(y,-1  ~  x/_,B)). 

T 

[8.3.15] 

If we knew the value of p, then  the  value  of B that  maximizes  [8.3.15]  could  be 
found  by an  OLS regression  of (y, —  py,-1) On (x,  ~  pX,-1)  fort  =  2, 3,..., 

8.3.  Generalized  Least  Squares 

223 

T (call  this  regression  A).  Conversely,  if we  knew  the  value  of B, then  the  value 
of  p  that  maximizes  [8.3.15]  would  be  found  by  an  OLS  regression  of  (y,  — 
x/B)  on  (y,_,  —  x/_,B)  fort  =  2,3,...  ,  T (call  this  regression  B).  We  can  thus 
start  with  an  initial  guess  for  p (often  p  =  0), and  perform  regression  A to  get  an 
initial  estimate  of  B.  For  p  =  0, this  initial  estimate  of  B would  just  be  the  OLS 
estimate  b.  This  estimate  of  B can  be  used  in  regression  B  to  get  an  updated 
estimate  of p,  for  example,  by regressing  the  OLS  residual  4,  =  y,  —  x,b  on  its 
own  lagged  value.  This  new  estimate  of p can  be used  to repeat  the  two  regressions. 
Zigzagging  back  and  forth  between  A  and B is  known  as  the  iterated  Cochrane- 
Orcutt  method  and  will  converge  to  a  local  maximum  of [8.3.15]. 

Alternatively,  consider  the  estimate  of p that  results  from  the  first  iteration 

alone, 

ee 

[8.3.16] 

where  u, = y,  —  x;b  and b is the  OLS  estimate  of B. To  simplify  expressions,  we 
have  renormalized  the  number  of  observations  in  the  original  sample  to  T  +  1, 
denoted yo, y,;,.  . 
. 
, Yr, SO that  T observations  are  used in the conditional  maximum 
likelihood  estimation.  Notice  that 

ui, >  (y,  a  B’x,  a  B’x,  a  b’x,)  =  U,  i  (B  ad  b)’x,, 

allowing  the  numerator  of [8.3.16]  to  be  written 

; 
(1/T)  & ati, 

t=1 

2 

iG 

=  (1/T)  2 [u, +  (B  —  b)’x,][u,_,  +  (B  —  b)’x,_,] 

=  (1/T)  > (u,u,_,)  +  (B  —  b)'(1/T)  ) (gO 

alls, #9  [8.3.17] 

+  (B  —  wy [ar >» ri.  ]6 —  b). 

As long as b is a consistent estimate  of B and  boundedness  conditions  ensure  that 
plims  of (1/7) 27_,u,x,_,,  (1/7) 

27. ,u,_,x,,  and  (1/T)27_,x,x/_,  exist,  then 

rh 

fi 

(1/7)  > G,t,_,;  > (1/7)  S uyu,_, 

ze  | 

t= 1 

i 

=  (1/T)  > (6, +  pu, :)u,-y 

t=] 

[8.3.18] 

4 p:Var(u). 
Similar analysis establishes  that the denominator  of [8.3.16] converges in probability 
to Var(u),  so  that  p> p. 

If u, is uncorrelated  with  x, fors  =  ¢t  —  1, t, andt  +  1, one  can  make  the 
stronger claim that an estimate  of p based on an autoregression  of the OLS residuals 
4, (expression  [8.3.16])  has  the  same  asymptotic  distribution  as  an  estimate  of p 
based  on  the true  population  residuals  u,.  Specifically,  if plim[(1/7)27_,u,x,_,]  = 
224  Chapter 8  | Linear  Regression  Models 

eee  =  0, then  multiplying  [8.3.17]  by \/T,  we  find 

WVT)  > aa... 

t=1 

T 

=  (VT) & (u,U,_,)  +  V7(B  —  b)‘(1/T)  S (u,x,_,  +  U,_4X,) 

+  VT(B  -  wan xxi. —  b) 

F 

4 (1/VT) >> (u,u,_1)  + VT(B  —  b)'0 

+ VT(B  —  b)' pim| (1/7  Dxx|o 

T 

=  (1/VT) p> (u,u,_,). 

7  ; a.  j 

; 

|  (1/7) p3 a6, 

TAY . pags 
/T|  ————_——_  |  +  ao ae 

ints  a 
age  an ¥ a, :  Lam z a | 

[8.3.19] 

[8.3.20] 

_ The OLS estim pe of p ba ont the population re residuals would have an 

iptotic 

GistriOutior 

oe.  “aor 

=  = 

nthe 

De  pre  ,  z3I¢ iaipay  Cotan  oa  oF  stk ad  oo wrens 

c 3  Li =) a piace  # 4 ne tag 9 arse 

longer  hold  if the  regression  contains  lagged  endogenous  variables.  For  example, 
consider  estimation  of 

Yr  =  Byr-1  +  YX,  +  Us 

[8.3.22] 

where  u, follows  an  AR(1)  process  as  in [8.3.7].  Since  (1) u, is correlated  with  u,_ 
and  (2) u,_,  is correlated  with  y,_,,  it follows  that  u, is correlated  with  the  explana- 
tory  variable  y,_,.  Accordingly,  it is not  the  case  that  plim[(1/7)27_  ,x,u,]  7 0, 
the  key  condition  required  for  consistency  of  the  OLS  estimator  b.  Hence,  f  in 
: 
[8.3.16]  is not  a  consistent  estimate  of p. 
If one  nevertheless  iterates  on  the  Cochrane-Orcutt  procedure,  then  the  al- 
gorithm  will  converge  to  a local  maximum  of [8.3.15].  However,  the  resulting  GLS 
estimate  b need  not  be a consistent  estimate  of  B.  Notwithstanding,  the  global 
maximum  of [8.3.15]  should  provide  a  consistent  estimate  of B. By experimenting 
with  start-up  values  for  iterated  Cochrane-Orcutt  other  than  p  =  0, one  should 
find  this  global  maximum.'° 

A simple  estimate  of p that  is consistent  in the  presence  of lagged  endogenous 
variables  was  suggested  by Durbin  (1960).  Multiplying  [8.3.22]  by (1  —  pL)  gives 

SS  ae  (p  +  B)y.-1  ee  pBY,-2  +  yX,  —  PyX,-1  t  &,. 

[8.3.23] 

This  is a  restricted  version  of the  regression  model 

Y,  =  MYp-1  F  AgY,-2  +  AX,  +  @yX,_,  +  E,, 

[8.3.24] 

where  the  four  regression  coefficients  (a,, a,  a3,  a4) are  restricted  to be nonlinear 
functions  of  three  underlying  parameters  (p, B,  y).  Minimization  of  the  sum  of 
squared  e’s in [8.3.23]  is equivalent  to maximum  likelihood  estimation  conditioning 
on  the  first  two  observations.  Moreover,  the  error  term  in  equation  [8.3.24]  is 
uncorrelated  with  the  explanatory  variables,  and  so  the  a’s  can  be  estimated  con- 
sistently  by OLS estimation  of [8.3.24].  Then  —  &,/a, provides  a consistent  estimate 
of p despite  the  presence  of lagged  endogenous  variables  in [8.3.24]. 

Even  if consistent  estimates  of p and  B are  obtained,  Durbin  (1970)  empha- 
sized  that  with  lagged  endogenous  variables  it will  still  not  be  the  case  that  an 
estimate  of  p  based  on  (y,  —  x/fs)  has  the  same  asymptotic  distribution  as  an 
estimate  based  on  (y, —  x, B). Tosee  this, note that if x, contains lagged endogenous 
variables,  then  [8.3.19]  would  no  longer  be valid.  If x, includes  y,_,,  for  example, 
then  x, and  u,_,  will  be correlated  and plim[(1/7)27_,u,_,x,]  # 0, as  was  assumed 
in  arriving  at  [8.3.19].  Hence,  [8.3.20]  will  not  hold  when  x,  includes  lagged  en- 
dogenous  variables.  Again,  an  all-purpose  procedure  that  will  work  is to maximize 
the  log likelihood  function  [8.3.15]  numerically. 

Higher-Order  Serial  Correlation" 

Consider  next  the  case  when  the  distribution  of u|X can  be  described  by a 

pth-order  autoregression, 

ey,  Os  “FT  Ost we tt  *s.  &  Pp4+—p  +.  €,, 

See  Betancourt  and  Kelejian  (1981). 
"This  discussion  is based  on  Harvey  (1981,  pp.  204-6). 

226  Chapter  8  | Linear  Regression  Models 

The  log likelihood  conditional  on  X  for  this  case  becomes 

—  (772)  log(2m)  —  (T/2)  log(a?)  —  (1/2)  log|V,| 

ia  [1/(207)}(y,  ™  X,B)'V,  '(y,  Bei  X,B) 

CL)  De G =  #8)  +  py,  -—  x/_,B) 

T 

[8:3-25) 

MAK  Sarmes<2B  erm 

sar 

7  04%,  7X;-pB) 

2 

where  the  (p  x  1) vector  y, denotes  the  first  p observations  on  y,  X pis the  (p  x  k) 
matrix  of  explanatory  variables  associated  with  these  first  p  observations,  and 
a°V,  is the  (p  X  p) variance-covariance  matrix  of (y,|X,).  The  row  i, column  j 
element  of a’ V, is given by y,,_,,  for y, the kth autocovariance  of an  AR(p)  process 
with autoregressive  parameters  p,,p2,...,  p, and  innovation  variance  a7.  Letting 
L, denote  a  (p  X  p) matrix  such  that  L}L,  =  V;>!,  GLS  can  be  obtained  by 
regressing  y,  Ke  L,Y,  on  X,  -  L,X,  and  y,  =  y,  —  Mises 
£29,235"  ***  = 
GaJj—p OP 2, =  KX,  —  Pi%1-1  —  P2X-2  —¢  °°  —  Pph-pfort=pt+i,p+2,..., 
T.  Equation  [8.3.14]  is a special  case  of [8.3.25]  with p =  1, V, =  1/(1  —  p?), and 
L, =  V1  —  p*. 

If we  are  willing  to  condition  on  the  first p observations,  the  task  is to  choose 

B and  p,,.p2,  . 

: 

- 

. 

,  p, SO  as  to  minimize 

. 

ar lo, =  SB)  Ae  —  x;-1B)  —  pA¥,-2  —  X;-2B) 

ee  PAV»  —  x-B)| : 

2 

Again,  in the absence  of lagged endogenous  variables  we can  iterate  as in Cochrane- 
Orcutt,  first  taking  the  p,’s  as  given  and  regressing  y, on  x,,  and  then  taking  B as 
given  and  regressing  4, on  @,_,,  U,_2,...,  U,_p. 

Any covariance-stationary  process  for the  errors  can  always  be approximated 
by a  finite  autoregression,  provided  that  the  order  of  the  approximating  auto- 
regression  (p) is sufficiently  large.  Amemiya  (1973)  demonstrated  that  by letting 
p go to  infinity  at  a slower  rate  than  the  sample  size  T, this  iterated  GLS  estimate 
will  have  the  same  asymptotic  distribution  as  would  the  GLS  estimate  for  the  case 
when V is known.  Alternatively,  if theory  implies  an  ARMA(p,  q) structure  for 
the  errors  with  p  and  qg  known,  one  can  find  exact  or  approximate  maximum 
likelihood  estimates  by adapting  the  methods  in  Chapter  5,  replacing  mw  in  the 
expressions  in Chapter  5 with  x;B. 

Further  Remarks  on  Heteroskedasticity 
Heteroskedasticity  can  arise  from  a  variety  of sources,  and  the  solution  de- 
pends  on  the  nature  of  the  problem  identified.  Using  logs  rather  than  levels  of 
variables,  allowing  the  explanatory  variables  to enter  nonlinearly  in the  regression 
equation,  or  adding previously  omitted  explanatory  variables  to the  regression  may 
all be helpful.  Judge,  Griffiths,  Hill,  and Lee  (1980)  discussed  a variety  of solutions 
when  the  heteroskedasticity  is thought  to  be  related  to  the  explanatory  variables. 
In  time  series  regressions,  the  explanatory  variables  themselves  exhibit  dynamic 
behavior,  and such specifications  then imply a dynamic structure  for the conditional 
variance.  An  example  of such  a  model  is the  autoregressive  conditional  hetero- 
skedasticity  specification  of Engle  (1982).  Dynamic  models  of heteroskedasticity 
will  be discussed  in Chapter  21. 

8.3.  Generalized  Least  Squares 

227 

APPENDIX  8.A.  Proofs  of Chapter  8 Propositions 

w  Proof  of  Proposition  8.2. 
[8.1.27]  can  be  calculated  using  the  Lagrangean: 

The  restricted  estimate  b*  that  minimizes  [8.1.2]  subject  to 

J  =  (1/2)  & (y,  -  x/B)?  +  A(RB  —  ©). 

Zr 

[8.A,1] 

r=1 

Here  A denotes  an  (m  X  1) vector  of Lagrange  multipliers;  A, is associated with the constraint 
represented  by the  ith'row  of RB  =  r.  The  term  } is a  normalizing  constant  to  simplify  the 
expressions  that  follow.  The  constrained  minimum  is  found  by  setting  the  derivative  of 
[8.A.1]  with  respect  to  B equal  to  zero:'* 

oJ 

apr  = (12) 2 20;  — xB) 
ball  eS 

2(y,  — 

Gg 
x/p)  ~+——*  +  AR 

a(y  =  x, B) 

- 

' 

‘ 

T 

=  ->  (y,  -  Bx,)x)  +  VR  =  0, 

* 

or 

1=1 

8 

7 6 

b*’  >" xX,  =  > yx,  —  A'R. 
t=1 

t=1 

Taking  transposes, 

b xx  |b =  D xy,  —  R’A 

[Sam] [Sx0]- [Zax] wa  tear 

t=1 

t=1 

b* 

b  —  (X'X)-'R’A, 

where b denotes  the  unrestricted  OLS  estimate.  Premultiplying  [8.A.2]  by R (and recalling 
that  b*  satisfies  Rb*  =  r), 

Rb  —  r  =  R(X'X)-'R’A 

or 

hd  =  [R(X'X)~'R’]-'(Rb  —  r). 

[8.A.3] 

Substituting  [8.A.3]  into  (8.A.2], 

b —  b*  =  (X’X)~'R’[R(X'X)  -'R’]-'(Rb  —  r). 

[8.A.4] 

Notice  from  (8.A.4]  that 

(b —  b*)’(X’X)(b  —  b*)  =  {(Rb  —  r)'[R(X’X)-'R’]-'R(X'X)-"}(X’X) 

x  {(X'X)~'R'[R(X'X)-'R’] 
’  =  (Rb  —  r)'[R(X’X)~'R’]-'[R(X'X)-'R’] 

“(Rb —  r)} 

[8.A.5] 

<x  [R(X'X)-'R’]-(Rb  —  r) 

=  (Rb  —  r)'[R(X’X)-'R’]~'(Rb  —  r). 

Thus,  the  magnitude  in [8.1.32]  is numerically  identical  to 

F=  (b —  b*)'X'X(b -— b*)/m  we 

52 

BR  b*)'X'X(b  —  b*)/m 
: 

RSS,/(T  -  k) 

Comparing  this with  [8.1.37],  we  will  have  completed  the demonstration  of the equivalence 
of [8.1.32]  with  [8.1.37]  if it is the case  that 

RSS,  —  RSS,  =  (b =  b*)'(X'X)(b  —  b*). 

(8.A.6] 

"We  have  used  the  fact  that  4x/B/aB’  =  x;/.  See  the  Mathematical  Review  (Appendix  A) at  the 

end  of the  book  on  the  use  of derivatives  with  respect  to vectors. 

: 

228  Chapter 8  | Linear  Regression  Models 

Now,  notice  that 

RSS,  =  (y —  Xb*)'(y  —  Xb*) 

=  (y —  Xb  +  Xb  —  Xb*)'(y  —  Xb  +  Xb  —  Xb*) 
=  (y —  Xb)'(y  —  Xb)  +  (b  —  b*)’X’X(b  —  b*), 

[8.A.7] 

where  the  cross-product  term  has  vanished,  since  (y  —  Xb)'X  =  0 by the  least  squares 
property  [8.1.10].  Equation  [8.A.7]  states  that 

RSS,  =  RSS,  +  (b —  b*)'X'X(b  —  b*), 

[8.A.8] 

confirming  [8.A.6].  @ 

@ Proof  of Proposition  8.3.  Assumption  8.6(¢)  guarantees  that  Q,— > Q,  so  the  issue  is 
whether  2, gives a  consistent  estimate  of 2.  Define  27  =  (1/T)Z/_,u;x,x,,  noting  that 
QF; oomengye  in  probability  to  2  by  Assumption  8. 6(c). Thus,  if  we  can  show  that 
0,  -  2350,  then 2, > 2. Now, 

f, -  27  =  (1/7)  ») (af  =  u;)x,%,. 

(8.A.9] 

(a? —  u?) =  (0, +  u,)(@, —  u) 

| 

: 

one 
de 

=  [(y, ~  byx,)  +O, - Bx,  -  byx)  -  (,  - B’ x,)) 
_  = (20, — B’x,)  —  (br — B)’x,][-(br — B)'x,] © 
Retin  Retliie tiie.  mem 

allowing [8.4.9] tobe writen a5  mae 
0, ~  95 = (-2'7) ¥ ulby ~  BY’, x) +  (WT) nS By x Pou)  [8.4.10] 
hhh iekiabigilciann  eT  - 

ahr 

4 

;  =  (8.A.11] 

os 
24, 

e 

Consider  the  first  term  in [8.A.12]: 

wr) > o—  PX, 2))(%,  —  Px,_,)’ 

*  (1/T) » [x, —  PX,-,  +  (p i? P)x,— iI,  —  pX,-1  +  (p ae p)x,—1]’ 

-  (1/T) > (x, a  px,  -)(X, wrt pX,-1)’ 

+p - by IT) Xa -  PX, —1)X—1 

[8.4.13] 

+  GF puns ae —  px,-1)' 

— 

™% 

. 

+ 

-  DRWT) S xsX1-+ 

@ 
But  (p —  p)—>  0, and  the  plims  of  (1/7)27_,x,_,x;_,  and  (1/T)2/7_,x,x;_,  are  assumed  to 
exist.  Hence  [8.A.13]  has  the  same  plim as  (OD) 2a- mee px,-:)(%, — px)’. 

Consider  next  the  second term  in ay A.12]: 

(VT) > (= Pau Pu)  : ae  Gn  ioe es) 

p= 

=  QivT) > tx -  prs  + @ xl sent mas ta 

Pe.  87}  r 

x  * (8  - =  Ans 

| 

Shs 

been) 

Zins-)« a  - A 

8.2. 
Consrder  a  null  hypothesis  H, involving  m  =  2 linear  restrictions  on  B.  How  large  a 
sample  size  r is  needed  before  the  5%  critical  value  based  on  the  Wald  form  of  the  OLS 
F test  of H, is  within  1%  of  the  critical  value  of  the  Wald  form  of the  OLS  > test  of  H,? 
8.3. 

Derive  result  [8.2.28). 
Consider  a  covariance-stationary  process  given  by 

8.4. 

y=ut>d  WE, —;; 

j=0 

where  {e,} is an  i.i.d.  sequence  with  mean  zero,  variance  a7,  and  finite  fourth  moment  and 
where  >7, |W,| <  ~.  Consider  estimating  a pth-order  autoregression  by OLS: 

y:  =  6.0  Piyi-1  +  $2Y,-2  Rev  dcek  $,Y1-p  +  Uu,. 

Show  that the  OLS coefficients  give consistent  estimates  of the  population  parameters  that 
characterize  the  linear  projection  of y, on  a constant  and p of its lags—that  is, the coefficients 
give  consistent  estimates  of the  parameters  c,  ¢,,... 

,  ¢, defined  by 

E(ydy,—  1  Ye-2»  ees  ~— 

=c¢c+t  7  ee  .  2Y,-2 

dee. 

3  $,Y1-p 

(HINT:  Recall  that  c,  ¢,,...  ,  @, are  characterized  by equation  [4.3.6]). 

Chapter  8 References 

Amemiya,  Takeshi.  1973.  ‘““Generalized  Least  Squares  with  an  Estimated  Autocovariance 
Matrix.”  Econometrica  41:723-32. 
Anderson,  T.  W.  1971.  The  Statistical  Analysis  of Time  Series.  New  York:  Wiley. 
Betancourt,  Roger,  and  Harry  Kelejian.  1981.  “‘Lagged  Endogenous  Variables  and  the 
Cochrane-Orcutt  Procedure.”’  Econometrica  49:1073-78. 
Brillinger,  David  R.  1981.  Time  Series:  Data  Anaysis  and  Theory,  expanded  ed.  San  Fran- 
cisco:  Holden-Day. 
Durbin,  James.  1960.  ‘Estimation  of Parameters  in Time-Series  Regression  Models.”  Jour- 
nal of the  Royal  Statistical  Society  Series  B, 22:139-53. 

.  1970.  “‘Testing  for  Serial  Correlation  in  Least-Squares  Regression  When  Some  of 

| 

the  Regressors  Are  Lagged  Dependent  Variables.”  Econometrica  38:410-21. 
Eicker,  F.  1967.  “‘Limit  Theorems  for  Regressions  with  Unequal  and  Dependent  Errors.” 
Proceedings  of the  Fifth  Berkeley  Symposium  on  Mathematical  Statistics  and  Probability, 
Vol.  1, pp.  59-62.  Berkeley:  University  of California  Press. 
Engle,  Robert  F.  1982.  “‘Autoregressive  Conditional  Heteroscedasticity  with  Estimates  of 
the  Variance  of United  Kingdom  Inflation.”  Econometrica  50:987-1007. 
Evans,  G.  B. A., andN.  E. Savin.  1981.  ‘Testing  for Unit  Roots:  1. Econometrica  49:753- 
Paw 
Flavin,  Marjorie  A.  1983.  ‘Excess  Volatility  in the  Financial  Markets:  A  Reassessment  of 
the  Empirical  Evidence.”  Journal  of Political  Economy  91:929-56. 
Gregory,  Allan  W.,  and  Michael  R.  Veall.  1985.  “Formulating  Wald  Tests  of Nonlinear 
Restrictions.”  Econometrica  53:1465-68. 
Hansen,  Lars  P.  1982.  ‘“‘Large  Sample  Properties  of Generalized  Method  of Moments  Es- 
timators.””  Econometrica  50:1029-54. 
Harvey,  A.  C.  1981.  The  Econometric  Analysis  of Time  Series.  New  York:  Wiley. 
Hausman,  Jerry  A.,  and  William  E.  Taylor.  1983.  ‘Identification  in  Linear  Simultaneous 
Equations  Models  with  Covariance  Restrictions:  An Instrumental  Variables  Interpretation.” 
Econometrica  51:1527-49. 
Imhof,  J. P.  1961.  “Computing  the  Distribution  of Quadratic  Forms  in Normal  Variables.” 
Biometrika  48:419-26. 
Judge,  George  G.,  William  E.  Griffiths,  R.  Carter  Hill, and  Tsoung-Chao  Lee.  1980.  The 
Theory  and Practice  of Econometrics.  New York:  Wiley. 
Kinderman,  A.  J.,  and  J.  G.  Ramage.  1976.  “Computer  Generation  of Normal  Random 
Variables.”  Journal  of the American  Statistical  Association  71:893-96. 
Lafontaine,  Francine,  and  Kenneth  J.  White.  1986.  “Obtaining  Any  Wald  Statistic  You 
Want.”  Economics  Letters  21:35-40. 

. 

Chapter 8 References 

231 

Maddala,  G.  S.  1977.  Econometrics.  New  York:  McGraw-Hill. 
Newey,  Whitney  K.,  and  Kenneth  D.  West.1987.  “A  Simple  Positive  Semi-Definite,  Heter- 
oskedasticity  and  Autocorrelation  Consistent  Covariance  Matrix.”  Econometrica  55:703-8. 
Nicholls,  D,  F.,  and  A.  R.  Pagan.  1983.  “Heteroscedasticity  in Models  with  seen  De- 
pendent  Variables.”  Econometrica  51;1233-42.. 
O’Nan,  Michael.  1976.  Linear  Algebra,  2d ed.  New  York:  Harcourt  Brace  Idan 
Phillips,  P. C. B., and Joon  Y. Park.  1988.  ‘On the Formulation  of Wald Tests  of Nonlinear 
Restrictions.”  Econometrica  56: 1065-83. 
Rao,  C.  Radhakrishna. 1973. Linear Statistical  Inference  and Its Applications,  2d ed. New 
York: Wiley. 
Theil,  Henri.  1971.  Principles  of Econometrics.  New  York:  Wiley. 
White,  Halbert.  1980. “A Heteroskedasticity-Consistent  Covariance  Matrix Estimator and 
a Direct Test for Heteroskpdtasticity  ””  Econometrica  48:817-—38. 
re 

. 

.  1984.  Asymptotic Thecry for Econometricians.  Orlando,  Fla.:  Academic Press. 

a 

+*  = 

{ie2 

168 

a 

i 

| a¥ sh  S L  shane  > 

|  boxilstsna®) 

eve 
SE-~EST  1+  os Mein 

otn 

i  STR IBA 
xine 

bak fy  hoy  yaw,  Reese SETS  tos e235 lant.  | see Sem,  efi  At vf  .W  T _nozrsbnA 

| 

fis?  Fuc aS i hsogh. Mh 48G}x, adigsioMoeright  Bas,  ag05)  ngbohsis4 
8 -£°0L2)  eamonosd “ subsort  nea1G-sasuieoD 

i 

st  a tyr pet  so aainsd,  sett  ae  > 

Sa.  -  wef  Pa 

ee  Ee  Fi  ae 

57 

= 

4 
p  nk  ors yoke  *T 

Ae 

*% 
a 

“ 

%  ype gg: hd obi uv  yar 
id NS 

PO all  At 

9 

Linear  Systems 
of Simultaneous  Equations 

The  previous  chapter  described  a  number  of  possible  departures  from  the  ideal 
regression  model  arising  from  errors  that  are  non-Gaussian,  heteroskedastic,  or 
autocorrelated.  We saw  that while  these  factors  can  make  a difference  for the small- 
sample  validity  of t and  F tests,  under  any  of  Assumptions  8.1  through  8.6,  the 
OLS  estimator  b; is either  unbiased  or  consistent.  This  is because  all  these  cases 
retained  the  crucial  assumption  that  u,,  the  error  term  for  observation  f¢, is uncor- 
related  with  x,,  the  explanatory  variables  for that  observation.  Unfortunately,  this 
critical  assumption  is unlikely  to  be  satisfied  in many  important  applications. 

Section  9.1  discusses  why  this  assumption  often  fails  to  hold,  by examining  a 
concrete  example  of simultaneous  equations  bias.  Subsequent  sections  discuss  a 
variety  of techniques  for  dealing  with  this  problem.  These  results  will  be  used  in 
the  structural  interpretation  of vector  autoregressions  in Chapter  11 and  for under- 
standing  generalized  method  of moments  estimation  in Chapter  14. 

9.1.  Swunultaneous  Equations  Bias 

To  illustrate  the  difficulties  with  endogenous  regressors,  consider  an  investigation 
of the  public’s  demand  for  oranges.  Let p, denote  the  log of the  price  of oranges 
in a particular  year  and  qg¢ the  log of the  quantity  the  public  is willing-to  buy.  To 
keep  the  example  very  simple,  suppose  that  price  and  quantity  are  covariance- 
stationary  and  that  each  is measured  as  deviations  from  its population  mean.  The 
demand  curve  is presumed  to  take  the  form 

[9.1.1] 

qi =  Bp,  +  ef, 
with  6 <  0; a  higher  price  reduces  the  quantity  that  the  public  is willing  to  buy. 
Here  ef represents  factors  that  influence  demand  other  than  price.  These  are 
assumed  to be independent  and identically  distributed  with  mean  zero  and variance 
a3. 

The  price  also  influences  the  supply  of oranges  brought  to  the  market, 
qi =  YP,  +  &; 
where  y > 0 and ej represents  factors  that influence  supply other  than  price.  These 
omitted  factors  are  again  assumed  to  be  i.i.d.  with  mean  zero  and  variance  o?, 
with  the supply  disturbance  ¢/ uncorrelated  with  the  demand  disturbance  e/. 

[9.1.2] 

uation  [9.1.1]  describes  the  behavior  of buyers  of oranges,  and  equation 
[9.1.2]  describes  the  behavior  of sellers.  Market  equilibrium  requires  q/ =  q}, or 
Bp,  +  e¢ =  yp,  +  €. 

233 

Rearranging, 

=  tt, 
€.,  xe 
py  ~  B 

[9.1.3] 

Substituting  this  back  into  [9.1.2], 

ed  —  ¢ 

BAT  ft  ef  = 

y 

5 

ef  = 

B 

ef 

ae 
Consider  the  consequences  of trying  to  estimate  [9.1.1]  by OLS.  A regression 

Sack  f 

ee 

[9.1.4] 

of quantity  on  price  will  produce  the  estimate 

br  = 

T 

(1/T)  2, Pid 

 T 

; 

(1/T)  2 py 

t=1 

[9.1.5] 

Substituting  [9.1.3]  and  [9.1.4]  into  the  numerator  in  [9.1.5]  results  in 

, 

1 

=  7 

4-42 

par 
— 
7  Dy Pedi  Apert  a, 

1  || Y 

B  | 
€ 
y—p" 
—  os  EEF 
=  |  09)? 
ges 
la  2  d 
Tolono.  Gai  oe 

Ss 
€..44:AnuG  witht.  2 
7. 
+ a  (81)? 
B 

- iil t m  ete 

d\2 

5 A 

2 

P  yor  +  Bo? 
(y —  B) 

Similarly,  for  the  denominator, 

ee  fe sige  «| ——— 

T 

T 

2 

2 

: 

t=1 

Pity 

=  6 

9. 

je 

'G aaeee 53) 

Hence, 

| 

|you+  Bos)  _ you  +  Bos 
b,—->  ae | je Be pure  hh JYNEN itet 
€o teek 

at+os| 
inet Wik adh %s wo Bindnase 

Tos 

9.1.6 

vale 

ue 

OLS  regression  thus  gives  not the  demand  elasticity  B but  rather  an  average 
of 8 and the supply elasticity  y, with  weights  depending  on  the sizes of the variances 
a2, and  o2. If the  error  in the  demand  curve  is negligible  (03  —  0) or  if the  error 
term  in the supply  curve  has a big enough  variance  (a? —  ©), then  [9.1.6]  indicates 
that  OLS would  give a consistent  estimate  of the demand  elasticity  B. On the other 
hand,  if 73 —~  =  or  a? —  0, then  OLS  gives  a  consistent  estimate  of the  supply 
elasticity  y.  In the  cases  in between,  one  economist  might  believe  the  regression 
was  estimating  the  demand  curve  [9.1.1]  and  a  second  economist  might  perform 
the  same  regression  calling  it the  supply  curve  [9.1.2].  The  actual  OLS  estimates 
would  represent  a  mixture  of both.  This  phenomenon  is known  as  simultaneous 
equations  bias. 

Figure  9.1  depicts  the problem  graphically.’  At any  date  in the  sample,  there 
is some  demand  curve  (determined  by the  value  of ¢7) and  a supply  curve  (deter- 
mined  by e), with  the  observation  on  (p,,  q,) given  by the  intersection  of these 
two  curves.  For  example,  date  1 may  have  been  associated  with  a small  negative 
shock  to  demand,  producing  the curve  D,,  and a large  positive  shock  to  supply, 
producing S,. The  date  1 observation  will then  be (p,, q,). Date  2 might have  seen 

‘Economists  usually display these  figures with  the axes  reversed  from  those  displayed  in Figure 9.1. 

234  Chapter  9  | Linear  Systems  of Simultaneous  Equations 

P, 

FIGURE  9.1 
supply  functions  and  demand  functions. 

Observations  on  price  and  quantity  implied  by disturbances  to both 

a  bigger  negative  shock  to  demand  and a negative  shock  to  supply,  while  date  3 
as  drawn  reflects  a  modest  positive  shock  to  demand  and a large  negative  shock 
to  supply.  OLS  tries  to  fit a  line  through  the  scatter  of points  {p,, q,}7-1. 

__ 

If the  shocks  are  known  to  be  due  to  the  supply  curve  and  not  the  demand 
‘curve,  then  the  scatter  of points  will  trace  out  the  demand  curve,  as  in Figure  9.2. 
If the  shocks  are  due  to the demand  curve  rather  than  the supply curve,  the scatter 
will  trace  out  the  supply  curve,  as  in Figure  9.3. 

.  The  problem  of simultaneous  equations  bias  is extremely  widespread  in the 
social  sciences.  It is rare  that  the  relation  that  we  would  like  to estimate  is the only 
possible  reason  why there  might  be  a correlation  among  a group  of variables. 

Consistent  Estimation  of the  Demand  Elasticity 
The  above  analysis  suggests  that consistent  estimates  of the demand  elasticity 
might  be obtained  if we  could  find  a variable  that  shifts  the  supply  curve  but  not 
the  demand  curve.  For  example,  let  w,  represent  the  number  of days  of below- 
freezing temperatures  in Florida  during year ¢. Recalling that the supply disturbance 
e{ was  defined  as  factors  influencing  supply other  than  price,  w, seems  likely to be 
an  important  component  of e/.  Define  h to  be  the  coefficient  from a linear  pro- 
jection  of ef on  w,,  and  write 

[9.1.7] 
ef =  hw, +  ut. 
Thus,  u‘ is uncorrelated  with w,, by the definition  of h. Although  Florida  weather 
is likely  to  influence  the  supply  of oranges,  it is natural  to  assume  that  weather 

7 

9.1.  Simultaneous  Equations  Bias 

235 

rca 

\ 

—. 

2 emryrnnnenteetapimene 

nckemtk mnie meme  ryan ASCs:  SmMmenees  ee ceshadaginine  Gm 

a 

Weg 

> 

+ 

fi  - 

_* 

,  oe 

Ee  - aaah 

Pe 

Bo; 

7 

©. 

7 

Go 

O 

2 

: 

i 

-— 

7 

ee. 

ip. 

' 

matters  for  the  public’s  demand  for  oranges  only  through  its effect  on  the  price. 
Under  this  assumption,  both  w, and  u; are  uncorrelated  with  e¢. Changes  in price 
that can  be attributed  to the weather  represent  supply shifts  and not  demand  shifts. 
Define  p*  to  be  the  linear  projection  of p, on  w,.  Substituting  [9.1.7]  into 

[9.1.3], 

and  thus 

e?  —  hw,  —  uS 
=  + 
ea 

Pr 

pr  = 

ah  W,, 

y-B 

[9.1.8] 
9.1.8 

[9.1.9] 

since  ef and uj are  uncorrelated  with  w,.  Equation  [9.1.8]  can  thus  be written 

; 

| 

= 

P, 

i 
* 
Pe 

a 

ef i  ur 
. 
y ale  B 

anid substituting this into {9,1-1),  =~ 

| 

) 

; 

| 

q  = =i * 4+ o—)  +  er =  Bp?  +  v,, 

ge —  uy; 

x 

aa 

; 

[9.1.10] 

‘ 

AVENE 

t24y; "y= 8 py 

: 

i545 

3 

5  343% 

% 

where  — oi 

mee  S  . AN  ISH  = Sas eine 

{OF 

+  Shit  tet  4B a  72 A 

| 

at  ——— oi ig 

statnihes  4  2: 

ti EQ 

v 
ae y+ pe ya  PR 
Since us and  e4 are both uncorrelated  with w,,  it follows  that  >: dy emcee 
Ee retereieet pa. tex h-a.a  rumen  saaueer 

a  fer x  Datslon 

—Tr 

ps Byryors: « & inoemeviani  bilev  «  Ac 

—F 

Rese  rOona  Jud  = BY] sidsnsy  ¥ cate 

*  ee Sade 

eee apaonen  on esti 

one  yori 

where 

T 

4 

(1/T)  > wp, 
“eatact 
6,  =  ——>—_ 
(1/7)  >) w? 

t=1 

The  estimator  [9.1.11]  with p* replaced  by f, is known  as-the  two-stage  least squares 
(2SLS)  coefficient  estimator: 

t=1 

Bosts  = 

tie 

(1/T) 2» BG 
{= 7 
(1/T)  >> (p.P 

[9.1.13] 

Like  8%, the  2SLS  estimator  is consistent,  as  will  be shown  in the  following  section. 

9.2.  Instrumental  Variables  and  Two-Stage  Least  Squares 

General  Description  of Two-Stage  Least  Squares 

A generalization  of the  previous  example  is as  follows.  Suppose  the  objective 

is to  estimate  the  vector  B in the  regression  model 

[9.2.1] 

y.  =  Bz,  +  u,, 
where  z,  is a  (kK  Xx  1) vector  of explanatory  variables.  Some  subset  n  =  k of the 
variables  in  z,  are  thought  to  be  endogenous,  that  is,  correlated  with  u,.  The 
remaining  k  —  n  variables  in  z, are  said  to  be predetermined,  meaning  that  they 
are  uncorrelated  with  u,.  Estimation  of B requires  variables  known  as  instruments. 
To  be  a  valid  instrument,  a  variable  must  be  correlated  with  an  endogenous  ex- 
planatory  variable  in z, but  uncorrelated  with  the  regression  disturbance  u,.  In the 
supply-and-demand  example,  the  weather  variable  w, served  as  an  instrument  for 
price.  At least  one  valid  instrument  must  be found  for each endogenous  explanatory 
‘ 
variable. 

Collect  the predetermined explanatory  variables  together with the instruments 
in an  (r X  1) vector  x,.  For  example,  to  estimate  the  demand  curve,  there  were 
no  predetermined  explanatory  variables  in equation  [9.1.1]  and  only a single  in- 
strument;  hence,  r  =  1, and  x,  would  be  the  scalar  w,.  As  a  second  example, 
suppose  that  the  equation  to  be estimated  is 

Y,  =  By  +  Bozy  +  BsZ3,  +  BaZq  +  Bszs,  +  Uy. 
In this example,  z,, and  zs, are  endogenous  (meaning  that  they are  correlated  with 
u,), Z>,  and  z3,  are  predetermined  (uncorrelated  with  u,), and  &,,,  &,,  and  &,, are 
valid  instruments  (correlated  with  z,,  and  z;, but  uncorrelated  with  u,). Then  r  = 
6 and x; =  (1, Z2,,  Z3,  €11» 2, €s,)- The  requirement  that there  be at least  as  many 
instruments  as  endogenous  explanatory  variables  implies  that  r  =  k. 

Consider  an  OLS  regression  of z;, (the ith explanatory  variable  in [9.2.1])  on 

s: 

Zi  =  8/x,  +  ej. 
The  fitted  values  for  the  regression  are  given  by 

Ly =  Oi xs, 

238  Chapter  9  | Linear  Systems  of Simultaneous  Equations 

[9.2.2] 

[9.2.3] 

where 

‘  B xi] [3 stu] 

If z,,  is one  of the  predetermined  variables,  then  z,,  is one  of the  elements  of x, 
and  equation  [9.2.3]  simplifies  to 

4 es  *  Zz it? 

This  is because  when  the dependent  variable  (z,,) is included  in the  regressors  (x,), 
a  unit  coefficient  on  z,  and  zero  coefficients  on  the  other  variables  produce  a 
perfect  fit and  thus  minimize  the  residual  sum of squares. 

Collect  the  equations  in  [9.2.3]  fori  =  1, 2,...,k  in  a  (k  x  1) vector 

equation 

where the (k x  r) matrix  8’ is given by 

2, ='6'z,, 

iti ay 

[9.2.4] 

9.2.5 

Kaan  Never.  SE\E) 

. 

fru  « 

The two-stage least squares (SLS) ssnhiate dep is félind frit an OLS onic 

ARM  AG vitos:s bc 

(ThNCvE 

f 

G  ait 

fais = {5 ws] bs to} 

Ri  S29901G  asf tsiit  givin 

“p24 

tive  way of writing [ [9. 2. 6] is NT  aa useful.  Let  é,, denote  the 

on  a  parager  aes that is, let 3 

Consistency  of 2SLS  Estimator 

Substituting  [9.2.1]  into  [9.2.8], 

r 

wil  Wy a 

Badi¥y  =  | Daar|  b z,(z,B,-+  u| 

t=1 

+  > tx  | ‘[s tu  | 

[9.2.9] 

where  the  subscript  T has  been  added  to  keep explicit  track  of the  sample  size  T 
on  which  estimation  is based.  It follows  from  [9.2.9]  that 

Gauss  =  p=  |) )  2, bz “| Ge > tu  |  [9.2.10] 

T 

“4 

T 

Consistency  of the  2SLS  estimator  can  then  be  shown  as  follows.  First  note 

from  [9.2.4]  and  [9.2.5]  that 

(1/T)  > z,z/  =  51-(1/T)  bs X,Z; 

=  wr ) zai || (UT) > xa  | Ge > x2} 

[9.2.11] 

Assuming  that  the  process  (z,, x,) is covariance-stationary  and  ergodic  for  second 
moments, 

(1/T)  > 2,2; > Q, 

; 

[9.2.12] 

where 

‘Turning  next  to  the  second  term  in  [9.2.10], 

=  [E(z,x})[E(x,x/)]-  '[E(x,2/)]. 

[9.2.13] 

hi 

a: 

jar) 2 tu =  6, (1/T)  > x,u,. 

Again,  ergodicity  for  second  moments  implies  from  [9.2.5]  that 

t=1 

t=1 

8: [E(z,x!)[E(x,x!)]-?, 

[9.2.14] 

while  the  law  of large  numbers  will  typically  ensure  that 

(1/T)  > x,u,—>  E(x,u,)  =  0, 

T 

t=1 

under  the  assumed  absence  of correlation  between  x,  and  u,.  Hence, 

jar) es i, +  0. 

Z 

P 

[9.2.15] 

Substituting  [9.2.12]  and  [9.2.15]  into  [9.2.10],  it follows  that 

t=1 

Basis.r  — 

B> Q-'0  =  0 

240  Chapter  9  | Linear  Systems  of Simultaneous  Equations 

Hence,  the  2SLS  estimator  is  consistent  as  long  as  the  matrix  Q  in  [9.2.13]  is 
nonsingular. 

Notice  that  if none  of the  predetermined  variables  is correlated  with  z,,,  then 
the  ith  row  of E(z,x/)  contains  all zeros  and  the  corresponding  row  of Q in [9.2.13] 
contains  all  zeros,  in  which  case  2SLS  is  not  consistent.  Alternatively,  if  z,,  is 
correlated  with  x, only  through,  say,  the  first  element  x,,  and  z,, is also  correlated 
with  x, only  through  x,,,  then  subtracting  some  multiple  of  the  ith  row  of Q from 
the  jth  row  produces  a  row  of  zeros,  and  Q again  is  not  invertible.  In  general, 
consistency  of  the  2SLS  estimator  requires  the  rows  of  E(z,x/)  to  be  linearly  in- 
dependent.  This  essentially  amounts  to  the  requirement  that  there  be  a  way  of 
assigning  instruments  to  endogenous  variables  such  that  each  endogenous  variable 
has  an  instrument  associated  with  it,  with  no  instrument  counted  twice  for  this 
purpose. 

Asymptotic  Distribution  of 2SLS  Estimator 

Equation  [9.2.10]  implies  that 

VT(B2sis,r  —  B)  =  Ge - tx] [avn > ta  [9.2.16] 

where 

te 

Jan Ss iu | =  6; (1/VT)  > X,U,. 

Hence,  from  [9.2.12]  and  [9.2.14], 

VT (Bosts.7  —  B) = QE ex  MEGAN“ >> xu)  [9.2.17] 

Suppose  that  x, is covariance-stationary  and  that  {u,} is an  i.i.d.  sequence  with 
mean  zero  and  variance  o? with  u, independent  of x, for all  s =  t. Then  {x,u,}isa  __ 
matrix  given  by 
martingale  difference  sequence  with  variance-covariance 
o?-E(x,x)).  If  u,  and  x,  have  finite  fourth  moments,  then  we  can  expect  from 
Proposition  7.9  that 

(wt ) S xu, + N(0,  0?-E(x,x’)). 

[9.2.18] 

Thus,  [9.2.17]  implies  that 

VT \(B2s1s,7 -  B) > N(O, V), 

where 

V  =  Q-E@,x/  MEQ)  [o?  Ex  x MEO  ))- TER 2)1Q- 

=  7Q-'-Q-:Q"! 

=  a’Q°' 

for  Q given in [9.2.13].  Hence, 

; 

Bosis.r ~  M(B,  (1/T)o’Q-'). 

[9.2.19] 

[9.2.20] 

[9.2.21] 

9.2.  Instrumental  Variables  and  Two-Stage  Least  Squares 

241 

Since  B25;5  is a  consistent  estimate  of B, clearly  a  consistent  estimate  of the 

population  residual  for  observation  ¢ is afforded  by 

Similarly,  it is straightforward  to  show  that  a? can  be  consistently  estimated  by 

ar  =y,  —  % Basis 

U, . 

[9.2.22] 

T 

a7  =  (VT)  2 (Y-  % Bosts.r)? 

t=1 

[9.2.23] 

(see  Exercise  9.1).  Note  well  that  although  B,;,5 can  be  calculated  from  an  OLS 
regression  of y, on  2,, the  estimates  @, and  6? in [9.2.22]  and  [9.2.23]  are  not  based 
on  the  residuals  from  this  regression: 

ui, #  :  aioe  Zz, Bosis 

a?  #  (UT)  & (y  ~  % Boss). 

z 

A 

The  correct  estimates  [9.2.22]  and  [9.2.23]  use  the  actual  explanatory  variables  z, 
rather  than  the  fitted  values  2Z,. 

A consistent  estimate  of Q is provided  by [9.2.11]: 

7  =  (1/T)  »> ab 

4 
es jw pa zai || wn  a xa  | un D> x2  | 

T 

-1 

e.% 

[9.2.24] 

Substituting  [9.2.23]  and  [9.2.24]  into  [9.2.21],  the  estimated  variance-covariance 
matrix  of the  2SLS  estimator  is 

V,/T  =  &3-(1/T)- {an S 22 “| 

aff] 

[9.2.25] 

A test  of the  null  hypothesis  RB  = rcan  thus  be  based  on 

[9.2.26] 
(RBasis.r  —  ¥)'[R(V7/T)R']-  (Basis.  —  ¥), 
which,  under  the  null  hypothesis,  has  an  asymptotic  distribution  that  is x? with 
degrees  of freedom  given  by m,  where  m  represents  the  number  of restrictions  or 
the  number  of rows  of R. 
_ 
estimation  will  be discussed in  Chapter  14. 

Heteroskedasticity-  and  autocorrelation-consistent  stdndard  errors  for  2SLS 

Instrumental  Variable  Estimation 

Substituting  [9.2.4]  and  [9.2.5]  into  [9.2.8],  the  2SLS  estimator  can  be written  as 

Bases =  | > sax] | > by, (Sele) 
Lee esi] Boo) 

[9.2.27] 

242 

Chapter.9  | Linear  Systems  of Simultaneous  Equations 

Consider  the  special  case  in  which  the  number  of instruments  is exactly  equal  to 
the  number  of  endogenous  explanatory  variables,  so  that  r  =  k, as  was  the  case 
for  estimation  of  the  demand  curve  in  Section  9.1.  Then  D7. ,2z,x; is  a  (k  x  k) 
matrix  and  [9.2.27]  becomes 

te 

Lm] 
<  {[Sax][Sax]  [Zx]} 

[Zm]Z-] | 

was 

Expression  [9.2.28]  is known  as  the  instrumental  variable  (IV)  estimator. 

A  key property  of the  /V estimator  can  be seen  by premultiplying  both  sides 

of [9.2.28]  by >7_,x,z;: 

‘ 

implying  that 

a 

. 

ue 

py X,Z;  Bry  =  >> X,Y» 
(= 

t= 

T 
> x(y,  —  z; Biv) =  0. 
t=) 

[9.2.29] 

Thus,  the  JV sample  residual  (y,  —  z, 6,,)  has  the  property  that  it is orthogonal 
to  the instruments  x,,  in contrast  to  the  OLS  sample  residual  (y,  —  z;b),  which  is 
orthogonal  to  the explanatory  variables  z,.  The  /V estimator  is preferred  to  OLS 
because  the  population  residual  of the  equation  we  are  trying  to  estimate  (u,)  is 
correlated  with  z, but  uncorrelated  with  x,.  © 

Since  the  JV  estimator  is a  special  case  of  2SLS,  it shares  the  consistency 
property  of the  2SLS  estimator.  Its estimated  variance  with  i.i.d.  residuals  can  be 
calculated  from  [9.2.25]: 

| > «| [3 xxi|| > oxi] 

[9.2.30] 

t=1 

t=1 

9.3.  Identification 
We  noted  in  the  supply-and-demand  example  in  Section  9.1  that  the  demand 
elasticity  B could  not  be  estimated  consistently  by an  OLS  regression  of quantity 
on  price.  Indeed,  in  the  absence  of a  valid  instrument  such  as  w,,  the  demand 
elasticity  cannot  be estimated  by any  method!  To  see  this,  recall  that  the  system 
as  written  in [9.1.1]  and  [9.1.2]  implied  the  expressions  [9.1.4]  and  [9.1.3]: 

B 

- 

Y 

ef - 

u”y-B 
bhatt 
Bec)  up 

If e¢ and  e  are  i.i.d.  Gaussian,  then  these  equations  imply  that  the  vector  (q,, p,)' 
is Gaussian  with  mean  zero  and  variance-covariance  matrix 

on 

[723 +  Bo?  yo}  +  ee] 

pe  [1/(y 

B)’) 

yo?  se Bo? 

o2  +  o? 

: 

This  matrix  is completely  described  by three  magnitudes,  these  being  the  variances 
of q and  p along  with  their  covariance.  Given  a  large  enough  sample,  the  values 
of these  three  magnitudes  can  be  inferred  with  considerable  confidence,  but  that 
is  all  that  can  be  inferred,  because  these  magnitudes  can  completely  specify  the 
process  that  generated  the  data  under  the  maintained  assumption  of  zero-mean 
i.i.d.  Gaussian  observations.  There  is no  way  to  uncover  the  four  parameters  of 
the  structural  model  (8, y, 73, 77) from  these  three  magnitudes.  For  example,  the 
values  (8,  y,  73, 72) = (1, 2, 3, 4) imply  exactly  the same  observable  properties 
for  the  data  as  would  (8,  y,  03, 02) = (2, 1, 4, 3). 

If two  different  values  for  a parameter  vector  @ imply  the  same  probability 

distribution  for  the  observed  data,  then  the  vector  @ is said  to  be  unidentified. 

When a third  Gaussian  white  noise  variable  w,  is added  to  the  set  of obser- 
vations,  three  additional  magnitudes  are  available  to  characterize  the  process  for 
observables,  these  being  the  variance  of w,  the  covariance  between  w and  p,  and 
the  covariance  between  w  and  q.  If the  new  variable  w  enters  both  the  demand 
and  the supply  equation,  then  three  new  parameters  would  be required  to  estimate 
the  structural  model—the  parameter  that  summarizes  the  effect  of  w on  demand, 
the  parameter  that  summarizes  its  effect  on  supply,  and  the  variance  of  w.  With 
three  more  estimable  magnitudes  but three  more  parameters  to estimate,  we  would  — 
be  stuck  with the  same  problem,  having  no  basis  for  estimation  of B. 

Consistent  estimation  of  the  demand  elasticity  was  achieved  by using  two- 
stage  least  squares  because  it was  assumed  that  w  appeared  in the  supply  equation 
but  was  excluded  from  the  demand  equation.  This  is known  as  achieving  identi- 
fication  through  exclusion  restrictions. 

We  showed in  Section  9.2  that  the  parameters  of an  equation  could  be  esti- 
mated  (and  thus  must  be  identified)  if (1)  the  number ig instruments  for  that 
explanatory  variables 
equation  is at  least  as  great  as  the  number  of endogenous 
for  that  equation  and  (2) the  rows  of E(z,x;)  are  linearly  independent.  The  first 
condition  is  known  as  the  order  condition  for  identification,  and  the  second  is 
known  as  the  rank  condition. 

The  rank  condition  for  identification  can  be  summarized  more  explicitly  by 
specifying  a complete  system  of equations  for all of the  endogenous  variables.  Let 
y,  denote  an  (nm  X  1) vector  containing  all  of  the  endogenous  variables  in  the 
system,  and  let  x, denote  an  (m  x  1) vector  containing  all  of the  predetermined 
variables.  Suppose  that  the  system  consists  of n  equations  written  as 

By,  +  Tx,  =  u,, 

[9.3.1] 

where  B and  [ are  (nm x  n) and  (n X  m) matrices  of coefficients,  respectively,  and 
u, is an  (n  X  1) vector  of disturbances.  The  statement  that  x, is predetermined  is 
taken  to  mean  that  E(x,u;)  =  0.  For  example,  the — and  supply  equations 
ios 
considered in  Section  9.1  were 

q.  =  Bp,  +  uf 
q:  =  yp,  +  hw,  +  uy 

(demand) 
(supply). 

[9.3.2] 
[9.3.3] 

244 

Chapter  9  | Linear  Systems  of Simultaneous  Equations 

For  this  system,  there  are  n  =  2 endogenous  variables,  with  y,  =  (q,,  p,)';  and 
m  =  | predetermined  variable,  so  that  x,  =  w,.  This  system  can  be  written  in  the 
form  of [9.3.1]  as 

ted pa 

l  —B  q; 

0 

co 

ud 

= 

Suppose  we  are  interested  in the  equation  represented  by the  first  row  of the 
vector  system  of equations  in [9.3.1].  Let  yo,  be  the  dependent  variable  in the  first 
equation,  and  let  y,,  denote  an  (nm,  x  1) vector  consisting  of  those  endogenous 
variables  that  appear  in  the  first  equation  as  explanatory  variables.  Similarly,  let 
x,,  denote  an  (m,  X  1) vector  consisting  of  those  predetermined  variables  that 
appear  in the  first  equation  as  explanatory  variables.  Then  the  first  equation  in the 
system  is 

Yor  +  Boyy,  +  Voixy  =  Yas 

where  Bo,  is a  (1  X  n,)  vector  and  Ip,  is a  (1  <  m,)  vector.  Let  y,,  denote  an 
(m2  X  1) vector  consisting  of those  endogenous  variables  that  do not  appear  in the 
first  equation;  thus,  y;  =  (yo,,  Yi,, Y>,)  and  1  +  n,  +  n,  =  n.  Similarly,  let  x2, 
denote  an  (mm,  X  1) vector  consisting  of those  predetermined  variables  that  do  not 
appear  in the  first  equation,  so  that  x;  =  (x},,  x3,)  and  m,  +  m,  =  m.  Then  the 
system  in [9.3.1]  can  be written  in partitioned  form  as 

1  By  98 

|[y¥o 

Po, 

0° 

: 

Uo, 

Bio  By  Biutiyn| 

+  [Tu  Pa  io = 

|U,,|- 

[9.3.5] 

Bo  B,,  BrIjLyx 

r,,  Vy 

r¢ 

Uy, 

Here,  for example,  B,, is an  (mn,  X  nm) matrix  consisting  of rows  2 through  (n,  +  1) 
and  columns  (nm,  +  2) through  n  of the  matrix  B. 

An  alternative  useful  representation  of the  system  is obtained  by moving  Ix, 

to  the  right  side  of [9.3.1]  and  premultiplying  both  sides  by B~’: 

y,  =  —B-'Irx,  +  B~1u,  =  II’x,  +  v,, 

where 

ll’  =  -B-'r 

[9.3.6] 

[9.3.7] 

v,  =  B~'u,. 
Expression  [9.3.6]  is known  as  the  reduced-form  representation  of the  structural 
system  [9.3.1].  In  the  reduced-form  representation,  each  endogenous  variable  is 
expressed  solely  as  a  function  of  predetermined  variables.  For  the  example  of 
[9.3.4],  the  reduced  form  is 
; 

1  -B]  [us 

1  -p] 

[9.3.8] 

[0 
Mie  be eae +  [ 24) i 
vif 7  EI) 
-y  BIO 

je 

oa 

+  [1B - a Ald 

=  [1B  -  oil +  [1B  -  | Lad  as 

—yus  +  sm 

9.3.  Identification 

245 

The  reduced  form  for  a  general  system  can  be  written  in partitioned  form  as 

You 

yy} 

Yx 

= 

Wo. 

‘ 
IIo, 
{Miu  My,  | |] +  | Vir], 

Vor 

II,  1, 

ft 

V2 

| 

[9.3.10] 

where,  for  example,  II, denotes  an  (n,  X  m,) matrix  consisting  of rows  2 through 
(nm,  +  1) and  columns  (m,  +  1) through  m  of the matrix  II’. 

tae 

To  apply  the  rank  condition  for  identification  of  the  first  equation  stated 
earlier,  we  would  form  the  matrix  of cross  products  between  the  explanatory  var- 
iables  in  the  first  equation  (x,,  and  y,,)  and  the  predetermined  variables  for  the 
whole  system  (x,, and  x,,): 

M  x 

E(x  1X)  E(x mn 

[9.3.11] 

E(Y  Xi).  E(Y,X) 

: 

In  the  earlier  notation,  the  explanatory  variables  for  the  first equation  consist  of 
Z,  =  (Xi,  Yj)’,  while  the predetermined  variables for the system as  a whole consist 
of x,  =  (x;,,  x3,)’.  Thus,  the  rank  condition,  which required  the  rows  of E(z,x;) 
to  be  linearly  independent,  amounts  to  the  requirement  that  the  rows  of the 
[(m,  +  n,)  X  m] matrix  M  in [9.3.11]  be linearly  independent.  The  rank condition 
can  equivalently  be  stated  in terms  of the  structural  parameter  matrices  B and  F 
or  the  reduced-form  parameter  matrix  IT. The  following  Proposition  is  adapted 
from  Fisher  (1966)  and  is proved  in Appendix  9.A  at  the  end  of this  chapter. 

Proposition  9.1: 
If the matrix  Bin [9.3.1] and the matrix  of second  moments  of the 
predetermined  variables  E(x,x;)  are  both nonsingular,  then the following  conditions 
are  equivalent: 

| 

(a)  The rows  of the [(m,  +  n,) X  m] matrix M 
(b)  The  rows  of the  [(n,  +  n)  X  (m,  +  np)] matrix 

in [9.3.11] are linearly independent. 

, 

ie  y 
Ei B,, 

9.3.12 

Magee. 

are  linearly  independent. 

(c)  The rows  of the  (n;  X  m) matrix II,, are  linearly  independent. 

For example,  for the system  in [9.3.4],  no  endogenous  variables  are  excluded 
from  the  first  equation,  and  so  yo,  =  9,  Yu  =  P,,  and  y>, contains  no  elements. 
No  predetermined  variables  appear  in  the  first  equation,  and  so  X;,  contains  no 
elements  and  x,,  =  w,.  The  matrix  in [9.3.12]  is then  just given by the  parameter 
I’,,. This represents  the coefficient  on  x2, in the equation  describing y,, and is equal 
to the  scalar  parameter  —h.  Result  (b) of Proposition  9.1  thus  states  that  the  first 
equation  is identified  provided  that  h #  0. The  value  of I,, can  be read  directly 
off the  coefficient  on  w, in the second  row  of [9.3.9]  and  turns  out  to be given  by 
h(B  —  y). Since  B is assumed  to  be nonsingular,  (8 —  y) is nonzero,  and  so  r,, 
is zero  if and  only if I1,, is zero. 

Achieving  Identification  Through  Covariance  Restrictions 
Another  way  in which  parameters  can  be identified  is through  restrictions  on 
the  covariances  of the  errors  of the  structural  equations.  For  example,  consider 
246  Chapter  9  | Linear  Systems  of Simultaneous  Equations 

again  the supply  and  demand  model,  [9.3.2]  and  [9.3.3].  We  saw  that  the  demand 
elasticity  B was  identified  by the  exclusion  of  w,  from  the  demand  equation.  Con- 
sider  now  estimation  of the  supply  elasticity  y. 

Suppose  first  that  we  somehow  knew  the  value  of  the  demand  elasticity  B 
with  certainty.  Then  the  error  in  the  demand  equation  could  be  constructed  from 

__ 

uy  =-@  7  Bp,. 
Notice  that  u# would  then  be a valid  instrument  for  the  supply  equation  [9.3.3], 
since  u/ is correlated  with  the  endogenous  explanatory  variable  for  that  equation 
(p,)  but  u? is  uncorrelated  with  the  error  for  that  equation  (u‘).  Since  w,  is also 
uncorrelated  with  the error  u;,  it follows  that  the  parameters  of the  supply  equation 
could  be  estimated  consistently  by  instrumental  variable  estimation  with  x,  = 
(us.  ,)": 

yt]  _ [2usp, 
K 4 
P 
2w.p, 
h* 

Ludw,] 
4 
Ww; 

‘[Zusq, 
2W.q, 

ae  en 

h 

(9.3.13] 

where  > indicates  summation  over t =  1, 2, .  . Pe 

Although  in practice  we  do  not  know  the  true  value  of B, it can  be estimated 

consistently  by /V estimation  of [9.3.2]  with  w, as  an  instrument: 

B -  (2w,p,) &  (2w,q,). 

Then  the  residual  u¢ can  be estimated  consistently  with  47 =  g,  —  Bp,. Consider, 
therefore,  the  estimator  [9.3.13]  with  the  population  residual  u¢ replaced  by the 
LV  sample  residual: 

¥ 

3 

2 

—1 

hr 

2Ww,P,  Zw? 

=w.4, 

It is straightforward  to use  the fact that B = B to deduce  that  the difference  between 
the  estimators  in [9.3.14]  and [9.3.13]  converges  in probability  to zero.  Hence,  the 
estimator  [9.3.14]  is also  consistent. 

Two  assumptions  allowed  the  parameters  of the  supply  equation  (y and  A) 
to  be  estimated.  First,  an  exclusion  restriction  allowed  B to  be  estimated  consis- 
tently.  Second,  a  restriction  on  the  covariance  between  u¢ and  us was  necessary. 
If u?¢ were  correlated  with  uj,  then  u? would  not  be  a  valid  instrument  for  the 
supply  equation  and  the  estimator  [9.3.13]  would  not  be  consistent. 

Other  Approaches  to  Identification 

A good deal  more  can  be said  about  identification.  For example,  parameters 
can also  be identified  through  the  imposition  of certain  restrictions  on  parameters 
such  as  B,  +  B,  =  1. Useful  references  include  Fisher  (1966),  Rothenberg  (1971), 
and  Hausman  and  Taylor  (1983). 

9.4.  Full-Information  Maximum  Likelihood  Estimation 
Up to  this  point  we  have  considered  estimation  of a  single  equation  of the  form 
y, =  B’z,  +  u,.  A more  general  approach  is to specify  a similar  equation  for every 
endogenous  variable  in the  system,  calculate  the  joint  density  of the  vector  of all 
of the endogenous  variables  conditional  on  the predetermined  variables,  and  max- 
imize  the  joint  likelihood  function.  This  is known  as full-information  maximum 
likelihood  estimation,  or  FIML. 

| 

9.4, Full-Information  Maximum Likelihood Estimation  247 

For  illustration,  suppose  in  [9.3.1]  that  the  (mn  x  1) vector  of structural  dis- 
turbances  u, for date  fis distributed  N(0, D). Assume,  further,  that  u, isindependent 
of  u,  for  ¢  #  7  and  that  u,  is independent  of x,  for  all  t and  r.  Then  the  reduced- 
form  disturbance  v,  =  B~'u,  is distributed  N(0,  B~'D(B~')'),  and  the  reduced- 
form  representation  [9.3.6]  implies  that 

y|x,  ~  ow, BD")  =  y(-B-1Pe, B'DB-*)’). 

The  conditional  log likelihood  can  then  be  found  from 
¥(B,  I, D) 

2 log f(y,|x,;  B, I’, D) 

—(Tn/2)  log(27)  —  (T/2)  log|B-'D(B-)'| 

—  (12)  ») ly,  +  B-’Tx,]'[B-'D(B~*)']“[y,  +  B-Ix,]. 

But 

[y, +  B~'Tx,]'[B-'D(B-)'}-"[y,  +  B-'Ix,] 

| 

=  [y,  + B-'Ix,]'[B’D-"B][y,  +  B-'Ix,] 
=  [B(y,  +  B~'Tx,)]'D-"[B(y,  +  B-'Tx,)}] 
=  [By,  +  I'x,]'D-'[By,  +  rx, ]. 

Furthermore, 

|B-'D(B-?)'|  =  [B-'|-[DJ-[B-}| 

=  |D|/|B)?. 

Substituting  [9.4.2]  and  [9.4.3]  into  [9.4.1], 

£(B,  T, D)  =  —(Tn/2)  log(2m)  +  (T/2)  log|Bi? 

T 

—  (T/2)  log|D|  —  (1/2)  D> [By,  +  I'x,]'D~'[By,  +  Ix,]. 

[9.4.1] 

[9.4.2] 

[9.4.3] 

[9.4.4] 

The  FIML  estimates  are  then  the  values  of B, ', and  D for which  [9.4.4]  is max- 
imized. 

For  example,  for  the  system  of [9.3.4],  the  FIML  estimates  of £, y,  h, o3, 

and  o} are  found  by maximizing 

L(B,  y, h, 3, 0?) 

=  —Tlog(27)  +  3 log  ;  wi 73°%  0  ‘I 

Povigns 

sahypaop 

POI 

bier 

ster i 

: 

¥ 

- 

-> {ta Bp,  q: 

a4  0  i 

qi  Bp, 

YP:  hw 0  m & YP, >. n| 

=  -Tlog(2m)  +  Tlog(y  —  B) —  (T/2) log(o3) 

~  (7/2) log(a?)  -  (1/2) 

(9, —  Bp,)?/03 

T 

si (1/2) > (q, ~  ee  hw,)?/o?. 

248  Chapter  9 | Linear  Systems  of Simultaneous  Equations 

[9.4.5] 

The  first-order  conditions  for  maximization  are 

T 

Dy (q. —  Bpdp. 

B =  °sTy sf <2 ° in =  0 

[9.4.6] 

y 4 

5 yee too”? 

> (4.  — yp,  — hw,)p, 

r 

o 5  fo 

ps (4,  —  yp,  —  hw,)w, 
; 

ce 

. 
cea 

») 
at  = 

(q, —  Bp.) 

=0 

ana  =  . 

Ti A a ®. - ae 

[9.4.7] 

[9.4.8] 

[9.4.9] 

oc  phoma 

AMinsrzs  LOANS  ods  od of  nals  juo  smut  pA 

"Wad  ri 26  Ge  520M sis  (A  bre 
‘=  pe  igsrbhasty  Bram 50  63 bs  wun) oy Gas: > ine 
out an  smez  of)  16 219!eeag16<} 

}  Rovisey 

yi Fa 

ye 

S23 wi} x0? 
:  a 

»  a etal P baci 92% 
SSAR TEES s23  sldainsy-ic 

tl  £2SsSNtEI 

(9. yy 

sy 

a ath va 

sidsina’ trunks a hw sigan. air og 4 2 
: 

hy bonita nibtniktnt nofadrcta: iii  ba. 

ae ienamanel  ot  Nees  Be 
nape 

inne  paneer  bap hos § aiay © 1941 

elm 

ene 

eee 
“hati  ote 
sa 

°  ‘  ac.  ~~, 

Boxscore  ea 

ot 

Similarly,  multiplying  [9.4.6]  by (y  —  )/T, 

r 

0  It 

—I  +  » (q,  a  Bp.) (vP,  —  4 

ao  Bp,)/(To4) 

T 

T- 

=]  ~  2, (4,  7  Bp.)(4.  ae  yp)!(To4)  ey »y (q, <i Bp,)?(To7). 

Using  [9.4.11], 

(q.  —  Bpd(q.  —  9%.)  =  9. 

Ms 
il  — 
t 

[9.4.15] 

Subtracting  [9.4.14]  from  [9.4.15], 

0=  > (q.  —  bpd(q.  -  9p.)  —  (qs  -  Sp,  ~  hw))  =  h > (q, —  Bp.)™,- 

Assuming  that  h #  0, the  FIML  estimate  of B thus satisfies 

| 

b (q.  —  Bp,)w,  =  0; 

that  is, the  demand  elasticity  is chosen  so  as  to  make  the  estimated  residual  for 
the  demand  equation  orthogonal  to  w,.  Hence,  the  instrumental  variable  estimator 
Br turns  out  also to  be the  FIML  estimator.  Equations  [9.4.8]  and  [9.4.14]  assert 
that  the  parameters  for  the  supply  equation  (y and  h) are  chosen  so  as  to  make 
the  residual  for  that  equation  orthogonal  to  w, and  to  the  demand  residual  a7  = 
—  Bp,.  Hence,  the  FIML  estimates  for  these  parameters  are  the  same  as  the 

instrumental-variable  estimates  suggested  in  [9.3.14]. 

For  this  example,  two-stage  least  squares,  instrumental  variable  estimation, 
and full-information  maximum  likelihood  all produced  the  identical  estimates.  This 
is because  the  model  is just  identified.  A model  is said  to  be  just  identified  if for 
any  admissible  value  for  the  parameters  of the  reduced-form  representation  there 
exists  a unique  value  for the  structural  parameters  that  implies  those  reduced-form 
parameters.  A model  is said  to  be  overidentified  if some  admissible  values  for  the 
reduced-form  parameters  are  ruled  out  by the structural  restrictions.  In  an  over- 
identified  model,  JV,  2SLS,  and  FIML  estimation  are  not  equivalent,  and FIML 
typically  produces  the  most  efficient  estimates. 

For a general overidentified  simultaneous  equation  system with no  restrictions 
on  the  variance-covariance  matrix, the  FIML  estimates  can  be  calculated  by iter- 
ating on  a procedure  known  as  three-stage  least squares,  see,  for example,  Maddala 
(1977,  pp.  482-90).  Rothenberg  and  Ruud  (1990)  discussed  FIML  estimation  in 
the  presence  of covariance  restrictions.  FIML  estimation  of dynamic  time  series 
models  will  be discussed  further  in Chapter  11. 

9.5  Estimation  Based  on  the  Reduced  Form 
If a system  is just identified  as  in [9.3.2]  and  [9.3.3]  with  u? uncorrelated  with  uw’, 
one  approach is  to  maximize  the  likelihood  function  with  respect  to  the  reduced- 
form  parameters.  The  values  of the  structural  parameters  associated  with  these 
values  for the  reduced-form  parameters  are  the  same  as  the  FIML  estimates  in a 

_  just-identified  model. 

250  Chapter 9 | Linear  Systems  of Simultaneous  Equations 

The  log likelihood  [9.4.1]  can n  be apron in  terms  of the  reduced-form 

parameters HandQas 

© 

~ 

ey 

= 

L(f,  )  =  >> log f(y,1x,;  Il,  2) 

=  —(Tn/2)  log(27)  ~  (T/2)  log|| 

eo 

T 

a  (1/2) 2 ly,  *  II'x,|'Q-  "Ly, a  II'x,], 

where  2  =  E(v,v;)  =  Bo'D(B™ ")'. The  value  of II that maximizes  [9.5.1] will be 
shown in  Coat 11  to  be given  by 

=  b rai|| 3 vai] 

in other words,  the ith row of I’  is obtained  from an OLS regression  of the ith | 
endogenous variable  on  all of the pPdsrermined variables: 

a 

fence & pitib sOS 
_  Retest Brings  sit} 
The MLE of © turns out to be. 

bos 

iF 

0  tO fi = al 

Get  SOG 

0 

Ie os 6: atc 

Lt Sa Sea  SRE VES, ic 
o%  cubait KOT  HH  te 

Multiplying  [9.5.3]  by B and  subtracting  the  result  from  [9.5.2]  produces 

0 

T 
hy (q,  -  tw,  —  Bp,  +  Bir.W,)W, 
r=1 

T 

II  M-x 

il 

(q,  ~~  Bp,)™,  re  ps (7  .  Bir.)w? 
t=1 

I  M- (q,  "¥  Bp,),, 

i] 

—_— 

by  virtue  of  the  first  row  of  [9.5.5].  Thus,  the  estimate  of  B inferred  from  the 
reduced-form  parameters  is the  same  as  the  /V or  FIML  estimate  derived  earlier. 
Similarly,  multiplying  [9.5.3]  by y and  subtracting  the  result  from  [9.5.2]  gives 

T 

0  =  by (4,  =  Ww,  ms  VR,  +  yT2w,)W, 

t=1 

M-~ 
i)  ba 
t 

(4,  Pit.  (7,  7  Yit2)w, |W, 

Ms (4,  ee 
I 

— 

|  Slem  hw,]w,, 

by virtue  of the  second  row  of [9.5.5],  reproducing  the  first-order  condition  [9.4.8] 
for FIML.  Finally,  we  need  to solve  D =  BQQB’  for D and  y (the remaining  element 
of B). These  equations  are 

| i 

OQ,  OD  a 

Lr 2a: all>  =] 
_1S)]1  -Blla-  mm, 
~i5  ii Alle >  Be  [a am  p.—  toml| 'y  ‘iH 
> {| —  Bp, -  (#1 -  pail (i —  Bp, -  (#1 -  penn 

daetaapete 

gee: 

‘ 

ty 

Ge  Vp  (7,  -  Yi), 

a  FP:  (7 ai yi), 

Lak 

- 

a  T = {l, tes Mg ur Bp,  Qi:  —  YP:  —  nwa. 

t=1 

The  diagonal  elements  of this  matrix  system  of  equations  reproduce  the  earlier 
formulas  for the FIML  estimates  of the variance  parameters,  while  the off-diagonal 
element  reproduces  the  result  [9.4.14]. 

9.6.  Overview  of Simultaneous  Equations  Bias 
The  problem  of simultaneous  equations  bias  is extremely  widespread  in the  social 
sciences.  It is rare  that  the  relation  that  we  are  interested  in estimating  is the only 
possible  reason  why the dependent  and  explanatory  variables  might be correlated. 
For  example,  consider  trying  to  estimate  the  effect  of military  service  on  an  indi- 
vidual’s  subsequent  income.  This  parameter  cannot  be estimated  by a  regression 
of income  on  a measure  of military service  and other  observed  variables.  The error 
252  Chapter  9  | Linear  Systems  of Simultaneous  Equations 

term  in  such  a  regression  represents  other  characteristics  of  the  individual  that 
influence  income,  and  these  omitted  factors  are  also  likely  to  have  influenced  the 
individual’s  military  participation.  As  another  example,  consider  trying  to  estimate 
the  success  of long  prison  sentences  in deterring  crime.  This  cannot  be  estimated 
by regressing  the  crime  rate  in  a  state  on  the  average  prison  term  in  that  state, 
because  some  states  may  have  adopted  stiffer  prison  sentences  in response  to higher 
crime.  The  error  term  in the  regression,  which  represents  other  factors  that  influ- 
ence  crime,  is  thus  likely  also  to  be  correlated  with  the  explanatory  variable. 
Regardless  of  whether  the  researcher  is  interested  in  the  factors  that  determine 
military  service  or  prison  terms  or  has  any  theory  about  them,  simultaneous  equa- 
tions  bias  must  be  recognized  and  dealt  with. 

Furthermore,  it is not  enough  to  find  an  instrument  x,  that  is  uncorrelated 
with  the  residual  u,.  In  order  to  satisfy  the  rank  condition,  the  instrument  x, must 
be  correlated  with  the  endogenous  explanatory  variables  z,.  The  calculations  by 
Nelson  and  Startz  (1990)  suggest  that  very  poor  estimates  can  result  if x,  is only 
weakly  correlated  with  z,. 

Finding  valid  instruments  is  often  extremely  difficult  and  requires  careful 
thought  and a bit  of good  luck.  For  the  question  about  military  service,  Angrist 
(1990)  found  an  ingenious  instrument  for  military  service  based  on  the institutional 
details  of the  draft  in the  United  States  during  the  Vietnam  War.  The  likelihood 
that  an  individual  was  drafted  into  military  service  was  determined  by a lottery 
based  on  birthdays.  Thus,  an  individual’s  birthday  during  the  year  would  be  cor- 
related  with  military  service  but  presumably  uncorrelated  with  other  factors  influ- 
encing  income.  Unfortunately,  it is unusual  to  be  able  to  find  such  a  compelling 
instrument  for  many  questions  that  one  would  like  to  ask  of the  data. 

APPENDIX  9.A.  Proofs  of Chapter  9 Proposition 

@  Proof of Proposition  9.1.  We  first  show  that  (a) implies  (c). The  middle  block  of [9.3.10] 
states  that 

Yi,  =  yx,  +  Wyoxa  +  Vu. 

Hence, 

m=  ali]  =] 

ths nA 3  xi]  +  | ex xl} 

[9.4.1] 

i,,.'"@ 

=  i [Gua ; 

Suppose  that  the  rows  of  M  are 

since  x, is uncorrelated  with  u, and  thus  uncorrelated  with  v,. 
This  means  that 
{A’  p’JM  #  0’ for  any  (m,  x  1) vector  A and  any  (n,  x  1) vector  p that  are not both 
zero.  In particular,  {—’N,,  »’JM  #  0’.  But  from  the  right side  of [9.A.1],  this  implies 
that 

linearly  independent. 

[-»'l,,  oft  i eax) =  pw 'TT,.]  E(x,x;)  # 0° 

for any nonzero  (n,  X  1) vector  ». But this could  be true  only if p'Tl,,  + 0’. Hence,  if the 
rows of M are linearly independent,  then  the rows  of I1,, are  also  linearly  independent. 

Appendix  9.A.  Proofs  of Chapter  9 Proposition 

253 

To  prove  that  (c) implies  (a), premultiply  both  sides  of [9.A.1]  by any  nonzero  vector 
pw’).  The  right  side  becomes 

[A’ 

[A’  ra 

1, Ex) =  [(’  +  w'll,,) 

p’M,,]E(x,x/)  =  9 E(x,x/), 

where  7’  =  [(A’  +  p’Il,,) 
p’T,,].  If the  rows  of  I,,  are  linearly  independent,  then  9’ 
cannot  be the zero  vector  unless  both  p and A are  zero.  To see  this,  note  that  if  : is nonzere, 
then  w’II,,  cannot  be  the  zero  vector,  while  if ~  =  0, then  » will  be  zero  only  if A is also 
the  zero  vector.  Furthermore,  since  E(x,x;)  is  nonsingular,  a  nonzero  4  means  that 
yn E(x,x;)  cannot  be  the  zero  vector.  Thus,  if the  right  side  of  [9.A.1]  is premultiplied  by 
any  nonzero  vector  (A’,  4’),  the  result  is not  zero.  The  same  must  be  true  of the  left  side: 
[A’  ’JM  #  0’ for  any  nonzero  (A’,  w’), establishing  that  linear  independence  of the  rows 
of II,,  implies  linear  independence  of the  rows  of M. 
To  see  that  (b) implies  (c), write  [9.3.7]  as 

We also  have  the  identity 

I,  Mo. 
Bic:  Oise  a. Lal, 

eT 

IT,  II, 

r,,  r,, 

[9.A.2] 

1  0’ 

0’ 

} 

Boy: 

2-0 

0  I,  0; 

=  B'|By  By  By}. 

[9.A.3] 

0  0  te 

B:,  B,,  B,, 

The  system  of equations represented  by the  second  block  column  of [9.A:2]  and  the  third 
block  column  of [9.A.3]  can  be collected  as 

| 

Il,:  0’ 

0’ 

0' ’ 

: 

II,,  0  =  B 4 

= 

B,.  e 

r 

[9.A.4] 

II,  I,,, 

Ta r,,  B,, 

If both  sides  of [9.4.4]  are  premultiplied  by the  row  vector [0  4;  0’] where  p,  is 

any  (m,  X  1) vector,  the  result  is 

(will,, 

0’ 

0’ 
0’)  =  (0  m;,  O'}B-'|-T,,  B,, 
-i,,  B 
0' 
"=  Ao  AL  AG]  -Biar  Bo 
-Fr,,  B., 

0’ 

ia  agf fe Be], 

[9.A.5} 

where 

implying 

[Ao  Ay  Ag]  =(0  wi  OB’, 

[0  mi  O') =  [Ao  AY  ASB. 

[9.A.6] 
Suppose  that  the rows  of the matrix  [fi3  13] are  linearly  independent.  Then  the only 
values  for 4, and  A, for which  the  right side  of [9.A.5]  can  be zero  are  A,  =  OandaA,  = 
0.  Substituting  these  values  into  (9.A.6],  the  only  value  of ,  for  which  the  left  side  of 
[9.A.5]  can  be zero  must  satisfy 

(0  wi  O'} =  [Ay  0°  O/B 

7  [Ao  AoBo,  0’). 

Matching  the  first  elements  in these  vectors  implies  Ajo =  0, and  thus  matching  the second 
elements  requires  »,  =  0. Thus,  if condition  (b) is satisfied,  then  the  only value  of m,  for 
254  Chapter  9 | Linear  Systems  of Simultaneous  Equations 

which  the left  side  of [9.A.5]  can  be  zero  is x,  =  0, establishing  that  the  rows  of  Il,,  are 
linearly  independent.  Hence,  condition  (c)  is satisfied  whenever  (b) holds. 

Conversely,  to  see  that  (c) implies  (b),  let  A, and  A, denote  any  (nm,  X  1) and  (nm,  x  1) 

vectors,  and  premultiply  both  sides  of [9.4.4]  by the  row  vector  [0  A‘  A4jJB: 

or 

where 

Il,, 

0’ 

0 

0 

(0  A,  AJB),  O]  =[0  A;  Aj  -P..  By 

II,  a 

-Py  B,, 

Il 

’ 

és  ‘ 
[Ho  BB,  #3]  1,  0} 

’ 

’ 

’ 
=  [Ai  d2] 

‘ 

-T 

Bip 

-P,  B, 

Il,,  I, 

[Ho  Mi  M2] =[0  Aj  AQ)B. 

[9.A.7] 

[9.A.8] 

‘Premultiplying  both  sides  of equation  [9.A.4]  by B implies  that 

1 

By, 

OO 

//Ny, 

0’ 

)—t(O 

By  By,  Bil.  0; 
Bo  B,,  ByjiM,  L,, 

=|}-Fy.  Bil. 

-F,  By 

The  upper  left  element  of this  matrix  system  asserts  that 

Substituting  [9.A.9]  into  [9.A.7], 

Il,.  +  By,II,,  =  0’. 

[9.A.9] 

[4  mi  m2) 

—  By, II,2  0 

Mi. 
Il, 

=O;  =[rr  Al] 
I,,, 

= 

B 

2” 

8". 

= 2  Br 

[9.4.10] 

In order  for the  left  side  of [9.A.10]  to  be zero,  it must  be the  case  that  wp,  =  0 and  that 

7 

—  BoM,  +  will,  =  (wi  —  HoBo)M,.  =  0’. 

— 

(9.A.11] 

But  if the  rows  of II,, are  linearly  independent;  [9.A.11]  can  be zero  only if 

Substituting  these  results  into  [9.A.8],  it follows  that  [9.A.10]  can  be zero  only if 

By  =  oBo.- 

- 

[9.A.12} 

[0  A;  AL]B  =  [Ho  MoBo  0’) 
1 

B,,  0 

=[u.  0’  v B,,  4 

Ba  Bz,  Bz 

[9.4.13] 

=  [bu  0’  O')B. 
Since  B is nonsingular,  both  sides  of [9.4.13]  can  be postmultiplied  by B~'  to  deduce  that 
[9.A.10]  can  be zero  only if 

(O  A}  Aj]  =  [mw  0’  O'). 

_  Thus,  the  right  side  of [9.A.10]  can  be  zero  only  if A, and  A, are  both  zero,  establishing 
"that the  rows  of the  matrix  in [9.3.12]  must  be linearly  independent.  @ 

Chapter 9 Exercise 

9.1. 

Verify  that  [9.2.23]  gives a consistent  estimate  of 0”. 

Chapter  9 Exercise 

255 

Chapter  9 References 

— 

Angrist,  Joshua  D.  1990.  “Lifetime  Earnings  and the  Vietnam  Era Draft tone Evidence 
from  Social  Security  Administration  any by |  American  Economic  Review  80:313-36. 
Errata,  1990,  80:1284—86. 
Fisher,  Franklin  M.  1966.  The Identification  Problem  i in iBipriamieteics: New York:  McGraw- 
Hill. 
Hausman,  Jerry  A.,  and  William  E.  Taylor.  1983.  “Identification  in Linear  Simultaneous 
Equations Models  with Covariance  Restrictions:  An Instrumental  Variables  Interpretation.” 
Econometrica  51: :1527-49. 
Maddala,  G.  S.  1977. Econometrics.  New York:  McGraw-Hill. 
Nelson,  Charles  R., and  Richard Startz.  1990.  “Some  Further  Results  on  the  Exact  Small 
Sample  Properties  of the  Instrumental  Variable  Estimator.”  Econometrica  58:967-76. 
hea Thomas  J. 1971.  “Identification in Parametric  Models.”  Econometrica  39:577—- 

—  and  Paul  A.  Ruud.  1990. “Simultaneous  Equations with Covariance  Restrictions.” 
Journal  of  Econometri 
f 

fees Mi  ay  *.  #2)  AGUS  tO gobie  dtodd  Baynes: 

4:25—  | 

eer 

- 

a 

. 

ai  | 

ees  Moe  am  zu  to  tnegsis  fal  toqge  of F 

ee eg 
oe! 8 phot ys om  see a  Cabana  is  |  e  Buh 

#3) 

mf 

p 

4 

* 
eek 

hi Pe ¥ arercvcn 

10 

Covariance-Stationary 
Vector  Processes 

This  is the first  of two  chapters  introducing  vector  time  series.  Chapter  10 is devoted 
to  the  theory  of  multivariate  dynamic  systems,  while  Chapter  11  focuses  on  em- 
pirical  issues  of estimating  and  interpreting  vector  autoregressions.  Only  the  first 
section  of Chapter  10 is necessary  for  understanding  the  material  in Chapter  11. 
Section  10.1  introduces  some  of the  key ideas  in  vector  time  series  analysis. 
Section  10.2  develops  some  convergence  results  that  are  useful  for  deriving  the 
asymptotic  properties  of certain.  statistics  and  for  characterizing  the  consequences 
of multivariate  filters.  Section  10.3  introduces  the  autocovariance-generating  func- 
tion  for  vector  processes,  which  is  used  to  analyze  the  multivariate  spectrum  in 
Section  10.4.  Section  10.5  develops  a  multivariate  generalization  of  Proposition 
7.5,  describing  the asymptotic  properties  of the sample  mean  of a serially  correlated 
vector  process.  These  last  results  are  useful  for  deriving  autocorrelation-  and  het- 
eroskedasticity-consistent  estimators  for  OLS,  for  understanding  the  properties  of 
generalized  method  of moments  estimators  discussed  in Chapter  14, and for deriving 
some  of the  tests  for  unit  roots  discussed  in Chapter  17. 

10.1.  Introduction  to  Vector  Autoregressions 

Chapter  3 proposed  modeling a scalar  time  series  y, in terms  of an  autoregression: 

| 

es  Piyi-1  *  drY,-2  3  SES. 

a 

+  €,, 

[10.1.1] 

where 

E(e,)  =  0 

[10.1.2] 

= 

foes  =  « 
a? 
Eee,)  ‘°  otherwise. 
Note  that  we  will  continue  to  use  the convention  introduced  in Chapter  8 of using 
lowercase  letters  to denote  either  a random  variable  or  its realization.  This  chapter 
describes  the dynamic  interactions  among a set  of variables  collected  in an  (n x  1)  © 
vector  y,.  For  example,  the  first  element  of y,  (denoted  y,,)  might  represent  the 
level  of GNP  in year  1, the  second  element  (y2,)  the  interest  rate  paid on  Treasury 
bills  in year  ¢, and  so  on.  A pth-order  vector  autoregression,  denoted  VAR(p),  is 
a vector  generalization  of [10.1.1]  through  [10.1.3]: 

10.1.3 

i, 

t+  Piy,-,  +  OH,-27°°°  t  9,91»  +  €,. 

[10.1.4] 

Here c denotes  an  (n  x  1) vector  of constants  and  ®, an  (n  x  n)  matrix  of 
autoregressive  coefficients  for  j =  1,2,...,p.The(n  x  1) vector  €, is a vector 

257 

generalization  of white  noise: 

E(e,)  =  0 

E(e,e!)  = 

fort  =  7 
(€.€;)  ‘s  otherwise, 

D 

[10.1.5] 

10.1.6 
| 

with  2 an  (n  X  n) Gaisnareic positive  definite  matrix. 

Let  c, denote  the  ith  element  of the  vector  ¢  and  let  ¢") denote  the  row  i, 
column  j element  of  the  matrix  ®,.  Then  the  first  row  of  the vector  system  in 
[10.1.4]  specifies  that 

Pin, 

Sy  +  OV Yia-1  + O)) Your  usdoul  aces  May  <i 

|  ri uP A  Oe  ys 

a 

ee Yn 2” 

. 

|  {10.1.7] 

Betsy  oe Yr, =p  + oA Bw.  a oe  as  +t ae 

Thus,  a vector 5  seiseeaeceiagi is a sabia in which  each  pee  Fe is oetame ona 
constant  and p  of its own lags as well as on p lags of each of the other variables 
in the VAR. Note that  each regression  has the. same explanatory  variables. 
___  Using lag operator notation,  [10.1.4]  can be written in eeteon  evistum:  he 
a  Mipitoage.  Henevhugn  9th}  sxyisna  G3  bsew  2:  tiotdw  .2sezs00  iq  i 
“veo  rasilgegn 22  Giprilum  «  292 »L*ly,  = e+e) rer  noitese 
= betelsres  yiicins ‘eto. shoe sili to zoineqoig aoigniyes  ott gridineab  2.6 
os  tod bn hoiteisnooois gAivizeh  wot  lutsep  ox  atiues;  test  sxodT  233g  INS 
or 
ort: gnibnanisbey  vot  .21©  10}  notemites  tastaicnes- Wiitesboaleors 
aatrvinat sot ban Sf cabs ita nt Gaxeusetb z10rmitzs zinomom  to bodigm  bagiie 

301  aon 

w 

|  RY;  vo)  ni FS  Fr, » 23003  linw  wi  £3233  att  9 ommoe 

Rewriting  a VAR(p)  as  a  VAR(1) 
As in the case  of the univariate  AR(p) process,  it is helpful  to rewrite  [10.1.8] 

in terms  of  a VAR(1)  process.  Toward  this  end,  define 

\  ie 

mi”  # 

: 

Yi-p+1  —  B 

(mp  x  1) 

®,  ®,  ®,  _—* 
r  O@ 
FeO  Ly  Ons 
Sbienj 
; 
4 
Oe  agro: 

®,_,  ®, 
0 
0); 
: 
0 

 Qavew  i 2@ 

of) 
OL 

iW 
6 

(mp  =  np) 

Ee, 

R69  .tm  *  2). 

ie: 

1s  ip 

6! 

[10.1.9] 

| 
[10.1.10} 

lim  Osisisnioony  ai 
‘The VAR(p) in [10.1.8] can then be  rewrite a8 the following VAR(1): one 
(10.1.11] 
é, =  Fes  +, 

an 

et  : 

i?" 

i} 
ee 

7? 

“7  t 

2 

De 

»  hyeee  NmeiaK 

ee 

tr Q-  meee 
a:  Fon) - fs RAs  heh og 

Hagh by  29499 150 

Vector  MA(~)  Representation 

The  first  n  rows  of  the  vector  system  represented  in  [10.1.12]  constitute  a 

vector  generalization  of equation  [4.2.20]: 

Mies  =  BT  S45  T  W  ie  can  +  W€,45-2  Tot?  tat  WY, _  18,41 

[10.1.14] 

+  Filly,  —  #)  +  P20) 1 ae 

ae  POU, 941 > 

Here  W, =  Fj) and  F{/) denotes  the  upper  left  block  of F’, where  F’ is the  matrix 
F raised  to the jth power—that  is, the  (n X  n) matrix  F‘/) indicates  rows  1 through 
n  and  columns  1 through  n  of the  (np  x  np)  matrix  F’.  Similarly,  F‘4? denotes  the 
block  of F’ consisting  of rows  | through  n  and  columns  (n  +  1) through  2n,  while 
F\/) denotes  rows  1 through  n  and  columns  [n(p  —  1)  +  1] through  np  of F’. 

If the  eigenvalues  of F all  lie  inside  the  unit  circle,  then  F* >  0 as 

s—  =  and 

y, can  be  expressed  as  a  convergent  sum  of the  history  of e: 

y=pte,  +  PV e,_,  +  Wre,_.  +  V3e,_; 

te  Hp  +  W(L)e,, 

[10.1.15] 

which  is a  vector  MA(*%)  representation. 

Note  that  y,_;  is  a  linear  function  of  €,_;,  €,_;_,,  . 

,  each  of  which  is 
uncorrelated  with  €,,,  forj  =  0,1,...  .  It follows  that  €,,,  is uncorrelated  with 
y,_;for  any  j =  0.  Thus,  the  linear  forecast  of y,,,  on  the  basis  of y,,  y,_,,.  . 
.  is 
given  by 

. 

. 

Yeo as  =  gk  +  ® (y,  ioe  #)  aA ®,(y,_,  a  #1) 5 ee  ea 

Ts |  At 

ad  1), 

and  €,,,  can  be  interpreted  as  the  fundamental  innovation  for  y,,,,  that  is,  the 
error  in forecasting  y,,,  on  the  basis  of a  linear  function  of a constant  and  y,,  y,_,, 
. 
..-  More  generally,  it follows  from  [10.1.14]  that  a  forecast  of y,,,  on  the  basis 
of y,,  y,_,,  . 

.  will  take  the  form 

. 

Vissis  =prt  FV,  or  1)  +  FY(Y,-.  —  #) 

[10.1.16] 

ae  oS  (Ae  ‘,  }). 

The  moving  average  matrices  W, could  equivalently  be calculated  as  follows. 

The  operators  ®(L)  and  W(L)  are  related  by 

requiring 

W(L)  =  [®(L)]-'. 

(I,  =  OL 

OL?  = 

+ 

=  ®,L°|I,  +  VL  +  WL?  +.--  io 

Setting the coefficient  on  L ' equal  to the zero  matrix,  as  in Exercise  3.3 of Chapter 
3, produces 

Similarly,  setting  the  coefficient  on  L? equal  to  zero  gives 

WV,  7  ®,  =  0. 

and  in general  for  L*, 

¥,  >  2,9  ..+  @,. 

VW,  =  DV, , +  OW,  + 
with W, =  I, and  W,  =  0 fors  <  0. 

ee  +  OMijagasiraforises  MA  «.  ¢ 

_ 

Note  that  the  innovation  in  the  MA(~)  representation  [10.1.15]  is ©,,  the 
fundamental  innovation  for y. There are alternative  moving average  representations 
based  on  vector  white  noise  processes  other  than  e,.  Let  H denote  a nonsingular 
260  Chapter  10 

Covariance-Stationary  Vector  Processes 

{10.1.17] 

[10.1.18] 

[10.1.19} 

(”  X  n)  matrix,  and  define 

Then  certainly  u,  is white  noise.  Moreover,  from  [10.1.15]  we  could  write 

u,  =  He,. 

[10.1.20] 

y  =»  +  H''He,  +  W,H~'He,,  +  WH  'He,_, 

+  W,H'He,  ,+--- 

[10.1.21] 

=e  +  Jum,  +  Ju,.,  +  bu,.2  +  Jw-3+°::. 

where 

J,=V,H"'. 

For  example,  H could  be  any  matrix  that  diagonalizes  1,  the  variance-covariance 
matrix  of  €,: 

HOH’  =  D, 

with  Da diagonal  matrix.  For such  a choice  of H,  the elements  of u, are  uncorrelated. 
with  one  another: 

E(u,u;)  =  E(He,e/H’)  =  D. 

Thus,  it is always  possible  to  write  a  stationary  VAR(p)  process  as  a  convergent 
infinite  moving  average  of  a  white  noise  vector  u,  whose  elements  are  mutually 
uncorrelated. 

| 

1.15] 
There  is one  important  difference  between  the  MA(=)  representations-10. 
and  [10.1.21],  however.  In  [10.1.15],  the  leading  MA  parameter  matrix  (W,,)  is the 
identity  matrix,  whereas  in  [10.1:21]  the  leading  MA  parameter  matrix  (J,)  is not 
the  identity  matrix.  To  obtain  the  MA  representation  for  the  fundamental  inno- 
vations,  we  must  impose  the  normalization  VW,  =  I,. 

Assumptions  Implicit  in  a VAR 

. 
. 

For  a  covariance-stationary  process,  the  parameters  c  and  ®,,...  ,  ®, in 
equation  [10.1.4]  could  be  defined  as  the  coefficients  of the  projection  of y, on  a 
- 
constant  and  y,_;,  - 
,  Y;-»-  Thus,  €, is uncorrelated  with  y,_,,...  ,  y,-,  by the 
definition  of ®,,  . 
, ®,. The  parameters  of a vector  autoregression  can  accord- 
. 
ingly be estimated  consistently  with  n  OLS  regressions  of the  form  of [10.1.7].  The 
additional  assumption  implicit  in a  VAR  is that  the  €, defined  by this  projection  is 
-  The  assumption  that  y, follows  a 
further  uncorrelated  with  y,_,-1,  Y,-p-2,  - 
vector  autoregression  is basically  the  assumption  that p lags are  sufficient  to  sum- 
marize  all of the  dynamic  correlations  between  elements  of y. 

- 

. 

10.2.  Autocovariances  and  Convergence  Results 
for Vector  Processes 

The  jth Autocovariance  Matrix 
For  a covariance-stationary  n-dimensional  vector  process,  the jth autocovar- 

iance  is defined  to  be  the  following  (n  x  n) matrix: 

[10.2.1] 
Pr, =  El(y,  —  w)(y,-;  —  »)'). 
Note  that  although  y,  =  y-,  for  a scalar  process,  the  same  is not  true  of a vector 
261 
10.2.  Autocovariances  and Convergence  Results for Vector  Processes 

process. 

r,#T-,. 
For  example,  the  (1, 2) element  of  I; gives  the  covariance  between  y,,  and  y>,_;- 
The  (1, 2) element  of I'_, gives  the  covariance  between  y,,  and  y,,,;.  There  is no 
reason  that  these  should  be  related—the  response  of y,  to  previous  movements  in 
y>  could  be  completely  different  from  the  response  of y,  to  previous  movements 
in yj. 

Instead,  the  correct  relation  is 

Pear 

7 

| 

[10.2.2] 

To derive  [10.2.2],  notice  that  covariance-stationarity  would  mean  that  ¢ in [10.2.1] 
could  be  replaced  with  ¢  +  J: 

P=  El(yie;  —  &)(¥e+p-7  —  &')  =  ElG.+;  —  »)(y.  —  B)’]- 

Taking  transposes, 

as  claimed. 

r;  =  E{(y,  a  B)(Y4;  =  »)’]  al r_,, 

Vector  MA(q)  Process 

A  vector  moving  average  process  of order  q takes  the  form 

1 

+  6.4  Me,  tT  Oe. 2 TO 

{10.2.3} 

where  €,  is  a  vector  white  noise  process  satisfying  [10.1.5]  and  [10.1.6]  and  ©, 
denotes  an  (n  X  n) matrix  of MA  coefficients  for  j =  1,2,...,q.  The  mean  of 
y, is ys,  and  the  variance  is 

| 

ry  =  El(y,  —  v)(y,  —  #)’] 

Efe,e;]  +  O,E[e,_,e€;_,JO;  +  O,E[e,_,€;_,]0; 
he 

age (210g 
2 ertPphle, 

[10.2.4] 

2  +  6,00;  +  8,20,  +--+  +  8,06), 

with  autocovariances 

O,2  +  O;,,NO;  +  O,,,2O;  +  --:  +  O,N8;_; 
forj = 1,2,...,q 
=)  90",  +  @,00',,,  +  @,00',,,+---+6,,,90; 

[10.2.5] 

: 

0 

| 

forj  =  -1,  -2,..., -q 
for |j| >, 

where  ©,  =  I,,. Thus,  any  vector  MA(q)  process  is covariance-stationary. 

Vector  MA()  Process 

The  vector  MA(=)  process  is written 

y=  pte,  +  We,  +  We.  +  °°: 

[10.2.6]  © 

for €, again  satisfying  [10.1.5]  and [10.1.6]. 

A  sequence  of  scalars  {h,}*._.  was  said  to  be  absolutely  summable  if 
2y. -2|h,|  <  %. For  H, an  (n x  m) matrix,  the sequence  of matrices  {H,}*_ _.  is 
262  Chapter 10  | Covariance-Stationary  Vector Processes 

absolutely  summable  if each  of  its  elements  forms  an  absolutely  summable  scalar 
sequence.  For  example,  if y{) denotes  the  row  i, column  j element  of the  moving 
average  parameter  matrix  W,  associated  with  lag s,  then  the  sequence  {W,}*_,.  is 
absolutely  summable  if 

> Iwi Moms 

eofori  =  1,2) .4..,  nandj  =  1,2,...,  n. 

{10.2.7] 

Many  of  the  results  for.  scalar  MA(%)  processes  with  absolutely  summable 
coefficients  go  through  for  vector  processes  as  well.  This  is  summarized  by  the 
following  theorem,  proved  in  Appendix  10.A  to  this  chapter. 

Proposition  10.2: 

Let  y, be  an  (n  X  1) vector  satisfying 

y,  =  ye  +  2 W,€,-,, 

where  €, is vector  white noise satisfying  [10.1.5] and [10.1.6] and {W,}z_, is absolutely 
summable.  Let y,,  denote  the  ith  element  of y,,  and  let 4;  denote  the  ith  element  of 
p.  Then 

(a)  the  autocovariance  between  the  ith  variable  at  time  t and  the  jth  variable  s 
periods  earlier,  E(y,  —  m,)(Yjr-.  —  My)»  exists  and  is  given  by  the  row  i, 
column  j element  of 

r,  =  > ee  tors  =  0, 152 

v=( 

) 

(6)  the sequence  of matrices  {1 ,}7_y  is absolutely  summable. 

If, furthermore,  {e€,}7— .  is an  i.i.d.  sequence  with  E|é; ,€;,.€i,0€i,.|  <  ©  for iy, iz, 
i,,  ig  =  1,2,...,  2,  then  also, 

(c)  ELY 5, 1  Vint VintyVis.tyl  <  oo for i,  b,  13,  l,  ¥  1.  2,  +22 

A  and  for  all  hh,  h, 

ty, 4; 

(d)  (VT)  > Yudjr—-s  > E(Wudjr-s) for i,j  =  1,2,...,.nand  for  all s. 

Is 

t=1 

Result  (a) implies  that  the  second  moments  of an  MA()  vector  process  with 
absolutely  summable  coefficients  can  be  found  by taking  the  limit  of [10.2.5]  as 
q—  =~.  Result  (b) is  a convergence  condition  on  these  moments  that  will  turn  out 
to ensure  that  the vector  process  is ergodic  for the mean  (see Proposition  10.5  later 
in this  chapter).  Result  (c) says  that  y, has  bounded  fourth  moments,  while  result 
(d) establishes  that  y, is ergodic  for  second  moments. 

Note  that the vector  MA()  representation  of a stationary  vector  autoregres- 
sion  calculated  from  [10.1.4]  satisfies  the  absolute  summability  condition.  To  see 
this,  recall  from  [10.1.14]  that  W, is a block  of the  matrix  F*.  If F has  np  distinct 
,  An»), then  any element  of W, can  be written  as  a weighted  - 
eigenvalues  (Aj, Az, ..  - 
average  of these  eigenvalues  as  in equation  [1.2.20]; 

vi ag c(i, jai  +  c(i, {):A3  a>  et  tot  Cupli, Dy  Ah» 
where‘c,(i,  /) denotes  a  constant  that  depends  on  v,  i, and j but  not  s.  Absolute 
summability  [10.2.7]  then  follows  from  the  same  arguments  as  in Exercise  3.5. 

iances  and Convergence  Results for Vector  Processes 

263 

Multivariate  Filters 

Suppose  that  the  (n  x  1) vector  y, follows  an  MA(%)  process: 

y,  =  py  +  W(L)e,, 

{10.2.8] 

with  {W,}Z_,  absolutely  summable.  Let  {H,}{_  _.  be  an  absolutely  summable  se- 
quence  of  (r  x  n) matrices  and  suppose  that  an  (r  x  1) vector  x,  is related  to  y, 
according  to 

x,  =  H(L)y,  =  > H,y,_,- 

[10.2.9] 

That  is, 

x,  =  H(L)[py + W(L)e,] 

=  H(1)py  +  H(L)W(L)e_, 

{10.2.10] 

where  pty  =  H(1)py  and  B(L)  is the  compound  operator  given  by 

=  ~Px  +  B(L)e,. 

B(L)  = 

>, B,L*  =  H(L)W(L). 

[10.2.11] 

k=-=x 

The  following  proposition  establishes  that  x, follows  an  absolutely  summable  two- 
sided  MA(=)  process. 

Let  {W,}~=)  be  an  absolutely  summable  sequence  of (n  X  n) 
Proposition  10.3: 
matrices  and let {H,}x;_  _.  be an  absolutely  summable  sequence  of (r X  n) matrices. 
Then the sequence  of matrices  {B,};-  —  .  associated  with the operator  B(L)  =  H(L)¥(L) 
is absolutely  summable. 

If {e,} in [10.2.8]  is i.i.d.  with  finite  fourth  moments,  then  {x,} in [10.2.9]  has 

finite  fourth  moments  and  is ergodic  for  second  moments. 

Vector  Autoregression 

Next we  derive  expressions  for the second  moments  for y, following  a VAR( p). 
Let  & be  as  defined  in equation  [10.1.9].  Assuming  that  — and y are  covariance- 
Stationary,  let  & denote  the  variance  of &, 

%  =  E(&£) 

y~—  B 
=p)  YB 
Vijay  me 

X(Y,  —  @)  Abc  Totes 

cies 

elkinenhi—  wi 

Ey 

r,, 

Radi 

tosh 

Bhat 

abel 

ee  iat  ita! 

r, 

: 

| 

{10.2.12} 

264  Chapter  10  | Covariance-Stationary  Vector  Processes 

where  I’; denotes  the jth autocovariance  of the  original  process  y.  Postmultiplying 
{10.1. i] by its own  transpose  and  taking  expectations  gives 

Elé,.&;]  =  El(FE_—,  +  v,)(FE,_,  +  v,)']  =  FE(€,_,€&-,)F’  +  E(v,v,). 

or 

>  =  FIF'+Q. 

(10.2.13] 

A closed-form  solution  to  [10.2.13]  can  be  obtained  in  terms  of the  vec  op- 
erator.  If A is an  (m  X  n) matrix,  then  vec(A)  is an  (mn  xX  1) column  vector, 
obtained  by stacking  the  columns  of A,  one  below  the  other,  with  the  columns 
ordered  from  left  to  right.  For  example,  if 

4);  j2 

A  = 

ar)  a2  I> 

Q3, 

432 

[10.2.14] 

7 

> 

>> 

ie ie 

Appendix 10.A establishes  the  following  useful  result. 

tye t  a  Cercle 

MAT 37M  <  Te  ‘ 

i 

.  WII  DO  38) Te  +  ata 

10.4:  Let A, Band C be marie whose dimension are such that te 

by substituting  [10.2.12]  into  [10.2.18]: 

ry 

r; 

rn -1 

on 

a 

ie:  =  [I,: —  A]-'  vec(Q). 

 [10.2.19] 

pe  pt . 

ae 

The  jth  autocovariance  of  € (denoted  %,) can  be  found  by postmultiplying 

(10.1.11]  by €/_; and  taking  expectations: 

E(é,&;_  ;) =  F-  E(&,_  .&_,;)  r  E(v,&-;): 

Thus, 

or 

2 =  FX ;_, 

fond  =  1,2,..°., 

[10.2.20] 

¥, =  Fi Set"  tok  jiael,  2... 

(10.2.21] 

The jth autocovariance  I, of the  original  process  y, is given  by the  first  n  rows  and 
n  columns  of [10.2.20]: 

r; =  ®J;_,  +  ®.J,_,  =  OOM 

«3 ®T;_, 

for j =  P.?Pp  +  Lp  +  Z.  owes 

[10.2.22] 

10.3.  The  Autocovariance-Generating  Function 
for Vector  Processes 

| 

Definition  of Autocovariance-Generating  Function 
for Vector  Processes 

Recall  that  for  a  covariance-stationary  univariate  process  y, with  absolutely 
summable  autocovariances,  the  (scalar-valued)  autocovariance-generating  function 
gy(z)  is defined  as 

with 

8y(z) 

2  2’ 

j=  -=x 

Sf 

E{(y,  a  MK) Y,-;  Se )] 

and  z  a  complex  scalar.  For  a  covariance-stationary  vector  process  y,  with  an 
absolutely  summable  sequence  of autocovariance  matrices,  the  analogous  matrix- 
valued  autocovariance-generating  function  Gy(z)  is defined  as 

Gy(z)=  ST z/, 

jJ#=  —x 

[10.3.1] 

where 

P= El(y,  -  w)(y,-,  —  w)'] 

and z is again  a complex  scalar. 
266  Chapter 10  | Covariance-Stationary  Vector  Processes 

_  Autocovariance-Generating  Function  for a  Vector  Moving 
Average  Process 

For  example,  for  the  vector  white  noise  process  €, characterized  by [10.1.5] 

and [10.1.6],  the  autocovariance-generating  function is 

G.(z)  =  ©. 

[10.3.2] 

For the  vector  MA(q)  process  of [10.2.3],  the  univariate  expression  [3.6.3]  for  the 
autocovariance-generating  function  generalizes  to 

Gy(z)  =  (I,  +  @,z  +  @.z7  +--+  +  O,2z7)0 

[10.3.3] 

mG  Oia  he  Bz04  +,  +  L279). 

This  can  be  verified  by noting  that  the  coefficient  on  z/ in [10.3.3] is  equal  to  I, 
as given  in [10.2.5]. 
_ 

For an  MA(=)  process  of the  form 

y=  B+  We, + Wye,  + We.  + °°  =p  +  V(Ly)e,, 

with {W,}z_» absolutely  summable, [10.3.3] generalizes to 
ons  Qed. i  heel “Ge = nui  4 web SehOre 
thn 

olor  at a 

tat 

© 

7: 

| 

j 

P 

-  03.4) 
sia 

1S)  D2-SHaMICT 

OS 

_  Autocovariance-Generating aig 

! 

a  Vector Autoregression 

aie 

Bee  TshINn  AT x  +) 
VA. 

D 

a 

with oie Wi of Fine 

aa eee vivir) = Q cla 

Note  also  that  if  an  (r  x  1) vector  &, is  premultiplied  by  a  nonstochastic 
(1  x  r)  matrix  H’,  the  effect  is  to  premultiply  the  autocovariance  by  H’  and 
postmultiply  by H: 

E((H’é,  —  H’w_)(H’&,_;  —  H’wg)']  =  H’E[(E,  —  we)(E-;  —  Me) JH. 

implying 

Gy(Z)  =  H’'G,(z)H. 

Putting  these  results  together,  consider  &, the  r-dimensional  VA R(1)  process 
é  =  Fé_,  +  v,  and  a  new  process  u,  given  by u,  =  H’é,  +  w,  with  w,  a  white 
noise  process  that  is uncorrelated  with  &,_; for  all j. Then 
+ Gy/(z), 
Gy(z) =  H’G,(z)H 

or,  if R is the  variance  of w,, 

Gy(z)  =  H'[I,  —  Fz]~'Q{I,  —  F’z7']"'H  +  R. 

[10.3.6] 

More generally,  consider  an  (n  X  1) vector  y, characterized  by 

y=  By  +  W(L)e,, 

where  €, is a  white  noise  process  with  variance-covariance  matrix  given  by 2 and 
where  W(L)  =  2i_,)W,  L*  with  {W,}%_,  absolutely  summable.  Thus,  the  auto- 
covariance-generating  function  for y is 

Gy(z)  =  V(z)Q[V(z-')]’. 

[10.3.7] 

Let  {H, }¢. -.,  be an  absolutely  summable  sequence  of (r x  n) matrices,  and suppose 
that an  (r X  1) vector  x, is constructed  from  y, according  to 

x  =  H(L)y,  =D) HeYs-n  =  Bx  +  B(L)e,, 

where  px  =  H(1)py  and  B(L)  =  H(L)W(L)  as  in  [10.2.10]  and  [10.2.11].  Then 
the  autocovariance-generating  function  for  x  can  be  found  from 

Gx(z)  =  B(z)Q[B(z~')]’  =  (H(z) (z)JQ[W(z-')]'[H(z-')]’. 
[10.3.8] 
Comparing  [10.3.8]  with  [10.3.7],  the  effect  of applying  the  filter  H(L)  tovy,  is to 
premultiply  the  autocovariance-generating  function  by H(z)  and  to  postmultiply 
by the  transpose  of H(z~'): 

Gx(z)  =  [H(z)]Gy(z)[H(z~')]’. 

[10.3.9] 

10.4.  The  Spectrum  for Vector  Processes 
Let  y, be an  (nm  x  1) vector  with  mean  E(y,)  =  p and  kth  autocovariance  matrix 
[10.4.1] 
El(y,  —  w)(y.-«  —  B)')  =  Ty. 
If {T, }/. _,,  is absolutely  summable  and if z is a complex  scalar,  the autocovariance- 
generating  function  of y is given  by 

Gy(z)  =D Mez'. 

[10.4.2] 

268  Chapter  10  | Covariance-Stationary  Vector  Processes 

The  function  Gy(z)  associates  an  (n x n) matrix  of  complex  numbers  with  the 
complex  scalar  z.  If [10.4.2]  is ovided by 27  and  evaluated  at  z  =  e~“,  where 
w  is  a  real  scalar  and  i  =  V—1,  the  result  is  the  population  spectrin of  the 
vector  y: 

Sy(w)  =  (27)~'Gy(e-) = (27)7!  5 re", 

[10.4.3] 

k=  -=x 

The  population  spectrum  associates  an  (n  X  n) matrix  of complex  numbers  with 
the  real  scalar  w. 

Identical  calculations  to  those  used  to  establish  Proposition  6.1  indicate  that 
when  any  element  of sy(w)  is multiplied  by e’“*  and  the  resulting  function  of w  is 
integrated  from  —7  to  7,  the  result  is  the  corresponding  element  of  the  kth 
autocovariance  matrix  of y: 

let erie dw  =  T,. 

[10.4.4] 

via? 

=: 

a special 

a! 

CASE, 

rene  0 

=  0,  equation 

WACR 

[10.4.4] 

implies 
[10.4.4]  impli 
>. 

. 
: 

at sy(w) dw  =  Ty, 

| 

(10.4.5) 

= _ 

; 

inn 

a 

pmate. the area  under  the  population bs seubee is the  unconditional  var- 

acmuerg “it y. 
CIGMER 

BORA 

a of the malate 

multivari 

h rt a 6th he goal elements 0 , fy) 

For  this  n  =  2 case,  the  population  spectrum  [10.4.3]  would  be 

Sy(w) 

ey vite  iat 

>  ye  iwk 

2a  s ry (ke —  iw 

Ss ye —iwk 

ko-= 

k=-= 

. yi{cos(wk)  —  i-sin(wk)}  a _Yxv{cos(wk)  —  i-sin(wk)} 

Press 

ae  zm _Y¥x {cos(wk)  —  i “sin(wk)}  #y _Yvv{cos(wk)  —  i-sin(wk)} 

[10.4.10} 

Using  [10.4.7]  and  [10.4.8]  along  with  the  facts  that  sin(—wk)  =  —sin(wk)  and 
sin(0)  =  0, the  imaginary  components  disappear  from  the  diagonal  terms: 

Sy (w) 

4 

»> xx cos(wk) 

2 y¥“Yoos(wk)  —  i-sin(wk)} 

a _v¥xtcos(wk)  —  i-sin(wk)} 

ey yy cos(wk) 

(10.4.11] 

However,  since  in  general  y¥)  # y\;"”,  the  off-diagonal  elements  are  typically 
complex  numbers. 

The  Cross  Spectrum,  Cospectrum,  and Quadrature  Spectrum 

.  The  lower  left  element  of the  matrix  in [10.4.11]  is known  as  the  population 

cross  spectrum  from  X to  Y: 

Syx(w) = (2m)7! “S _7¥3cos(wk)  —  i-sin(wk)}. 

[10.4.12] 

The  cross  spectrum  can  be  written  in terms  of its real  and  imaginary  components 
as 

Syx(w)  =  cyx(w)  +  i-qyx(w). 

[10.4.13] 

The  real  component  of the  cross  spectrum  is known  as  the  cospectrum  between  X 
and  Y: 

Cyx(w) = (2m)*! > _7¥x cos(wk). 

[10.4.14] 

One  can  verify  from  [10.4.9]  and  the  fact  that  cos(—wk)  =  cos(wk)  that 

Cyx(w)  =  Cyy(w). 

{10.4.15] 

270  Chapter  10  | Covariance-Stationary  Vector  Processes 

The imaginary  component  of the  eross  spectrum  is known  as  the  quadrature  spec- 
trum  from  X to  Y: 

qvx(w) = ~(2m)-" SD yify sin(wk). 

k=  -=x 

(10.4.16] 

One  can  verify  from  [10.4.9]  and  the  fact  that  sin(—wk) = —sin(wk)  that  the 
quadrature  spectrum  from Y to X  is  the  negative  of the  quadrature  spectrum  from 
X to  ¥: 

Gyx(w)  =  —qxy(w). 

Recalling  [10.4.13],  these  results  imply that  the off-diagonal  elements  of s(w)  are 
complex  conjugates  of  each  other;  in general,  the  row  j, column  m  element  of 
Sy(@) is the  complex  conjugate  of the  row  m,  column  j element  of sy(w).  ° 

Note  that  both  cy,(w)  and  qgy,(w)  are  real-valued  periodic  functions  of w: 

Cyx(w  +  27)  cs Cyx(w) 

for j =  +].  +72. pene 

| 

Irx(@ + 2nj) = =  Iyx(@)  for i= + 1, +2, . ee 

irther alo from-{10.4.14] that 

F  a 

t 

cyx(- -w) =  Svifer. 

while ib 4.16] implies that 

Oe asl)  = ale).  oan 

> 

S 

£ptstw  oigar  IOI  wie 

Pat 
1  and i 

quadrature een are fly specified by the te i 

The  Sample  Multivariate  Periodogram 
To  gain  further  understanding  of the  cospectrum  and  the  quadrature  spec- 
trum,  let y,,  yo...  ,¥,  and  x,,X2,...  ,  7 denote  samples  of  T observations  on 
the  two  variables.  If for  illustration  T is odd,  Proposition  6.2  indicates  that  the 
value  of y, can  be  expressed  as 

y  =yrt  > {&)-cos[w;(t  —  1)]  +  §,-sin[w,(¢  —  1)... 

[10.4.19] 

where  y is the  sample  mean  of zi =  (T —  1)/2,  w,  =  2nj/T,  and 

= QT) 2 eeoste(t —  1) 

[10.4.20} 
geen s eine  ~ De  po.42y 

- 

AS Ire: i  7  TICK  16s 

An  analogous  representation  for x, is 

M 

x,  = 

+  2,  {a;- cos[w,(t  -  )) +d: as (t =  ~  [10.4.22] 

uM 

£.\ 

7  se. 

_ 

¢ 

. 

Se Fe  DIL 
1 ais  = int ~e sd 

ban 

UY 

[Sisbey) Age tiie iz 

i023 5] 

Hence, the portion  of the sample  covariance  between  x and y that  is due  to  their 
common  dependence  on  cycles  of frequency  w, is given  by 

(1/2)(@)4;  +  5,d;). 

[10.4.28] 

_  This magnitude  can  be  related  to  the  sample  analog  of the  cospectrum  with 
calculations  similar  to those  used  to  establish  result  (c) of Proposition  6.2.  Recall 
that since 

Y 

s 

>> cos[w,(*  —  1)]  = 

the magnitude  4; in [10.4.20]  can  alternatively  be expressed  as 

; 

4, =  (UT)  ¥ (y, ~  ¥)-cosfu,(t  -  1). 

gies 

Gore's 
teases 

sil So 

P| 

Pp AVR)  (ood as 

ay 

: 

= Reade = oD + by ts Te a  Ree  oi 

ye «8 (y, ae jen  4 ae 4 3 0. - 5): -sinfu,(7 — ai  4 

© ee 
\ Va? See 

r=! 

Be 

bed  Bs 21  iS Pegi x 

wa) 

ee (T? 36 Fi exo  - y {So OP: expl-i: @ (7 — of 
(aT?) a «© - DO, -9) +  3 6  DOr - W)-ex- io) 

-  pres  i 

et  tere |  = | hep is met i  fv) Ree 2 ¥; ia): a ave 

anpsttan! 

650 

Result  [10.4.29]  implies  that 

14,  +  i-dj)(a,  —  1-5)  =  (2/T) 

T-1 

DB  FP -exp[—kia,] 

k=-T+1 

(10.4.31] 

=  (4n/T)-S,,(w)), 

where  §,,(w,)  is the  sample  cross  periodogram  from x to  y at  frequency  w,,  OF  the 
lower  left element  of the  sample  multivariate  periodogram: 

T—1 

T-1 

a 

2 

=< 

k=-T+  Pia 

a  (k) 

il  A 

—  iwk 

a  (k) 

—  iwk 

e 

a 

a 

k=  be fad 

ee  §,x(@)  I 

Hie)  4  ; 2 

aig 
>  sped  de, 

: 

a 
vat 
>  o,  ei 

(k) 

§,,(w) 

§,,(w) 

k=-T+I 

k=-Trl 

Expression  [10.4.31]  states  that  the  sample  cross  periodogram  from x to y at 

frequency,  w; can  be  expressed  as 

§,,(w,)  =  [Ti(8x)]-(4,  +  i-d))(4;  —  1-8) 

=  [T/(87)]: (4,4,  +  4,5)  +  i-[T/(8m)]-(4,4;  —  4,4). 

The  real  component  is the  sample  analog  of the cospectrum,  while  the  imaginary 
component  is the  sample  analog  of the  quadrature  spectrum: 

where 

$y. (@)  =  Eye(@)  +  i Gyx(M), 

é,.(@)  =  [T/(8z)]+(4;4,  +  4,5) 
Gyx(,)  =  [T87)]-(d)4,  —  4,8). 

[10.4.32] 

| 

[10.4.33] 
[10.4.34] 

Comparing  [10.4.33]  with  [10.4.28],  the  sample  cospectrum  evaluated  at  w, 
is proportional  to  the  portion  of  the  sample  covariance  between  y and x that  is 
attributable  to  cycles  with  frequency  w;.  The  population  cospectrum  admits  an 
analogous  interpretation  as  the  portion  of the  population  covariance  between  Y 
and  X attributable  to  cycles  with  frequency  w  based  on  a  multivariate  version  of 
the  spectral  representation  theorem. 

What  interpretation  are  we  to  attach  to  the  quadrature  spectrum?  Consider 
using  the  weights  in [10.4.22]  to  construct  a  new  series  x*  by shifting  the  phase  of 
each  of the  periodic  functions  by a  quarter  cycle: 

Ze 

X.c  ») {d;-cos[w;(t  —  1) +  (7/2)] 

+  dj-sin[w;(¢  —  1) +  (7/2)}}. 

[10.4.35] 

The  variable  x;  1s  driven  by the  same  cycles  as  x,,  except  that  at date  ¢ =  1 each 
cycle  is one-quarter  of the  way  through  rather  than  just  beginning  as  in the  case 
of x,. 

Since  sin[@ +  (7/2)]  =  cos(@) and since cos[@ +  (7/2)]  =  —sin(@), the variable 

x} can  alternatively  be  described  as 
| 

n 

( 

xt  =¥+  >» {d;-cos{w;(t  —  1)] —  4-sinfw,(t  -  1)]}. 

[0.4.36] 

274  Chapter  10  | Covariance-Stationary  Vector  Processes 

As  in  [10.4.27],  the  sample  covariance  between  y,  and  x*  is found  to  be 

\ 

T 

T'S (,-  et  -  B=  (12) > (4,4,  —  §,4)). 

Comparing  this  with  [10.4.34],  the  sample  quadrature  spectrum  from  x  to  y  at 
frequency  ,  is  proportional  to  the  portion  of  the  sample  covariance  between  x  * 
and  y  that  is due  to  cycles  of  frequency  w,.  Cycles  of  frequency  w,  may  be  quite 
important  for  both  x  and y individually  (as reflected  by large  values  for  §,,(w)  and 
S,,(@))  yet fail  to produce  much  contemporaneous  covariance  between  the  variables 
because  at  any  given  date  the  two  series  are  in  a  different  phase  of the  cycle.  For 
example,  the  variable  x  may  respond  to  an  economic  recession  sooner  than  y.  The 
quadrature  spectrum  looks  for  evidence  of such  out-of-phase  cycles. 

Coherence,  Phase,  and  Gain 

The  population  coherence  between  X  and Y is a  measure  of  the  degree  to 
which  X  and  Y are  jointly  influenced  by  cycles  of  frequency  w.  This  measure 
combines  the  inferences  of  the  cospectrum  and  the  quadrature  spectrum,  and  is 
defined  as! 

hyx(o) 

=  Ae vx(w)P? + [Gyx(@)}? 

Syy(w)sSxx (w) 

’ 

assuming  that  sy,(w)  and  sy,(w)  are  nonzero.  If syy(w)  or  syy(w)  is zero,  the 
coherence  is defined  to  be  zero.  It can  be  shown  that  0 <  hy,(w)  = 1 for  all  w as 
long as  X and Y are  covariance-stationary  with  absolutely  summable  autocovariance 
matrices.” 
If hyx(w) is large,  this  indicates  that  Y and  X have  important  cycles  of 
frequency  w  in common. 

The  cospectrum  and  quadrature  spectrum  can  alternatively  be  described in 
polar  coordinate  form.  In  this  notation,  the  population  cross  spectrum  from  X to 
Y is written  as 

Syx(w)  =  cyx(w)  +  i-qyx(w)  =  R(w)-expli-Ow)],  _ [10.4.37] 

where 

R(w)  =  {[eyx(@)]?  +  [ayx(o)  7}? 

[10.4.38] 

and  @(w)  represents  the  radian  angle  satisfying 

sin[O(w))  _ —  Irx\%) 
yx(w) 
Cyx(w)  | 
cos[A(w)] 

|

[10.4.39] 

The  function  R(w)  is  sometimes  described  as  the  gain  while  @(w)  is  called  the 
phase.’ 

|

'The  coherence  is sometimes  alternatively  defined  as  the square  root  of this magnitude.  The sample 

coherence  based  on  the  unsmoothed  periodogram  is identically  equal  to  1. 

*See, for example,  Fuller  (1976,  p.  156). 
The gain is sometimes  alternatively  defined  as  R(w)/syx(w). 

10.4.  The Spectrum for Vector  Processes 

275 

 
 
The  Population  Spectrum  for  Vector  MA  and  AR  Processes 
Let  y,  be  a  vector  MA(~)  process  with  absolutely  summable  moving  average 

coefficients: 

where 

y;  mf  »  +  W(L)e,, 

for  1o=.7 
.e 
EMmihs),  7  {,  otherwise. 

Substituting  [10.3.4]  into  [10.4.3]  reveals  that  the  population  spectrum  for  y, can 
be  calculated  as 

sy(w)  =  (27)-'"[W(e-)]O[W(e)y’. 

[10.4.40] 

For example,  the population spectrum  for a stationary  VAR( p) as  written  in [10.1.4] 
i 

= 

Sy (w) 

-1 

= 

-iw  _ 

|  i) |  a 

ee 

ne 

pee 

—piw\—1 

{I,,  Pg 

(27) 
'  ®,e 
x  {I,  —  Die™  7  Oey  SS  —  Or 

Pre 

}  Q 

[10.4.41] 

Estimating  the  Population  Spectrum 

If an  observed  time  series  y,,  y>,  . 

,  yr can  be  reasonably  described  by a 
. 
pth-order  vector  autoregression,  one  good  approach  to  estimating  the  population 
spectrum  is  to  estimate  the  parameters  of  the  vector  autoregression  [10.1.4]  by 
OLS  and  then  substitute  these  parameter  estimates  into  equation  [10.4.41]. 

. 

Alternatfvely,  the  sample  cross  periodogram  from  x  to y at  frequency  w;  = 
2nj/T can  be calculated  from  [10.4.32]  to  [10.4.34],  where  4;, 5, 4), and  @; are  as 
defined  in [10.4.20]  through  [10.4.24].  One  would  want  to  smooth  these  to obtain 
a  more  useful  estimate  of the  population  cross  spectrum.  For  example,  one  rea- 
sonable  estimate  of the  population  cospectrum  between  X and Y at  frequency  , 
would  be 

y 
€yx(w;)  Fj  be  At (h  +  1)?  | ealoyem) 

fa+i=  {nll 

— 

a 

, 

where  ¢,,(w;,,,)  denotes  the  estimate  in  [10.4.33]  evaluated  at  frequency 
®j 4m  =  20(j  +  m)/T and  h is a bandwidth  parameter  reflecting  how many different 
frequencies  are  to  be  used  in estimating  the  cospectrum  at  frequency  w;. 

Another  approach  is to  express  the  smoothing  in terms  of weighting  coeffi- 
cients  xf to  be applied  to  f, when  the  population  autocovariances  in expression 
[10.4.3]  are  replaced  by sample  autocovariances.  Such  an  estimate  would  take  the 
form 

- 

84(0)  =  nyt {fo +S atltveniet  « them} 

k=1 

276  Chapter 10  | Covariance-Stationary  Vector  Processes 

— 

_—_—s 

R=  TD  (-a  -  57 

. a 

P’ 

Zz 

y=T"!  py y,. 

For example,  the  modified  Bartlett  estimate  of the  multivariate  spectrum  is 

fe  0 Qn)"  {fy +  $ |1-- 

k=1 

+. if.e 7  +  ten}.  [10.4.42] 

Filters 
net x, be an  r-dimensional  covariance-stationary  process  with  absolutely 
riances  and with — _ r) population  spectrum denoted  s,(w). 
922 V2 46b-Ne “ablatttutely sinhehal e sequence of (n x ee igvriems sad, let 

: 

yea he n-dimensional ear poe  Bren OY bes  zoaisiem 

sonsits 

} [re noe  oe  — y= HL, = - Se 

- 

na 

, 

nie } 

> 

sae nae 

tat the population spectrum ofy (denoted sy) isrlated 
loo  NOI9p o7g Iwettill to.  consepse  SAT 

1  oo  Wi OFF BuO  a st Saw 
‘He NoSjoIg  wesc 

nit  26 

CUB 

z)  ¢ 

f 

p 

sa  her “Sy(w)  = [H(e7* ee hs  cenit  iti  ff 

262 PAM hoitnleeith  ,{  4 OP is [Th.b.61}  to  ain anisis shel 

oP  ed 

It  follows  from  [10.4.43]  that  the  spectrum  of y is given  by 

~  |  aN 
Trem)  UL 
Syx() 

0 

0  iF ed 

suo) 

1 
JLo 
Sxx(w)h(e) 

(10.4.45] 

| 

[h(e-)syx(w)  he) Sxx(w)h(e)  +  syu()} 

VO 

where 

i 
~ 

h(e-‘*)  =  Ss her  *, 
k=  -= 

[10.4.46] 

The  lower  left  element  of the  matrix  in [10.4.45]  indicates  that  when  Y, and  X,  are 
related  according  to  [10.4.44],  the  cross  spectrum  from  X to  Y can  be  calculated 
by multiplying  [10.4.46]  by the  spectrum  of X. 

We  can  also  imagine  going  through  these  steps  in reverse  order.  Specifically, 
suppose  we  are  given  an  observed  vector  y,  =  (X,,  Y,)’  with  absolutely  summable 
autocovariance  matrices  and  with  population  spectrum  given  by 

sy(w)  =  koe in. 
Syx(w) 

Syy(@) 

| 

[10.4.47] 

Then  the linear  projection  of Y,on{X,_,}%-  —~  exists  and is of the form  of [10.4.44], 
where  u,  would  now  be  regarded  as  the  population  residual  associated  with  the 
linear  projection.  The  sequence  of linear  projection  coefficients  {h,};_  _.  can  be 
summarized  in terms  of the  function  of w  given  in [10.4.46].  Comparing  the  lower 
left  elements  of [10.4.47]  and  f10.4.45],  this  function  must  satisfy 

"In other  words,  the functron  h(e ~) can  be  calculated  from 

h(e~"*)sxx(w)  =  Syx(w). 

h(e-iw)  =  Svat) 
Sxx(w) 

| 

[10.4.48] 

assuming  that  5yy(w)  is not  zero.  When  syy(w)  =  0,  we  set  h(e~)  =  0.  This 
magnitude,  the  ratio  of the  cross  spectrum  from  X to  Y to  the  spectrum  of X,  is 
known  as  the  transfer function  from  X to  Y. 

The  principles  underlying  [10.4.4]  can  further  be  used  to  uncover  individual 

transfer  function  coefficients: 

hy =  (2a)7!  [" h(e-i*)el  dw, 

In other  words,  given  an  observed  vector  (X,,  Y,)’  with  absolutely  summable  au- 
tocovariance  matrices  and  thus  with  continuous  population  spectrum  of the  form 
of [10.4.47],  the  coefficient  on  X,_,  in the  population  linear  projection  of  Y, on 
{X,_,}%-  —.  can  be calculated  from 
| 

h, =  (2n)>!  [ Syx() oink diy, 

 Syy(w) 

[10,4.49} 

278  Chapter 10  | Covariance-Stationary  Vector  Processes 

10.5.  The  Sample  Mean of a Vector  Process’ 

Variance  of the  Sample  Mean 
Suppose  we  have  a  sample  of  size  7,  {y,,  y2,  . 

dimensional  covariance-stationary  process  with 

. 

. 

.  yy},  drawn  from  an  n- 

E(y,)  =  » 
Ei(y,—  w)(y,>—  »)'),=  F;, 

-  Consider  the  properties  of the  sample  mean, 
f 

[ 

| 

Yr  =  (1/T)  a Y,. 

| 

[10.5.1] 
[10.5.2] 

[10.5.3] 

As in  the  discussion in  Section  7.2  of the  sample  mean  of a  seemed process,  it is 
clear  that  E (¥7) =  a and 

E(yr -  wir = wh +s Ss  > 

eG  Ha  ve 

| 

) +  [ee nt ies af Gr- wr  bs 

; 

tat ma  (y> sain) ire  cota  (ype Bw) 56 a> mite  fT 
pb) + gy  py  He +. ees  ee Yo {0}  mish 

Ps 
7 
©  PMR,  Hol) Hextecet  Piro] 

+--+  +  (yz - wil, —~p)  +  (y2-  pw)  +e  HQ  8 videm: 
[10.5.4]  . 

- y: ~ wl 

) 

“oh 

Gy ty  +  a  eaag: stalscoealt To  non 

ts  + fe  X87) )  $F  e+  Parisi) 

BAe  ir 343% aa. ‘ 

“4n 

RaSh  Siqennd, sit Yo” 

8  2323)  2: ior ogni 
pees  ~ (aj  3 ines 

tC = ths 

~~ 

be 

oo 

. 

ke 

‘ 

Maa.  g* 

self Nes Bers ait  aoee 

"es 

Pte ee anes? 

‘  ~~  I 

SOS 

ey. 

a 

eee 

, 
> 
eee  ral  qs  xe:  foes 
Se  AVA 

oi) Geer  aa 

ae 

_ 

oe 

as 

ee 

7 

= 

—%) 

| 
er 

a 

The  proof of Proposition  10.5  is virtually  identical  to  that  of Proposition  7.5. 

Consider  the  following  (”  <  n) matrix: 

v=  -= 

DN  TG  -  wGr-  w=  T+ 
where  the equality  follows  from  [10.5.5].  Let  y”) denote  the row  i, column j element 
of  f,.  The  row  i, column  j element  of the  matrix  in  [10.5.6]  can  then  be  written 

DWE.  [10.5.6 

T-\ 

> 
|vl=T 

T-1 

+ 

Dd  (vir pyf". 

w=  —(7T-1) 

Absolutely  summability  of  {f',}*._.  implies  that  for  any  e  >  0 there  exists  a  q 
such  that 

2 lyf]  <  €/2. 
lwi>q 

Thus, 

| vy  + 
eT 

T -1  uit) yp| <  en  +  S (wll. 
( 
v=  -(T-1 

v=—q 

This  sum  can  be  made less  than  e  by choosing T sufficiently  large.  This  establishes 
claim  (b)  of  Proposition  10.5.  From  this  result,  E(y,7  —  ,)? —  9 for  each  i, 
implying  that  y,7—>  14. 

Consistent  Estimation  of T  Times  the  Variance 
of the  Sample  Mean 

Hypothesis  tests  about  the  sample  mean  require  an  estimate  of the  matrix  in 

result  (b) of Proposition  105.  Let  S represent  this  matrix: 

s=  as T-E[(yr  —  #)(y¥r  —  #)’). 

[10.5.7] 

If the  data  were  generated  by a vector  MA(q)  process,  then  result  (b) would 

imply 

S =  Sr 

[10.5.8] 

A natural  estimate  then  is 

where 

S =f,  +  » Gort  RY) 

[10.5.9] 

ft, = (VT)  YY,  -  yy  - 9)’. 

(ey  + 

[10.5.10] 

As  long  as  y, is ergodic  for  second  moments,  [10.5.9]  gives  a consistent  es- 
timate  of [10.5.8].  Indeed,  Hansen  (1982) and White  (1984, Chapter 6) noted that 
[10.5.9]  gives a consistent  estimate  of the asymptotic  variance  of the sample  mean 
for a  broad  class  of processes  exhibiting  time-dependent  heteroskedasticity  and 
autocorrelation.  To  see  why,  note  that  for  a  process  satisfying  E(y,)  =  p with 
280  Chapter 10  | Covariance-Stationary  Vector  Processes 

time-varying  second  moments,  the  variance  of  the  sample  mean  is given  by 

El(yr  —  w)(¥r  -  #)') 

rT 

T 

F 

el wn) 2 (y,  -  »)| jan Dy YY,  -  »)| 

(10.5.11] 

r 

oT 

(UT?)  & D Ely,  ~  wy.  -  w)'). 

Suppose,  first,  that  E[(y,  —  p)(y,  —  p)’)  =  0 far  |t —  s| >  g,  as  was  the  case  for 
the  vector  MA(q)  process,  though  we  generalize  from  the  MA(q)  process  to  allow 
El(y, —  »)(y,  —  p)'] to  be  a function  of ¢ for  |t —  s| =  q. Then  [10.5.11]  implies 

T-El(¥r  -  w¥r  -  w)’) 

T 

=  (VT)  & ElQ,  —  WO,  -  HY 

+  (1/T)  2 {E[(y,  a  wy,  — By] +  E{(y,-\  =  B)(y,  =  »)'}} 

+  (WT)  > {ELQ,  —  w)Y-2  =  #)') +  ElQs-2  -—  W.-W)  + 

a 

| 

+  (1/T)  > {E[(y,  -  w)(y,-,  —  »)']  +  El(y,-4  -—  wy,  —  »)'}. 

t=qr+il 

* 

The  estimate  [10.5.9]  replaces 

{10.5.12] 

(UT) & EQ:  wy»  -  wy] 

[10.5.13] 

in [10.512] with 

| 

| 

(UT) & i —  Yr)»  — Yr)’, 

_— 

[10.5.14} 

and thus  [10.5.9]  provides  a consistent  estimate  of the  limit of [10.5.12]  whenever 
[10.5.14]  converges  in probability  to  [10.5.13].  Hence,  the  estimator  proposed  in 
[10.5.9]  can  give a consistent  estimate  of T times  the  variance  of the  sample  mean 
in the  presence  of both  heteroskedasticity ‘and  autocorrelation  up through  order  q. 

More  generally,  even  if E[(y,  —  )(y,  —  )’] is nonzero  for  ail  ¢ and  s,  as 
long as  this  nratrix  goes  to  zero  sufficiently  quickly  as  |¢ —  s| —»  ~,  then  there  is 
still  a sense  in which  §, in [10.5.9]  can  provide  a consistent  estimate  of S.  Specif- 
ically,  if, as the  sample  size  T grows,  a  larger  number  of sample  autocovariances 
q is used  to form  the  estimate,  then  $,—> S (see White,  1984,  p.  155). 

The  Newey-West  Estimator 
Although  [10.5.9]  gives a consistent  estimate  of S, it has  the  drawback  that 
[10.5.9]  need  not  be  positive  semidefinite  in  small  samples.  If § is not  positive 
semidefinite,  then  some  linear  combination  of the elements  of y is asserted  to have 
a negative  variance,  a considerable  handicap  in forming  a hypothesis  test! 

Newey  and  West  (1987)  suggested  the  alternative  estimate 

S=f,  +  : [ ~~  Jc. +2), 

+  1 

[10.5.15] 

10.5.  The Sample  Mean  of a  Vector  Process 

281 

where  Ff, is given  by [10.5.10].  For  example,  for  g  =  2, 

Ge  T2490).  4  (0,  +  Pp. 

Newey  and  West  showed  that  § is  positive  semidefinite  by  construction  and  has 
the  same  consistency  properties  that  were  noted  for  S,  namely,  that  it g  and  T 
both  go  to  infinity  with  q/T'*  —  0,  then  S;—  S. 

Application:  Autocorrelation-  and  Heteroskedasticity-Consistent 
Standard  Errors  for  Linear  Regressions 

As  an  application  of  using  the  Newey-West  weighting,  consider  the  linear 

regression  model 

y,  =  xB  +  4u, 

for  x, a  (k  X  1) vector  of explanatory  variables.  Recall  from  equation  [8.2.6]  that 
the  deviation  of the  OLS  estimate  b; from  the  true  value 

satisfies 

VT(b;  —  B)  =  jun > xa iv pi xu  (10.5.16] 

In calculating  the asymptotic  distribution  of the  OLS estimate  b,. we  usually  assume 
that  the  first  term  in [10.5.16]  converges  in  probability  to  Q  ': 

jun) ) x  | >  Q-'. 

(10.5.17] 

The  second  term  in [10.5.16]  can  be  viewed  as  \/T times  the  sample  mean  of the 
(kK  X  1) vector  x,u,: 

Tr 

T 

Jan) 2 su =  (VT)(/T)  x y,. 

[10.5.18] 

where  y, =  x,u,.  Provided  that  E(u,|x,)  =  0, the  vector  y, has  mean  zero.  We  can 
allow  for conditional  heteroskedasticity,  autocorrelation,  and  time  variation  in the 
second  moments  of y,,  as  long  as 

=  VT-yr. 

exists.  Under  general  conditions,’  it then  turns  out  that 

S =  lim  T: E(y,y;7) 

Tox 

Juv) >> «| =  VFy, 5 NO,  8). 

be 

t= 

Substituting  this  and  [10.5.17]  into  [10.5. 16}, 

VT(b;  —  B) > N(O,  Q-'SQ-'). 
In  light of the  foregoing  discussion,  we  might  hope  to  estimate  S by 

(10.5.19] 

A 
S;  = 

Vv 
Tyr  +  > [ =  q+  | (f+  Papi 

[10.5.20} 

‘See,  for example,  White  (1984,  p.  119).  : 

282  Chapter  10  | Covariance-Stationary  Vector  Processes 

Here, 

. 

T 

ae  =  (1/T)  es (x,d,74,_,7X;_-,), 
4, , is the  OLS  residual  for date  ¢ in a  sample  of size  T (u, 7; =  y,  —  xi br),  and  q 
is a lag length  beyond  which  we  are  willing  to assume  that  the  correlation  between 
x,u,  and  x,_,.u,_,  is  essentially  zero.  Clearly,  Q  is  consistently  estimated  by 
Q,  =  (1/T)27_,x,x;.  Substituting  Q;, and  §; into  [10.5.19],  the  suggestion  is to 
treat  the  OLS  estimate  b;, as  if 

t=v+i 

ne 
where 

V;  =  Q;'S,Q  z- 

b; ~  N(B,  (V,/ T)) 

I 

. 

T 

-1 
(1/T) >> “x;| (WT)  [> a?x,x; 
— 

= 

+$[1-  1 5 (did  Rina  +  Hts dx)| 

 —_ 

, 

vel  o.  q  +1  =  ue 

Bt  sapped  Sa) as ene 2  a 

s 

.. = 

&  (geet  PRA.  ana  veooins  |  SOMES  te  if  Seu  2sagnrG 
that is, the variance of b; is approximated  by RICOWD  Sts  SIMMS  Gihiwbrie: 
(V,/T) 
ee  St 

He Rees  wo  a = 

iat  |S 

o> 

aiken 

| 

2 

| 

= 

; 

Mi. 
: 

: 
ve) 

- 

: 

a 

c 

"3  ye 

“ 

-1 

D Mitty  oXae + Mle  di)  |] DHX 

APA 

, 

A 

A 

, 

Jee 

Shab  >  Sri  BAY  E  geri  £7  |  ie  ee  ta  ewerhde; 

Lt  t© 

s 
UO. 
5.9 

- 

4) 

~ecatar 

'AITICLY 
| 

<¢ 

Ee 

} 

. 

of 

If 

AN  a  ets  ah  hay 

" 

= 

- 

4 

~ 

7" 

, 

~ 

A 

-).  ot 

= 

’ 

e 

i: 

a 

Se  are  Ee Pe a  a  On  Pe 

: 

.  - 

i=  ay  ng 

Neer  i 

—e 
a 

- 

bes 
Ke 

: 

: 

4  ¥ in 

g 

is as  |”  a  =e  — 

where 

1-627  +6z? 

k(z) =  12(1  -  zp 
0 

for0szs3 
for}  szs1 
otherwise. 

For  example,  for  gq  =  2, we  have 

| 

§ =f,  +  if,  +  fh)  +  sf,  +  &%). 

Andrews  (1991)  examined  a  number  of alternative  estimators  and  found  the 

best  results  for  a  quadratic  spectral  kerne!: 

3  [sieez® 

=  cos(6nzi)  | 

mat  =  (67rz/5)?  |  6742/5 

In contrast  to the Newey-West  and  Gallant  estimators,  Andrews’s  suggestion  makes 
use  of all  T —  1 estimated  autocovariance  estimators: 
3 

f 

v 

ae  rn  T-1 

: 

Even  though  [10.5.22]  makes  use  of all  computed autocovariances,  there  is still  a 
bandwidth  parameter  q to  be chosen  for constructing  the  kernel.  For  example,  for 
g  =  2, 

mr, +  = k(vi3)(f,  +  Ph)  =f,  +  0.85,  +  F)) 

+  0.50(f,  +  1)  +  0.14¢(F,  +  Fy)  +  --- 

Andrews  recommended  multiplying  the  estimate  by 7/(T  —  k), where  y,  =  x,d, 
for  u, the  sample  OLS  residual  from  a  regression  with  k explanatory  variables. 
Andrews  (1991)  and  Newey  and  West  (1994)  also offered  some  guidance  for choos- 
ing an  optimal  value  of the  lag truncation  or  bandwidth  parameter  q for  each  of 
the  estimators  of S that  have been  discussed  here. 

The  estimators  that  have  been  described  will  work  best  when  y, has a finite 
moving  average  representation.  Andrews  and  Monahan  (1992)  suggested  an  al- 
ternative  approach  to estimating S that  also  takes  advantage  of any  autoregressive 
structure  to  the  errors.  Let  y, be  a  zero-mean  vector,  and  let S be the  asymptotic 
variance  of the  sample  mean  of y.  For  example,  if we  want  to  calculate  hetero- 
skedasticity-  and  autocorrelation-consistent 
standard  errors  for OLS estimation,  y, 
would  correspond  to  x,4,  where  x, is the  vector  of explanatory  variables  for  the 
regression  and  d, is the  OLS  residual.  The  first  step in estimating S is to  fit a low- 
order  VAR  for  y,, 

oO  ®,y.-»  +  V¥,, 

i  Oy-ct  Ages 

{10.5.23} 
where  v, is presumed  to  have  some  residual  autocorrelation  not  entirely  captured 
by the  VAR.  Note  that  since  y, has  mean  zero,  no  constant  term  is included  in 
[10.5.23].  The ith row  represented  in [10.5.23]  can  be estimated  by an  OLS regres- 
sion  of  the  ith  element  of y, on p lags  of  all  the  elements  of y,  though  if any 
eigenvalue  of |I,,A”  —  @,ar~'  —  @,ar-?  —  - 
-  —  &| =  0 is too close to the  unit 
circle  (say,  greater  than  0.97  in modulus),  Andrews  and  Monahan  (1992,  p. 957) 
recommended  altering  the  OLS  estimates  so  as  to reduce  the  largest  eigenvalue. 
__  The  second  step  in the  Andrews  and  Monahan  procedure  is to calculate  an 
estimate  S*  using  one  of the  methods  described  previously  based  on  the  fitted 

- 

284  Chapter 10  | Covariance-Stationary  Vector Processes 

residuals  ¥, from  [10.5.23].  For  example, 

S$: =  fe +  > [ -  lee: +  fr’), 

[10.5.24] 

where 

rt =  (1/7)  s ¥,¥/_, 

and  where  q is a  parameter  representing  the  maximal  order  of  autocorrelation 
assumed  for v,. The matrix  $# willbe  recognized as an  estimate  of 277s, (0), where  © 
Sy(w) is the spectral  density  of v: 

sw) = (27)7' p dE (vivi- ere. 

Notice  that the ae  en series  y, can  be obtained  from  v, by applying  the following 

_  filter: 

=  [l,  - aE  4 WP  RANGH 

allow »L?)-!v,. 

Thus, teuenhtichtbesbe sperma density oly rented Leathe, seesieal Sree of 
¥  acoonding fo 
ese pind ES af  ae ee  mee = oS  ®, e “Pray 5y(w) 

1  20) treonndis 

Hale ov 

@ 

a.  = ~  &e~ - ®,c2 -_*  Aad Bem}, +} 

estimate of 2m times the spectral density of y at  fre 

200s given 

TOD  ih 

: 
{¢.A.0f} 

~~ 

s 

ae  @, -  6, - +++  —  Op  'st 

"  H,  - -  i'- j ni ae -4 br 

(105.25 

cileiperidet  5 =  su  , 

.  ED  iglteoe> oe 

mm 

et 

=. 

2:  ie,  ee RE 

+  abe 

Oe  le 

- 

- 

a) 

oe o  a  - 
> 

> 
Y, 

ows 

ie 
, 

S 

“ 

= 

Kale ss 

on 

t 

A 

o 
‘ 

i’ 

Y 

hs 

= 

ne 

a 

¥ 

¥ 

and  X, is a  related  [n  x  n(p  — 

1)] matrix.  Let  S denote  the  following  (np  x  np)  matrix: 

a 

0  | Pe  1) 

;  ft  0  | 

and  note  that  its  inverse  is given  by 

Ss!  — 

0 

I 
7] 

‘ 

FE 1)  4 

as  may  be verified  by direct  multiplication.  Premultiplying  a  matrix  by S and  postmultiplying 
by S~'  will  not  change  the  determinant.  Thus,  [10.A.2]  is equal  to 

0  Lina 

ee 

X, 

X, 

Suet  i Ric)  0 

0  A 

~ALp-1  9 

10.A.3 
eg  ee 

Applying  the  formula  for calculating  a  determinant  [A.4.5]  recursively,  [10.A.3]  is equal  to 

(=Ayr-9)X,|  =  (— Ayr,  —  AL,  +  (@/A)  +  (@/2)  + 

+  A") 

—  (-—1)”|I,,A”  ie  ®, A’!  ay  ®,)”~°  ae  ed Be  eae 

®,|. 

Setting  this  to  zero  produces  equation  [10.1.13]. 

8  Proof  of Proposition  10.2. 
reflects  the  cumulative  effects  of the  /th  element  of e€: 

It is helpful  to  define  z,(i,  /) to  be  the  component  of y,,  that 

z,(i, !) 

oe  +  Wir e,, 

i  BS  On  ei; ~2  9  a 

> Wie,  ie 
r=0 

[10  A  4| 

where  yw“? denotes  the  row  i, column  / element  of the  matrix  W,..  The  actual  value  of the 
ith  variable  y,, is the  sum  of the  contributions  of each  of the  / =  1, 2,...  , 
1 components 
of €: 

=  pe 

»> z,(i,  1). 

[10.A.5] 

The results of Proposition  10.2 are  all established  by first demonstrating  absolute  summability 
of the  moments  of  z,(i,  /) and  then  observing  that  the  moments  of y, are  obtained  from 
finite  sums  of these  expressions  based  on  z,(i,  /). 

Proof of (a).  Consider  the  random  variable  z,(i, /)-z,_,(j,  m), where  i, |, j, and  m  represent 
arbitrary  indices  between  | and  n  and  where s is the  order  of the  autocovariance  of y that 
_ 
is being calculated.  Note  from  {10.A.4]  that 

E{z,(i,  !)-z,  ,(j, m)}  =  E {| ee I x  3 toss | or 

* 

i  b> > {Wry}  EXe,, 7  rem i  m? 

r-0  v=0 

The  expectation  operator  can  be  moved  inside  the  summation  here  because 

YY woos  =  & d wer wer  =  {> wer} x  {3 waa} <= 

r=0  v= 

Now,  the  product  of e's in the  final  term  in [10.A.6]  can  have  nonzero  expectation  only  if 
the  e’s have  the  same  date,  that  is, if  r =  s  +  v.  Thus,  although  [10.A.6]  involves  a sum 
over  an  infinite  number  of values  of r, only the  value  at  r =  s  +  v contributes  to this sum: 

E{z,(i,  !) *Zyag(Js  m)}  oo  2 {yi7  2  wn} j  E{e,,-.- ae  an  2 WW  Oi  [10.A.7] 

ve=0 

where  a,,,  represents  the  covariance  between  e¢,, and  e,,,  and  is given  by the  row  /, column 
m  element  of 2. 

The  row  i, column  j element  of I’, gives the  value  of 

Vy =  E(y,  -  HM Yja-s  —  p,). 

286  Chapter  10  | Covariance-Stationary  Vector  Processes 

Using [10.A.5]  and  [10.A.7]},  this  can  be expressed  as 

E(¥,  ~  MMos  ~  Hy) =  E  b zi, lf Ss. 2,  m|} 

| 

= 

M: 
~ Ui]  -  =  i  - 

E{z,(i,  !)-z,_.(j,  m)} 

[10.A.8] 

I  M: M: M« 

~ v]  -  = =  ‘  -  -  i]  - — 

( 

Wi ¥ Poon 

Ms: 

iM 
i]  ~ - ~ i 

= 

i) 

wi z Vjn Fon: 

Tia  T., ="... Vi  Vin Tim is the  row  i, column  j element  of W,,,QW;.  Thus,  [10.4.8] 
states  that  the  row  /, column j element  of ri is given  by the  row  i, column  / element  of 
=. 0: ai  as wok  wk in part (2). 

Proof of (b).  Det ee es pe ar in ae in i‘ A. af 

eae it Ot: ma areas eiajke $57 "ie 

“Satis st  ‘sees 3  jl, mis 3S weed  tp  i vite tof. 
tft? AGIGHB  8  gnivtove:  esr?  si = tied 
;  [10.0.9 

-  =  loinl 2 iol  |e 

ad cele  teum  doihe  x 
fos. 110-4 Rah trees 
os 

= 

5 

. 

=. 

tort  2xI0¥' 

fi)  te loert 

. 

Proof of (c). 
that 

Essentially  the  identical  algebra  as  in the  proof of Proposition  7.10  establishes 

7 

E|z,  (i, Z,,(é>,l2)  62h by, Ly) °Z,  (bas 1,)| 

=f  i ys We Penn  aa >  eure} 

a8. 

°* 

vye 

13, wire,  ae > We ll 

(10.A.11] 

=>  yd  DS lw 

yO  oy  Oo  <0  rye 

x  Ele,,.,, Ebates  vy Ehy  vy Elaty  het 

Now, 

<x, 

Fy,  Wee  G3) =E  a, +  5, Z,(i- 1) +  x Z,(is. | 

of.  +  '3 z, (4. In} 

M,,  +  ro! z, (is. | 

ia 

=E  {i Pas tlt -{ I +  D bealis 1 
flaad +  3 tests tol} flan +  3 tein tat} 

But  this  is a  finite  sum  involving  terms  of the  form  of [10.A.11]—which  were  seen  to  be 
finite—along  with  terms  involving  first  through  third  moments  of  z,  which  must  also  be 
finite. 

Proof  of (d).  Notice  that 

z,(i,  4)-z,  (j, m) = > S O86  ensiemcatizae 

The  same  argument  pai to  [7.2.14]  can  be_used  to  establish  that 

(1/T)  > z,(i. 1)+z,  i.  m)  > E{z,(i.  1) -z,  Us. m)}. 

{10.A.12] 

To  see  that  [10.A. - implies  ergodicity  for  the  second  moments  of y,  notice  from 

[10.A.5]  that 

(1/T)  > Yuin  5 =  (WT) ap *  : z,(i, || +  = z, 

m)| 

=  ye,  +  my > Jaw Ps 2, .AJs m| +  p, > Jam  z,(i, 0] 

+  ey Jum > 2,(i, Nz,  i. m| 

aim  +  oy ~ 

Elz,  .(j,m)]  +  w, > E[z,(i, )] 

+  DD Eledie De, im) 

“+a  Ssco)far S00} 

as  Claimed. 

@ 

™  EL yi¥in  als 

288  Chapter  10  | Covariance-Stationary  Vector  Processes 

® Proof of Proposition  10.3. 

Writing  out  [10.2.11]  explicitly, 
H(L)W(L)  =  (0°  +  HL)  +  HL°  +  HL!  +  °°) 

x  (WL  +  WL!  +  VL  +:  *), 

from  which  the  coefficient  on  L*  is 

(10.A.13] 
B,  =  H.W,  +  H,  WW,  +  A  w, +  oe 
Let  b\‘? denote  the  row  i, column  j element  of  B,,  and  let  h‘*' and  i}?  denote  the  row  /, 
column j elements  of  H,  and  W,.  respectively.  Then  the  row  /, column  j element  of  the 
matrix  equation  [10.A.13]  states  that 

si 

oy  =  rz Arps ~ +  YY al  tapi h is +2 hi  2)  a dole  tu 

a“ 

nm 

% 

ae  S  S hit  yt), 

‘ 

»  Ou  * 

Thus. 

4 

« 

ne 

> b=  Sa)  2 hin”  " 

=  3 > > Ih Rin  Wil 
Tie : +35 Bis 3  a, 

«vv  Om  | 

Ug ee SyG  uM yeZ  Sues  DL  Hay 2s yt 

{10.A.14] 

. 

“SRN  OW: tessa ltrsins  ody  ye e's  Se  a  a  ha 
tte  tae 
at  ead 188). .  sblnnd {23 

Oi  | gests 

=  ah ~,y  ie 

atadch 

Le 

— 

¢ 

“ 

.2 

; 

Be  i 
ft)  tu  ob  eowens  aie  aor isiee 

ee 

ee 

ae 

Vig 

* 

;  A 

ee 

_ 

lie 

relirac  (3S PAE  -4 
sive  -_ 

sa 

Me!  +3 

be 

: 

2 

Ee pt iz  "IG 
1}  (Pos  rig gYb App.  > (tf  UD  $2 i=  4 t  22? $54 
CE HE es eres Gia 

A} 

pee  2 

comes 

yp 

7 

‘ 

‘ 

ews 

y 

nee 
Chapter  10  Exercises 

Consider  a  scalar  AR(p)  process  (n  = 

10.1. 
(p  x  1) vector  consisting  of  the  variance  and  first  (p  — 

1).  Deduce  from  equation  [10.2.19]  that  the 

1) autocovariances, 

Yu 
Yi 

Y2 

’ 

Yp  1 

can  be  calculated  from  the  first  p  elements  in  the  first  column  of  the  (p>  x  p°)  matrix 
o{l,:  —  (F®  F)]  '  for F the  (p  x  p) matrix  defined  in  equation  [1.2.3]  in  Chapter  1]. 
10.2. 

Let  y,  =  (X,.  Y,)’  be  given  by 

X,  =  €,  +  GE,  , 

Y¥, 

=.jX%,5 
where  (¢,.  «,)'  is vector  white  noise  with  contemporaneous  variance-covariance  matrix  given 
by 
E(;)  ad J  [° 0 
E(ue,) 

0  of] 

E(u;) 

FU; 

(a)  Calculate  the  autocovariance  matrices  {T,}(.  _,.  for  this  process. 
(b)  Use  equation  [10.4.3]  to  calculate  the  population  spectrum.  Find  the  cospectrum 

between  X and  Y and  the  quadrature  spectrum  from  X to  Y. 

(c)  Verify  that  your  answer  to  part  (b) could  equivalently  be calculated  from  expres- 

sion  [10.4.45]. 

(d)  Verify  by integrating  your  answer  to  part  (b)  that  [10.4.49]  holds;  that  is,  show 

that 

Safe 
1 

Syx(w)  mk  dep 

al  J 7  Syx(w)  fs  vm 

=  ‘i 
() 

fork  =  1 

for  other  integer  k. 

Chapter  10 References 

Andrews,  Donald  W.  K.  1991.  ‘‘Heteroskedasticity  and  Autocorrelation  Consistent  Co- 
variance  Matrix  Estimation.”’  Econometrica  59:817-S8. 

and  J.  Christopher  Monahan.  1992.  ‘An  Improved  Heteroskedasticity  and  Auto- 

correlation  Consistent  Covariance  Matrix  Estimator.’  Econometrica  60:953-66. 
Fuller,  Wayne  A.  1976.  Introduction  to  Statistical  Time  Series.  New  York:  Wiley. 
Gallant,  A.  Ronald.  1987.  Nonlinear  Statistical  Models.  New  York:  Wiley. 
Hansen,  Lars  P.  1982.  “Large  Sample  Properties  of Generalized  Method  of Moments  Es- 
timators.”’  Econometrica  50):1029-54. 
Newey,  Whitney  K.,  and  Kenneth  D.  West.  1987.  “A  Simple  Positive  Semi-Definite,  Het- 
anit  and  Autocorrelation  Consistent  Covariance  Matrix.”  Econometrica  5S: 

and | 

.  1994.  “Automatic  Lag  Selection  in  Covariance  Matrix  Estimation.” 

Review  of Economic  Studies  61:631—54. 
Sims,  Christopher  A.  1980.  “Macroeconomics  and  Reality."’  Econometrica  48:1-48. 
oo Halbert.  1984.  Asymptotic  Theory  for Econometricians.  Orlando,  Fla.:  Academic 
ress. 

i 

290  Chapter  10  | Covariance-Stationary  Vector  Processes 

11 

Vector  Autoregressions 

The  previous  chapter  introduced  some  basic  tools  for describing  vector  time  series 
processes.  This  chapter  looks  in greater  depth  at  vector  autoregressions,  which  are 
particularly  convenient  for  estimation  and  forecasting.  Their  popularity  for  ana- 
lyzing  the  dynamics  of economic  systems  is due  to  Sims’s  (1980)  influential  work. 
The  chapter  begins  with  a  discussion  of  maximum  likelihood  estimation  and  hy- 
pothesis  testing.  Section  11.2  examines  a concept  of causation  in bivariate  systems 
proposed  by Granger  (1969).  Section  11.3  generalizes  the  discussion  of Granger 
causality  to  multivariate  systems  and  examines  estimation  of restricted  vector  au- 
toregressions.  Sections  11.4  and  11.5  introduce  impulse-response  functions  and 
variance  decompositions,  which  are  used  to  summarize  the  dynamic  relations  be- 
tween  the  variables  in  a  vector  autoregression.  Section  11.6  reviews  how  such 
summaries  can  be  used  to  evaluate  structural  hypotheses.  Section  11.7  develops 
formulas  needed  to  calculate  standard  errors  for  impulse-response  functions. 

11.1. Maximum  Likelihood  Estimation  and  Hypothesis 
Testing for an  Unrestricted  Vector  Autoregression 

The  Conditional  Likelihood  Function 
for a  Vector  Autoregression 

- 

Let y, denote  an  (n x  1) vector  containing  the values  that  1 variables  assume 
at date  t. The  dynamics  of y, are  presumed  to be governed  by a pth-order  Gaussian 
vector  autoregression, 

y,  =  ¢  +  Diy,.,  +  P.y,-2  + 

°°  +  @Y,_»  +  &, 

[11.4.1] 

with  e, ~  i.i.d.  N(O,  2). 

Suppose  we  have  observed  each  of these  n variables  for (T + p) time  periods. 
As  in the  scalar  autoregression,  the  simplest  approach  is to,condition  on  the  first 
» Yo) and  to base  estimation  on  the  last 
p observations  (denoted  y_,,,;. 
.  yr).  The  objective  then  is to  form  the  con- 
T observations  (denoted  y,,  y2..  . 

¥.-»+2.-  - 

. 

- 

_  ditional  likelihood 

Pres.  tec  c¥ilVo.¥  2  1.0..¥-pe  V7 fat  *  2s  Yi1¥o»  Fs] 

re  ow  f  Y-pe+is  0)  [11.1.2] 

and  maximize  with  respect  to  8, where @ is  a vector  that  contains  the  elements  of 
c,  ®,, ®,,...,  ®,, and  .  Vector  autoregressions  are  invariably  estimated  on 
the basis of the conditional  likelihood  function  [11.1.2]  rather  than  the  full-sample 
291 

unconditional  likelihood.  For  brevity,  we  will  hereafter  refer  to  [11.1.2]  simply  as 
the  “likelihood  function”  and  the  value  of @ that  maximizes  [11.1.2]  as  the  “max- 
imum  likelihood  estimate.” 

The  likelihood  function  is  calculated  in  the  same  way  as  for  a  scalar  auto- 
regression.  Conditional  on  the  values  of y observed  through  date  ¢  —  1, the  value 
of y for  date  ¢ is equal  to  a  constant, 

c+  Py, , +  Dy,.  +--+:  +  ®y,-p> 

[11.1.3] 

plus  a  N(0,  92) variable.  Thus, 

YAY,—1  ¥i-3s 

F's  Y-p+i 

>  Ne  +  Oy,  yt  Dy.  +  °*°  +  ®,Y,-p):  0}. 

[11.1.4] 
- 

It  will  be  convenient  to  use'a  more  compact  expression  for  the  conditional 
mean  [11.1.3].  Let  x, denote  a  vector  containing  a constant  term  and p lags of eac 
of the  elements  of y: 
: 

R=} 

] 

Yi-1 
¥ro2 

Yi-p 

2 

T° 
[11.1.5] 

Thus,  x, is an  [(np  +  1) X  1] vector.  Let  II’  denote  the  following  [n x  (np  +  1)] 
matrix: 

. 

| 

| 

T'=[¢  ®,  ®,  ---  ®]. 

[11.1.6] 

Then  the  conditional  mean  [11.1.3]  is equal  to  II’x,.  The  jth  row  of II’  contains 
the  parameters  of the  jth equation  in  the  VAR.  Using  this  notation,  [11.1.4]  can 
be  written  more  compactly  as 

YA Yi-1»  Y-25  © 
Thus,  the  conditional  density  of the  rth  observation  is 

+  Y-pay  ~  N(x,  Q). 

+ 

+ 

[11.1.7] 

fray, EX  sh  Mae \  ae  Poe  eee  Y-p+  1s 60) 

=  (2m)-"7|Q-'|"2  exp{(—1/2)(y,  —  M'x)'2-\y,  —  Hx].  E118) 
The  joint  density  of observations  1 through  ¢ conditioned  on  yy,  y_,,  ...  . 

Y_»+1  Satisfies 

FON  MG  AE  On Yr-ty  sees  yilYos  Yoisi+  ©  v0  Y--p+is  6) 

Pi teins re 

ee,  eee)  eee er 

ee  Y—p+i;  9) 

x  FAW  10  2  ¥ po  Yel Yrs  Yi-25+  ++  5  Y—p+is  8). 

Applying  this formula  recursively,  the likelihood  for the  full sample  y;, y7_,..-  . 
y, conditioned  on  yy, y_,,..  . 
densities: 

. 
» Y_»41  is the  product  of the  individual  conditional 

Fyrvi  sin.  VVON  o1ck¥  pe  lY Ts Vr-abys* 
s 

YilYos  Yess  ses  Y-peis  0) 

[11.1.9] 

=  Il fryay,  1.¥;  gxawe  pi  VdVekor  Y;-25  *"t2%  Y-p+is  6). 

The sample  log likelihood  is found  by substituting  [11.1.8]  into  [11.1.9]  and taking 
292  Chapter 11  | Vector Autoregressions 

fg 

£(8)  x  p> log fyi.  v,> ae Y  a 

St  ee  Yi-2>  .  «tt»  Y-p+  6) 

=  —(Tn/2)  log(2m)  +  (T/2)  logiQ~'| 

[1.1.10] 

F 

—  (1/2)  >> e —  H’x,)’'Q~(y,  -  nx). 

Maximum  Likelihood  Estimate  of 11 
Consider  first  the  MLE  of I, which  contains  the  constant  term  ¢ and  auto- 

regressive  coefficients  ®,. This  turns  out  to  be given  by 

1  =  b vai|| 5 xxi] 

[ax (ap  +  1)) 

t=1 

(11.1.11] 

which can  be viewed  as  the  sample  analog  of the population  linear  projection  of 
LRN PAR  sd  Bi equation [4.1.23]).  The jth row  of  I is 

uf  -[3 vas] [3 ox] 

[i x<(np +L 

4  “Ty 

|  which is just the eer coefficient  vector from an OLS. eh ddait of yy on 2 
Thus,  maximum  li CSE Re estimates  of the coeffi icients for the jth equation of a 
eek end e  found id  saeeniee of yj, On a constant term and p lags of all 

ir 

m appearing  in  the last term in (11.1. 0) as 
ee mo  siet-as> [Ti.4  11] 19d3  aulev  tzoeine 
|  i ese ‘YG bosiminien  gt (E111)  sone 

md 

ane 

> 

on 

- 

> 

. 

x, . Aegigg, ane Cae 

— 

by applying  the  “‘trace’’  operator: 

al 
T 
> @/2-\(  —  M)’x, 

i=l 

T 

“ 

trace|  ead  -  ny 

=I 

trace| 5 o-  (I  -  myx; |  [11.1.16] 

T 

~ 

(=1 

trace| 0-1  -  Ml)’  > xéi| 

Y fe 

=! 

But  the  sample  residuals  from  an  OLS  regression  are  by  construction  orthogo- 
nal  to  the  explanatory  variables,  meaning  that  >7_,x,é,  =  0  for  all j and  so 
=, x,é;  =  0.  Hence,  [11.1.16]  is identically  zero,  and  (11.1.15]  simplifies  to 

T 

—  I’x,)'n-!   —  ix, 

» a.  ree 

3) 

[11.1.17] 

T 

Ms 
=  ¥ ¢/0-'2,+  > x1  -  MOQ-(M  —-  M’x,. 

T 

% 

t= 

=! 

Since  2  is  a  positive  definite  matrix,  ~'  is  as  well.'  Thus,  defining  the 

(n  x  1) vector  x7 as 

the  last  term  in  [11.1.17]  takes  the  form 

x? =  (Il —  W)’x, 

> x(t  —  M)a-(  —  M)'x,  =  > [x*]'Q-'x?. 

T 

t=1 

T 

=! 

This  is positive for  any  sequence  {x/*}/_,  other  than  x*  =  0 for  all  ¢.  Thus,  the 
smallest  value  that  [11.1.17]  can  take on  is achieved  when  x?  =  0, or  when  II  = 
HI.  Since  {11.1.17]  is  minimized  by  setting  II  =  Il,  it  follows  that  [11.1.10]  is 
maximized  by setting  II  =  II, establishing  the  claim  that  OLS  regressions  provide 
the  maximum  likelihood  estimates  of the  coefficients  of  a vector  autoregression. 

Some  Useful  Results  on Matrix  Derivatives 

The  next  task  is to  calculate  the  maximum  likelihood  estimate  of 2.  Here 
two  results  from  matrix  calculus  will  prove  helpful.  The  first  result  concerns  the 
derivative  of a quadratic  form  in a matrix.  Let a,, denote  the row  i, column j element 
of an  (n x  n) matrix  A. Suppose  that the matrix  A is nonsymmetric  and unrestricted 
(that  is, the  value  of a,; is unrelated  to  the  value  of a,, when  either  i #  k or j #  /). 
Consider  a quadratic  form  x’Ax  for  x  an  (n  X  1) vector.  The  quadratic  form  can 
be written  out  explicitly  as 

from  which 

x’‘Ax  =  > > X;jX;, 

=! 

j=) 

~itaa! 
Eee  a  ey 

{11.1.18] 

(11.1.19} 
11.1.19 

ie be immediately  from  the  fact  that  ~'  can  be written  as  L'L  for L a nonsingular  matrix 

as  in 

[8.3.1]. 

294  Chapter  11  | Vector  Autoregressions 

Collecting  these  n? different  derivatives  into an  (n x  n) matrix,  equation  [11.1.19] 
can  conveniently  be expressed in  matrix  form  as 

dx’Ax 
aA 

: 
OX 

{11.1.20] 

The  second  result  concerns  the  derivative  of the  determinant  of a  matrix.  Let 
A be a nonsymmetric  unrestricted  (n x  n) matrix  with  positive  determinant.  Then 

d log|A 
SL 
0a; 

2 

where  a” denotes  the  row  j, column  i element  of A~'.  In  matrix  form, 
d log|A| 
—————_  = 

(A')~'. 

| 

(11.1.21] 

‘A. 

_ 

To derive  [11.1.22],  recall  the formula  for  the  determinant  of A  (equation 

[A.4.10]  in the  Mathematical  Review,  Appendix  A, at  the  end  of the  book): 

|A|  =  2 (Vail Agl, 

{11.1.23] 

where  A; denotes  the  (n —  1) x  (m —  1) matrix  formed  % on row  i and 
column j from A. The derivative  of [11.1 .23] with respect  to a 

| 

ir 

in the matrix Ay It follows that 

BOC Eth a,  | 

g  su 
be  ay 

- 
23:.cniga 

pt) 

y 

adi! Hts rg | 4 

ereonme 
h  5 mn seem fo equation [A. 4. 12] ag the Bae Peaami i element 
A~', 

asc  siege  manmraas ni os ‘ pn 

"=  (WAL): leg Danse nee: 

: 

of 

The  matrix  2 that  satisfies  [11.1.27]  maximizes  the  likelihood  among  the class 
of all  unrestricted  (n  x  n) matrices.  Note,  however,  that  the  optimal  unrestricted 
value  for  2  that  is specified  by [11.1.27]  turns  out  to  be  symmetric  and  positive 
definite.  The  MLE,  or  the  value  of  2  that  maximizes  the  likelihood  among  the 
class  of all  symmetric  positive  definite  matrices,  is thus  also  given  by [11.1.27): 

M  =  (1/T)  > @é/. 

T 

t=l 

The  row  i, column  i element  of 2 is given  by 

6? =  (1/T)  > &, 

T 

t=] 

(11.1.28] 

(11.1.29] 

which  is just  the  average  squared  residual  from  a  regression  of the  ith  variable  in 
constant  term  and p lags of all  the  variables.  The  row  i, column  j 
the  VAR  on  a 
element  of 22 is 

E 

6, =  (1/T)  D> 6, £); 

[11.1.30] 

which  is the average  product of the OLS residual  for variable  i and the  OLS residual 
for variable  j. 

Likelihood  Ratio  Tests 

To  perform a likelihood  ratio  test,  we  need  to  calculate  the  maximum  value 

achieved  for [11.1.25].  Thus,  consider 

—  £(Q, TI) =  —(Tni2)  log(27)  +  (7/2)  log|Q-"| 

pi 

—  (1/2)  > #/0-'é, 

11.1.31 
[ 

for  2 given  by {11.1.28].  The  last  term  in  [11.1.31]  is 

t=1 

(1/2)  > €/Q-"é,  =  (1/2)  trace| AD ea, 

T 

T 

a 

A 

t=1 

t=1 
- 
=  (1/2)  trace| > aes; | 
t=] 

=  (1/2)  trace[Q.-'(TAQ)) 

(1/2)  trace(7:I,,) 

=  Tn/2. 

Substituting  this  into  [11.1.31]  produces 

£(Q, Ml) =  -(Tn/2)  log(2m)  +  (772) top|" “  (Tnl2).  (11.1.33] 
This  makes  likelihood  ratio  tests  particularly  simple to perform.  Suppose  we 
want  to test the null hypothesis that a set of variables was generated  from a Gaussian 
VAR  with po lags against  the  alternative  specification  of p,  > po lags. To estimate 
the  system  under  the  null  hypothesis,  we  perform  a set  of  n OLS  regressions  of 
each variable  in the system  on  a constant  term  and’6n py lags of all the  variables 
in the system.  Let y =  (1/7) D7, €,(po)[@,(po)]’  be the variance-covariance  matrix 
of the residuals  from  these  regressions.  The  maximum  value  for the log likelihood 
296  Chapter 11  | Vector Autoregressions 

under  H, is then 

L§  =  —(Tn/2)  log(2m)  +  (7/2)  log|Q="|  -—  (Tn/2). 
Similarly,  the System  is estimated  under  the  alternative  hypothesis  by OLS  regres- 
sions  that  include  p,  lags  of all  the  variables.  The  maximized  log likelihood  under 
the  alternative  is 

LF  =  —(Tn/2)  log(2m)  +  (T/2)  logiQ="|  —  (Tn/2), 
where  @2,  is  the  variance-covariance  matrix  of  the  residuals  from  this  second  set 
of regressions.  Twice  the  log likelihood  ratio  is then 

ALF  —  L5)  = 2{(T/2)  log|M>"|  —  (7/2)  tog|A5"}} 

T 

log(1|Q,))  y  T log(1/Q4l) 

~T log|Q,|  +  T log|Q,| 
T{log|Q|  —  log|®, |}. 

(11.1.33] 

Under  the  null  hypothesis,  this  asymptotically  has  a  x? distribution  with  degrees 
of freedom  equal  to  the  number  of restrictions  imposed  under  H,.  Each  equation 
in the  specification  restricted  by Hy has  (p,  —  py) fewer  lags on  each  of n  variables 
compared  with  H,;  thus,  Hy,  imposes  n(p,  —  py)  restrictions  on  each  equation. 
Since  there  are  n  such  equations,  H, imposes  n7(p,  —  py) restrictions.  Thus,  the 
magnitude  calculated  in [11.1.33]  is asymptotically  x? with  n2(p,  —  po) degrees  of 
freedom. 

For  example,  suppose  a  bivariate  VAR  is  estimated  with  three  and  four 
lags  (n  =  2, po  =  3, p,  =  4).  Say  that  the  original  sample  contains  50  obser- 
,  Ya)  and  that  observations  1 
vations  on  each  variable  (denoted  y_;,  y_2,  . 
through  46  were  used  to  estimate  both  the  three-  and  four-lag  specifications  so 
that  T  =  46.  Let  é,(po)  be  the  sample  residual  for  observation  t from  an  OLS 
regression  of  y,  on  a  constant,  three  lags  of  y,,,  and  three  lags  of  yz,. 
2.5,  and 

2.0,  (1/7) 2/7, [é(po)P  = 

. 

. 

-  Suppose  that  (1/7) 27, [é,(po)P  = 
(1/7) 27. €1,(Po)é2(Po)  =  1-0.  Then 

A 
2)  = 

2.0 

1.0 

O25 

and  log|{,|  =  log 4  =  1.386.  Suppose  that  when  a  fourth  lag is added  to  each 
regression,  the  residual  covariance  matrix  is reduced  to 

oe  i A 
0.9 

2.2 

for which  log|®,|  =  1.147.  Then 

(Lt  —  L*)  =  46(1.386  —  1.147)  =  10.99. 
The  degrees  of freedom  for  this  test  are  27(4  —  3) =  4.  Since  10.99  >  9.49  (the 
5% critical  value  for a y7(4) variable),  the null hypothesis is rejected.  The dynamics 
are not completely  captured  by a three-lag  VAR, and a four-lag  specification  seems 
preferable. 

Sims (1980, p.  17) suggested  a modification  to the likelihood  ratio test  to take 

into  account  small-sample  bias.  He  recommended  replacing  [11.1.33]  by 

(11.1.34] 
(T —  k){log|M|  -  log}, 
where  k  =  1  +  np,  is the  number  of parameters  estimated  per  equation.  The 

11.1.  Estimation  and Hypothesis  Testing for an  Unrestricted  VAR  297 

adjusted  test  has  the  same  asymptotic  distribution  as  [11.1.33]  but  is less  likely  to 
reject  the  null  hypothesis  in  small  samples.  For  the  present  example,  this  test 
statistic  would  be 

(46  —  9)(1.386  —  1.147)  = 

and  the  earlier  conclusion  would  be  reversed  (H, would  be  accepted). 

Asymptotic  Distribution  of 1 
The  maximum  likelihood  estimates  I and  Q will  give consistent  estimates  of 
the population  parameters even  if the true  innovations  are  non-Gaussian.  Standard 
errors  for II can  be based PP, the usual OLS abe  ao as the following  proposition 
demonstrates. 

Proposition Fea:  Let 
on  a Fema sit. fii BP Ps 5 + 1  Riadoca tere 

t+ talon +  ee 

_ 

| 

ae  -  — — 2? - ay rer = oO  : 

lie outside the unit circle.  Let  k =np + K reas let x! be the  x Kk) vector 

a ORs  oun Pte eee wes 

po 

ieee 

re  3 :  oc 3 rf Pra: ; 

Phe SELMA oa i. 

- eee 

mie 

mT 

= i 

- 

a  ne 
m 

—e 

(c)  2,4  0; 
(dq)  V7(#,  - 

ye 

product. 

i 

tm)  —  NO,  (2  @®  Q-')),  where  ®  denotes  the  Kronecker 

A  proof  of  this  proposition  is  provided  in  Appendix  11.A  to  this  chapter. 
If we  are  interested  only  in  7,7,  the  coefficients  of  the  ith  regression  in  the 

VAR,  result  (d)  implies  that 

VI(i;7  —  1) > NO,  020-'), 

[11.136] 

where a;  =  E(e7)  is the  variance  of the  innovation  of the  ith  equation  in the  VAR. 
But  a; is estimated  consistently  by  @? =  (1/T)2/_,  é7, the  average  squared  residual 
from  OLS  estimation  of this equation,  Similarly,  Q~'  is estimated  consistently  by 
[(1/T)>7_ , x,x/]~'.  Hence,  [11.1.36]  invites  us  to  treat  %; approximately  as 

it, ~  n(x. G? > <x; | ) 

[11.1.37] 

T 

-1 

r=1 

But  this  is  the  standard  OLS  formula  for  coefficient  variances  with  s?  = 
{1(T  —  k)]=7_,  é2 in  the  standard  formula  replaced  by the  maximum  likelihood 
estimate  &?  in  [11.1.37].  Clearly,  s? and  G? are  asymptotically  equivalent,  though 
following  Sims’s  argument  in  [11.1.34],  the  larger  (and  thus  more  conservative) 
standard  errors  resulting  from  the  OLS  formulas  might  be preferred.  Hence,  Prop- 
osition  11.1  establishes  that  the  standard  OLS  t and F statistics  applied  to  the 
coefficients  of any  single  equation  in  the  VAR  are  asymptotically  valid  and  can  be 
evaluated  in the  usual  way. 

A more  general  hypothesis  of the  form  Ra  = r involving  coefficients  across 
different  equations  of the  VAR  can  be  tested  using  a  generalization  of the  Wald 
form  of  the  OLS  x? test  (expression  [8.2.23]).  Result  (d)  of  Proposition  11.1 
establishes  that 

VI(Ri;  —  r) > (0, R(X @ Q-  oR’) 

In  the  light  of  results  (a)  and  (c),  the  asymptotic  distribution  could  equivalently 
be described  as 

VI(Rit,  —  1) > N (0. R(Q,  @ QR’), 

where  0, =  (1/T) 37. , €,é/ and  Q,  =  (1/T) 27, x,x/. Hence,  the following  statistic 
has  an  asymptotic  x? distribution: 

| 

x*(m) 

T(Ray  ~  r)'(RQy @ QF')R')-  (Rat —  ¥) 
(Ri,  —  r)'(R(Q,@  (TQr) IR’)  "(Rit — 

[1.1.38] 

=  (Ri;  —  r)’ {R ja,8 (3 xxi) x'} one —  r). 

The  degrees  of freedom  for this  statistic  are  given  by the  number  of rows  of R, or 
the number of restrictions  tested. 

jo example, suppose we  wanted  to test  the hypothesis  that the constant term 
in the first equation  in the  VAR  (c,) is equal  to  the  constant  term  in the  second 
anon (c,). Then  R  is a (1 x ils vector with  unity  in the  first  position,  —  1. in 
Wl Estimation and Hypothesis  Testing for an Unrestricted  VAR  299 

the  (k  +  1)th  position,  and  zeros  elsewhere: 

R=[i"0  @  =< 

@ 

+4 

°O 

07°  ++ 

OF 

To  apply  result  [11.1.38],  it  is  convenient  to  write  R  in  Kronecker  product  form 
as 

R  =  R,  @R,, 

(11.1.39] 

where  R, selects  the  equations  that  are  involved  and  R,  selects  the  coefficients. 
For  this  example, 

R,  =[1  -1  0  0 

0] 

(1  xa) 

R,  ={1  0  0  0 

0) 

(I  xk) 

We  then  calculate 

n| 0 ® (> xx] |e =  (R, ® Ry| a ® (> =x;]  Jew: @® R}) 

=  (R,OR;,)  ® IR. (> xx)  Ri| 

=  (G7  —  262  +  63) @ €"', 

where  G2  is the  covariance  between  é,, and  é,, and  é''  is  the  (1,  1) element  of 
(=7_,x,x;)~'.  Since  €'!  is a  scalar,  the  foregoing  Kronecker  product  is  a  simple 
multiplication.  The  test  statistic  [11.1.38]  is then 

2 

OY  GE 

= 

(¢,  —  ¢2)° 

ia  +  DE" 

Asymptotic  Distribution  of Ce) 

In  considering  the  asymptotic  distribution  of the  estimates  of variances  and 
covariances,  notice  that  since  2 is symmetric,  some  of its elements  are  redundant. 
Recall  that  the  “‘vec’’  operator  transforms  an  (nm  Xx  n) matrix  into  an  (n*  x  1) 
vector  by stacking  the  columns.  For  example, 

{Oy  G2  Oy 
O23) 

G22 

vec  | Oy; 

3,  G32 

Ar, 

= 

712 
O27 

02 

|. 

{1 OS | 40} 

An analogous  “‘vech"’  operator  transforms  an (n X  n) matrix  into an  ([n(m  +  1)/2] 
x  1) vector  by  vertically  stacking  .thesé elements  on  or below  the  principal 

300  Chapter  11  | Vector  Autoregressions 

diagonal.  For example, 

Fy, 

Fir»  Dy 

03, 

31 

@C 

32. 

F733 

* 

a1; 

02; 
ya 

P22 
O32 
033 

Proposition 11.2:  Let 

y 

=ce+  @My,_,  + By, , +--+  +  ®,y,-»  +  &, 

where  ©, ~  i.i.d.  N(O,  Q) and  where  roots of 

IL, —  Bz  —  ®z? 

--+- - @ 2"  =0 

lie outside  the unit  circle.  Let itp, My, and  Q be as  defined  in  Proposition  11.1. 
Then 

| 

ne  tae  ee “([° me  0  ) 
|  VT[vech(2)  -  vech()] 

Lo]}’ 

22. 

0 

Let o; denote the row  i, column  j peniaie of Q:; for example,  o,, is the variance of 
£,,-  Then the element  of X, corresponding  to  the covariance  between  6; and G;,, is 
given by (00%. +  Tn  ta j.l.m  =1,2,...,2,  including i =  j  =1= 
m. 

| 

| 

s = 
eT  ——E————<— 

D0 i tion i Wialcy that 

a a  Kae 201272) 

For  example,  for  n  =  2,  equation  [11.1.43]  is, 

a 

1  0  0 
|| 
OE 
O71 
01  0  en] = 
@  0  J 

2 

O1; 
oe 
O10 |" 
Orn 

11.1.44] 

Further,  define  D;  to be  the  following  [n(n  +  1)/2  x  n?]  matrix:? 

D,;  =  (D,,D,)~  'D;,. 

[11.1.45] 

Notice  that  D*D,  =  14,412.  Thus,  premultiplying  both  sides  of  [11.1.43]  by D; 
reveals  D*  to  be  a  matrix  that  transforms  vec({)  into  vech(@2)  for  symmetric  2: 
[11.1.46] 
vech(Q)  =  D}  vec(Q). 

For  example,  for  n  =  2, equation  [11.1.46]  is 

or 

o,,| 
O22 

l 

= 

3 
TO a 

10 
0  0 

os, ses 

a 

(11.1.47] 

It turns  out  that  the  matrix  ,, described  in  Proposition  11.2  can  be  written 

as 

) 

y,.  =  2Dt(2 @ 2)\(D;)’. 

(11.1.48] 

For  example,  for  n  =  2, expression  [11.1.48]  becomes 

100  0 
2D7  (Q  &® QHDF)'  =  2)  9  5  +  O 
0  0  OA 

AyF, 

AFiAMi2 

AizFi1  = Fi2Fi2 

eu)  0 

F192)  = %19%22  F129  = F%12922 

OTs  6 

021%) 

F21%12_ 

92291,  9 F22F12 

0.4.9) 

92;92; 

F%21%22 

F229%21 

%22%22 

00  1 

o2 
207; 

20) 102 

2 
2072. 

iss 
=  | 204,012 

2 
O1022  +  Fiz  20,2022], 

2 
2072 

202022 

2 
)  20% 

which  reproduces  [11.1.42]. 

11.2.  Bivariate  Granger  Causality  Tests 

One  of the  key questions  that  can  be addressed  with  vector  autoregressions  is how 
useful  some  variables  are  for forecasting  others.  This  section  discusses  a particular 
summary  of the  forecasting  relation  between  two  variables  proposed  by Granger 
(1969)  and  popularized  by Sims  (1972).  A  more  general  discussion  of a  related 
question  in larger  vector  systems  is provided  in the  following  section. 

“It can  be shown  that  (D/D,,)  is nonsingular.  For  more  details,  see  Magnus  and  Neudecker  (1988. 

pp.  48-49). 

‘Magnus and Neudecker  (1988, p. 318) derived  this expression directly  from  the information  matrix. 

302  Chapter  11  | Vector  Autoregressions 

Definition  of Bivariate  Granger  Causality 
The question  investigated  in this section  is whether  a scalar y can  help forecast 
another  scalar  x.  If it cannot,  then  we  say  that  y does  not  Granger-cause  x.  More 
formally,  y fails  to  Granger-cause  x  if for  all  s >  0 the  mean  squared  error  of a 
.  .) is the  same  as  the  MSE  of a  forecast  of 
forecast  of x,,,  based  on  (x,, x,,,  . 
.  .).  If we  restrict  ourselves  to 
x,,,  that  uses  both  (x,, x,_,,  .  . .) and  (y,, y,_,,  . 
linear  functions,  y fails  to  Granger-cause  x  if 

MSE(E(x,..1%,,  X;-ts+-  )) 

: 

ay  Ser  ok a 

11.2.1] 

rs 

| B 

Equivalently,  we  say  that  x is exogenous  in the time  series  sense  with  respect  to y if 
[11.2.1] holds.  Yet a third expression  meaning  the same  thing is that y is not linearly 
informative  about future  x. 

Granger’s  reason  for  proposing  this  definition  was  that  if an  event  Y is the 
cause of another  event  X, then  the event  Y should  precede  the event  X.  Although 
one  might  agree  with this  position  philosophically,  there  can  be serious  obstacles 
to practical implementation  of this  idea  using  aggregate  time  series  data,  as  will 
be seen  in the examples considered later  in this section.  First, however,  we explore 
the mechanical  implications  of Granger  causality  for the time series  representation 
of a bivariate  system. 

: 

_  Alternative  Implications  of Granger  Causality 
Ina  bivariate VAR — x va y aa not  Granger-cause  x if the 

a 0 1 1 pK 

33 iy  gt  v. 

ite ae at eae We 

+  2  2 a 

cies 
tof  x 

nead 

forecast. 

oor  sk ‘  #43 

LO  See,  ee  ae  en 

F electe 

Pix 1—p+l 9 

®, is lower  triangular  for  all  j, then  the  moving  average  matrices  W,  for  the  fun- 
damental representation  will  be  lower  triangular  for all s.  Thus,  if y fails  to Granger- 
cause  x,  then  the  MA(*)  representation  can  be  written 

mh»  bs .  ee  0  ile 
yi 

W(L)  Y(L) 

M2 

Ex, 

2 

(11.2.4) 

where 

Wi (L)  =  oh  +  AOL!  +  GPL?  +  WPL?  + 

with  yw)?  =  pf  =  1 and  ys)  =  0. 

Another  implication  of Granger  causality  was  stressed  by Sims  (1972). 

Proposition  11.3: 
xs, 

Consider  a  linear  projection  of y,  on  past,  present,  and  future 

y=  c+  > byx,-;  +  4 reat 

j=0 

= 

[11.2.5] 

where  b; and  d; are  defined  as  population  projection  coefficients,  that  is, the  values 
for which 

E(n,.x,)  =  0 

forall t and  rt. 

Then y fails to  Granger-cause  x  if and  only  if d; =  0 forj  =  1,2,.... 

Econometric  Tests for Granger  Causality 

Econometric  tests  of whether  a particular  observed  series  y Granger-causes 
x  can  be  based  on  any  of the  three  implications  [11.2.2],  [11.2.4],  or  [11.2.5].  The 
simplest  and  probably  best  approach  uses  the  autoregressive  specification  [11.2.2]. 
To  implement  this  test,  we  assume  a  particular  autoregressive  lag  length  p  and 
estimate 

Kj  =  Cy  +  OX, 2g F  kg  + 

OS  G&S  t+  Pie 

+"  Baypag  Ee  amt  GF,»  +  u, 

by OLS.  We  then  conduct  an F test  of the  null  hypothesis 

[11.2.6] 

Hy:  B,  =y B,  et 

eg  B,  =  0. 

[11.2.7] 

Recalling  Proposition  8.2,  one  way  to  implement  _ test  is to  calculate  the  sum 
of squared  residuals  from  [11.2.6],‘ 

hi 

RSS,  =  > a2, 

r=) 

and  compare  this  with  the  sum  of squared  residuals  of a univariate  autoregression 
for x,, 

T 

RSS)  =  > é;., 

t=] 

‘Note  that in  order  for ¢ to  run  from  | to  T as  indicated,  we  actually  need  T +  p observations  on 

x and y, namely, x44.  per  es  Lr ANd Yn gts  Vapade  ses  Dye 

304  Chapter  11  | Vector Autoregressions  _ 

where 

x4,  =  Cy  +  Yizt,-1  +  Y2%,-2  +  ++*  +  Yp%i=p  +  é, 

[11.2.8] 

is also  estimated  by OLS.  If 

= 

1" 

(RSS),  a  RSS  ,)/p 
RSS.AT  =  eng 

[11.2.9] 

is greater  than  the  5%  critical  value  for  an  F(p,  T  —  2p  — 
1) distribution,  then 
we  reject  the  null  hypothesis  that  y  does  not  Granger-cause  x;  that  is,  if  S,  is 
sufficiently  large,  we  conclude  that  y does  Granger-cause  x. 

The  test  statistic  [11.2.9]  would  have  an  exact  F distribution  for  a  regression 
with  fixed  regressors  and  Gaussian  disturbances.  With  lagged  dependent  variables 
as  in  the  Granger-causality  regressions,  however,  the  test  is valid  only  asymptot- 
ically.  An  asymptotically  equivalent  test  is given  by 

S, 

a  T(RSS,  —  RSS,) 

RSS, 

[11.2.10] 

We  would  reject  the  null  hypothesis  that y does  not  Granger-cause x if S, is greater 
than  the  5%  critical  values  for  a y7(p)  variable. 

An  alternative  approach  is to  base  the  test  on  the  Sims  form  [11.2.5]  instead 
of the  Granger  form  [11.2.2].  A problem  with  the  Sims  form  is that  the  error  term 
7, is in general  autocorrelated.  Thus,  a standard F test  of the  hypothesis  that  d; =  0 
for all  j in  [11.2.5]  will  not  give  the  correct  answer.  One option  is to  use  autocor- 
relation-consistent  standard  errors  for  the  OLS  estimates  as  described  in  Section 
10.5.  A second  option  is to  use  a generalized  least  squares  transformation.  A third 
option,  suggested  by Geweke,  Meese,  and  Dent  (1983),  is as  follows.  Suppose  the 
error  term  7, in [11.2.5]  has  Wold  representation  n,  =  4 (L)v2,.  Multiplying  both 
sides  of [11.2.5]  by A(L)  =  [¥22(L)]~'  produces 

x 

y= 

—-  dD hy,  +  Dd bfx,  +  Dd axa;  +  ve 

j=l 

j=0 

j=' 

{11.2.11] 

The  error  term  in  [11.2.11]  is white  noise  and  uncorrelated  with  any  of  the  ex- 
planatory  variables.  Moreover,  d? =  0 for  all j if and  only  if d; =  0 for  all j. Thus, 
by truncating  the  infinite  sums  in  [11.2.11]  at  some  finite  value,  we  can  test  the 
df =  dy  = 
null  hypothesis  that  y does  not  Granger-cause  x  with  an  F test  of 
co. 

eis  ap) 
A  variety of other  Granger-causality  tests  have  been  proposed;  see  Pierce 
and  Haugh  (1977)  and  Geweke,  Meese,  and  Dent  (1983)  for  selective  surveys. 
Bouissou,  Laffont,  and  Vuong  (1986)  discussed  tests  using  discrete-valued  panel 
data.  The  Monte  Carlo  simulations  of Geweke,  Meese,  and  Dent  suggest  that  the 
simplest  and  most  straightforward  test—namely,  that  based  on  {11.2.10]—may 
well  be the  best. 

The  results  of any  empirical  test  for  Granger  causality  can  be  surprisingly 
sensitive  to the choice  of lag length  (p) or  the  methods  used  to deal  with  potential 
nonstationarity  of the series.  For demonstrations  of the  practical  relevance  of such 
issues,  see  Feige  and  Pearce  (1979),  Christiano  and  Ljungqvist  (1988),  and  Stock 
and Watson  (1989). 

Interpreting  Granger-Causality  Tests 
How is “Granger  causality”  related  to  the  standard  meaning  of “causality”? 

We explore  this  question  with  several  examples. 

11.2.  Bivariate  Granger  Causality  Tests 

305 

Example  11.1—Granger-Causality  Tests 
and  Forward-Looking  Behavior 
The  first  example  uses  a  modification  of  the  model  of  stock  prices  described 
in  Chapter  2.  If an  investor  buys  one  share  of  a  stock  for  the  price  P, at  date 
t,  then  at  ¢  +  1 the  investor  will  receive  D,,,  in  dividends  and  be  able  to  sell 
the  stock  for  P,,,.  The  ex  post  rate  of return  from  the  stock  (denoted  r,,  ,) Is 
defined  by 

1  +  Gr,  @  2.4  teas: 

[11.2.12] 

A  simple  model  of stock  prices  holds  that  the  expected  rate  of return  for  the 
stock  is a constant  r at  all  dates:° 

(1  +  +P,  =  E[ Piz)  +  Dio). 

[11.2.13] 

Here  E,  denotes  an  expectation  conditional  on  all  information  available  to 
stock  market  participants  at  time  ¢. The  logic behind  [11.2.13]  is that  if investors 
had information  at time  ¢ leading  them  to anticipate  a higher-than-normal  return 
to  stocks,  they  would  want  to  buy more  stocks  at date  ¢.  Such  purchases  would 
drive  P,  up  until  [11.2.13]  was  satisfied.  This  view  is  sometimes  called  the 
efficient  markets  hypothesis. 

As  noted  in  the  discussion  of equation  [2.5.15]  in  Chapter  2, equation 

[11.2.13]  along  with  a  boundedness  condition  implies 

cl 

es a 

PB =Eb>.  Feel Disj- 

[11.2.14] 

Thus,  according  to  the  theory,  the  stock  price  incorporates  the  market’s  best 
forecast  of the  present,  value  of future  dividends.  If this  forecast  is based  on 
more  information  than  past  dividends  alone,  then  stock  prices  will  Granger- 
cause  dividends  as  investors  try  to  anticipate  movements  in dividends. 

For  a  simple  illustration  of this point,  suppose  that 

D,  =  d  +  u,  +  6u,_,  +  v,, 

[11.2.15} 

where  u, and  v, are  independent  Gaussian  white  noise  series  and  d is the  mean 
dividend.  Suppose  that  investors  at  time  ¢ know  the  values  of {u,,  u,_,,  - 
-and  {v,, v,_;,  . 

-  -} 
.  -}. The  forecast  of D,,, based  on  this  information  is given  by 
 forj=1 

d+  6u, 

E(D,,))  = 

(r+)  {!  i  <  ae 

2 
paren, 

Substituting  [11.2.16]  into  [11.2.14],  the  stock  price  would  be given  by 

P, =  dir  +  5u,/(1  +  r). 

{1.2.17} 

‘A related  model  was  proposed  by Lucas  (1978): 

U'(C,)P,  “= EABU'(C,.  (Pi.  °  D,.1)} 

with  U'(C,)  the  marginal  utility  of consumption  at  date  1.  If we  define  P, to  be  the  marginal-utility-. 
weighted  stock  price  P,  =  U'(C,)P,  and  D, the  marginal-utility-weighted  dividend,  then  this  becomes 

p-'8  -  EAP oni  +  D.<i. 

which  is the same  basic  form  as  (11.2.13].  With  risk-neutral  investors,  U'(C,)  is a constant  and  the two 
formulations  are  identical.  The  risk-neutral  version  gained  early  support  from  the  empirical  evidence 
in Fama  (1965). 

306  Chapter  11  | Vector  Autoregressions 

Thus,  for  this example,  the  stock  price  is white  noise  and  could  not  be forecast 
on  the  basis  of  lagged  stock  prices  or  dividends.°  No  series  should  Granger- 
cause  stock  prices. 

On  the  other  hand,  notice  from  [11.2.17]  that  the  value  of  u,_,  can  be 

uncovered  from  the  lagged  stock  price: 

du,_,  =  (1  +  r)P,_,  —  (1  +  r)d/r. 
Recall  from  Section  4.7  that  u,_,  contains  additional  information  about  D, 
beyond  that  contained  in {D,_,,  D,_2,  . 
.  .}. Thus,  stock  prices  Granger-cause 
dividends,  though  dividends  fail  to  Granger-cause  stock  prices.  The  bivariate 
VAR  takes  the  form 

| "  | dir  | | “weg 
bette 
—dlr 

D, 

ey phy 
DL  LD, 20 

, 

[mul  +r) 
u,+v, 

| 

Hence,  in  this  model,  Granger  causation  runs  in  the  opposite  direction 
from  the  true  causation.  Dividends  fail  to  ‘“‘Granger-cause”’  prices,  even  though 
investors’  perceptions  of  dividends  are  the  sole  determinant  of  stock  prices. 
On  the  other  hand,  prices  do  “Granger-cause”  dividends,  even  though  the 
market's  evaluation  of the stock  in reality has no  effect  on  the dividend  process. 

In  general,  time  series  that  reflect  forward-looking  behavior,  such  as  stock 
prices  and  interest  rates,  are  often  found  to  be  excellent  predictors  of many  key 
economic  time  series.  This  clearly  does  not  mean  that  these  series  cause  GNP  or 
inflation  to move  up or  down.  Instead,  the  values  of these  series  reflect  the market’s 
best  information  as  to  where  GNP  or  inflation  might  be headed.  Granger-causality 
tests  for  such  series  may  be  useful  for  assessing  the  efficient  markets  view  or 
investigating  whether  markets  are  concerned  with  or  are  able  to  forecast  GNP  or 
inflation,  but  should  not  be  used  to  infer  a  direction  of causation. 

There  nevertheless  are  circumstances  in  which  Granger  causality  may  offer 
useful  evidence  about  the  direction  of  true  causation.  As  an.  illustration  of  this 
theme,  consider  trying to measure  the effects  of oil price increases  on  the economy. 

Example  11.2—Testing for Strict  Econometric  Exogeneity’ 
All  but  one  of the  economic  recessions  in the  United  States  since  World  War 
Il  have  been  preceded  by a  sharp  increase  in  the  price  of crude  petroleum. 
Does this  mean  that  oil  shocks  are  a  cause  of recessions? 

One  possibility  is that  the  correlation  is a  fluke—it  happened  just  by 
chance  that  oil  shocks  and  recessions  appeared  at  similar  times,  even  though 
the  actual  processes  that  generated  the  two  series  are  unrelated.  We  can  in- 
vestigate  this  possibility  by testing  the  null  hypothesis  that  oil  prices  do  not 
Granger-cause  GNP.  This  hypothesis  is rejected  by the  data—oil  prices  help 
predict  the  value  of GNP,  and  their  contribution  to  prediction  is statistically 
-  significant.  This argues  against viewing the correlation  as simply a coincidence. 
To  place  a  causal  interpretation  on  this  correlation,  one  must  establish 
that oil price increases  were  not reflecting some  other macroeconomic  influence 
that  was  the  true  cause  of the  recessions.  The  major  oil  price  increases  have 

*This  result  is due  to the particular  specification  of the time  series  properties  assumed  for dividends. 
A completely general  result  is that the excess  return  series  defined  by P,,,  + D,,,  —  (1 +  r)P, (which 
for this example  would  equal 6u,, ,/(1  +  r) +  u,.1  + V,+:) Should  be unforecastable.  The  example  in 
the text  provides a simpler  illustration  of the general  issues. 
This  discussion  is based  on  Hamilton  (1983,  1985). 

11.2.  Bivariate ‘Granger Causality  Tests  307 

been  associated  with  clear  historical  events  such  as  the  Suez  crisis  of  1956-57, 
the  Arab-Israeli  war  of  1973-74,  the  Iranian  revolution  of  1978-79,  the  start 
of the  Iran-Iraq  war  in  1980,  and  Iraq’s  invasion  of Kuwait  in  1990.  One  could 
take  the  view  that  these  events  were  caused  by forces  entirely  outside  the  U.S. 
economy  and  were  essentially  unpredictable.  If this  view  is correct,  then  the 
historical  correlation  between  oil  prices  and  GNP  could  be  given  a  causal 
interpretation.  The  view  has  the  refutable  implication  that  no  series  should 
Granger-cause  oil  prices.  Empirically,  one  indeed  finds  very  few  mac- 
roeconomic  series  that  help  predict  the  timing  of these  oil  shocks. 

The  theme  of  these  two  examples  is  that  Granger-causality  tests  can  be  a 
useful  tool  for  testing  hypotheses  that  can  be  framed  as  statements  about  the 
predictability  of a particular  series.  On  the  other  hand,  one  may  be skeptical  about 
their  utility  as  a  general  diagnostic  for  establishing  the  direction  of causation  be- 
tween  two  arbitrary  series.  For  this  reason,  it seems  best  to  describe  these  as  tests 
of whether  y helps  forecast  x  rather  than  tests  of  whether  y causes  x.  The  tests 
may  have  implications  for  the  latter  question,  but  only  in  conjunction  with  other 
assumptions. 

Up to  this  point  we  have  been  discussing  two  variables,  x  and  y,  in isolation 
from  any  others.  Suppose  there  are  other  variables  that  interact  with  x or y as  well. 
How  does  this  affect  the  forecasting  relationship  between  x  and  y? 

Example  11.3—Role  of Omitted  Information 
Consider  the  following  three-variable  system: 

Fr 

Pp  ODS 

LY 

bre 

be sad  es 

Yr 

0 

0 

1  O}  | €,], 

aie 

£3, 

with 

Ele#,)  = 

ee 
a 
0  a3  0 
ile | ak 

fort=s 

—~0 

otherwise. 

Thus,  y3 can  offer  no  improvement  in a  forecast  of either  y,  or y, beyond  that 
achieved  using  lagged  y,  and  y,. 

Let  us  now  examine  the  bivariate  Granger-causality  relation  between  yy 

and y;.  First,  consider  the  process  for y,: 

Vu 

Seon 
Notice  that y, is the sum  of an  MA(1) process  (€,, +  5€,,_,) and an  uncorrelated 
white  noise  process (€ ,_ ;). We know from equation  [4.7.15] that the univariate 
representation  for y,  is an  MA(1)  process: 

S71 |. 

eh, 

From  [4.7.16],  the  univariate  forecast  error u, can  be  expressed  as 

Yu  =  u,  +  Ou,_,. 

U,  =  (€,  —  06,,-,  +  076,,_2  —  6°e,,_3  #  ***) 

+  (€),-;  —  06,42  +  O76,,_3  -  Gey pice shits  ree 
+  (€2,-1  —  0€2,-2  +  076243  -  6°62 ,-4  bgeiaby  2 

308  Chapter 11  | Vector Autoregressions 

The  univariate  forecast  error  u,  is, of course,  uncorrelated  with  its own  lagged 
values.  Notice,  however,  that  it is correlated  with  yo 

E(u,)(¥3,-1)  =  E(u,)(€3,-,  as  €>,-2)  = 

—  6%. 

Thus,  lagged  y,  could  help  improve a forecast  of  y,  that  had  been  based 
on  lagged  values  of  y,  alone,  meaning  that  y,  Granger-causes  y,  in  a  bivariate 
system.  The  reason  is that  lagged  y,  is correlated  with  the  omitted  variable  y,, 
which  is also  helpful  in  forecasting  y,.* 

11.3.  Maximum  Likelihood  Estimation  of Restricted 
Vector  Autoregressions 

Section  11.1  discussed  maximum  likelihood  estimation  and  hypothesis  testing  on 
unrestricted  vector  autoregressions.  In  these  systems  each  equation  in  the  VAR 
had  the  same  explanatory  variables,  namely,  a  constant  term  and  lags  of  all  the 
variables  in the system.  We showed  how  to calculate  a Wald  test  of linear constraints 
but  did  not  discuss  estimation  of the  system  subject  to  the  constraints.  This  section 
examines  estimation  of a  restricted  VAR. 

Granger  Causality  in  a  Multivariate  Context 

As an  example  of a restricted  system  that  we  might be interested  in estimating, 
consider  a  vector  generalization  of  the  issues  explored  in  the  previous  section. 
Suppose  that the variables  of a  VAR  are  categorized  into two  groups,  as  represented 
by the  (m,  x  1) vector  y,,  and  the  (mn,  x  1) vector  y,,.  The  VAR  may  then  be 
written 

Vif 

Cyt.  ApXt—ty  Aske,  +.  €1, 

[11.3.1] 

Y2,  =>  Cc,  +  Bi X1,  +  B3X>,  +  E>,- 

{11.3.2} 

Here  x,,  is an  (n,p  X  1) vector  containing  lags  of y,,,  and  the  (n,p  x  1) vector 
x,, contains  lags of y,,: 

Fir 
a  Py  ivi 

Xi,  or 

You0-1 
Vou —2 
a 

- 

Yd 
X>,  = 

Yiu-p 

: 

Jzi~p 

The  (n,  <  1) and  (n,  x  1) vectors  c,  and  ¢,  contain  the  constant  terms  of  the 
VAR, while  the matrices  A,, A>, B,, and  B, contain  the autoregressive  coefficients. 
The  group  of variables  represented  by y,  is said  to  be block-exogenous  in the 
time  series  sense  with  respect  to  the  variables  in y,  if the  elements  in y,  are  of no 
help in improving  a forecast  of any  variable  contained  in y, that  is based  on  lagged 
values  of all the  elements  of y,  alone.  In the  system  of [11.3.1]  and  [11.3.2],  y,  is 
block-exogenous  when  A,  =  9. To  discuss  estimation  of the  system  subject  to  this 
constraint,  we  first  note  an  alternative  form  in  which  the  unrestricted  likelihood 
can be calculated  and  maximized. 

"The reader may  note  that  for this example  the correlation  between  y,, and y,,  , is zero.  However, 
there are  nonzero  correlations  between  (1) y,, and y,,.  and  (2) y,,.,  and ya,..,,  and  these  account 
for the contribution of ys,-,  to a forecast  of y,, that  already  includes y, , -,. 

11.3.  Estimation  of Restrieted  Vector  Autoregressions 

309 

An  Alternative  Expression  for the  Likelihood  Function 
Section  11.1  calculated  the  log  likelihood  function  for  a  VAR  using  the  pre- 

diction-error  decomposition 

where  y, =  (Yirs  Yar)»  X=  (Yi—-15  Yr-2»  + 

£(0)  =  >. log fujx.(yslx,3  8)» 
+  Yr=p),  and 
+ 
+ 

log fvxylx:  60) 

=  car a log(2m)  —  $10 

ny, 

2),  2, 

2,  222} 

[11.3.3] 

[11.3.4] 

—  Au  —  Cy  —  Aix,  —  Axe)’  (Ya  —  e2  —  Bix,  —  Bx2,)’] 

”  la Bia} fF pel 

ie. 

6  PAS  cat 

D2,  2» 

Yar  —  C2  —  Bix,  —  Bzx2, 

Alternatively,  the  joint  density  in  [11.3.4]  could  be  written  as  the  product  of  a 
marginal  density  of y,,  with  the  conditional  density  of y2,  given  y,,: 

fix,  1X,  0)  =  frvax,  (Yul Xs  9) -fysiv,.x,  Yala  x,;  @). 

[11.3.5] 

Conditional  on  x,,  the  density  of y,,  is 

Fviix, (Yul  0)  =  Qm)-"7{0,,|-” 

x  exp[—H(y,, — ¢;  —  Aix,  —  A5x2,)'Q5' 

[11.3.6] 

while  the  conditional  density  of y2,  given  y,,  and  x, is also  Gaussian: 

X  (Y¥y,  —  ¢;  —  Aix,  —  A5Xx2,)], 

Frat  ex(Y2rl¥ ues  Xs 8)  =  (2mr)~"2?| 

HI -'7 

[11.3.7] 

The  parameters  of this  conditional  distribution  can  be  calculated  using  the  results 
from  Section  4.6.  The  conditional  variance  is given  by equation  [4.6.6]: 

x  exp[—2(y2,  —  m,)’H~'(y2,  —  m;,)]. 

while  the  conditional  mean  (m3,)  can  be  calculated  from  [4.6.5]: 

H  =  22  —  2,,07;'0); 

m2,  =  E(y2,|x,) 

2,07  TY.  ms  E(y,,|x,)}- 

[11.3.8] 

Notice  from  [11.3.1]  that. 

while  from  [11.3.2], 

E(y|x,)  =  ¢  +  Aix,  +  Ax, 

agua these  expressions  into  [11.3.8], 

E(yo,|x,)  =  ¢2  +  Bix,  +  B3x,- 

where 

=  (c,  +  Bix,,  +  B3x,,)  +  2,,05'[y,, — (ce,  +  Alx,  + Ags) 
=  d +  Diy,  +  Djx,,  +  Dix, 

d=  c,  —  2,,Qj;'c, 
Di = 2,,9;;' 
D,  =  By —  2,,07'A; 
Ds  =  B, —  0,,0;,'A}. 

, 

[11.3.9] 
(11.3.10] 
[11.3.11] 
peperepesirs  hb 

310  Chapter  11  | Vector  Autoregressions 

The  log of  the  joint  density  in  [11.3.4]  can  thus  equivalently  be  calculated  as 
nt “git of  the  logs  of  the  marginal  density  [11.3.6]  and  the  conditional  density 

1.3.7]: 

log fy,ix,(y, 1x5 ®)  =  €,,  +  €3,, 

[11.3.13] 

where 
€,,  = (—n,/2)  log(27)  —  + log|Q,,| 

i 

—  S[(¥,,  —  Cp  Aim,'=  A2X2,)'Qi  (yi,  —  ¢;  —  Aix,  - 
(—n,/2)  log(27m)  —  3 log|H| 

A>X2,)| 

€;, 

< 

(11.3.14] 

re  *[(y2,  ~  @  ~  Diy,  >:  D)Xx;,  od  D;>x2,)'H~' 

[11.3.15] 

x  (y2,  —  d  —  Dyy,,  —  Dix,,  —  Dx;,)]. 

The  sample  log  likelihood  would  then  be  expressed  as 

T 

T 

20)  =  > €,,  +  >d €,. 

r=1 

‘=I 

(11.3. 16] 

Equations  [11.3.4]  and  [11.3.13]  are  two  different  expressions  for  the  same 
magnitude.  As  long  as  the  parameters  in the  second  representation  are  related  to 
those  of the  first  as  in [11.3.9]  through  [11.3.12],  either  calculation  would  produce 
the  identical  value  for  the  likelihood.  If [11.3.3]  is maximized  by choice  of (c,, A,, 
A>,  ¢>,  B,,  Bz,  2,,,  2,2,  52),  the  same  value  for  the  likelihood  will  be  achieved 
as  by maximizing  [11.3.16]  by choice  of  (c,,  A,,  Az,  d, Dy,  D,,  D,,  2,,;,  H). 

The second  maximization  is as easy  to achieve  as  the first.  Since  the parameters 
(c,, A,, A>) appear  in [11.3.16]  only through  2 7_, ¢,,, the MLEs  of these  parameters 
can  be  found  by OLS  regressions  of the  elements  of y,,  on  a  constant  and  lagged 
values  of y,  and  y,,  that  is,  by OLS  estimation  of 

Yi),  =  C,  +  Ajx,,  +  Ajx,  +  &,,. 

[11.3.17] 

The  MLE  of @,,  is the  sample  variance-covariance  matrix  of  the  residuals  from 
these  regressions,  2,,  =  (1/7) 2/_, €,,€;,.  Similarly,  the  parameters  (d,  Dy,  D,, 
D,) appear  in [11.3.16]  only through  2/_, €,,, and  so  their  MLEs  are  obtained  from 
OLS  regressions  of the  elements  of y2, on  a constant,  current  and  lagged  values  of 
y,.  and  lagged  values  of y,: 

Yn  =  d  +  Doy,,  +  Dix,  +  D5x2,  +  v2, 

[11.3.18] 

The  MLE  of H is the  sample  variance-covariance  matrix  of the  residuals  from  this 
second  set  of regressions,  H =  (1/T) 27_, ¥,,¥3,. 

Note  that  the  population  residuals  associated  with  the  second  set  of regres- 
sions,  ¥>,,  are  uncorrelated  with  the  population  residuals  of the  first  regressions. 
This  is because  v,,  =  yz,  —  E(y2,ly,,.  X,) is uncorrelated  by construction  with  y,, 
and  x,,  whereas  €,,  is a linear  function  of y,,  and  x,.  Similarly,  the  OLS  sample 
residuals  associated  with  the  second  regressions, 

V2, =  ¥x,  =  d -  Diy  "  D;x,, re D3x,,, 
are  orthogonal  by construction  to y,,,  a constant  term,  and x,. Since  the OLS sample 
residuals  associated  with  the first  regressions,  é,,, are  linear  functions  of these  same 
elements,  9%, is orthogonal  by construction  to  €,,. 

Maximum  Likelihood  Estimation  of  a VAR  Characterized 
by Block  Exogeneity 
Now  consider  maximum  likelihood  estimation  of the  system  subject  to  the 
constraint  that  A,  =  0. Suppose  we  view  (d, Dy, D,,  D., H) rather  than  (c2, B,, 

11.3.  Estimation  of Restricted  Vector  Autoregressions 

311 

B,,  2,,,  2,2)  as  the  parameters  of  interest  for  the  second  equation  and  take our 
objective  to  be to choose  values  for  (¢,, A,, 2,,,  d, Dy,  Di,  D2, H) so  as  to  maximize 
the  likelihood  function.  For  this  parameterization,  the  value  of A, does  not  affect 
the  value  of  ¢,, in  [11.3.15].  Thus,  the  full-information  maximum  likelihood  esti- 
mates  of c,,  A,, and  9, , can  be  based  solely  on a restricted  version  of the  regressions 
in [11.3.17], 

¥,  =  Cy  +  Aj,  +.  &,,- 
Let  ¢,(0),  A,(0), Q, ,(0) denote  the  estimates  from  these  restricted  regressions.  The 
maximum  likelihood  estimates  of  the  other  parameters  of  the  system  (d,  Dy,  D,, 
D,,  H)  continue  to  be  given  by unrestricted  OLS  estimation  of  [11.3.18],  with 
estimates  denoted  (d, Dy, D,, D2,  H). 

[11.3.19] 

The  maximum  value  achieved  for  the  log  likelihood  function  can  be  found 

by applying  [11.1.32]  to  [11.3.13]: 

£[6(0)]  Nai  3 €,,{€,(0),  A,(0), 2, ,(0)]  +  x ¢,,[d,  Dy, D,, D,. H] 

=  [-(Tn,/2)  log(2m)  +  (7/2)  log|Q®j(0)|  —  (Tn,/2)] 
+  [—(Tn,/2)  log(27)  +  (7/2)  log|H~-'|  —  (Tn,/2)}. 

[1.3.20] 

By contrast,  when  the  system  is estimated  with  no  constraints  on  A;,  the  value 
achieved  for  the  log likelihood  is 

(6)  7  > .[¢1,  A,, A,, 01]  +  > €,,[d, Dp, D,, D,, H] 

=  [—(Tn,/2)  log(2m)  +  (7/2) log|Q5'|  —  (Tn,/2)] 

[11.3.21] 

+  [—(Tn,/2)  log(2m)  +  (7/2)  log|H-'|  —  (Tn,/2)), 

where  (é,, A,, A,, 9,,) denote  estimates  based  on  OLS  estimation  of [11.3.17].  A 
likelihood  ratio  test  of the  null  hypothesis  that  A,  =  0 can  thus  be  based  on 

2A£[6]  -  £[6(0)}}  = T{log|Qji'|  —  log|%j'(0)) 
T{log|®,,(0)|  —  log|,,)} 

[11.3.22] 

This will  have  an  asymptotic  y? distribution  with  degrees  of freedom  equal  to  the 
number of restrictions.  Since  A, is an  (m,  X  mp) matrix,  the  number  of restrictions 
is n\nzp. 

Thus,  to  test  the  null  hypothesis  that  the  n,  variables  represented  by y,  are 
block-exogenous  with  respect  to  the  n, variables  represented  by y,,  perform  OLS 
regressions  of each  of the elements  of y, on  a constant,  p lags of all of the elements 
of y,,  and p lags of all of the  elements  of y,.  Let  €,, denote  the  (m,  x  1) vector  of 
sample residuals for date ¢ from  these  regressions  and 22,, their variance-covariance 
matrix  (Q,,  =  (1/T)2/_,  €,,€},).  Next  perform  OLS  regressions  of each  of  the 
elements  of y, on  a constant  and p lags of all the  elements  of y,.  Let  €,,(0)  denote 
the (n,  x  1) vector  of sample  residuals  from  this  second  set  of regressions  and 
,,(0)  their  variance-covariance  matrix  (,,(0)  =  (1/7) =7_, [é,,(0)][é,,(0)]’). 
If 

T {log |, ,(0)|  =  log| 9,1} 
is greater than the 5% critical  value for a y7(n,n2p) variable,  then the null hypothesis 
is rejected,  and  the  conclusion  is that  some  of the  elements  of y,  are  helpful  in 
forecasting  y,. 

Thus,  if our  interest  is in estimation  of the  parameters  (c,, A,, 2,,,  d, Do, 
D,,  D,,  H) or  testing  a  hypothesis  about  block  exogeneity,  all  that  is necessary 

312  Chapter 11  | Vector Autoregressions 

is OLS  regression  on  the  affected  equations.  Suppose,  however,  that  we  wanted 
full-information  maximum  likelihood  estimates  of the  parameters  of the  likelihood 
as  Originally  parameterized  (c,,  A,,  Q,,,  c>,  B,,  B,, 2,,,  2,.).  For  the  parameters 
of  the  first  block  of  equations  (c,,  A,,  2,,),  the  MLEs  continue  to  be  given  by 
OLS  estimation  of  [11.3.19].  The  parameters  of  the  second  block  can  be  found 
from  the  OLS  estimates  by inverting  equations  [11.3.9]  through  [11.3.12]:” 

2,,(0)  =  DQ, :(0)) 

€,(0)  =  d +  [,,(0)}[®,,(0)]-  '[é,(0)} 
[B,(0)]’  =  Dj +  [2,,(0)][2,,(0)}-  '[A,()]’ 
[B,(0)]'  =  D; 
2,,(0)  =  A +  [0,,(0)][2,,(0)]- '[0,.(0)). 
Thus,  the  maximum  likelihood  estimates  for  the  original  parameterization  of [11.3.2] 
are  found  from  these  equations  by combining  the  OLS  estimates  from  [11.3.19] 
and  [11.3.18]. 

Geweke’s  Measure  of Linear  Dependence 

The  previous  subsection  modeled  the  relation  between  an  (n,  X  1) vector  yj, 
and  an  (n,  X  1) vector  y2,  in  terms  of  the  pth-order  VAR  [11.3.1]  and  [11.3.2], 
where  the  innovations  have  a  variance-covariance  matrix  given  by 

glee  od a  a mie 

£2,E}, 

€2,€, 

Q,,  OQ» 

. 

To test  the null  hypothesis  that y, is block  exogenous  with  respect  to y,, we  proposed 
calculating  the  statistic  in  [11.3.22], 

[11.3.23] 
T{log|®,,(0)|  —  log|®,,|}  ~  x?(mn2p). 
where  2,, is the variance-covariance  matrix  of the  residuals  from  OLS  estimation 
of [11.3.1]  and  2, ,(0) is the  variance-covariance  matrix  of the  residuals  from  OLS 
estimation  of  [11.3.1]  when  lagged  values  of y,  are  omitted  from  the  regression 
(that  is, when  A,  =  0 in [11.3.1)). 

Clearly,  to  test  the  parallel  null  hypothesis  that  y,  is block-exogenous  with 

respect  to  y,,  we  would  calculate 

T{log|2,,(0)|  —  log|Q,.|}  ~  x?(n2n,p), 

(11.3.24] 

where  22, is the variance-covariance  matrix  of the  residuals  from  OLS estimation 
of [11.3.2]  and 22,,(0)  is the  variance-covariance  matrix  of the  residuals  from  OLS 
estimation  of [11.3.2]  when  lagged  values  of y,  are  omitted  from  the regression 
(that is, when  B,  =  0 in [11.3.2]). 

| 
Finally,  consider  maximum  likelihood  estimation  of the  VAR  subject  to  the 
restriction  that  there  is no  relation  whatsoever  between  y,  and  y,,  that  is, subject 

| 

"To confirm  that  the  resulting  estimate  (0) is symmeteric and  positive  definite,  notice  that 

11,,(0)  =  H+  Bif®,,(0)]5, 

and so 

‘ 

,,(0)  4,0)}  _  [tO  o  bb 
|)  L,]| 
,,(0)  0,,0)| 

AjLo  Ly 

9 

11.3.  Estimation  of Restricted  Vector  Autoregressions  313 

to  the  restrictions  that  A,  =  0,  B,  =  0,  and  2,,  =  0.  For  this  most  restricted 
specification,  the  log  likelihood  becomes 

ig 

£(0)  =  > { =  (12)  log(2m)  —  (1/2)  log|®,,| 

—  (1/2)(y;,  —  ¢)  —  Ajx,,)'Q5 (yn —  C1  =  ain) 

T 

+>  { ~(l2)  log(2m)  —  (1/2)  log|2,9| 

=  ic Ya ~-C2  —  B>x>,)'Q'(y2,  —  ¢2  -  Bix,)| 

and  the  maximized  value  is 

£(6(0))  =  {-(Tn,/2)  log(2m)  —  “(Ti2)  log|M,,(0)|  —  (Tn,/2)} 

+  {—(Tnj/2)  log(27)  —  (T/2)  log|®,.(0)|  —  (Tn2/2)}. 

A  likelihood  ratio  test  of the  null  hypothesis  of no  relation  at  all  between  y,  and 
y2  is thus  given  by 

2{L(6)  —  £(6(0))} 

(11.3.25] 

A 
r  {og | 

A 
0)|  +  log},,(0 
g|22,,(0)| 

= 

Q,,  Ms 
Ys 
—  log  a,,  a 

lo 

where 0,, is the  covariance  matrix  between  the  residuals  from  unrestricted  OLS 
estimation  of  [11.3.1]  and  {11.3.2].  This  null  hypothesis  imposed  the  (n,n3p)  re- 
Strictions  that  A,  =  0, the  (n,n,p)  restrictions  that  B,  =  0, and  the  (n,n,)  restric- 
tions  that  ©,,  =  0. Hence,  the  statistic  in  [11.3.25]  has  a  x? distribution  with 
(n,n)  X  (2p  +  1) degrees  of freedom. 

Geweke  (1982)  proposed  (1/7)  times  the  magnitude  in  [11.3.25]  as  a  measure 
of the  degree of linear  dependence  between  y,  and  y,.  Note  that  [11.3.25]  can  be 
expressed  as  the  sum  of three  terms: 

r {tog +  log|,,(0)|  —  log} ae 

acl} 

: 
ie ae | 

=  T{log|Q,,(0)|  —  log|®,|}  +  Tflog]®,,(0)| — log|Q,,|}  [1.3.26] 

+  7} losld,  +  log|®,,|  —  log} - 

2,,  ri 

ids,  is } 

The  first  of  these  three  terms,  T{log|®, ,(0)|  —  log|Q,,[},  is  a  measure  of  the 
strength of the linear  feedback  from  y, to y, and is the y(n,n,p)  statistic  calculated 
in [11.3.23].  The  second  term,  T{log|®,,(0)|  -  log|22I}, is an  analogous  measure 
of the  strength  of linear  feedback  from  y,  to  y,  and  is the  y?(n,n,p)  statistic  in 
[(11.3.24].  The  third  term, 

_ 

r{ tos  +  log|®,.|  —  log] a,  © 

2,  || 

1 

22 

is a measure  of instantaneous  feedback.  This  corresponds  to a likelihood  ratio  test 
of the  null hypothesis  that  2,  =  0 with  A, and  B, unrestricted  and  has a x?(n,n3) 
distribution. under  the  null. 

Thus,  [11.3.26]  can  be  used  to summarize  the  strength  of any  linear  relation 
between  y,  and  y,  and  identify  the  source  of that  relation.  Geweke  showed  how 
these  measures  can  be further  decomposed  by frequency. 

314  Chapter 11  | Vector  Autoregressions 

Maximum  Likelihood  Estimation  Under  General 
Coefficient  Constraints 

We  now  discuss  maximum  likelihood  estimation  of a  vector  autoregression 
in which  there  are  constraints  that  cannot  be  expressed  in a  block-recursive  form 
as  in the  previous  example.  A VAR  subject  to  general  exclusion  restrictions  can 
be viewed  as  a system  of “seemingly  unrelated  regressions”  as  originally  analyzed 
by Zellner  (1962). 

Let x,,bea(k,  x  1) vector  containing  a constant  term  and  lags of the variables 

that  appear  in the  first  equation  of the  VAR: 

Yu  =  Xi Bi +  &y, 

Similarly,  let x,, denote  a (k,  x  1) vector  containing  the  explanatory  variables  for 
the second  equation  and  x,,,  a  (k,,  X  1) vector  containing  the variables  for  the  last 
equation.  Hence,  the  VAR  consists  of the  system  of equations 

iT  eed  xB,  :  Ey 

= 

+ 

ig  bs  Mtl 

tp 

7 

[11.3.27] 

: 

E 

3} 

1: 

A | ‘7  : 

Yur  =  x/,B, r. Ew  i 

oe  es 

=r  k, +  ky + -+-  +  k, denote  the total number  of coefficient to be 
2 
ese  in a  (k i “4 Lepr: 

Pe  3 

pas shese in ck 

3D 

Bs’ 

ol 

:  anoissups in fié : attitden'  . 

He. pe  q 

(  gS  ra tS  is  “ id  bsc  ninin 1  Bi  Aidt ° 

demiitas  B 

<aecomen  lag 288T |noi  19} gid eek cen  =, 

C1<% 

. 

ia  ie 

Sos 

+ 

cd a:  vig  > 

é 

<« “ , 

i  ii foem as  | 

2 bee 
Amie ro) nije  eee 

oh  a x j  0 peat  Ses  seeneney 

|  SR  ‘Sige ar rie 

ee 

*. 
te 

If 

@~'  is written  as  L’L,  this  becomes 

D (y, ~ ¥/B)Q-"(y,  ~ BiB)  = 2 (Ly,  ~ LEB)  (ly,  —  PB) 
~  L¥'B)'(Ly, — L&¥,; 
d 
Sy —  #By'O-“y,  -  vB) = 

es ay 

- £8). 
=  ¥ (5, —  £,B)'(5, 

where  y, =  Ly, and 

But  [11.3.31]  is simply 

> (¥, —  £/B)'(¥,  —  ¥/B) 

apts  —  fuP) 
>  Yu  —  By 

yy —  ZaP 
Y2  —  £28 

jw  - £1,B| 

|5u  - E18 

=  2 [Gu  —  BuBY  +  Fa  —  €2BYP  +  >>>  + (Gm —  £18), 

{= 

which  is minimized  by an  OLS  regression  of y,, on  %,,,  pooling  all  the  equations 
(=  1,2 
,  n) into  one  big regression.  Thus,  the  maximum  likelihood  estimate 
is given  by 

_ 

T 

= 

p 7  {> [(%,,21,)  +  (%2,%3,)  5 

44:1} 

T 

[11.3.32] 

x  {> (ZY us)  7  (%2,V2,)  Sh 

X  eet  asad} 

Noting  that  the  variance  of the  residual  of this  pooled  regression  is unity  by 
construction,'’  the  asymptotic  variance-covariance  matrix  of B can  be  calculated 
from 

E(B  —  B)(B  -  B)'  = p> (CC 

Gua,))}  ; 

Construction  of the  variables  y,, and  %;, to  use  in this  pooled  OLS  regression 
requires  knowledge  of L and  hence  (2. The parameters  in B and  2 can  be estimated 
jointly  by maximum  likelihood  through  the  following  iterative  procedure.  From  n 
OLS  regressions  of y,,  on  x,,,  form  an  initial  estimate  of  the  coefficient  vector 

That  is, 

E(¥, ~  8/BN9,  -  7B)’  =  LOL’  =  L(L'L)'L’  =  1,. 

316  Chapter  11  | Vector  Autoregressions 

B(0)  = 
matrix, 

(b}  b,  --- 

 bj{)'.Use  this  to  form  an  initial  estimate  of the  variance 

(0) = (1/7)  > Ly, — %/6(0)}[y,  —  %/B(0)]’. 

Find  a  matrix  L(0) such  that  (L(0)')L(0) = [(0)]-', say,  by  Cholesky factor- 
ization,  and  form  y,(0) = L(0)y,  and  %'(0)  =  L(O)%.  A  pooled  OLS  regression 
then  yields  the  new  estimate  6(1), 
of y;,(0)  on  %,(0)  combining  i =  1, 2,..., 
from  which  Q(1) = (1/7)>/_,[y, - #'A(1)[y,  -  2; 'B(1)]’.  Iterating  in  this 
manner  will  produce  the  maximum  likelihood  estimates  (f, 2), though  the  esti- 
mate  after  just  one  iteration  has  the  same  asymptotic  distribution  as  the  final 
MLE  (see  Magnus,  1978). 

An alternative  expression  for the MLE  in [11.3.32]  is sometimes  used.  Notice 

that 

(ui)  +  aR) +  + 

+ EAI] 

(11.3.33] 

Substituting  [11.3.33]  and  [11.3.34]  into  [11.3.32],  the  MLE  satisfies 

’ 
a!  SX 1,X1, 

o'?X4,X>, 
Ge  ZX  Xv 
oe  o**SX2,X5)  ea Ke,  **?  >> OZ S,, 

ros 

’ 

, 

’ 

a  | 

o's  Se  of*  Sx  X57 

lalla 

gi" LX, Xr 

[1 1.3.35] 

(oxy,  +  ee  RF  +  x  *  +  oO  "%  Var) 

1 

X(a?'x2,y1,  +  =  a  a  Be  »  a 

O"Xa, Vi) 

Lo"  x Yr  ©  ax,  2,  cr  Stem 

2  OXY) 

where  © denotes  summation  over ¢ =  1,2,...,  T. 

The  result  from  Section  11.1  was  that  when  there  are  no  restrictions  on  the 
VAR,  maximum  likelihood  estimation  is achieved  by OLS  equation  by equation. 
= 
This  result  can  be seen  as  a  special  case  of [11.3.35]  by setting  x,,  =  x2,  =  °°" 
x,,.  for  then  [11.3.35]  becomes 

6B  =  [0-'@  (%x,x/)]-'2[(Q-'y,)  @ x] 
=  [0 @ (2x,x/)-"J2[((Q-'y,)  @ x] 
—  [L, ® (2x,x/)~'J2ly,  © x,] 
ae 

(2x,x;)~' 

0 

0 

LyX, 

(2x,x,)  > 

i= 

0 

LVy2/X, 

0 

mat  (2X,K) YL 2%, 

0 

0 

_ 

b, 
|b 
b, 

as  shown  directly  in Section  11.1. 

Maximum  likelihood  estimation  with  constraints  on  both  the coefficients  and 

the  variance-covariance  matrix  was  discussed  by Magnus  (1978). 

11.4.  The  Impulse-Response  Function 

In equation  [10.1.15]  a VAR  was  written  in vector  MA(*)  form  as 

‘ 

Y=  -  +_€,  +  We;  +  W,€,_,  ” ire 

ii 

[11.4.1] 

Thus,  the  matrix  W, has  the  interpretation 

OY, +5 
ae  2 

4. 

that  is, the  row  i, column  j element  of W, identifies  the  consequences  of a  one- 
unit  increase  in the jth variable’s  innovation  at  date  ¢ (¢,,) for the  value  of the  ith 
variable  at  time  ¢ +  s (y,,,,),  holding  all  other  innovations  at  all dates  constant. 
If we  were  told  that  the  first  element  of ©, changed  by 6, at  the  same  time 
. 
,  and the nth element  by 6,,, then  the 

that  the  second  element  changed  by 5,, . 

. 

318  Chapter  11  | Vector Autoregressions 

combined  effect  of these  changes  on  the  value  of the  vector  y,,,  would  be  given 
by 

Me  ee  +t  tie...  +  Sg  ews 

,., 

OY, 4, 

OY, +5 

ital 

where  6  =  (6,,5......,  6,,)'. 

Several  analytic  characterizations  of W, were  given  in Section  10.1.  A  simple 
way  to  find  these  dynamic  multipliers  numerically  is by simulation.  To  implement 
the simulation,  sety,  ,  =  y,_»  =  °°:  =  y,_,  =  0.  Sete,  =  | and  all other  elements 
of €, to zero,  and  simulate  the  system  [11.1.1]  for  dates#,¢  +  1,4  +  2,....  with 
c and  €,,,,  €,,2,  . 
.  all  zero.  The  value  of  the  vector  y,,,  at  date  ¢  +  s  of  this 
simulation  corresponds  to  the  jth column  of  the  matrix  W,.  By doing  a  separate 
simulation  for  impulses  to  each  of the  innovations  (j =  1, 2,...  ,  /),  all  of  the 
columns  of W, can  be  calculated. 

. 

JA plot of the  row  /, column  / element  of W,, 
ee, 

ae 
; 
Ae; 

4, 
{11.4.4] 

as  a function  of s is called  the  input  Meepon function.  \t describes  the  response 
of y,,,,  10  a one-time  impulse in y,, with  al other variables dated ¢ or earlier held 

: 

Is there a sense  in which  this  multiplier can be viewed  as measuring the causal 

, 
that we should  © 
wanes on y;? The  discussion  of Granger-causality  tests suggests 
7 
,  be wary'of such a claim.  We are  on surer ground  with an atheoretical  VAR if we 
: 
to statements  about  forecasts.  Consider, therefore, the following 
4  = Let. 

confine  ourselves 

Big  sen 

p 

' 

- 

7 

| 

Bi bit 

are  then 
was  higher 

=O ux i ace 

to 

x  inf Bipesticn received about ‘the system as  of date  f — 

1.  Suppose  we 
isaegenal t value of the first  variable  in the autoregression, Vir 
Pres positive.  How  does this cause  us to révise 
s. what wiiitsy  .xntem  Isnogeib  & 2G  tok 
2By 
,  NSM IIS 0  aff  battles 

aig, 

tha 

f  aS O1-6.  1) 26 eabie shod hy 4.5] 

:  ew ni a  aaa Ds pene 

1 j =  bont 

:  ay  age" Age a 
Aci 

Ue 

: 

: 

Similarly,  for  the  variable  designated  number  3,  we  might  seek 

aE(y,  hae,  Vo,.  Vip.  %,  ) 

(11  4.7] 

OV, 

: 

and  for  variable  n, 

OE(Yis¢6LYors  Yo  tus s+ 

+ 

+  Vu Be) 

[11.4.8] 

IV yy 

This  last  magnitude  corresponds  to  the  effect  of  &,,  with  €),...-.  E,, 
and is  given  simply  by the  row  /, column  n  element  of  W,. 

-1,  constant 

The  recursive  information  ordering  in  [11.4.5]  through  [11.4.8]  is quite  com- 
monly  used.  For  this  ordering,  the  indicated  multipliers  can  be  calculated  from  the 
moving  average  coefficients  (W,)  and  the  variance-covariance  matrix  of e,  (QL)  by 
a  simple  algorithm.  Recall  from  Section  4.4  that  for  any  real  symmetric  positive 
definite  matrix  2,  there  exists  a  unique  lower  triangular  matrix  A  with  ls  along 
the  principal  diagonal  and  a  unique  diagonal  matrix  D with  positive  entries  along 
the  principal  diagonal  such  that 

Using  this  matrix  A  we  can  construct  an  (m  x  1) vector  u,  from 

Q.  =  ADA’. 

fe  —  A  le 

{11.4.9] 

[11.4.10] 

Notice  that  since  €,  is uncorrelated  with  its  own  lags  or  with  lagged  values  of y,  it 
follows  that  u,  Is also  uncorrelated  with  its  own  lags  or  with  lagged  values  of  y. 
The  elements  of u, are  furthermore  uncorrelated  with  each  other: 

cf 

E(u,u;)  = [A  'JE(e,e/)[A°  ']’ 

[A  'JQ[Ay 
[A>  ']ADA‘[A‘} >! 

Il 

=  D. 

(11.4.11] 

But  D is a  diagonal  matrix,  verifying  that  the  elements  of  u,  are  mutually  uncor- 
related.  The  (j, /) element  of D gives  the  variance  of u,,. 

If both  sides  of [11.4.10]  are  premultiplied  by A,  the  result  is 

| 

Au,  =  €,. 

[11.4.12] 

Writing  out  the  equations  represented  by [11.4.12]  explicitly, 

P'? 
Bs 

 NEPEES 
OPSY 

aS 
N) swede 

at 

Skp 

1, 
y; 

ani  ly 

I 

Son) 

| 

ay  P=) 

Sapa. 

{11.4.13] 

a,  a>  a,  oni 

bani. 

Ch, 

Thus,  ,,  is simply  ¢,,.  The  jth  row  of [11.4.13]  states  that 

ui,  =  ey  ‘TE  ayy,  x  jx,  st)? 

=  ffi,  a; j -  Lene 

But  since  ,,  is uncorrelated  with  u,,.  us...  . 
interpretation  as  the  residual  from  a  projection  of Fy ON  Uy,  U3,. 

.  U1,  it follows  that  uw, has  the 

A: 

E(e, |),  th 

ne  |  Mj; 14)  =  a, \44,  +  Q)2Us,  +  Alte 

as  ayy.  (4, le 

{ll 4, 14] 

The  fact  that  the  w,, are  uncorunlaied fucnee implies  that  the  coefficient  on 
, 14)  IS the same  as  the  coefficient  on 

M4),  iN a  projection  of €, ON  (ty).  Us...  . 

320  Chapter  11  | Vector Autoregressions 

4),  IN  a  projection  of  ¢,,  on  u,,  alone: 

[11.4.15] 
E(e,|uy,)  =  ajtt,,. 
Recalling  from  [11.4.13]  that  ¢,,  =  u,,,  we  see  that  new  information  about  the 
value  of ¢,,  would  cause  us  to  revise  our  forecast  of  ¢, by  the  amount 

ake, |e,,)  ‘a JE(e,,|u,,) 

OF), 

OU, 

=  M,,. 

[1 1.4.16] 

Now  ¢,,  has  the  interpretation  as  y,, — E(y,,|x,  _,) and  e,  has  the  interpretation 
as  y,  —  E(y,|x,  ,).  From  the  formula  for  updating  a  linear  projection  [4.5.14], 
the coefficient  on  y,,  in a  linear  projection  of ¥, ON  y,,  and  x,.,  is the  same  as  the 
coefficient  on  ¢,,  in a linear  projection  of ¢, on  &,,.''  Hence, 

JE (ei | Yu  X,_1)  na 

ayy, 

: 
aj 

11.4.17 
[ 

] 

rr  these “equations aa A, 2, . .., 

into  a  vector, 

i” 

; 

he  wos 

. 

oF 

y  phaaee 

r  iad 

BO.  fi 

where a, denotes  the  first  column  of A 

1 

—  = 

z 

pi 

<  (8 

"sa!  To  Amiens  a $4}  207003! 

‘ 
se  ied ences es for y,,, of new  information 

Htid’st 5; a ROTI, Sa feu 

where 

a.  = 

ay» 
te 
a4 

a, 

In  general, 

IE 
AE (e+. LY Ys ser  2 Yu Mt)  ayy ge 

sere:  FA 

[11.4.19] 

OY, 

where  a, denotes  the  jth  column  of  the  matrix  A defined  in  [11.4.9]. 

The  magnitude  in  [11.4.19]  is  a  population  moment,  constructed  from  the 
population  parameters  W,  and  92  using  [11.4.9].  For  a  given  observed  sample  of 
® ,  by OLS  and 
size  T, we  would  estimate  the  autoregressive  coefficients  ®,, 
construct  W, by simulating  the  estimated  system.  OLS  estimation  would  also  pro- 
vide  the  estimate  Q =  (1/T)  >/_, €,é;,  where  the  ith  element  of  €, is  the  OLS 
sample  residual  for  the  ith  equation  in  the  VAR  for  date  ft.  Matrices  A and  D 
satisfying  Q  =  ADA’  could  then  be  constructed  from  © using  the  algorithm  de- 
scribed  in Section  4.4.  Notice  that  the  elements  of the  vector  a,  =  A~'é,  are  then 
mutually  orthogonal  by construction: 

KE 

(UT)  >, &6).=  (VT)  >) Ac ee) (A)  =A  QA!)  =  D. 

ee 

1=1 

; 

The  sample estimate  of [11.4.19]  is then 

Wa, 

(11.4.20] 

where  a, denotes  the  jth  column  of the  matrix  A. 

é,,)  into  a  set  of  uncorrelated  components  (u,,,  .. 

A plot of [11.4.20]  as  a  function  of s  is known  as  an  orthogonalized  impulse- 
response  function.  It  is based  on  decomposing  the  original  VAR  innovations  (€,,, 
,  u,,)  and |  calculating 
the  consequences  for  y,,,  of a  unit  impulse  in u,;,.  These  multipliers  describe  how 
new  information  about  y,, causes  us  to  revise  our  forecast  of y,, ,,  though  the  implicit 
definition  of “new”  information  is different  for  each  variable  /. 

. 

What  is the  rationale  for  treating  each  .ariable  differently?  Clearly,  if the 
VAR  is being  used  as  a  purely  atheoretical  summary  of  the  dynamics  of a  group 
of variables,  there  can  be  none—we  could  just  as  easily  have  labeled  the  second 
variable  y,, and  the  first  variable  y,,,  in which  case  we  would  have obtained  different 
dynamic  multipliers.  By choosing  a  particular  recursive  ordering  of the  variables, 
the  researcher  is implicitly  asking  a  set  of questions  about  forecasting  of the  form 
of [11.4.5]  through  [11.4.8].  Whether  we  should  orthogonalize  in this  way  and  how 
the  variables  should  be  ordered  would  seem  to  depend  on  why  we  want  to  ask 
such  questions  about  forecasting  in  the  first  place.  We  will  explore  this  issue  in 
more  depth  in Section  11.6. 

Before  leaving  the  recursive  orthogonalization,  we  note  another  popular  form 
in  which  it is implemented  and  reported.  Recall  that  D is a  diagonal  matrix  whose 
(j. /) element  is  the  variance  of  u,,.  Let  D'?  denote  the  diagonal  matrix  whose 
(j. /) element  is the  standard  deviation  of u,,.  Note  that  [11.4.9]  could  be  written  as 

2  =  AD'@D'7A'  =  PP’. 

[11.4.21] 

where 

P =  AD'?. 

322 

Chapter  11  | Vector  Autoregressions 

Expression  [1.4.21]  is the  Cholesky  decomposition  ot  the  matrix  Q.  Note  that, 
like  A,  the  (m  x  ») matrix  P  is  lower  triangular,  though  whereas  A  has  Is  along 
its principal diagonal,  P has  the standard  deviation  of u, along its principal  diagonal. 

In place  of u, defined  in , 1.4.10],  some  researchers  use 

y,  =P!  pa  Pr VALS ,=D-'?u, 

= v,, iS just  uw,  divided  by its  standard  deviation  Vd,,. 

A one-unit  increase  in 

,  ls the  same  as  a  One-standard-deviation  increase  in  u,, 

In  place  of the  dynamic  multiplier  dy, ,, ,/du;,,  these researchers  then  report 

ay,,,,/dv,,.  Thesrelation  between  these  multipliers  is clearly 

a 
LEE  =  Yi +5 ys  Vd...  =  Wa a,  Vd,,- 
av, 

re) 
OU), 

Buta, Va; is just the jth column  of AD'”,  which is the jth column  of the  Cholesky 
factor matrix  P.  Denoting  the jth column  of P by p,,  we  have 

~ 

. 

Scutiqas 

IY, +5  = 

Yop, 

| 
11.4.22 

ee (u. 4: 22] is th [1.4.19]  nidstiphed by the constant VVar(u,).  | 
eee (11.4, 19] gives  the  consequences  of a  one-unit increase  in Yi where 
the - are those in  which  y,, itself is  measured.  Expression  [t1.4.22] gives the 

uences  if Yu up eeeemeand Var(u,,) 

(v 

vA4  a  Tee 

2  Eijuitiond 10. a fauitdohtdlemsifyithx ‘ester n forecasting a VAR s periods 
;  eee sc  a mmoness  To  29}  a1  sish  ad}  isigq 

bes  i tN ae 

‘ 

B.nseieRioyelzt  > [say 
anid itp onde son ac 
ae  meg  tie  We 
wes  = ees)’ dvoliehadt Oe STAGIIS  Be 

3a  cast  AaA Nt RES  aE aps 

BT 

i 

er =H 

iy plads sonata aa) 

am nme an b.1 Thiet Melba: — 
SN | :  wethey H  an 
Bell  Pavel ests  sods: hee  5  ae 

ein 

eet  cx 

> 

aah 

Vs  oie  eae  a  bed tite  o  Daag  S ‘lena’  beni Sa ae packs  domi 

ame  WH ,, 

(ae  Sag 

= 

G 

¢ 

= 

With  this  expression,  we  can  calculate  the  contribution  of  the  jth  orthogonalized 
innovation  to  the  MSE  of  the  s-period-ahead  forecast: 
Var(u,,):[a,a}  +  Wiaa/W)  +  Waals  +--+ 

+  Waa.  mF 

Again,  this  magnitude  in  general  depends  on  the  ordering  of  the  variables. 

As  s  —  x  for  a  covariance-stationary  VAR,  MSE(¥,,,\,)  >  To.  the  uncon- 
ditional  variance  of  the  vector  y,.  Thus,  [11.5.6]  permits  calculation  of the  portion 
of  the  total  variance  of  y,  that  is  due  to  the  disturbance  u,  by  letting  s  become 
suitably  large. 

Alternatively,  recalling  that  a;-\V/Var(u,)  is  equal  to  p,,  the  jth  column  of 

the  Cholesky  factor  P,  result  [11.5.6]  can  equivalently  be  written  as 

MSE(¥,..1))  =  2 [pp;  +  Wipp;¥,  +  W2pjp;¥> 

tot  pps  Wy]. 

[11.5.7] 

11.6.  Vector  Autoregressions  and  Structural 
Econometric  Models 

Pitfalls  in  Estimating  Dynamic  Structural  Models 

The  vector  autoregression  was  introduced  in  Section  10.1  as  a  statistical  de- 
scription  of the  dynamic  interrelations  between  n  different  variables  contained  in 
the  vector  y,.  This  description  made  no  use  of prior  theoretical  ideas  about  how 
these  variables  are  expected  to  be  related,  and  therefore  cannot  be  used  to  test 
our  theories  or  interpret  the  data  in  terms  of  economic  principles.  This  section 
explores  the  relation  between  VARs  and  structural  econometric  models. 

Suppose  that  we  would  like  to  estimate  a  money  demand  function  that  ex- 
presses  the  public's  willingness  to  hold  cash  as  a  function  of  the  level  of  income 
-  and  interest  rates.  The  following  specification  was  used  by some  early  researchers: 

M,  Hoke  =  Bots  Behe  toBstiot:  BMY  i  yP, ~ 

re 

[11.6.1] 

Here,  M, is the  log of the  nominal  money  balances  held  by the  public  at  date  1,  P, 
is the  log of the  aggregate  price  level,  Y, is the  log of real  GNP,  and  /, is  a nominal 
interest  rate.  The  parameters  B, and  B, represent  the  effect  of income  and  interest 
rates  on  desired  cash  holdings.  Part  of  the  adjustment  in  money  balances  to  a 
change  in  income  is thought  to  take  place  immediately,  with  further  adjustments 
coming  in subsequent  periods.  The  parameter  B, characterizes  this  partial  adjust- 
ment.  The  disturbance  v)? represents  factors  other  than  income  and  interest  rates 
that  influence  money  demand. 

It  was  once  common  practice  to  estimate  such  a  money  demand  equation 
with  Cochrane-Orcutt  adjustment  for  first-order  serial  correlation.  The  implicit 
assumption  behind  this  procedure  is that 

vP  =  pv? , +  u?, 

[11.6.2] 

where  u/ is white  noise.  Write  equation  [11.6.2] as (1 —  pL)v?  =  u? and multiply 
both  sides  of [11.6.1]  by (1  —  pL): 

M,  -  P,  =  (1  ~  p)By  +  BY,  -  BipY,.,  +  Bl,  -  Bl... 

[11.6.3] 

+  (By  +  p)(M,.,  — 

P,.,)  =  Bsp(M,-2  —  P,-2)  +  u?. 

324  Chapter  II  | Vector  Autoregressions 

Equation  {11.6.3}  is a  restricted  version  of 

M,  -  Pi =  a@  +  ai, Y,  +  anY,  !  +  al,  +  ag,  1 

+  a(M,.,  —  P,_,)  +  a(M,-, 

—°P,2s)eouPs 

[11.6.4] 

where  the  seven  parameters  (a,  a,.....  a.)  are  restricted  in  [11.6.3]  to  be 
nonlinear  functions  of  the  underlying  five  parameters  (p,  B,.  B,.  B>.  By).  The 
assumption of [11.6.2]  can  thus  be  tested  by comparing  the  fit of [11.6.3]  with  that 
from  unconstrained  estimation  of [11.6.4]. 

By definition,  v? represents  factors  influencing  money  demand  for  which  the 
researcher  has  no  explicit  theory.  It therefore  seems  odd  to  place  great  confidence 
in  a  detailed  specification  of its  dynamics  such  as  [11.6.2]  without  testing  this 
assumption  against  the data.  For example,  there  do not  seem  to be clear  theoretical 
grounds  for  ruling  out  a specification  such  as 

D v! 

D 
=  pv, 

+  pv?,  +  uP 

or,  for  that  matter,  a specification  in which  yP is correlated  with  lagged  values of 
Yor  /. 
Equation [11.6.1] further assumes that the dynamic multiplier relating money 
_  demand  to income is senate to that haus money ASAT to the interest 
entie 1G.28 Hlitow 

— 

t 

7 

© 

a(M, tisha 
eT  CF 

oa  Fics  oS,  BB’ 
3 

COMisl>  ONS  Ss°RgRC is i2 
02  .  love!  SHG  S1k.bes 
—-- basmob  iieias: ni A  liswe  es  suc 

; 
Biagiu’  SisgsmaRe  no  ek  Az  ohh 

'  a(M,,.,  =  Pex 

Or 

f 

se 

it 

od  idea to test teialoautngibathideneiengoninpit ‘on comparing 
the ft of  ‘ iil sith that  of a  more  general  dynamic model. Finally, inflation 
‘ffect:  on oe semend oy are not i aieatie by nounuy! interest rates. 
may 

Although  [11.6.5]  relaxes  many  of  the  dubious  restrictions  on  the  dynamics 
implied  by [11.6.1],  it is still  not  possible  to  estimate  [11.6.5]  by  OLS. because  of 
simultaneous  equations  bias.  OLS  estimation  of  [11.6.5]  will  summarize  the  cor- 
relation  between  money,  the  price  level,  income,  and  the  interest  rate.  The  public's 
money  demand  adjustments  are  one  reason  these  variables  will  be  correlated,  but 
not  the only  one.  For  example,  each  period,  the  central  bank  may  be  adjusting 
the  interest  rate  /, to  a  level  consistent  with  its policy  objectives,  which  may  depend 
on  current  and  lagged  values  of.income,  the  interest  rate,  the  price  level,  and  the 
money  supply: 

I, =  ky +  BSIM,  +  BWP,  +  BYY, 

+;  Bay My.  4  F BaP  +  BS)  1+  Bastin 
+  BLM,»  +  BY Pio  +  BAY 22 +  BY hort  27  - 
+  BWM, _, + BYP,  +  BLY 2g +  BY»  +  af. 

[11.6.6] 

Here,  for example,  B{> captures  the  effect  of the  current  price  level  on  the  interest 
rate  that  the  central  bank  tries  to  achieve.  The  disturbance  u* captures  changes 
in policy  that  cannot  be described  as  a deterministic  function  of current  and  lagged 
money,  the  price  level,  income,  and  the  interest  rate.  If the  money  demand  dis- 
is unusually  large,  this  will  make  M, unusually  large.  If BY)?  >  0, this 
turbance  uw? 
would  cause  /, to  be  unusually  large  as  well,  in which  case  u? would  be  positively 
correlated  with  the  explanatory  variable  /, in  equation  (11.6.5].  Thus,  [11.6.5] 
cannot  be  estimated  by OLS. 

af-3-0 

Nor  is central  bank  policy  and  endogeneity  of /, the  only  reason  to  be  con- 
cerned  about  simultaneous  equations  bias.  Money  demand  disturbances  and  changes 
in central  bank  policy  also-have  effects  on  aggregate  output  and  the  price  level,  so 
that  Y, and  P, in [11.6.5]  are  endogenous  as  well.  An  aggregate  demand  equation, 
for  example,  might  be  postulated  that  relates  the  level  of  output  to  the  money, 
supply,  price  level,  and  interest  rate: 

Y, ~  ky +  BY)M,  +  BSP,  +  BS2I, 

+  BY)M,_,  + 1 aE 

PBR  VEE  BYE 

[11.6.7] 

+  BS) M,  2+  BOP  2+  BYY,-2  +  BOL_2  +  °° 
+  BPM,»  +  BY Pip  +  BY Y-p  +  BL»  +  uA, 

with  u/  representing  other  factors  influencing  aggregate  demand.  Similarly,  an 
aggregate  supply  curve  might  relate  the  aggregate  price  level  to  the  ether  variables 
being  studied.  The  logical  conclusion  of  such  reasoning  is that  alt  of  the  date  ¢ 
explanatory  variables  in [11.6.5]  should  be  treated  as  endogenous.  | 

a 

Relation  Between  Dynamic  Structural  Models 
and  Vector  Autoregressions 
The  system  of equations  [11.6.5]  through  [11.6.7]  (along  with  an  analogous 
aggregate  supply  equation  describing  P,} can  be  collected  and  written  in vector 
form  as 

' 

Buy,  =k  +  Byy,_,  +  B,y,.  a 

i  ate +  U,. 

: 

{11.6.8}. 

326  Chapter  11  | Vector  Autoregressions 

where 

y,  =  (M,  P,.  Y,, 1,)' 

= 

4 
u,  =  (u/  »uU,,  us.  us y’ 

dD 

y 

—~ 

wud 
By  = 

% 

l 

Pes  (0) 
B\> 

* 

(QO) 
13 

at 

(0) 
Bis 

RW) 
21 

(0) 
31 

2 
ar 

l 

(0) 
—  B>, 

(0) 
—  Bo, 

— 

al) 
32 

al). 
42 

2) 

| 

_— 

Atv 
Bi, 

a 
14 

l 

k  =  (k,,  ka. ky, ky)’ 

and B, is a  (4  x  4) matrix  whose  row  /, column  j element  is given  by B\” for  s  = 
1,2,....,p.A 
large  class  of structural  models  for  an  (#7  x  1) vector  y,  can  be 
written  in the  form  of [11.6.8]. 

Generalizing  the  argument  in {11.6.3},  it is assumed  that  a  sufficient  number 
of lags of p are included and the matrices B, are defined  so  that  u, is vector  white 
RABE bdh segs say, u, followed an rth-order VAR, with 

Th HiMie  to  z2tostis 

u, =  stacéenaal  Ph gliiragy 
sen we could prema ny [11.6.8] by (ly — FL! ~ FL? ~~~  ~ BL) toarrive 
rae 

the same ee form as [11 by with p replaced by Me  + ryan nd with 

Ae ai noise disturbance  e,. 

ni  onitet  Ot} ribs 

SL  sere 

ea 

is  ie is # 

: 

x 

$ 

each side of  [11.6.8] is premultiplied  is Bec ‘the resulfis 

P 

sf ot bstsior 318 

2998 

pig  tale 

:.  tee  ti 

4 

e 

y,=c¢ 

+  Dy,  !  #  2y,  = 

+  ®,y, , /P  ie +e a ai  [116.9] 

it  might  turn  out  that 

e,,  =  0.3u?  —  0.6u*  +  0.1lu4  —  0.Suf. 

In  this  case,  if the  cash  held  by the  public  is larger  than  would  have  been  forecast 
using  the  VAR  (e,,  is positive),  this  might  be  because  the  public's  demand  for  cash 
is higher  than  is normally  associated  with  the  current  level  of  income  and  interest 
rate  (that  is, «” is positive).  Alternatively,  ¢,,  might  be  positive  because  the  central 
bank  has  chosen  to  ease  credit  (uf  is  negative),  or  a  variety  of  other  factors.  In 
general,  ¢,,  represents  a  combination  of all  the  different  influences  that  matter  for 
any  variables  in  the  economy.  Viewed  this  way,  it is not  clear  why  the  magnitude 
[11.6.13]  is of particular  interest. 

By contrast,  if we  were  able  to  calculate 

dy, +, 
nae 
au‘ 

11.6.14 
| 

this  would  be  of considerable  interest.  Expression  [11.6.14]  identifies  the  dynamic 
consequences  for  the  economy  if the  central  bank  were  to  tighten  credit  more  than 
usual  and  is a  key  magnitude  for  describing  the  effects  of monetary  policy  on  the 
economy. 

Section  11.4  also  discussed  calculation  of an  orthogonalized  impulse-response 
function.  For  2  =  E(e,e;),  we  found  a  lower  triangular  matrix  A and  a  diagonal 
matrix  D such  that  2  =  ADA’.  We  then  constructed  the  vector  A~'e,  and  calcu- 
lated  the  consequences  of changes  in each  element  of this  vector  for  future  values 
of y. 

. 

Recall  from  [11.6.12]  that  the  structural  disturbances  u,  are  related  to  the 

VAR  innovations  e,  by 

u,  =  Boe,. 

(11.6.15] 

Suppose  that  it happened  to  be  the  case  that  the  matrix  of structural  parameters 
B,, was  exactly  equal  to the  matrix  A~'.  Then  the orthogonalized  innovations  would 
coincide  with  the  true  structural  disturbances:  | 

[11.6.16] 
u,  =  Bye,  =  A-'e,. 
In this case,  the  method  described  in Section  11.4 could  be used  to find  the  answers 
to  important  questions  such  as  [11.6.14). 

Is there  any  reason  to  hope that  B,, and  A ~~!  would  be the same  matrix?  Since 
A is lower  triangular,  this  clearly  requires  B, to be lower  triangular.  In the  example 
[11.6.8].  this  would  require  that  the  current  values  of P, ’Y.  and  / do  not  influence 
money  demand,  that  the  current  value  of  M  but  not  that  of  Y or  / enters  into  the 
aggregate  supply  curve,  and  so  on.  Such  assumptions  are  rather  unusual.  though 
there  may  be  another  way  to  order  the  variables  such  that  a  recursive  structure  is 
more  palatable.  For example.  a Keynesian  might argue  that  prices respond  to other 
economic  variables  only  with  a  lag, so  that  the  coefficients  on  current  variables  in 
the  aggregate  supply  equation  are  all  zero.  Perhaps  money  and  interest  rates  in- 
fluence  aggregate  demand  only with  a lag, so  that  their  current  values  are  excluded 
from  the  aggregate  demand  equation.  One  might  try  to  argue  further  that  the 
interest  rate  affects  desired  money  holdings  only with  a  lag as  well.  Because  most 
central  banks  monitor  current  economic  conditions  quite carefully,  perhaps  all the 
current  values  should  be included  in the equation  for /,, These  assumptions  suggest 
ordering  the variables  as y,  =  (P,.  Y,. M,, /,)’, for which  the structural  model  would 
328  Chapter  11  | Vector  Autoregressions 

P, 
ge 
M,\ 
, 

, 

{k, 
k, 

0. 

Bs 
ps 
Bs 

0 

0 

(a) 
32 

(0) 
42 

Gan-4 

O 

P, 
Yy, 

M,. 

0 

I, 

0 

) 

(1h) 
43 

q)) 

r,. 

Bs 

Yoni 

[11.6.17] 

aly 
P) 

M,  | 
L 

Bi} 1) 

Bs 
Bs 

(1) 
42 

(1) 

13 BY 
BY 
py 
pi 
pe  aw 

BY) 
“py 

i 

BY +m 
Ase AR 
ae 

|| yo 

Fern  +ip 
L-p 
36) 

tht 

uit  > 

Ti 

ates Pepe si is9 nie si nom 

ee 

ba ozeevinale the variables f¢ fc 

dynam  structural model on 6.8) a as 

aij  th 

Bay, = Ps + a pape 

Git 

tisli#litisins  to wol  ai  fsbon, 

Tape 8. 

) 

pyattte  1 — 
F116. 18] 

Viiess  hy 

— 

P 

f 

—~VI 

‘ 

a 

5 

; 

Tey. 

7 

io  te  § is  a ‘ 
opal!  AB: 

Seta 

Note  that  if the  only  restrictions  on  the  dynamic  structural  model  are  that  B, 
is  lower  triangular  with  unit  coefficients  along  the  principal  diagonal  and  that  D  is 
diagonal,  then  the  structural  model  is  just  identified.  To  see  this,  note  that  these 
restrictions  imply  that  B,,'  must  also  be  lower  triangular  with  unit  coefficients  along 
the  principal  diagonal.  Recall  from  Section  4.4  that  given  any  positive  definite 
symmetric  matrix  92,  there  exist  a  unique  lower  triangular  matrix  A  with  Is  along 
the  principal  diagonal  and  a  diagonal  matrix  D  with  positive  entries  along  the 
@  =  ADA’.  Thus,  unique  values  By  '  and  D  of  the 
principal  diagonal  such  that 
required  form  can  always  be  found  that  satisfy  {11.6.23],  Moreover,  any  B, matrix 
of this  form  ts elas ea  so  that  [ in  [11.6.21]  can  be  calculated  uniquely  from 
—B,  Il‘.  Thus,  given  any  allowable  values  for  the  reduced- form 
B,, and  Il as  T 
parameters  (Il ia (2),  there  exist  unique  values  for  the  structural  parameters  (B,,. 
lr,  and  D)  of  the  specified  form,  establishing  that  the  structural  model  is  just 
identified. 

Since  the  model  is just  identified,  full-information  maximum  likelihood  (F/ML) 
estimates  of  (B,,  I’.  and  D)  ean  be  obtained  by  first  maximizing  the  likelihood 
function  with  respect  to  the  reduced-form  parameters  (II  and  (2)  and  then  using 
the  unique  mapping  from  reduced-form  parameters  to  find  the  structural  param- 
eters.  The  maximum  likelihood  estimates  of IT are  found  from  OLS regressions  of 
the elements  of y, on  x,,  and  the  MLE of Q1 is obtained  from  the  variance-covariance 
matrix  of the  residuals  from  these  regressions.  The  estimates  B,, '  and  D are  then 
found  from  the  triangular  factorization  of 2.  This,  however,  is precisely the  pro- 
cedure  described in  calculating  the  orthogonalized  innovations  in Section  11.4.  The 
estimate  A described  there  is  thus  the  same  as  the  F/ML  estimate  of  B,'.  The 
vector  of orthogonalized  residuals  u,  =  A~'e,  would  correspond  to  the  vector  of. 
structural  disturbances‘,  and  the orthogonalized  impulse-response  coefficients  would 
give the  dynamic  consequences  of the  structural  events  represented  by u,,  provided 
that  the  structural  model  is lower  triangular  as  in  [11.6.17]. 

Nonrecursive  Structural  VARs 

Even  if the  structural  model  cannot  be written  in  lower  triangular  form,  it 
may  be  possible  to  give  a  structural  interpretation  to  a  VAR  using  a  similar  idea 
to  that  in  equation  {11.6.23].  Specifically,  a  structural  model  specifies  a  set  of 
restrictions  On  B, and  D,  and  we  can  try  to  find  values  satisfying  these  restrictions 
such  that  B,  'D(B,  ')’ = Q.  This  point  was  developed  by Bernanke  (1986).  Blan- 
chard  and  Watson  (1986).  and  Sims  (1986). 

For  illustration,  consider  again  the  model  of supply  and  demand  discussed  in 
equations  [9.3.2]  and  [9.3.3].  In that  specification,  quantity  (q,) and  price  (p,) were 
endogeneous  variables  and  weather  (w,)  was  exogenous,  and  it was  assumed  that 
both  disturbances  were  1.i.d.  The  structural  VAR  approach  to  this  model  would 
allow  quite  general  dynamics  by adding p lags of all  three  variables  to  equations 
[9.3.2]  and  [9.3.3].  as  well  as  adding  a  third  equation  to  describe  the  dynamic 
behavior  of weather.  Weather  presumably  does  not  depend  on  the  behavior  of the 
market.  so  the  third  equation  would  for  this  example  va be  a  univariate  auto- 
regression.  The  model  would then  ne 

q,  =  Bp,  +  Bd  “q  + Bp,  r  BY, 

+  Bs q,  2  +  Bp,  >  +  pi)’ W,.  ?  sys 

[11.6.24) 

he  BY’ q,  - “p  >  BiS’P,- ay  +  fie? W, . Pp  4  uj! 

330  = Chapter  11 [i Vector  Autoregressions 

- 

1  =  yp,  +  hw,  + 

BS)q,  ,  +  Bs;'p,  ,  + ids  , 

+ 

ety  ear  tee  P 
Bo, 
(yp 

a 
Pixs  *  BS ,-,  +  GF; 
, 
‘ 
(p) 

.. 

[11.6, 25] 

= 

Phos 

2 

w=  RR w,  ,  +  BY Als,  Sythe 

" 

») 

us 
A  BY Wp»  +  Uy. 

[11.6.26] 

We could  then  take  (w‘’, u},  u)")'  to  be  a  white  noise  vector  with  diagonal  variance- 
covariance  matrix  given  by D.  This  is an  example  of  a  structural  model  11.6.18] 
in which 

PiL-8B 
By=|}lo-y  —At. 

0 

6  | 

[11.6.27] 

There  is no  way  to order  the  variables  so  as  to  make  the  matrix  B, lower 
triangular.  However,  equation [11.6.22]  indicates  that  the  structural  disturbances 
u, are  related  to  the  VAR  residuals  e, by €,  =  B, 'u,.  Thus,  if B, is estimated  by 
_  maximum  likelihood,  then  the  impulse-response  functions  could  be calculated  as 
in Section  11.4 with  A teplaced by B,', and the results would  give the effects  of 
each of the structural  disturbances on ‘eee  values of variable és of the system. 
—  wiieo SWVAAOS.  di  11} oteé  {O€.4.1 

{] baw  f1E.0.1  1 gatiutinedue 
HUXEM  OF  FE  OB | of bri  a  : M2003  48  faatsc }  Sy ‘ PISISINGIS  | 

i681) 

iilest 
i@igot 

; 

(250).  — “loathed 
(SY) 

Oa 
ent) Bas  hee cca 
ee  =  Bo — (wy) 

. 

de, 

Tt 

: 
‘a 
CT  Gay 

= 

>  tha sates o one, of the jth sect disturbance « Aiy is given by b’, the jth 

nn  oO 

rapt 

Ft): Thuis, we would calculate esariiam  Simin  teixs  919d)  vi  seri 

bas,  estemites  souborq  a ay te. 

eehicet  the unknow 

hee  De  “ = Wb! 

iteximingen  rad. 2.3  C  UA  z 
= 
“gaivizive  a 

Pe  aie 
ue 

a.  ee 
43 
ne 

leas te Pisa Per ee 

y 

a 

ae 

~~  os 

;  ae  ae 

ty, 

. 

. 

. 

[11.1.25]  produces 

‘£(B,,  D.  I) =  —(Tn/2)  log(2m)  —  (77/2)  log|B,  'D(By  ')’| 

1 

-  (1/2)  > é;[B;  'D(B,')']  '€,. 

[11.6.29] 

But 

7 

> é/[B,  'D(B,  ')']-'é, 
tel 

T 

'D(By ')’]~'€d 
=  > trace{é;/[B; 

r= | s 

= 

py prpeell iy "DCB 

5:')  _  (11.6.30] 

Bs)  ez, 

=  trace{{B,;  'D(B,  ')‘]~'7-  2} 

=x  trace{(B,  'D(B,  ')']-'Q} 
=  T  x  trace{(B/D~'B,)Q}. 

Furthermore, 

log|B,;  'D(B,  ')'|  =  log{/B,'|-|D]-|By'|}  =  —log|B,I?  +  log|D|. 

[11.6.31] 

_  Substituting  [11.6.31]  and  [11.6.30]  into  [11.6.29],  FJ/ML  estimates  of the  structural 
parameters  are  found  by choosing  B, and  D so  as  to  maximize 

¥(By.  D,  FH) =  —(Tni2) log(2m)  +  (7/2)  log|Bal?  -  (7/2)  log|D| 

=  (7/2)  trace{(BD~  'B,)Q}. 

. 
fecal 

Using  calculations  similar  to  those  used  to  analyze  [11.1.25].  one  can  show 
ss if  there  exist  unique  matrices  B,  and  D  of  the  required  form  satisfying 
B, 'D(B,;')' = Q,  then  maximization  of  [11.6.32]  will  produce  estimates  B, and 
D satisfying 

i¢ 

peasy  sa. 

[1.6.33] 

This  is a  nonlinear  system  of equations,  and  numerical  maximization  of [11.6.32] 
offers  a  convenient  general  approach  to  finding  a  solution  to  this  system  of 
equations. 

| 

Identification  of Structural  VARs 

The  existence  of  a  unique  maximum  of  [11.6.32]  requires  both  an  order 
condition  and  a  rank  condition  for  identification.  The  order  condition  is  that  B, 
and  D have  no  more  unknown  parameters  than  2.  Since  2  is symmetric,  it can 
be summarized  by n(n  +  1)/2 distinct  values.  If D is diagonal,  it requires  n  param- 
eters,  meaning  that  B, can  have  no  more  than  n(n  —  1)/2 free  parameters.  For  the 
supply-and-demand  example  of [11.6.24]  through  [11.6.26],  1  =  3, and  the  matrix 
B, in [11.6.27]  has 3(3  —  1)/2  =  3 free  parameters  (8, y, andh).  Thus,  that  example 
satisfies  the  order  condition  for  identification. 

Even  if the order condition is satisfied,  the  model  may  still  not  be  identified, 

For  example,  suppose  that 

B,,  = 

et 

l 

a 

Cn’ 

0 

0) 

. 

A 

332  Chapter  11  | Vector Autoregressions 

, 

Even though this specification  satisfies  the  order  condition.  it fails  the  rank  con- 
dition,  since  the value  of the  likelihood  function  will  be  unchanged  if B and  y are 
switched  along  with  o2 and  o?. 

To characterize  the  rank  condition,  suppose  that  there  are  /y elements  of B, 
that  must  be  estimated;  collect  these  in  an  (n, x 1) vector  64.  The  identifying 
assumptions  can  be  represented  as  a  known  (nn?  x  ng)  matrix  Sp  and  a  known 
(n*  x  1) vector  s,  for  which 

{1 1.6.34] 
vec(B,)  ™  $,9,  +  Sag. 
For example,  for the dynamic  model  of supply  and  demand  represented  by [11.6.27], 

44  puede  pumA 

wataoy 

labor  att.  Re 
Ges0  Of  ven  Dhury 

Bart) 

ots 

‘  beable. 
.
w
F

A 

Re 

Pao  cree  ee 
, 

: 

= iy 

teat  Ain’  bhtoy 
dil —  adie igen  bias 

is 

3a} ipets  Rit  ABOF 

“ee witiles: Ai 

Te 

pdiga® VSbasasini  Se = 
BK, : 
SqHRM OSE  (0% 1 
ee. oh, ae dyna eS 

| 

:  gnidtdinglic mw  3 
RRA Tis xcs bas el isonet fi 

Oi }  wel 

x > LS +34} Pesta Rid nig ae 

erence 
sees 

re 

: 

ae  ROLE, 4 an3  3 tSB5 

ron al y. 

<a  recs i  mn  Gi: hata ie oe papeniirns 2.  ro  es5 

% 

nl 

vee 

: 

: 

. 
moet.  a  a  I 

vi 

ee 

q 

. 

a 

* 

: 

ee  eC  a  aad  ‘Wick 

i, 

. 

és 

 
Since  [11.6.33]  is  an  equation  relating  two  symmetric  matrices,  there  are 

n*  =  n(n  +  1)/2  separate  conditions,  represented  by 

vech(Q)  =  vech((B.(0y)  (0(0,  )K(B,(@,)1-"Y) 

([11.6.36] 

Denote  the  right  side  of  [[1.6.36]  by f(@,,  @,,),  where  f:  (R’"  x  R’'”)  >  R”  : 
vech(Q)  =  f(0,.  8,,). 

[1.6.37] 

Appendix  11.B  shows  that  the  [7°  xX  (ny,  +  A,)]  matrix  of  derivatives  of  this 
function  is given  by 

|! vech(Q) 
a0, 

seat 
a0}, 

(11.6.38] 

[ —  2D;  (2 @ Br')Sp) 

DY  [(Bs')  @ (Be "18>. 

where  D;  is the  (1 * x  n?)  matrix  defined  in  [11.1.45]. 

Suppose  that  the  columns  of the  matrix  in  [11.6.38]  were  linearly  dependent; 
that  is, suppose  there  exists  a  nonzero  [(m,  +  np)  X  1] vector  A such  that  JA  =  0. 
This  would  mean  that  if a  small  multiple  of  A were  added  to  (@;,  6;,)',  the  model 
would  imply  the  same  probability  distribution  for the  data.  We  would  have  no  basis 
for  distinguishing  between  these  alternative  values  for  (6,,  8;,),  meaning  that  the 
model  would  be  unidentified. 

Thus,  the  rank  condition  for  identification  of a  structural  VAR  requires  that 
the-(n,  +  ”,) columns  of the  matrix  J in [11.6.38]  be  linearly  independent.'>  The 
order  condition  is that  the  number  of rows  of J (n' *  =  n(n  +  1)/2)  be  at  least  as 
great  as  the  number  of columns. 

To  check  this  condition in  practice,  the simplest  approach  is usually  to  make 
a  guess  as  to  the  values  of  the  structural  parameters  and  check  J numerically. 
Giannini  (1992)  derived  an  alternative  expression  for  the  rank  condition  and  pro- 
vided  computer  software  for  checking  it numerically. 

Structural  VAR  with  Restrictions  on ll 

The  supply-and-demand  example  of [11.6.24]  to  [11.6.26]  did  not  satisfy  the 
assumptions  behind  the  derivation  of  [11.6.32],  because  [11.6.26]  imposed  the 
restriction  that  lagged  values  of p and  q did  not  belong  in the  weather  equation. 
Where  such  restrictions  are  imposed,  it  is  no  longer  that  case  that  the  FIML 
estimates  of TI]  are  obtained  by  OLS,  and  system  parameters  would  have  to  be 
estimated  as  described  in Section  11.3.  As an  alternative,  OLS estimation  of [11.6.24] 
through  [11.6.26]  would  still  give  consistent  estimates  of  II,  and  the  variance- 
covariance  matrix  of the  residuals  from  these  regressions  would  provide  a consistent 
estimate  2.  One  could  still  use  this  estimate  in  [11.6.32],  and  the  resulting  max- 
imization  problem  would  give  reasonable  estimates  of B, and  D. 

Structural  VARs  and  Forward-Looking  Behavior 

The  supply-and-demand  example  assumed  that  lagged  values of price  and 
quantity  did  not  appear  in  thé  equation  for  weather.  The  spirit  of  VARs  is that 

"This  condition  characterizes  /ocal  identification;  it may  be  that even  if a  model  satisfies  both  the 
rank  and  the  order  condition,  there  are  two  noncontiguous  values  of (@},.  @},)  for which  the likelihood 
has  the  same  value  for all  realizations  of the  data.  See  Rothenberg  (1971,  Theorem  6.  p.  585). 

334  Chapter  11  | Vector  Autoregressions 

such  assumptions  ought  to  be  tested  before  being  imposed.  What  should  we  con- 
clude  if,  contrary  to  our  prior  expectations,  the  price  of  oranges  turned  out  to 
Granger-cause  the  weather  in  Florida?  It  certainly  cannot  be  that  the  price  is  a 
cause  of  the  weather.  Instead.  such  a  finding  would  suggest  forward-looking  be- 
havior  on  the  part  of  buyers  or  sellers  of  Oranges;  for  example,  it  may  be  that  if 
buyers anticipate  bad  weather  in  the  future,  they  bid  up  the  price  of oranges  today. 
If  this  should  prove  to  be  the  case,  the  identifying  assumption  in  [11.6.24]  that 
demand  depends  on  the  weather  only  through  its  effect  on  the  current  price  needs 
to  be  reexamined.  Proper  modeling  of  forward-looking  behavior  can  provide  an 
alternative  way  to  identify  VARs,  as  explored  by Flavin  (1981  ), Hansen  and  Sargent 
(1981).  and  Keating  (1990),  among  others. 

j 

Other  Approaches  to  Identifying  Structural  VARs 
Identification  was  discussed  in  previous  subsections  primarily  in  terms  of 
exclusion  restrictions  on  the  matrix  of  structural  coefficients  B,.  Blanchard  and 
Diamond  (1989,  1990)  used a priori  assumptions  about  the  signs  of  structural 
parameters  to  identify  a  range  of  values  of  B, consistent  with  the  data.  Shapiro 
and  Watson  (1988)  and  Blanchard  and  Quah  (1989)  used  assumptions  about  long- 
run  multipliers  to  achieve  identification. 

A  Critique  of Structural  VARs 

Structural  VARs  have  appeal  for  two  different  kinds  of  inquiry.  The  first 
potential  user  is  someone  who  is  primarily  interested  in  estimating  a  structural 
equation  such  as  the  money  demand  function  in  [11.6.1].  If  a  model  imposes 
restrictions  on  the  dynamics  of the  relationship,  it seems  good  practice  to  test  these 
restrictions  against  a  more  general  specification  such  as  [11.6.5]  before  relying  on 
the  restricted  model  for inference.  Furthermore,  in order  to  estimate  the  dynamic 
consequences  of,  say,  income  on  money  demand,  we  have  to  take  into  account  the 
fact  that,  historically,  when  income  goes  up,  this  has  typically  been  associated  with 
future  changes  in  income  and  interest  rates.  What  time  path  for  these  explanatory 
variables  should  be assumed  in order  to assess  the consequences  for money  demand 
at  time  f  +  s  of  a  change  in  income  at  time  tf?  A  VAR  offers  a  framework  for 
posing  this  question — we  use  the  time  path  that  would  historically  be  predicted 
for  those  variables  following  an  unanticipated  change  in income. 

A  second  potential  user  is  someone  who is interested  in  summarizing  the 
dynamics  of a  vector  y,  while  imposing  as  few  restrictions  as  possible.  Insofar  as 
this  summary  includes  calculation  of  impulse-response  functions.  we  need  some 
motivation  for what  the  statictics  mean.  Suppose  we find that there  is a  temporary 
rise  in income  following  an  innovation  in money.  One  is tempted  to  interpret  this 
finding  as  suggesting  that  expansionary  monetary  policy has  a  positive  but  tem- 
porary  effect  on  output.  However,  such  an  interpretation  implicitly  assumes  that 
the  orthogonalized  “money  innovation”  is the  same  as  the  disturbance  term  in  a 
description  of central  bank  policy.  Insofar  as  impulse-response  functions  are  used 
to make  statements  that  are  structural  in nature,  it seems-reasonable  to  try  to  use 
an orthogonalization  that  represents  our  understanding  of these  relationships  as 
well as possible.  This point has been  forcefully  argued by Cooley and  LeRoy (1985), 
Leamer  (1985),  Bernanke  (1986),  and  Blanchard  (1989),  among  others. 

Even  so,  it must  be  recognized  that convincing  identifying  assumptions  are 
is clearly  somewhat  ar- 
hard to come  by. For  example.  the  ordering  in [11.6.17] 
bitrary, and the exclusion  restrictions  are difficult  to defend.  Indeed,  if there  were 
compelling  identifying  assumptions  for  such  a  system,  the  fierce  debates  among 
11.6.  Vector  Autoregressions  and Structural Econometric  Models 

335 

; 

macroeconomists  would  have  been  settled  Jony  ago!  Simultaneous  equations  bias 
is  very  pervasive  in  the  social  sciences,  and  drawing  structural  inferences  from 
observed  correlations  must  always  proceed  with  great  care.  We  surely  cannot  always 
expect  to  find  credible  identifying  assumptions  to  enable  us  to  identify  the  causal 
relations  among  any  arbitrary  set  of  n  variables  on  which  we  have  data. 

11.7.  Standard  Errors  for Impulse-Response  Functions 

Standard  Errors for Nonorthogonalized  Impulse-Response 
Function  Based  on  Analytical  Derivatives 

Section  11.4  discussed  how  W,,  the  matrix  of  impulse-response  coefficients 
at  lag s,  would  be  constructed  from  knowledge  of  the  autoregressive  coefficients. 
In  practice.  the  autoregressive  coefficients  are  not  known  with  certainty  but  must 
be estimated  by OLS  regressions.  When  the  estimated  values  of the  autoregressive 
coefficients  are  used  to  calculate  W,,  it  is  useful  to  report  the  implied  standard 
errors  for  the  estimates  W,.'* 

Adopting  the  notation  from  Proposition  I1.1,  let  kK =  np  +  I denote  the 
number  of coefficients  in  each  equation  of  the  VAR  and  let  m  =  vec(If)  denote 
the  (1k  X  1) vector  of parameters  for  all  the  equations;  the  first  k clements  of tr 
give  the  constant  term  and  autoregressive  coefficients  for  the  first  equation.  the 
next  k elements  of am  give  the  parameters  for  the  second  equation,  and  so  on.  Let 
w, =  vec(W;)  denote  the  (n°?  x  1) vector  of moving  average  coefficients  associated 
with  lag s.  The  first  1  elements  of wp, are  given  by the  first  row  of W, and  identify 
the  response  of y,,,,  to  €,.  The  next  1  elements  of Ws, are  given  by the  second  row 
of W, and  identify  the  response  of y,,,,  to  €,,  and  so  on.  Given  the  values  of the 
autoregressive  coefficients  m a,  the  VAR  can  be  simulated  to  calculate  w,.  Thus, 
, could  be  regarded  as  a  nonlinear  function  of  a,  represented  by the  function 
(77).  b:  RX  SOR 

| 

The  impulse-response  coefficients  are  estimated  by replacing  am  with  the  OLS 
estimates  7,  generating  the  estimate  wb,  =  wW,(7,).  Recall  that  under  the  con- 
ditions  of Proposition  11.1,  \/T(#,  —  m)  —>  X.  where 

x~  M0. (2@e), 

; 

[11.7.1] 

Standard  errors  for , can  then  be  calculated  by applying  Proposition  7.4: 

— W.) > GX, 
VT(h,.7 

where 

That  is, 

G, 
(ie  x  nk) 

=  lash 
on 

| 

[11.7.2] 

VT.  —  WO) v(0. G(X 

BQ NG), 

[11.7.3] 

Standard  errors  for  an  estimated  impulse-response  coefficient  are  given  by the 

"Calculations  related  to those  developed  in this section  appeared  in Buillic  ( 1987), Litkepohl  (1989,  | 
1990),  and  Giannini  (1992).  Giannini  provided  computer  software  for  calculating  some  of these  mag- 
nitudes. 

336  Chapter  11  | Vector  Autoregressions 

Square  root  of the associated  diagonal  element  of (1/T)G,  ,(Q, ®Q,  ')G! ,.  where 

&.s  %.(t) 

onr 

a 

v 

Q,  *  (1/T)  > Fa; 

r=1 

with  x, and  re) ,  as  defined  in  Proposition  11.1. 

To  apply  this  result,  we  need  an  expression  for  the  matrix  G,  in  [11.7.2]. 
Appendix  11.B  to  this  chapter  establishes  that  the  sequence  {G,}’"_,  can  be  cal- 
culated  by iterating  on 

G.=(1.@@,  Wo,  Wo.  es  WL) + (@, @1)G,  , 

+  (®, @1,)G,_.  +  ---  +  (®, @1,)G,_,.° 

[11.7.4] 

Here 0,,, denotes  an  (n  x  1) vector  of zeros.  The  iteration  is initialized  by setting 
G, ="Glp  =  G_,.,; =  9,:,,.  It  is also  understood  that  W,  =  1, and 
wv,  =  0,, Ht < 0. Thus, for  example,  | 

of 

is  [L, 3) (0,,, 

L,  0,,,,  or 

0,,,)] 

=[1@ 0,  Wi  b  --+ 

0,,))  +  (%,  @1)G6). 

A Penne: solution for [11.7.4] is  given by 

dint 

‘@AOHBVONAL  9/1! 

G, = ¥ [W,  1® (0,,,  i  '  WV’  is  ||  feu  A  ie hI [11.7,5] 

| 

3 

a  mem - Each ont, to Garey. Standard  Errors 
|  eeitedilineetthegeeplinedslmnnise- Response, .kncti9" 

The matrix  of derivatives G, can attermaively be  calculated numerically a “as 

ere  rie we use the: OLs ml.  ace ge  RS for $5  ls Ls 2. es 

287  £ f° 

hope  is that  the  distribution  of u  is similar  to  the  distribution  of the  true  population 
e's.  Then  take  a  random  draw  from  this  distribution  (denoted  uj’),  and  use  this 
to  construct  the  first  innovation  in  an  artificial  sample;  that  ts,  set 

! 

7 

; 

- 

: 

: 

. 

A 

(1) 
yi  =O  +  Dy,  t  Dy,  +0  +  +  Dry_pss  +  uy’. 
where  Yu.  ¥-0.- 4  and  y_,,,,  denote  the  presample  values of y that  were  actually 
observed  in  the  historical  data.  Taking  a  second  draw  uS’’,  generate 

; 

; 

; 

yy  =  &+  @,y\"  +  ®,y,  Fit  cc 

Pel cenach  us’ 

. 

,  yf}  can  be  generated. 

Note  that  this  second  draw  is  with  replacement;  that  is,  there  is  a  (1/7)  chance 
that  u‘'  is  exactly  the  same  as  u$').  Proceeding  in  this  fashion,  a  full  sample 
{yi  yS?,  ... 
A  VAR  can  be  fitted  by  OLS  to  these 
simulated  data  (again  taking  presample  values  of  y  as  their  historical  values), 
producing  an  estimate  7‘').  From  this  estimate,  the  magnitude  ,(#"'’)  can  be 
calculated.  Next,  generate  a  second  set  of  7 draws  from  the  distribution  of  u, 
denoted  {u‘\?,  ul,  ...,  u?},  fit  aw?)  to  these  data  by  OLS,  and  calculate 
W,(7'?).  A  series  of  10,000  such  simulations  could  be  undertaken,  and  a  95% 
confidence  interval  for  W,,(7)  is  then  mferred  from  the  range  that  includes  95% 
of the  values  for  w,,(#"”). 

Standard  Errors  for Parameters  of a  Structural  VAR 
Recall  from  Proposition  11.2  and  equation  [11.1.48]  that  if the  innovations. 

are  Gaussian, 

VT[vech(Q,)  —  vech(Q)]  > n(o 2D, (2 @ 2)\(D;  }. 

The  estimates  of the  parameters  of a  structural  VAR  (B, and  D) are  determined 
as  implicit  functions  of Q from 

0 =  B;'D(B5")'. 
[11.7.6] 
As  in  equation  [11.6.34],  the  unknown  elements  of  B,  are  summarized  by  an 
(ng  X  1) vector  @,  with  vec(B,).  =  S,0,  +  sg.  Similarly,  as  in  [11.6.35],  it is 
assumed  that  vec(D)  =  Sp)@p  +  sp for 8, an  (np  X  1) vector.  It then  follows  from 
Proposition  7.4  that 

VT(6, 7 —  9%)  (0. 26,D,* (2 @ 2)\(D;)'G ,)  [11.7.7] 

p 

L 

VT(8p.r  -  8) >  v(0. 26D,"  (Q @ 2)(D;  Gi).  [11.7.8] 

where 

Gyoo= 

(mer) 

08, 
a[vech(Q)]' 
30, 

G 

Wines  S  a{vech(Q)]' 

[11.7.9] 

[1.7.10] 

and  n*  =  n(n  +  1)/2. 

Equation  [11.6.38]  gave an expression  for  the  [n*  x  (ny  +  Ny)]  matrix: 
 — 

J-  avech(Q) 

00), 

a vech(N) 
00 

We  noted there  that if the  model  is to  be identified,  the  columns  of this  matrix 
must  be linearly  independent.  In the just-identified  case.  n*  =  (ny,  +  n,) andj! 
338  Chapter  11  | Vector  Autoregressions 

exists,  from  which 

ee | srt, 

(11.7.11] 

Standard  Errors  for Orthogonalized 
Impulse-Response  Functions 

Section  11.6  described  calculation  of  the  following  (n  x  n) matrix: 

H,  =  WB, '. 

(11.7.12] 

The  row  i, column  j element  of this  matrix  measures  the  effect  of the jth structural 
disturbance  (u,,.) on  the  ith  variable  in  the  system  (y,,,,)  after  a  lag of s  periods. 
Collect  these  magnitudes  in  an  (nm?  x  1) vector  h, =  vec(H’‘).  Thus,  the  first  n 
elements  give  the  effect  of 
elements  of h, give  the  effect  of u, on  y,,,,,  the  next 
u, On  y2,,,,  and so  on. 

Since  W, is  a function  of # and  since  B, is  a  function  of vech(2),  the  distri- 
butions  of both  the  autoregressive  coefficients  and  the  variances  affect  the  asymp- 
totic  distribution  of  h,.  It  follows  from  Proposition  11.2  that  with  Gaussian  in- 
novations, 

VT(h,.7  =  h,) 

L 
4  (0, E.  =.1|  0  wren;  | =|)  (11.7.13] 

,[29@o° 

zi 

0 

_ 

+  (a, [=,.(Q@Q-')E,  + 2E,D;7(Q  ®@ 2)(D;7)'=. I) 

where  Appendix  11.B  demonstrates  that 

; 

=.  =  dh,/an’  =  [1,  © (By) ~'|G, 
Be =  Zoey 7 

~  lH  ®@ Bi" 'SeGe 

[11.7.14] 
(11.7.15] 

Here G, is the  matrix  given  in [11.7.5],  G, is  the  matrix  given  in (11.7.11],  and  S,. 
is an  (n?  X  ng)  matrix  that  takes  the  elements  of 6, and  puts  them  in the  corre- 
sponding position  to  construct  vec(B,)): 

For  the  supply-and-demand  examples  of [11.6.24]  to  [11.6.26], 

vec(B;)  =  S,8,  +  Sq. 

S28) 

6 
6 
5: 
ey  ee 
0 
Cisoa 
Oe  0 
D 
nk. 
Oyo 
QA  Ly 
ud 
2 
a 
co 
.0 
a 
op  0 

: 

Practical  Experience  with  Standard  Errors, 
In practice,  the standard  errors  for dynamic  inferences  based  on  VARs  often 
turn  out  to  be  disappointingly  large  (see  Runkle,  1987,  and  Litkepohl,  1990). 
339 

11.7.  Standard  Errors for Impulse-Response  Functions 

Although  a  VAR  imposes  few  restrictions  on  the dynamics,  the cost  of this generality 
is  that  the  inferences  drawn  are  not  too  precise.  To  gain  more  precision,  it  Is 
necessary  to  impose  further  restrictions.  One  approach  is  to  fit  the  multivariate 
dynamics  using  a  restricted  model  with  far  fewer  parameters,  provided  that  the 
A  second  approach  is  to  place  greater 
data  allow  us  to  accept  the  restrictions. 
reliance  on  prior  expectations  about  the  system  dynamics.  This  second  approach 
is explored  in  the  next  chapter. 

APPENDIX  11.A.  Proofs  of Chapter  11  Propositions 

@  Proof of Proposition  11.1. 
The  condition  on  the  roots  of  [11.1.35]  ensures  that  the  MA(~) 
representation  is absolutely  summable.  Thus  y,  is ergodic  for  first  moments,  from  Propo- 
sitions  10.2(b)  and  10.5(a),  and  is also ergodic  for second  moments,  from  Proposition  10.2(d). 
This  establishes  result  11.1(a). 

The  proofs  of  results  (b)  and  (c)  are  virtually  identical  to  those  for  a  single  OLS 

regression  with  stochastic  regressors  (results  [8.2.5]  and  [8.2.12]). 

To  verify  result  (d),  notice  that 

VT (a7  es  1,)  a  jan > “| jai Ss xe, 

and  so 

where 

Q;'  (VT)  > xe, 

VI (#;  —  %)  = 

Q;'  (1/VT)  S X,E>, 

a 

[11.A.1] 

Qz'(UVT)  DY xen 

Q;=  jury > xa] 

Define  &, to  be  the  following  (nk  x  1) vector: 

XE 1, 

é, = 

X/E>, 

XE, 

Notice  that  &, is  a martingale  difference  sequence  with  finite  fourth  moments  and  variance 

E(x, x;) :  E(e;,) 

E(x, x;) ‘  E(€,,€2,) 

pai 

E(x, x) ;  E(€,£,,,) 

E(é€;)  = 

E(x, x;) “E(€y€1) 

E(x,x;): E(e3,) 

E(x,x/)  E(€x€,,) 

E(x,x!)-E(Cq8,).  E(x!)  El€nts) 

s+ 

E(xx’)-E(e2) 

E(e;j,) 

E(€,,€2,) 

ge 

E(€,,€,,) 

E(ex€,) 

E(e3, 

~ 

s 

E(€3,€,)  ® E(x  x!) 

E (ety)  ElCts)- 11: . Eled) 

2 ®& Q. 

It can  further  be shown  that 

T 

(VT)  2 E> 2 QQ 

[11.4.2] 

340  Chapter  11  | Vector  Autoregressions  — 

(see Exercise  11.1).  It follows  from  Proposition  7.9  that 

— 

(1MV/T)  > eS n(o. (a ®Q)). 

(11.A.3] 

Now,  expression  [11.A.1]  can  be  written 

Supe, 

VI (az, —-  7m)  = 

Q;' 

0 
ett  nn 

,  (VT) > xe, 

5 

bree 

IKLA/T),  >, xe2, 

& 

0 

0 

i 

1 

(VT)  x i. 

=,  OQ WUT) DE. 

¢ 

| 

But result  (a) Superetant aeitoa- ", Thlstetip  tof  Weliacdeyt 

Yo  toot: 

Vii,  -  2) 51, OO navn Se  ILA 

c 

mri  sf BH  2) 

£ 3a  tnemols  isviay:  A 

3 

But from [11.A. 3), this has a distribution that i is Gaussian with mean 0 ) and variance 

a  4,.@@ "at siaidicaead ys -  (I, 21,) @ (Q- '9Q° Te -990'._ 
Peet: s 

— 
(Patti 

Pei Le, e/ to be ne estimate  of 2 based 
she Parecapenptoti distribution  as  Q3. 

th " ite ~ oa  2,  ts  be  thet retian, 

@ she on stem coefficren 

hy  ey 

ABA.  ij  eyii 

.ce  bas 

where  &  =  €,  ® x,  and 

h.  =  vech 

ul 

ei, 7  oi, 
Ex),  —  Fr, 

E\,€x  oa  CO; 
€3,  —  On 

<= 
fi 
"88 

. 

‘ 

EE,  =  Cin 
ExyEmy  ~  Fyn 

: 

: 

EE,  >  o,,;  EE  isi  0,2  i 

rn  i  Tun 

It is straightforward  to  show  that  (E’,  X/)'  is  a martingale  difference  sequence  that  satisfies 
the  conditions  of  Proposition  7.9,  from  which 

(/VT) 2 E, ‘ a((9}. d a 
(VT) > d, 

; 

(11.A.7] 

where 

, a  ed  Eth) FR  ND 

>  > 

fe!  E(4,€;) 

E(X,A/)  ; 

Recall  from  the  proof  of Proposition  11.1  that 

=  E(&E;) = A@Q. 

A  typical  element  of %,,  is of the  form 

E(x,€,)(€i€  —  Fi)  =  E(%,)*  E(€n€,€,)  —  9; E(x) - E(e,). 

which  equals  zero  for  all  i, 7, and  /. Hence,  [11.A.7]  becomes 

VT)  Se], 

mola  et  2D. 

and  so,  from  [11.A.6], 

} 

. 

VT[a,  -  7] 
VT{[vech(2,)  —  vech(Q)] 

fy  0 
2 

2@Q'  0 

ee 

rf] 

Hence,  Proposition  11.2  wilt  be  established  if we  can  show  that  E(A,A;)  is given  by the 
matrix  2.. described in  the  proposition;  that is,  we  must  show  that 

E (€,,€,  =  05  (EvE mn  he,  Om)  a  C47, yen  7  Finn 

[11.A.8] 

for all  i, j, /, and  m. 

To derive  [11.A.8],  let  & =  PP’ denote  the Cholesky  decomposition  of N, and  define 

v, =  P-'e,. 

[11.A.9]} 

Then  E(v,v;) = P-'(X(P-')’  =  I,. Thus,  v,, is Gaussian  with  zero  mean,  unit  variance,  and 
fourth  moment  given  by E(v})  = = Moreover,  v,, is independent  of v,, fori  # j. 

Equation  [11.A.9]  implies  — 

e,  =  Pv,. 

[11.A.10] 

Let p,, denote  the  row  i, column  j element  of P.  Then  the  ith  row  of (11.A.10]  states  that 

€  =  PiYu  +  PrYs  +  8 

pay 

[11.A.11] 

and 

Even  =  (PiaYir  +  PirYn + °°*  +  PinYan)  X  (PaVu  +  PaVy + °° 

+  PinVar)-  11. A.12] 
Second  moments  of e, can  be found  by taking expectations  of [11.A.12],  recalling that 

E(v,v,)  =  Lif i =  j and is  zero  otherwise: 

E(€,€,)  =  PrP  +  PaPa  +  °**  +  PinPin- 

[11.A.13] 

342  Chapter 11  | Vector Autoregressions 

Similarly,  foarth  moments  can  be  found  from 

E(E€  Fn& mw) =  El(pvy,  +  Povy  +  °*>  +  Puiu  Pu  +  Peau  +  °°  °F  PY 

X  (Pay  +  PrP»  +  +  +  PM  PiniYir  +  Pinz¥y  +  °°  *  %  PinnYar) 

=  [3(PaPrPaPmnr  +  PeP2PePm2  +  ***  +  PinPinPinPinn)| 

+  [((PaPiMPePir  +  PaPins  + 

°°  +  PinPinn) 

+  (P2P2(PrPmi  +  PisPinx  +  °°  +  PinPinn)  +  °° 
+  (PiiPinMPrnPur  +  PePme  +  °°  *  +  Pin-1Pms-  ») | 

+  [(Pi Pa  PrP uz  >  PisPina  te  tit  PinPinn) 

+  (P2PrHAPirPur  +  PisxPms  +  °°  +  RyPin)  +  °° 

s  (PinPm)(PirPon  +  PP  m2  7°  69"  >  Pin-  1Pin.n-  )] 

+  [(PiuPmiMP2Pe  +  PisPs  +  °**  +  PinPind 

+  (P2Pm2)(PrPn  +  PisPi  +  °°  +  PinPm)  +  °  °° 
ae  (PinPun)( Pir Pu  +  PePr  +  °°  +  Pin-sPin-1)) 

=  [(PaPia  +  P2P2  +  °°*  +  PinPinMPaPm  +  PePm2  +  °°*  +  PuPnn)| 

+  [(PaPa  +  P2Pe  +  *°*  +  PinPi (Pi Pini +  P2Pm2  +  °°*  +  PinPinn)| 
+  [CPP  +  PPPs + °°  +  PnP  PrPn  + PaPe  +  °**  +  PinPu)| 

=  0,0  +  Oyo  + FinT is 

7 

| 

: 
[11.A.14] 

where the last line follows from [11.A.13}. Then: 

| 

El(euty  —  Oj Menem  —  ab =  E (cut yéutma) ~  OP m * = Oyu  + Cond 

ert: in [11.A. -2 = 
Sil} 
tj} 208!  iG 

7 

MOE 

are 

nhs 

Sat? 

” 

; 

; 

‘ 

‘ 

me 

§2  30t 

ITEC  Pots 

- 

te 
| Rssuctate  4  wrth 

univariate  Wold  representation  for  x.  There  also  exists  a  univariate  Wold  representation 
for  the  error  term  in  [11.2.5],  denoted 

N,  =  Wo2(L)v2,, 

with  y)  = 
follows  that  v,,  is  uncorrelated  with  x,  or  e,,  for  all  ¢ and  7. 

1.  Notice  that  7,  as  defined  in  [11.2.5]  is uncorrected  with  x,  for  all 

Substituting  [11.A.18]  and  [11.A.19]  into  [11.2.5], 

4 a 

y,  =  c  +  D(l)p,  +  b(L)W,  (L)é,  +  Wo2(L)v>,. 

Define 

for  b, the  coefficient  on  L°  of  b(L)  and 

En, 

=  Va,  +  bye, 

[11.A.19] 

sands.  It 

[11.A.20] 

[11.A.2]] 

fr  =c  +  D(1)py. 
Observe  that  (€,,,  €2,)’  is  vector  white  noise.  Substituting  [11.A.21]  and  [11.A.22]  into 
[11.A.20]  produces 

[11.A.22] 

y,  =  Me  +  [D(L)4,(L)  -  bybAL)  ley,  +  H22(L)es,- 

[11.A.23] 

Finally,  define 

noting  that  wy?  =  0.  Then,  substituting  this  into  [11.A.23]  produces 

f2,(L)  =  [b(L)b,,(L)  re  byf2(L)]. 

y,  =  Ms  +  Yo (Ley,  +  Wr2(L)é2- 
This  combined  with  [11.A.18]  completes  the demonstration  that  [11.2.5]  implies  [11.2.4]. 

APPENDIX  11.B.  Calculation  of Analytic  Derivatives 

This  appendix  calculates  the  derivatives  reported  in Sections  11.6  and  11.7. 

Let  the  scalar  € represent  some  particular  element  of @, or  8,,, 
®  Derivation  of [11.6.38]. 
and  let  d92/dé  denote  the  (n*  x  n?) matrix  that  results  when  each  element  of 2 is differ- 
entiated  with  respect  to  €. Thus,  differentiating  [11.6.33]  with  respect.  to  é results  in 

dQ/dE  =  (dB, '/9€)D(B, ')’ +  By '(AD/ag)(B, ')’ +  (By ')D[A(B>')'/oé]. 

[11.B.1] 

Define 

and  notice  that 

xX =  (0B, '/0€)D(B;  ')’ 

[11.B.2] 

x’  =  (By, ')D[a(B,  ')'/0€], 

since  D is a  variance-covariance  matrix  and  must  therefore  be  symmetric.  Thus,  [11.B.1] 
can  be written 

aMag  =  x  +  By '(AD/aé)(B,  ')' +  x’. 

Recall  from  Proposition  10.4  that 

Thus,  if the  vec  operator  is applied  to  [11.B.3],  the  result  is 

vec(ABC)  =  (C'  @ A) -vec(B). 

4 vec(N 

bee  to nag  AF  [(By') @ (By')] vec(aD/ae). 

{11.B.3} 

{11.B.4] 

[11.B.5] 

344  Chapter  1] 

Vector Autoregressions 

Let  D,, denote  the  (n?  x  n*) duplication  matrix  introduced  in [11.1.43].  Notice  that 
for any  (nm x  n) matrix  x, the elements  of D;, vec(x)  are  of the  form  x,, for diagonal  elements 
of x and  of the form  (x,,  +  x,,) for off-diagonal elements.  Hence,  Dj, vec(x)  =  D/, vec(x’). 
If [11.B.5] is  premultiplied  by D; = (D;,D,)  'Dj,,  the  result  is thus 

ara =  2D,  vec(x)  +  D;[(B;')  © (B, ')] vec(aD/aé), 

[11.B.6] 

since  from  [11.1.46]  D+  vec(Q) = vech(Q). 

Differentiating  the  identity  B;'B,  =  1, with  respect  to  é produces 

(0B, '/d€)B,  +  B, '(0B,/aé)  =  0,,, 

or 

Thus,  [11.B.2]  can  be  written 

aBy /aé  =  —B,  (AB, /a€)B,  '. 

(11.B.7] 

x  =  —B, '(4B,/4€)B,  'D(B,  ')'=  —B,  '(AB,/ag)Q._ 

Applying  the vec  metas as  in [11.B.4] results in 

| 

vee(x) = -(0 OB are  6)  = 

a vec (By) 

,  Substituting this expression into [11. B.6] gives...  | 
 Byech() _ 20:0 ©  Bi =f + tm, 10 ny 220200) 
ep  aaghed ip 
fe BiG.  ¢ BAM 

+  D510: D ® B; Is, 

me -20;(0 ® By ng, 

{Bove | hai 

er, 

ras we fit. B. 8] , 

B.8]  is an (n* 

x  a8 vector  that  gives  the  cies ot wctagemaaiale 

1B cach or the * ele ee  ee  >  + 

Let  the  scalar  € denote  some  particular  element  of a,  and  differentiate  [11.B.13]  with 

respect  to  &: 

aw’ 
ry:  wi,  FY:  Vv,  ae  + 

ap; 

av’ 

as qa 

SE 

7_—_—_— 

= 

. 

+ 

YW;  / 

a®, 
ag 

av 
ag 

+ 

a 

} 

aw! , 

Ss 

FY:  ®,  + 

s 

- 

aw!  , 
+  E  sO 

’ 

=(0,  Wi.  ¥ 

aw’  , 

. 

a€é  = 

+ 

dc'/dé 

AD  /a€ 
W!  ,] | a@s/ag 

a@’  /aé 

aw:  , 
= 
ele 

Dot 

76) 

OW, -p 
h  ————  OF 
d€ 

| 

[11.B.14] 

=  (0, 

W..  W., 

- 

foul 
v  “lee 

aw:  , 
s 

oe  ®,  +  ap 
=  ®' 
' 

aw:  , 

a: 
Wi 
ie 
®’ 

~ 

Recall  result  [11.B.4],  and  note  the special  case  when A is the  (n  X  1) identity  matrix, 

B is an  (nm  X  r) matrix,  and  C is an  (r X  q) matrix: 

vec(BC)  =  (C’  @ I,) vec(B). 

[11.B.15] 

For  example, 

vee( Mes :  o:) =  (®, ®1,)  vee( 

aw: 

7  ') =  (®, @1)(% *  ), (11.B.16] 

mp, 

Another  implication  of [11.B.4]  can  be  obtained  by letting  A be  an  (m  x  q) matrix, 

B a(q  X  n) matrix,  and  C the  (m  X  n) identity  matrix: 

vec(AB)  =  (I,,  ® A) vec(B). 

[11.B.17] 

For  example, 

ver( 0,  Ww!  1  WV  e 

W!  sa a ") 

; 

r 

yeu 

age 

ge 

=[1,@(0,  Wo,  Ww, 

v'  I (vee) 

(11.B.18] 

-  (I, ® (0,,  ;:  t  wv; .  te 

Applying  the  vec  operator  to  [11.B.14]  and  using  [11.B.18]  and  [11.B.16]  gives 

= 7  [I, 3) (0,,,  Vv:  i  Vv;  > ie 

Wrinlt  (=) 

+  (®, @1,)  (2) +  (©,  @1,) (4 4) 

{11.B.19] 

++ @, @1) (2). 

Letting  é successively  represent  each  of the  elements  of m  and  stacking  the  resulting 

346  Chapter 11  | Vector Autoregressions 

equations  horizontally  as in [11.B.9]  results in’ 

w= 11,  @ 0,  Wi  Fs  *F 

¥:-,)) 

+  (®%,  @L,)  ee | T° 

YP  @,  ol  Keak 

as claimed  in [11.7.4]. 

®  Derivation  of (11.7.5). 
[11.7.4]  holds: 

Here  the  task  is to  verify  that  if G,  is  given  by [11.7.5].  then 

G,  =  (L, ® (0,,  v1  . ee  et  scall  +  "2 (®,  ® |  Car 

{11.B.20} 

Notice  that for G, given by [11.7.5], 
S@@nG. 
=> 1) 3 18 Ou Woy Wane ce Sa 

> 5 (OF,  @ On.  ¥ red  vw wig  ge 

ark 

oe. zis  ed]. 

For any given value  for  k and i, define v = k + i. When i = JI, then v =  k +  1: when 
fed  bees  2  eae  ? 

ne  Ps  — 

pw 

S@.@n G1 =  FF HOO  ee  Yer 

seh  50 

—.  ko  435 

Yor eed 

— 

«1 

=  Oforv = 2,3,....k,  we could equally well write 

4 Reference 4 

2  (MH. -n- 1@0,  ¥ = ee ms = e  sa 

ei  se. a, ry  tre  <r  OF 2 

We) 

on) 
gaya  wed  ts 

ae 

Applying  the  vec  operator  to  [11.B.23]  and  using  [11.B.15]  and  [11.B.17], 

(H,  © 1,)(0  vec(B,)/aE)  +  (I, @ Bi)(a  vec(H/)/dé)  =  4 vec(W{)/o€, 

implying  that 

ah  /aé  =  —(I,  @ By)  (Hy  @L,)(0  vec(Bi)/ag)  +  (1, @ Bu)  |  ob,/a€ 

—[H,  ® (B;)  '](@ vec(B,)/aé)  +  [I,,  ® (Bi)  '] aw, /d€. 

I 

(11.B.24] 

Noticing  that  B,, does  not  depend  on  m,  if [11.B.24]  is stacked  horizontally  for  € =  7,.  7, 

.  7,4.  the  result  is 

dh, /dte'  =  [L,  ® (Bi)  |] ap, /ar’, 
as  claimed  in  [11.7.14].  Similarly,  if € is an  element  of 2,  then  € has  no  effect  on  W,,  and 
its  influence  on  B,, is given  by 

Stacking  [11.B.24]  horizontally  with  € representing  each  of  the  elements  of  vech({2)  thus 
produces 

a vec(B;,) 
ae  =  Sy 
og 

00, 
ag 

ah, 

a[vech(Q)]’ 

ee 

as  claimed  in  [11.7.15]. 

© 

UV 

I  _is  . 5 

HOB e "Be a[vech(Q)]' ° 

Chapter  11  Exercises 

11.1. 

Verify  result  [11.A.2]. 

11.2. 

Consider  the  following  three-variable  VAR: 

Yu  =  ay,  1  +  Byr,  | 

Ya  FT ee A 

+  €, 

+  €:, 

Yu  =  Ei  vt  Yon  a  +  Way 

t-  Ew. 

(a)  Is y,,  block-exogenous  with  respect  to  the  vector  (y2,. yx,)'? 
(b)  Is the  vector  (y,,. y2,)  block-exogenous  with  respect  to y,,? 
(c)  Is y,,  block-exogenous  with  respect  to  the  vector  (y,,. y>,)’? 

11.3. 

Consider  the  following  bivariate  VAR: 

Vin.  =  Cen  net  Oia  3st 

OTN 

a p 

+  Bi Yrs 

+  BrY2.  xt  "* 

at  B,Yrrey  +  Ex 

=  Tia  t  =  aie  st  Oo!  +  Dis. 

¥  OW2..5  *  esr.  *  8,Y21-p  +  €x, 

2,  OQ, 

fort  =  7 

E(e,e;) 

a 2, 
0 

otherwise, 
Use  the  results  of Section  11.3  to  write  this  in the  form 
a 

Pe  ORI  6  THIF  at 

+ 

PMV245)  +  Manir.t.> 

42  ch  MWI24-p  +  Uy 

Yu  =  Mu  t+  Ai  1 

+  Aap 
+  §:Yora  +  $2¥on  t+  +  bY20-p  +  Uy, 

FAM  a 

ht 

where 

: 
v3 

fort  =r 

0 

otherwise. 

What  is the  relation  between  the  parameters  of the  first  representation  (a;, 
= aaa of the  second  representation  (¢,, n.  Aj. &. 07)?  What  is the  8  bey ae 
and 

B,.  y,. 

5, 

u,? 

348  Chapter 11 | Vector Autoregressions 

11.4. 

Write  the  result  for  Exercise  11.3  as 

L-  ¢(L) 
—Ay  —  A(L) 

or 

 ~n(L)  | -  “| 
1  ~  &L)J} 
Ly, 

Lue 

J) 

Premultiply  this  system  by the  adjoint  of A(L), 
avy  =|  &(L)  = »(L)  |. 
&(L 

1- 

A(L)y,  =  u,. 

to  deduce  that  y,,  and y., each  admit  a  univariate  ARMA(2p,  p) representation.  Show  how 
the  argument  generalizes  to  establish  that  if the  (n  x  1)  vector  y,  follows  a  pth-order 
autoregression,  then  each  individual  element  y,  follows  an  ARMA[np,  (n  —  1)p|  process. 
(See  Zellner  and  Palm,  1974). 

Ay  +  A(L)  v=  ¢(L) 

11.5. 

Consider  the  following  bivariate  VAR: 

Yue  =  9.3 yn  aot  O.8y.,05  +  Ex 

Va. 

0.9y,,.  +  0.4y,,  i  +  Ex, 

with  E(e,,e,,)  =  1 for t =  +  and  0 otherwise,  E(e,.,e.,)  =  2 fort  =  7  and  0 otherwise,  and 
E(e,,€2,)  =  0 for  all  ¢ and  r. 

(a)  Is this  system  covariance-stationary? 
(b)  Calculate  W,  =  dy,,  ,/de}  for  s =  0,  1, and  2.  What  is the  limit  as  s —  ~? 
(c)  Calculate  the  fraction  of  the  MSE  of  the  two-period-ahead  forecast  error  for 

variable  1, 

that  is due  to  e,  ,,,  and  €, ,,>. 

El[yise2  Tr  E(y1621¥  y,-  it?s  FF: 

7 

Chapter  11  References 

Ashley,  Richard.  1988.  ““On  the  Relative  Worth  of  Recent  Macroeconomic  Forecasts.” 
International  Journal  of Forecasting  4:363-76. 
Baillie,  Richard  T.  1987.  “Inference  in Dynamic  Models  Containing  ‘Surprise’  Variables.” 
Journal  of Econometrics  35:101-17. 
Bernanke,  Ben.  1986.  “Alternative  Explanations  of the  Money-Income  Correlation.’  Car- 
negie-Rochester  Conference  Series  on  Public  Policy  25:49-100, 
Blanchard,  Olivier.  1989.  ‘A  Traditional  Interpretation  of Matroeconomic  Fluctuations.” 
American  Economic  Review  79:1146—64. 

 ° 

Ww 

: 

and  Peter  Diamond.  1989.  “The  Beveridge  Curve.”  Brookings  Papers  on  Economic 

Activity  1:1989,  1-60. 

and 

.  1990.  “The  Cyclical  Behavior  of the.Gross  Flows  of  U.S.  Workers.” 

Brookings  Papers  on  Economic  Activity  11:1990,  85-155. 

and Danny Quah.  1989.  “The  Dynamic  Effects  of Aggregate  Demand  and Aggregate 

Supply  Disturbances."’  American  Economic  Review  79:655-73. 

and  Mark  Watson.  1986.  ‘Are  Business  Cycles  All  Alike?”  in  Robert  J.  Gordon, 

ed.,  The  American  Business  Cycle.  Chicago:  University  of Chicago  Press. 
Bouissou,  M.  B.,  J.  J.  Laffont,  and  Q.  H.  Vuong.  1986.  “Tests  of  Noncausality  under 
Markov  Assumptions  for  Qualitative  Panel  Data.”  Econometrica  54:395-414. 
Christiano,  Lawrence  J., and  Lars  Ljungqvist.  1988.  “Money  Does  Granger-Cause  Output 
in the  Bivariate  Money-Output  Relation.”  Journal  of Monetary  Economics  22:217-35. 
Cooley,  Thomas  F.,  and  Stephen  F.  LeRoy.  1985.  “‘Atheoretical  Macroeconometrics:  A 
Critique.”  Journal  of Monetary  Economics  16:283—308. 
Fama,  Eugene  F. 1965.  “The  Behavior  of Stock  Market  Prices."  Journal of Business  38:34- 
105. 
Feige, Edgar L., and Douglas  K.  Pearce.  1979.  “The  Casual  Causal  Relationship  between 
Money  and Income:  Some  Caveats  for Time  Series  Analysis.”  Review of Economics  and 
Statistics  61:521-33. 

Chapter  11  References 

349 

Flavin,  Marjorie  A.  1981.  “The  Adjustment  of  Consumption  to  Changing  Expectations 
about  Future  Income."  Journal  of Political  Economy  89:974—  1009. 
Geweke.  John.  1982.  ‘Measurement  of Linear  Dependence  and  Feedback  between  Multiple 
Time  Series."*  Journal  of the  American  Statistical  Association  77:304-13. 
_  Richard  Meese,  and  Warren  Dent.  1983.  “Comparing  Alternative  Tests  of Causality 
in  Temporal  Systems:  Analytic  Results  and  Experimental  Evidence.”  Journal  of Econo- 
metrics  21:161—94. 
Giannini,  Carlo.  1992.  Topics  in Structural  VAR  Econometrics.  New  York:  Springer-Verlag. 
Granger,  C. W.  J.  1969.  “Investigating  Causal  Relations  by Econometric  Models  and  Cross- 
Spectral  Methods.”  Econometrica  37:424-38. 
Hamilton,  James  D.  1983:  “Oil  and_the  Macroeconomy  since  World  War  II.”  Journal  of 
Political  Economy  91:228-48. 

_  1985.  “Historical  Causes  of Postwar  Oil  Shocks  and  Recessions.”  Energy  Journal 

6:97-116. 
Hansen,  Lars  P.,  and  Thomas  J.  Sargent.  1981.  ‘Formulating  and  Estimating  Dynamic 
Linear  Rational  Expectations  Models,”  in  Robert  E.  Lucas,  Jr.,  and  Thomas  J.  Sargent, 
eds.,  Rational  Expectations  and  Econometric  Practice,  Vo\.  1.  Minneapolis:  University  of 
Minnesota  Press. 
Keating,  John  W.  1990.  “Identifying  VAR  Models  under  Rational  Expectations.”  Journal 
of Monetary  Economics  25:453-76. 
Leamer,  Edward.  1985.  **  Vector  Autoregressions  for Causal  Inference?”  Carnegie-Rochester 
Conference  Series  on  Public  Policy  22:255-303. 
Lucas,  Robert  E., Jr.  1978.  “*Asset  Prices  in an  Exchange  Economy.”  Econometrica  46: 1429- 
45. 
Litkepohl,  Helmut.  1989.  “*A  Note  on  the  Asymptotic  Distribution  of Impulse  Response 
Functions  of Estimated  VAR  Models  with  Orthogonal  Residuals.”’  Journal  of Econometrics 
42:371-76. 

.  1990.  “Asymptotic  Distributions  of Impulse Response  Functions  and  Forecast  Error 
Variance  Decompositions  of  Vector  Autoregressive  Models.”  Review  of Economics  and 
Statistics  72:116-25. 
Magnus,  Jan  R.  1978.  ‘Maximum  Likelihood  Estimation  of the  GLS  Model  with  Unknown 
Parameters  in the  Disturbance  Covariance  Matrix.”  Journal  of Econometrics  7:281-312. 

and  Heinz  Neudecker.  1988.  Matrix  Differential  Calculus  with  Applications  in  Sta- 

tistics  and  Econometrics.  New  York:  Wiley. 
Pierce,  David  A.,  and  Larry  D.  Haugh.  1977.  “Causality  in Temporal  Systems:  Character- 
ization  and  a Survey.”  Journal  of Econometrics  5:265-93. 
Rothenberg,  Thomas  J. 1971.  ‘Identification  in Parametric  Models.”  Econometrica  39:577- 
91. 

.  1973.  Efficient  Estimation  with  a  Priori  Information.  New  Haven,  Conn.:  Yale 

University  Press. 
Runkle,  David  E.  1987.  ‘Vector  Autoregressions  and  Reality.’  Journal  of Business  and 
Economic  Statistics  5:437-42. 
Shapiro, Matthew  D., and Mark  W. Watson.  1988.  “Sources of Business Cycle Fluctuations,” 
in Stanley Fischer,  ed., NBER  Macroeconomics  Annual 1988.  Cambridge,  Mass.:  MIT Press. 
aon Christopher  A.  1972.  “Money,  Income  and  Causality.”  American  Economie  Review 

: 

:40-52. 

——.  1980.  “Macroeconomics  and  Reality.”  Econometrica  48:1-—48. 

~.  1986.  “Are  Forecasting  Models  Usable  for  Policy  Analysis?”  Quarterly  Review  of 

the Federal  Reserve  Bank  of Minneapolis  (Winter),  2-16. 
Stock, James H., and Mark  W. Watson.  1989.  “Interpreting  the Evidence  on  Money-Income 
Causality.”  Journal  of Econometrics  40:161-81. 
Theil,  Henri.  1971.  Principles  of Econometrics.  New  York:  Wiley. 
Zellner,  Arnold.  1962.  “An  Efficient  Method  of Estimating  Seemingly  Unrelated  Regres- 
tm and Tests for Aggregation  Bias.” Journal of the American  Statistical Association  57:348— 

— 

and  Franz  Palm.  1974.  “Time  Series  Analysis  and  Simultaneous  Equation  Econo- 

metric  Models.’  Journal  of Econometrics  2:17-54. 

350  Chapter 11  | Vector  Autoregressions 

12 

Bayesian  Analysis 

The  previous  chapter  noted  that  because  so  many  parameters  are  estimated  in  a 
vector  autoregression,  the standard  errors  for inferences  can  be large.  The  estimates 
can  be  improved  if the  analyst  has  any  information  about  the  parameters  beyond 
that contained  in the sample.  Bayesian  estimation  provides  a convenient  framework 
for  incorporating  prior  information  with  as  much  weight  as  the  analyst  feels  it 
merits. 

Section  12.1  introduces  the  basic  principles  underlying  Bayesian  analysis  and 
uses  them  to  analyze  a  standard  regression  model  or  univariate  autoregression. 
Vector  autoregressions  are  discussed  in  Section  12.2.  For  the  specifications  in 
Sections  12.1  and  12.2,  the  Bayesian  estimators  can  be  found  analytically.  Nu- 
merical  methods  that  can  be used  to analyze  more  general  statistical  problems  from 
a Bayesian  framework  are  reviewed  in Section  12.3. 

12.1.  Introduction  to  Bayesian  Analysis 

- 

Let  @  be  an  (a  X  1) vector  of parameters  to  be  estimated  from  a.sample  of 
observations.  For  example,  if y,  ~  i.i.d.  N(u,  a7),  then  @  =  (jp,  7)’  is to  be 
, yr)’.  Much  of the  discussion  up to this 
- 
estimated  on  the  basis  of  y =  (y,, y2,  - 
point  in the  text  has  been  based  on  the  classical  statistical  perspective  that  there 
exists  some  true  value  of @.  This  true  value  is regarded  as  an  unknown  but  fixed 
number.  An  estimator  6 is constructed  from  the data,  and 6 is therefore  a random 
variable.  In  classical  statistics,  the  mean  and  plim  of the  random  variable  @ are 
compared  with-the  true  value  8.  The  efficiency  of the estimator  is judged  by the 
mean  squared  crror  of the  random  variabje,  E(®  —  6)(®@  —  6)’. A popular classical 
estimator  is the value @ that maximizes  the sample  likelihood,  which for this example 
would  be 

T 
f(y; 8) =  T] aoe 

pe 

> 
eee) 
|. 

(12.1.1) 

In Bayesian  statistics,  by contrast,  @ itself  is regarded  as  a random  variable. 
All  inference  about  @ takes  the  form  of statements  of probability,  such  as  ‘there 
is only a 0.05  probability  that  0, is greater  than  zero.”  The  view  is that  the  analyst 
will always  have  some  uncertainty  about  8, and  the  goal of statistical  analysis  is to 
describe  this  uncertainty  in terms  of a probability  distribution.  Any  information 
the analyst  had  about  @ before  observing  the  data  is represented  by a prior density 

351 

f(®).'  Probability  statements  that  the  analyst  might  have  made  about  6  before 
observing  the  data  can  be  expressed  as  integrals  of f(@);  for  example,  the  previous 
statement  would  be  expressed  as  fi; f(0,)  d0,  =  0.05  where  f(0,)  = 
f=.  f*=.°*: 
J*..  f(®)  d0,  dd,  ---  d@,.  The  sample  likelihood  [12.1.1]  is viewed  as  the  density 
of y conditional  on  the  value  of the  random  variable  @, denoted f(y|®).  The  product 
of the  prior  density  and  the  sample  likelihood  gives  the  joint  density  of y and  6: 

f(y.  ®)  =  f(y|®)-f(6). 

[12.1.2] 

Probability  statements  that  would  be made  about  6 after  the  data  y have  been 

observed  are  based  on  the  posterior  density  of 8, which  is given  by 

f(®ly)  = 

fly,  0) 

f(y) 

[12.1.3] 

Recalling  [12.1.2]  and  the  fact  that  f(y)  =  f%.  f(y.  ©) d@,  equation  [12.1.3]  can 
be  written  as 

f(@ly)  =  wer OT eh 
[- role)-f(@)  a0 

[12.1.4] 

which  is known  as  Bayes's  law.  In  practice,  the  posterior  density  can  sometimes 
be  found  simply  by rearranging  the  elements  in  [12.1.2]  as 

f(y,  8)  =  f(®ly)-f(y). 

where  f(y)  is a  density  that  does  not  involve  0;  the  other  factor,  f(®|y),  is then 
the  posterior  density. 

Estimating  the  Mean  of a  Gaussian  Distribution 
with  Known  Variance 

7 

To  illustrate  the  Bayesian  approach,  let  y,  ~  i.i.d.  N(w,  0”) as  before  and 

write  the  sample  likelihood  [12.1.1]  as 

f(y|u;  0?)  =  es expt [-z]o —  pI)(y  -  no}.  [12.1.5] 

where  I denotes  a  (T x  1) vector  of  1s.  Here  w  is regarded  as  a  random  variable. 
To  keep  the  example  simple,  we  will  assume  that  the  variance  o2  is known  with 
certainty.  Suppose  that  prior  information  about  w  is  represented  by  the  prior 
distribution  4 ~  N(m,  o7/v): 

2 

1 

—(u  -  m)° 

f(u;  07)  =  (iaolv) exp| ES) 

[12.1.6] 

Here  m  and v are  parameters  that  describe  the  nature  and  quality  of prior  infor- 
mation  about  ».  The  parameter  m  can  be  interpreted  as  the  estimate  of u  the 
analyst  would  have  made  before  observing  y, with  a/v the  MSE  of this  estimate. 
Expressing  this  MSE  as  a  multiple  (1/v)  of the  variance  of the  distribution  for y, 
turns  out  to  simplify  some  of the  expressions  that  follow.  Greater  confidence  in 
the  prior  information  would  be  represented  by larger  values  of v. 

‘Throughout  this chapter we  will omit  the subscript  that  indicates  the random  variable  whose density 
is being described;  for example, f.,(8) will simply  be denoted f(@). The  random  variable  whose density 
is being described  should  always  be clear  from  the  context  and  the  argument  of f(-). 

. 

352  Chapter  12  | Bayesian  Analysis 

To make  the  idea  of a  prior  distribution  more  concrete.  suppose  that  before 
observing  y the  analyst  had  earlier  obtained  a  sample  of N separate  observations 
{z,,¢  =  1,2,...,  N} from  the  Mz,  o*) distribution.  It would  then  be  natural  to 
take  m  to  be  the  mean  of this  earlier  sample  (m  =  Z  =  (1/N)=%.,z,)  and  o?/v  to 
be  the  variance  of Z,  that  is,  to  take  v  =  N.  The  larger  this  earlier  sample  (N), 
the  greater  the  confidence  in  the  prior  information. 

The  posterior  distribution  for  4 after  observing  the  sample  y is described  by 

the  following  proposition. 

.,  Proposition  12.1: 

The  product  of [12.1.5]  and  [12.1.6]  can  be  written  in  the form 

fluly:  o7)-f(y;  07),  where 

SO  ee:  —~(  —  m*y 
Maly:  ©)  =  aot  +  TVA ee | 

f(y;  0)  =  (Qmoayra  ltr +  Ben |- "7 

] 

_*  exp} [-U20)IG —  m:1)'(I,  +  1-1'/v)-"(y  -  m-)| 

Big  ys  (. a )m +  ()s 

T 

¥= (VT) D y,. 

Het) 

[12.1.8] 

[12.1.9] 

In  other  words,  the  distribution  of yw  conditional  on  the  data  (y,,  y2,  -.-.  Yr) 
is  N(m*,  o7/(v  +  T)),  while  the  marginal  distribution  of  y  is  N(m-1, 
o7(1,  +  1-1'/v)). 

With  a quadratic  loss  function,  the  Bayesian  estimate  of wu is the  value  4 that 
minimizes  E(u  —  ,)*.  Although  this  is the same  expression  as  the  classical  MSE, 
its interpretation  is different.  From  the Bayesian  perspective,  pu is  a random  variable 
with  respect  to  whose  distribution  the  expectation  is taken,  and  4 is a  candidate 
value  for  the  estimate.  The  optimal  value  for  / is  the  mean  of  the  posterior 
distribution  described  in Proposition  12.1: 

+ 

(ethereal 

il  v+T 

e+  Te 

This  is  a weighted  average  of the  estimate  the  classical  statistician  would  use  (y) 
ahd an estimate  based on prior information  alone (m). Larger values of  y correspond 
to  greater  confidence  in  prior  information,  and  this  would  make  the  Bayesian 
estimate  closer  to  m.  On  the  other  hand,  as  v  approaches  zero,  tHe  Bayesian 
estimate  approaches  the classical  estimate  y. The  limit of [12.1.6] as  »—>  0 is known 
as  a  diffuse  or  improper  prior  density.  In  this  case,  the  quality  of prior  informa- 
tion  is so  poor  that  prior  information  is completely  disregarded  in  forming  the 
estimate  ji. 

The  uncertainty  associated  with  the  posterior  estimate  , is described  by the 
variance  of the  posterior  distribution. To use  the  data  to  evaluate  the  plausibility  of 
the  claim  that  4,  <  2  <  ,,  we  simply  calculate  the  probability  J f(uly;  o°) du. 
For  example,  the  Bayesian  would  assert  that  the  probability  that  4  is within  the 
range  +  20/Vv  +  Tis 0.95.  | 

| 

12.1.  Introduction  to  Bayesian  Analysis 

353 

Estimating  the  Coefficients  of a  Regression  Model 
with  Known  Variance 
Next,  consider  the  linear  regression  model 

y,  =  xB  +  u,, 
where  u, ~  i.i.d.  N(O,  72), x, is  a (k X  1) vector  of exogenous  explanatory  variables, 
and  B is a  (k  X  1) vector  of coefficients.  Let 

3 

(Tx  1) 

yi 
y2 

° 
Yr 

)  ae 

(Txk) 

x; 

x: 

. 
XT 

Treating  B as  random  but  a?  as  known,  wé  have  the  likelihood 

f(y|B.  X; 0’)  =  Ad (2 ae | blo  «6? 

-—. |(y, - 

x/B) 

: 

[12.1.10] 

=  oe exp{ [a] —  XB)'(y  —  XB)  ¢. 

Suppose  that  prior information  about B is represented  by a N(m,  07M) distribution: 

f(B;  0?) = Qn eae |[M|~'? exp| [-sts |e —  m)’M~'(B  -  =} 

7 

[12.1.1] 

Thus,  prior  to  observation  of the  sample,  the  analyst’s  best  guess  as  to  the  value 
of B is represented  by the  (k  x  1) vector  m,  and  the  confidence  in  this  guess  is 
summarized  by the  (k  x  k) matrix  o?M;  less  confidence  is represented  by larger 
diagonal  elements  of M.  Knowledge  about  the  exogenous  variables  X is presumed 
to  have  no  effect  on  the  prior  distribution,  so  that  [12.1.11]  also  describes  - 
AS (B|X; 2’). 

Proposition  12.1  generalizes  as  follows. 

Proposition  12.2: 
f(Bly,  X; 07): f(y|X;  02),  where 

The product  of {12.1.10]  and [12.1.11]  can  be written  in the form 

t 

Sud 

1 
oe.  ae eS 

f(Bly.  X;  0’) =  (27 oii  IM 

-| 

, 

172 

+  X  x| 

{12.1.12} 

x  ra 1/(207)|(B  —  m*)'(M~-'  +  X’X)(B  -  m*)| 

X;  G82  Desemel 

f(y|X;  0”) = (On oe Hani  RRA  9 

r|-172 

| 

[12.1.13] 

x  exp|[- 1/(207)](y  —  Xm)'(I;  +  XMX’)~'(y  —  xm} 

=  (M~'  +  X'X)-'(M-'m  +  X'y). 

{12.1.14] 
In  other  words,  the  distribution  of  B  conditional  on  the  observed  data  is 
ne ,  o(M~'  +  X’X)~')  and  the marginal  distribution  of y  given  X  is 
N(Xm, o*(l;  +  XMX’')). 

354  Chapter  12  | Bayesian  Analysis 

Poor  prior  information  about  B corresponds  to  a  large  variance  M,  or  equiv- 
alently  a  small  value  for  M~'.  The  diffuse  prior  distribution  for  this  problem  is 
often  represented  by the  limit  as 
M~'  —  0, for  which  the  posterior  mean  [12.1.14] 
becomes  m*  =  (X’X)~'X’y,  the  OLS  estimator.  The  variance  of  the  posterior 
distribution  becomes  o7(X'X)~'.  Thus,  classical  regression  inference  is reproduced 
as  a  special  case  of  Bayesian  inference  with  a  diffuse  prior  distribution.  At  the 
other  extreme,  if X'X  =  0,  the  sample  contains  no  information  about  B and  the 
posterior  distribution  is N(m,  77M),  the  same  as  the  prior  distribution. 

If the  analyst’s  prior  expectation  is that  all coefficients  are  zero  (m  =  0) and 
this  claim  is made  with  the  same  confidence  for  each  coefficient  (M~'  =  A-I,  for 
some  A  >  0),  then  the  Bayesian  estimator  [12.1.14]  is 
m*  =  (A:I,  +  X'X)7'X’y, 

(12.1.15] 

which  is  the  ridge  regression  estimator  proposed  by  Hoerl  and  Kennard  (1970). 
The  effect  of ridge  regression  is to  shrink  the  parameter  estimates  toward  zero. 

Bayesian  Estimation  of a  Regression  Model 
with  Unknown  Variance 

Propositions  12.1  and  12.2  assumed  that  the residual  variance  0? was  known 
with  certainty.  Usually,  both  a?  and  B would  be  regarded  as  random  variables, 
and  Bayesian  analysis  requires  a  prior  distribution  for  07.  A  convenient  prior 
distribution  for  this  application  is  provided  by  the  gamma  distribution.  Let 
{Z;}“ , be  a  sequence  of  i.i.d.  N(O,  7”) variables.  Then  W  =  XN |Z?  is said  to 
have  a  gamma  distribution  with  N  degrees  of  freedom  and  scale  parameter  A, 
indicated  W ~  I'(N,  A), where  A  =  1/1”.  Thus,  W has  the  distribution  of 7? times 
a x?(N)  variable.  The  mean  of W is given  by 

E(W)  =  N-E(Z2)  =  Nz?  =  N/A, 

and  the  variance  is 

E(W?)  —  [EQV)P  =  N-{E(Zi)  -  (E(Z3)P) 

=  N-(31r4  —  14)  =  2Nr*  =  2N/?. 

The  density  of W takes  the  form 

[12.1.16] 

<r 

p(w)  =  AZ AWIN" expl — Aw/2]  er 

I'(N/2) 

. 

where  I'(-) abotks the  gamma  function.  If N is an  even  integer,  then 

P(N/2)  =  1-2:3  +++  [(N/2)  —  1], 

with  ['(2/2)  =  1; whereas  if N is an  odd  integer,  then 
P(N/2)  =  Va-4-3-8 

+++ [(N/2)  -  1), 
pigs 

; 

with  T(3)  =  Vz. 
Following  DeGroot  (1970)  and  Leamer  (1978),  it  is  convenient  to  describe 
the  prior  distribution  not  in  terms  of the  variance  a?  but  rather in  terms  of the 
reciprocal  of the  variance,  a~”,  which  is known  as  the  precision.  Thus,  suppose 
that  the  prior  distribution  is  specified  as  a ~  T(N, A),  where  N  and  A are 
parameters  that  describe  the  analyst's  prior  information: 

f(o~?|X)  =  (A/2) "20~2N2)=l1  exp[ - Ao */2} 

T'(N/2) 

[12.1.19] 

12.1.  Introduction  to  Bayesian Analysis  355 

‘ 

Recalling  [12.1.16],  the  ratio  N/A  is the  value  expected  for  Pe  on the basis of prior 
information.  As  we  will  see  shortly  in  Proposition  12.3,  if the  prior  information  is 
based  on  an  earlier  sample  of  observations  {z,,  22,  ---  >  zn},  the  parameter  N 
turns  out  to  describe  the  size  of  this  earlier  sample  and A is  the  earlier sample’s 
sum  of squared  residuals.  For  a given ratio  of N/A,  larger  values  for N imply greater 
confidence  in  the  prior  information. 

The  prior  distribution  of B conditional  on  the  value  for  ao?  is  the  same  as  in 

f12/01 i): 

B-besbormseiorcbiak: 

f(Bla-?,  X)  =  (202)?  |M| 

-12 

{12.1.20] 

x  exp |-s|@ —  m)'M~—  '(p  >  m)  a 

Thus, f(B,  o-2|X),  the  joint  prior  density  for B and  o~?,  is given  by the product 
of [12.1.19]  and  [12.1.20].  The  posterior  distribution  f(B,/o~*|y,  X) is  described 
by the  following  proposition. 

Proposition  12.3: 
[12.1.19]  and  [12.1.20],  and  let the sample  likelihood  be 

Let  the  prior  density  f(B,  o~?|X)  be  given  by the  product  of 

f(y|B,  0-7,  X)  =  pectin oxe| [-sts or —  XB)'(y  -  xe)}. 

(12.1.21] 

Then  the following  hold: 

(a)  The joint posterior  density  of B and  o~?  is given  by 

f(B.  o~*ly,  X)  =  f(Blo~7,  y,  X)-f(o~7|y,  X), 
where  the posterior  distribution  of B conditional  on  o~?  is N(m*,  o7M*): 

[121.22] 

f(Blo~?,  y, X) 

1 

| 

1 

=  moe IM*I-  cp Fac —  m*)'(M*)""(B  —  =}. 

with 

m*  =  (M~'  +  X’X)~'(M-'m  +  X’y) 
M*  =  (M~!  +  X’X)-!. 

(12.1.23] 

(12. 1.24] 
[12.1.25] 

Furthermore,  the  marginal posterior  distribution  of o ~?  is T(N*,  A*): 

f(a-*ly,  X)  =  —_1(N*/2). 

exp[—A*o~  2/2], 

(12.1.26] 

o —2|(N  */2)-  (A*/2)N*?2 

with 

N*=N+T 
At  =A  +  (y —  Xb)'(y  —  Xb) 

[12.1.27] 

+  (b  ite  m)'M~'(X'X  +  M~')~'X'X(b  sia  m) 

[12.1.28] 

forb  =  (X'X)~'X'y  the  OLS  estimator. 

(b)  The marginal posterior distribution  for B is  a k-dimensional  t distribution  with 

N*  degrees  of freedom,  mean  m*,  and scale  matrix  (A*/N*)-M?*: 

356  Chapter  12  | Bayesian  Analysis 

(Bly,  X) 

_  | Pf(k  +  N*)2 
=  a 

(aN*)°P(N  #/2) 

-< &  |(A*/N*)M*|  - 1/2 

(12. 1.29] 

x  [1  +  (I/N*)(B  -  m*)‘((A*/N*)M*]-'(B  —  m*)]  onal, 

(c)  Let  R be a  known  (m  x  k) matrix  with  linearly  independent  rows,  and  define 

a 

(12.1.30] 

Then  Q has  a  marginal  posterior  distribution  that  is F (m,  N*): 

fiqly.  X)  = 

m2(N*)"’2T1(N*  Pres m)/2]q'"?)  -i| 

P(m/2)P(N*/2)(N*  rt mg)\(N"  +2)  : 

{12.1.31] 

Recalling  [12.1.16],  result  (a)  implies  that  the  Bayesian  estimate  of the  pre- 

vg 
cision  is 

E(o~2|y,  X)  =  N*/A*. 

(12.1.32] 

Diffuse  prior  information  is sometimes  represented  as  N =  A  =  0 and  M~'  = 0. 
Substituting  these  values  into  [12.1.27]  and  [12.1.28]  implies  that  N*  =  T and 
A*.=  (y  —  Xb)'(y  —  Xb).  For  these  values,  the  posterior  mean  [12.1.32]  would 
be 

: 

E(a~*|y,  X) =  Ti(y  —  Xb)'(y  —  Xb), 

which  is the  maximum  likelihood  estimate  of a~.  This  is the  basis  for  the  earlier 
claim  that  the parameter N for the prior distribution  might be viewed  as  the  number 
of presample  observations  on  which  the  prior information  is based  and  that  A might 
be  viewed  as  the  sum  of squared  residuals  for  these  observations. 

Result  (b) implies  that  the Bayesian  estimate  of the  coefficient  vector  is 

E(Bly,  X)  =  m*  =  (M~!  +  X’X)~'(M~'m  +  X’y), 

[12.1.33] 

which  is identical  to  the  estimate  derived  in  Proposition  12.2  for the  case  where 
a” is known.  Again,  for  diffuse  prior  information,  m*  =  b, the  OLS  estimate. 

Result  (c) describes  the  Bayesian  perspective  on  a hypothesis  about  the  value 
of RB, where  the  matrix  R characterizes  which  linear  combinations  of the elements 
of B are  of interest.  A classical  statistician  would  test  the  hypothesis  that  RB  =  r 
by calculating  an  OLS F statistic, 

(Rb  —  r)'[R(X’X)~'R’]~'(Rb  —  r)/m 
: 
g? 
and evaluating  the  probability  that an  F(m,  T —  k) variable  could  equal  or  exceed 
this magnitude.  This represents  the probability  that  the estimated  value  of Rb could 
be  as  far  as  it is observed  to  be  from  r given  that  the  true  value  of B satisfies 
RB  =  r.  By contrast,  a Bayesian  regards RB as a random  variable,  the distribution 
~  for which  is described  in result  (c). According  to [12.1.30],  the probability  that  RB 
would  equal r is related  to the probability  that  an  F(m, N*) variable  would  assume 
the  value 

A*/N* 

12.1.  Introduction  to  Bayesian  Analysis  357 

The  probability  that  an  F(m,  N*)  variable  could  exceed  this  magnitude  represents 
the  probability  that  the  random  variable  RB  might  be  as  far  from  the posterior 
r.  In  the  case  of  a  diffuse  prior 
mean  Rm*  as  is  represented  by  the  point  RB  = 
distribution,  the  preceding  expression  simplifies  to 

(r  —  Rb)'‘[R(X'X)~'R’]~'(r  —  Rb)/m 
(y  —  Xb)’(y  —  Xb)/T 
which  is to  be  compared  in  this  case  with  an  F(m,  T) distribution.  Recalling  that 
s*  =  (y  —  Xb)'(y  —  Xb)(T  —  k), 
it  appears  that,  apart  from  a  minor  difference  in  the  denominator  degrees  of- 
freedom,  the  classical  statistician  and  the  Bayesian  with  a  diffuse  prior  distribution 
would  essentially  be  calculating  the  identical  test  statistic  and  comparing  it  with 
the  same  critical  value  in evaluating  the  plausibility  of the  hypothesis  represented 
by RB  =  r. 

Bayesian  Analysis  of Regressions  with  Lagged 
Dependent  Variables 

| 

In  describing  the  sample  likelihood  (expression  [12.1.10]  or  [12.1.21]),  the 
assumption  was  made  that  the  vector  of  explanatory  variables  x,  was  strictly  ex- 
ogenous.  If x, contains  lagged  values  of y,  then  as  long  as  we  are  willing  to  treat 
presample  values  of y as  deterministic,  the  algebra  goes  through  exactly  the  same. 
The  only  changes  needed  are  some  slight  adjustments  in  notation  and  in  the  de- 
scription  of  the  results.  For  example,  consider  a  pth-order  autoregression  with 
»  Yx-p)’.  In  this  case,  the  expression  on  the  right  side  of 
- 
- 
x,  =  (1, y,-1,  -2,  - 
, yy) conditional  on  yp, y_;,---, 
[12.1.21]  describes  the  likelihood  of (y,, y2,.  . 
_Y-p+ts  that  is,  it describes  f(y|B,  7~,  x,).  The  prior  distributions  [12.1.19]  and 
[12.1.20]  are  then  presumed  to  describe  f(a ~?|x,)  and f(B|o~2,  x,),  and  the  pos- 
terior  distributions  are  all  as  stated  in  Proposition  12.3. 

. 

Note  in  particular  that  results  (b)  and  (c)  of  Proposition  12.3  describe  the 
exact  small-sample  posterior  distributions,  even  when  x, contains  lagged  dependent 
variables.  By contrast,  a classical  statistician  would  consider  the  usual  ¢ and F tests 
to  be valid  only  asymptotically. 

Calculation  of the  Posterior  Distribution  Using 
a  GLS  Regression 

It is sometimes  convenient  to describe  the prior information  in terms  of certain 

linear  combinations  of coefficients,  such  as 

Pn 

RB|o -? ~  Nr,  o?V). 
[12.1.34] 
Here  R denotes  a  known  nonsingular  (k  x  k) matrix  whose  rows  represent  linear 
combinations  of B in terms  of which  it is convenient  to describe  the  analyst's  prior 
information.  For  example,  if the  prior  expectation  is that  B,  =  B>,  then  the  first 
row  of R could  be  (1,  —1,  0, . 
,  0) and  the  first  element  of r  would  be  zero. 
The  (1, 1) element  of V reflects  the  uncertainty  of this  prior  information.  If B ~ 
N(m, 07M), then RB ~  N(Rm, o?RMR’). Thus, the relation between the parameters 
for the prior distribution  as expressed  in [12.1.34]  (R, r, and V) and the parameters 
for the  prior distribution  as  expressed  in [12.1.20]  (m and  M) is given  by 

. 

. 

r  =  Rm 
V  =  RMR’. 

{12.1.35] 
[12.1.36] 

358  Chapter 12  | Bayesian  Analysis 

Equation  [12.1.36]  implies 

V-'  =  (R’)-'M"'R-!. 

(12.1.37] 

If equation  [12.1.37]  is premultiplied  by R’  and  postmultiplied  by R,  the  result  is 

R'V-'R  =  M"'. 

[12.1.38] 

Using  equations  [12. 1.35]  and  [12.1.38],  the  posterior  mean  [12.1.33]  can  be  re- 
written  as. 

*  =  (R'V~'R  +  X’X)~'(R'V~'r  +  X’y). 

(12.1.39] 

To  obtain  another  perspective  on  [12.1.39],  notice  that  the  prior distribution 

[12.1.34]  can  be written 

=  RB  +, 

—(12.1.40] 

where  ©  ~  N(0, a?V).  This  is of the  same form  as  the  observation  equations  of 
the regression  model, 

=  Xp + 

3 

[12.1.41] 

with  u ~  N(0, 071).  The ‘thea estimation  strategy  described by Theil (1971,  pp. 
'  347-49)  thus  regards  the  prior  information  as  a  set  of k additional  observations, 

with r, treated  as  if it were another  observation  on  y, and  the ith  row  of R corre-  — 
to its vector  of explanatory  variables  x;. Specifically, equations (12.1.40] 

4 
and (12.1 .41] are s  Stacked  to form  the  system 

| 

oi a 

y*  =  X*B  + u’, 

ar 

Sy 

| 

[12.1.4 | 

a% 

POY  Sos: 
2 ih.  eaedies 
~ 
ee ore eee “fi eS eee 

se 
|  a 
Sagi  3  Adis.  ! brs. (rims  ae #15 ec  ee ; 

ae  ae  | a 
Se  BP  Na.  ge  "ns 

: 
oe  ON  yee  ¥ 

sine 

; 

a}; 

x: 

;  et aF  oa Ly  ae SUBORES Pe 

“76: 

a 

i  a 

ot  a  ROHL Hag, eeoeogme 

{12.1.34]  is written  as 

R, 

mse  ([rf-e*[ 0 

r 

5 

Ve  ae 

vf) 

where  R,  is an  (m  X  k) matrix  consisting  of  those  linear  combinations  for  which 
the  prior  information  is good  and  R, is a  [(K  —  m)  x  k] matrix of the  remaining 
linear  combinations.  Then  diffuse.  prior  information  about  those  linear  combina- 
tions  described  by R, could  be  represented  by the  limit  as  Vz ' —  0, for  which 

R'V-'  =  [R'  R:)| 

(i ileas 

:  vs Re  0). 

The  Bayesian  estimate  [12.1.39]  then  becomes 

(R;V;'R,  +  X’X)~(RiV,'r,  +  X’y), 

which  can  be  calculated  from  GLS  estimation  of a  [((T  +  m)  xX  1] system  of the 
form  of [12.1.42]  in  which  only  the  linear  combinations  for  which  there  is  useful 
_prior  information  are  added  as  observations. 

12.2.  Bayesian  Analysis  of Vector  Autoregressions 

Litterman’s  Prior  Distribution  for Estimation  of an  Equation 
of a  VAR 

. 

; 

: 

This  section  discusses  prior information  that  might  help improve  the estimates 
of a single equation  of  a VAR.  Much  of the early econometric  research  with  dynamic 
relations  was  concerned  with  estimation  of distributed  lag relations  of the  form 

y,  =  C  +  WX,  +  WX,_}  = le  es 

@),X;_p  +  U,. 

[12.2.1] 

For  this  specification,  w,  has  the  interpretation  as  dy,/dx,_,,  and  some  have  argued 
that  this  should  be  a  smooth  function  of s;  see  Almon  (1965)  and  Shiller  (1973) 
for  examples.  Whatever  the  merit  of  this  view,  it  is  hard  to  justify  imposing  a 
smoothness  condition  on  the  sequences  {w,}”_,  or  {¢,}?_,  in  a  model  with  auto- 
regressive  terms  such  as 

a 

ee  Diy,  1  7  $2Y,-2  Seta 

ast:  $yYi-p 

-  WX,  +  WX,  oe  W,X,_p  2  u,, 

since  here  the  dynamic  multiplier  dy,/ax,_,  is  a complicated  nonlinear  function  of 
the  #’s and  w’s. 

Litterman  (1986) suggested  an  alternative  representation  of prior information 

based  on  the  belief  that  the  change  in the  series  is impossible  to  forecast: 

Wt 

ya 

a 

BG 

[12.2.2] 

where  ¢,  is  uncorrelated  with  lagged  values  of  any  variable.  Economic  theory 
predicts  such  behavior  for  many  time  series.  For  example,  suppose  that  y, is the 
log of the real  price of some  asset  at time  #, that  is, the  price adjusted  for inflation. 
Then  y,  —  y,-,  is approximately  the  real  rate  of  return  from  buying  the  asset . 
at  ¢  —  1 and  selling  it at  ¢.  In  an  extension  of Fama’s  (1965)  efficient  markets 
argument  described  in Section  11.2,  speculators  would  have  bought  more  of the 
asset  at time  t ~  1 if they had  expected  unusually  high returns,  driving y,_,  up in 

360  Chapter  12 | Bayesian  Analysis 

relation  to the  anticipated  value  of y,.  The  time  path  for  { y,} that  results  from  such 
speculation  would  exhibit  price  changes  that  are  unforecastable,  Thus,  we  might 
expect  the  real  prices  of  items  such  as  stocks,  real  estate,  or  precious  metals  to 
satisfy  [12.2.2].  Hall  (1978)  argued  that  the  level  of spending  by consumers  should 
also  satisfy  [12.2.2],  while  Barro  (1979)  and  Mankiw  (1987)  developed  related 
arguments  for  the  taxes  levied  and  new  money  issued  by the  government.  Changes 
in  foreign  exchange  rates  are  argued  by many  to  be  unpredictable  as  well;  see  the 
evidence  reviewed  in  Diebold  and  Nason  (1990). 

Write  the  ith  equation  in  a  VAR  as 

- 

(1) 

Yn  =  6  +  OiViu-1  +  Op Y2u-) 

to 

+  OMY, 

+  Oi  Yis-2  +  $12’ You-2  GAs  bin Yna-2  | Re 

[12.2.3] 

(2) 

(p) 

2 

(p) 

+  Pit  Yis-p  +  D3 'Y24-p  ties 

PO Vira  +  Ej, 

where  $°) gives the  coefficient  relating  y,, to y;,-y-  The  restriction  [12.2.2]  requires 
¢),’  =  1 and  all  other  $‘)  =  0.  These  values  (0 or  1) then  characterize  the  mean 
of the  prior  distribution  for  the  coefficients.  Litterman  used  a  diffuse  prior  distri- 
bution  for  the  constant  term  c,. 

Litterman  took  the  variance-covariance  matrix  for  the  prior  distribution  to 
be —apenel,  with  y  denoting  the  standard  deviation  of  the  prior  distribution 
for  $;,’: 

};  SING,  ve) 

Although  each  equation  i  = 
1,  2,...,  n  of  the  VAR  is  estimated  separately, 
typically  the  same  number  y is used  for  each  7.  A  smaller  value  for  y represents 
greater  confidence  in the  prior  information  and  will  force  the  parameter  estimates 
to  be  closer  to  the  values  predicted  in  [12.2.2].  A  value  of y  =  0.20  means  that, 
before  seeing  the  data,  the  analyst  had  95%  confidence  that  ¢‘) is no  smaller  than 
0.60.and  no  larger  than  1.40. 

} 
The  coefficients  relating  y,  to  further  lags  are  predicted  to  be  zero,  and 
Litterman  argued  that  the  analyst  should  have  more  confidence  in this  prediction 
the  greater  the  lag.  He  therefore  suggested  taking  #2  ~  M(0,°  (y/2)?), 
6°) ~  N(0, (7/3)*),.  . 
,and d”) ~  N(0, (y/p)*),  tightening  the prior distribution 
with  a  harmonic  series  for  the  standard  deviation  as  the  lag increases. 

. 

Note  that  the  coefficients  ¢° are  scale-invariant;  if each  value  of y,, is mul- 
tiplied  by 100,  the  values  of ¢/?) will  be  the  same.  The  same  is not  true  of ¢{;) for 
i # j; if series  i is multiplied  by 100  but  series j is not,  then  $/") will  be  multiplied 
by 100.  Thus,  in  calculating  the  weight  to  be  given  the  prior  information  about 
¢‘),  an  adjustment  for  the  units  in  which  the  data  are  measured  is  necessary. 
_  Litterman  proposed  using the  following  standard  deviation  of the prior distribution 

ij ; 

. 

. 

for $/7?: 

ee 

(12.2.4] 

Here  (7,/7;) is  a correction  for the scale of series  i compared  with  series j. Litterman 
suggested  that  7, could  be  estimated  from  the  standard  deviation  of the  residuals 
from  an  OLS  regression  of y,,  on  a  constant  and  on p of its own  lagged  values. 
Apart from this  scale  correction,  [12.2.4]  simply  multiplies  y/s  (which  was  the 
standard  deviation  for the  prior distribution  for  /;’) by a parameter  w.  Common 
experience  with  many time  series  is that  the  own  lagged values y,,_,  are  likely  to 

12.2.  Bayesian  Analysis of Vector  Autoregressions 

361 

be  of more  help  in  forecasting  y,,  than  will  be  values  of other  variables  y, ,_,.  Hence 
we  should  have  more  confidence  in  the  prior  belief  that  #{)  =  0 than  the  prior 
belief  that  #‘)  =  0,  suggesting  a  value  for  w  that  is  less  than  1.  Doan  (1990) 
recommended  a  value  of  w  =  0.5  in  concert  with  y  =  0.20. 

Several  cautions  in  employing  this  prior  distribution  should  be  noted.  First, 
for  some  series  the  natural  prior  expectation  might  be  that  the  series  is white  noise 
rather  than  an  autoregression  with  unit  coefficient.  For  example,  if y;,  is  a  series 
such  as  the  change in  stock  prices,  then  the  mean  of ¢‘') should  be  0 rather  than  1. 
Second,  many  economic  series  display  seasonal  behavior.  In such  cases,  $';’ is  likely 
to  be  nonzero  for  s  =  12 and  24 with  monthly  data,  for  example.  Litterman’s  prior 
distribution  is not  well  suited  for seasonal  data,  and  some  researchers  suggest  using 
seasonally  adjusted  data  or  including  seasonal  dummy  variables  in  the  regression 
before  employing  this  prior  distribution.  Finally,  the  prior  distribution  is not  well 
suited  for  systems  that  exhibit  cointegration,  a  topic  discussed  in detail  in Chapter 
be 

Full-Information  Bayesian  Estimation  of  a VAR 

Litterman’s  approach  to  Bayesian  estimation  of  a  VAR  considered a single 
equation  in isolation.  It is possible  to analyze  all of the  equations  in  a VAR  together 
in  a  Bayesian  framework,  though  the  analytical  results  are  somewhat  more  com- 
plicated  than  for  the  single-equation  case;  see  Zellner  (1971,  Chapter  8) and  Roth- 
enberg  (1973,  pp.  139-44)  for  discussion. 

12.3.  Numerical  Bayesian  Methods 

In  the  previous  examples,  the  class  of densities  used  to  represent  the  prior  infor- 
mation  was  carefully  chosen  in order  to obtain  a simple  analytical  characterization  | 
for the  posterior  distribution.  For  many  specifications  of interest,  however,  it may 
be  impossible  to  find  such  a class,  or  the  density  that  best  reflects  the  analyst’s 
prior  information  may  not  be  possible  to  represent  with  this  class.  It is therefore 
useful  to  have  computer-based  methods  to  calculate  or  approximate  posterior  mo- 
ments  for  a quite  general  class  of problems. 

Approximating  the  Posterior  Mean  by the  Posterior  Mode 

One  option  is to  use  the  mode  rather  than  the  mean  of the  posterior  distri- 
bution,  that  is,  to  take  the  Bayesian  estimate  6 to  be  the  value  that  maximizes 
f(®|y).  For  symmetric  unimodal  distributions,  the  mean  and  the  mode  will  be  the 
same,  as  turned  out  to  be the  case  for  the  coefficient  vector  B in Proposition  12.2. 
Where  the  mean  and  mode  differ,  with  a  quadratic  loss  function  the  mode  is a 
suboptimal  estimator,  though  typically  the  posterior  mode  will  approach  the  pos- 
terior  mean  as  the  sample  size  grows  (see  DeGroot,  1970,  p.  236). 

Recall  from  [12.1.2]  and  [12.1.3] that  the  posterior  density  is given  by 

foly)  =  & ee : 
E16) f(@) 

[12.3.1] 

and  therefore the  log of the  posterior  density is 

log  (Oly)  =  log f(y|®)  +  log f(6)  —  log f(y). 
[12.3.2] 
Note  that  if the  goal  is to  maximize  [12.3.2]  with  respect  to  @, it is not  necessary 

362  Chapter  12  | Bayesian  Analysis 

\ 

to calculate f(y),  since  this does  not  depend  on  @. The  posterior  mode  can  thus  be 
found  by maximizing 

log f(®,  y)  =  log f(y|®)  +  log f(@). 

[12.3.3] 

To evaluate  [12.3.2],  we  need  only  to  be  able  to  calculate  the  likelihood  function 
f(y|®) and the density that describes  the prior information, f(@).  Expression  [12.3.2] 
can  be maximized  by numerical  methods,  and  often  the  same  particular  algorithms 
that  maximize  the  log likelihood  will  also  maximize  [12.3.2].  For  example,  the  log 
likelihood  for  a Gaussian  regression  model  such  as  [12.1.21]  can  be  maximized  by 
a GLS  regression,  just  as  the  posterior  mode  [12. 1. 39] can  be  calculated  with  a 
GLS regression. 

Tierney  and  Kadane’s  Approximation 
for Posterior  Moments 

Alternatively,  Tierney  and Kadane  (1986)  noted  that  the  curvature  of  the 
likelihood  surface can  be used to estimate  the distance  of the posterior  meee from 
he posterior  mean.  Suppose  that  the  objective i is  to calculate 

Ale @lr1=  J e@)-fly) do, 

2.3.4) 

re  9 is an (a X  1) vector  of parameters  and g: R’ > >Rii is a function of i interest. 
example, if g(@)  =  0,, then [12.3. 4] is the posterior mean  of the first parameter, 
while wig) Gi gives the secend moment.  nae (12.3. 1] ce can be perks to write” : 

ore -~  - (0) fa10) 10 do af a0) fly|®) £0) dé 

ia 

ee  | -  Sn  ee  ee  ae  cla 

Ny ana at ice 

[12.3.5] 

Assuming  that 
(0h(0)/d0']  |.»  is 0.  Then  [12.3.9]  could  be  expressed  as 

interior  optimum  of  h(-), 

is  an 

@* 

the 

first 

derivative 

h(®)  =  h(@*)  —  (1/2)(@  —  @*)'(2*)~'(0  —  8"), 

where 

ve  [a 

ee  : 

(12.3.10] 

1231) 

2.3.11 

When  [12.3.10]  is substituted  into  the  numerator  of  [12.3.8],  the  result  is 

[- exp[7-h(0)]  d0 

=|  exp| 7-110") —  (7/2)(@  —  0*)’(2*)- ‘(8 —-  | de 

=  exp[7-h(0*)]  ie exp] (726 —  9*)'(2*)*'@  -  0)| d® 

(12.3.12] 

=  exp[7-h(0*)]  (27)“7|2*/T|"” 

x  [ On  AZS/T?  exp| 5 (0 

1  —~ 

I 

: 

arty 

—0*)'(2"/T) 

* 

se 

*@— 

= 

* 

@  ) d® 

exp[7-A(0*)]  (277)“2|*/T|!2. 

i 

The  last  equality  follows  because  the  expression  being  integrated  is a  N(0*,  %*/T) 
density  and  therefore  integrates  to  unity. 

Similarly,  the function  k(®)  can  be  approximated  with  an  expansion  around 

the  posterior  mode  8, 

k(0) =  k(6)  —  (0 —  6)'=-'(@  —  6), 

where  @ maximizes  [12.3.7]  and 

ee 

[| ak(0) 

~ 

% 

The  denominator  in  [12.3.8]  is then  approximated  by 

is exp[7-k(@)]  d®  =  exp[7-k(6)]  (22r)*2|3/T]!2. 

[12.3.14] 

Tierney  and  Kadane’s  approximation  is obtained  by substituting  {12.3.12]  me 

[12.3.14]  into  [12.3.8]: 

Els@)ivl  =  eft KO] Qay?|S/T]"2 

(12.3.15] 

- 

exp{7T:[h(0*)  —  k(6)]}. 

To calculate  this  approximation  to  the  posterior  mean  of g(@),  we  first  find  the 
value  @*  that  maximizes  (1/T)-{log  g(@)  +  log f(y|®)  +  log f(®)}.  Then  h(@*)  in 
(12.3. 15] is the  maximum  value  attained  for  this  function  and  &*  is the  negative 
of the  inverse  of the  matrix  of second  derivatives  of this  function.  Next  we  find 
_  the  value  @ that  maximizes  (1/7) - {log f(y|®)  +  log f(®)},  with  k(6) the  maximum 
value  attained  and & the  negative  of the inverse  of the matrix  of second  derivatives. 

364  Chapter  12  | Bayesian  Analysis 

The required  maximization  and  second  derivatives  could  be calculated  analytically 
or  numerically.  Substituting  the  resulting  values  into  [12.3.15]  gives  the  Bayesian 
postenor  estimate  of 2(8). 

Monte  Carlo  Estimation  of Posterior  Moments 
Posterior  moments  can  alternatively  be  estimated  using  the  Monte  Carlo 
approach  suggested  by Hammersley  and  Handscomb  (1964,  Section  5.4) and  Kloek 
and  van  Dijk  (1978).  Again,  the  objective  is taken  to  be calculation  of the  posterior 
mean  of g(@).  Let  /(@)  be  some  density  function  defined  on  @ with  /(®)  > 0 for  all 
8. Then  [12.3.5]  can  be  written 

[ 5(@)-f0y10)-s(0)  a0 
Bigte) bi '=*h | 
_  (y|8)-f(@)  40 

[12.3.16] 

| % {g(0)-f(y|@)  -£(@)//(8)}1(0)  de 

| : _  tf (yl®)-f (6)/1()}/(@)  d@ 

The  numerator  in  [12.3.16]  can  be  interpreted  as  the  expectation  of the  random 
variable  {g(@) -f(y|®)-f(®)//(@)},  where.this  expectation  is taken  with  respect  to 
the  distribution  implied  by  the  density  /(@).  If  /(@)  is  a  known  density  such  as 
multivariate  Gaussian,  it may  be simple  to generate  N separate  Monte  Carlo  draws 
from  this  distribution,  denoted  {0,  6,  . 
,  @°™}.  We  can  then  calculate  the 
average  realized  value  of the  random  variable  across  these  Monte  Carlo  draws: 

. 

. 

S (1/N)-{g(0) f(y ]0) -f(O)1(0)}. 
i=l 

[12.3.17] 

From  the  law  of large  numbers,  as  N >  ~,  this  will  yield  a consistent  estimate  of 

EnoXg(®)-F(y®)-f(0)/1(0)}  =  | {e(0)-F(yl0)-F(0)/1(0)M(@)  do, 

(12.3.18} 

provided  that the  integral  in [12.3.18]  exists.  The  denominator  of [12.3.16]  is sim- 
ilarly estimated  from 

| 

N 

D (UN) {F(y10)-F(0°)/1(0)}. 

The  integral  in  [12.3.18]  need  not  exist  if the  importance  density  /(®)  goes 
to zero  in the  tails  faster  than  the  sample  likelihood  f(y|@).  Even  if [12.3.18]  does 
exist,  the Monte  Carlo  average  [12.3.17]  may  give a poor estimate  of [12.3.18]  for 
moderate N if /(@) is poorly  chosen.  Geweke  (1989)  provided  advice  on  specifying 
1(®).  If the set  of allowable  values  for @ forms  a compact  set,  then  letting  /(@)  be  — 
the density  for the asymptotic  distribution  of the  maximum  likelihood  estimator  is 
usually  a good approach. 

A nice  illustration  of the  versatility  of  Bayesian  Monte  Carlo  methods  for 
analyzing  dynamic  models  is provided  by Geweke  (1988a).  This  approach  was 
extended  to multivariate  dynamic  systems  in Geweke  (1988b). 

, 

12.3.  Numerical  Bayesian  Methods 

365 

APPENDIX  12.A.  Proofs  of Chapter  12 Propositions 

@  Proof of Proposition  12.1.  Note  that  the  product  of [12.1.5]  and  [12.1.6]  can  be written 
| 
f12.A.1] 
Fly.  ws)  =  ayer  AL ox  30"? va}. 

where 

a  Sete 

y  ae  pl 

(Ti  typxt 

ee 

| 

|

ee 1)  a  | 0 

al |. 

The  goal is to  rearrange  «  so that  » appears only in  the  first element.  Define 

(v + 

-V/(v  + 

A 
(Teiyx(T#eD 

-|"  ese  it 
ple: i 

: I; 

T) 
T) ; 

[12.4.2] 

E: 

' 

[12.A.3]} 

7 
[12.A.4] 

Since  1'1  =  T and  l'y  =  Ty, we  have 

ld  nie + T)\(H — m)  —  VyM(v  +  T) + [Tv  +  T)Ju 

=  — —  m* | t v)  ;  4)  4 

[12.A.5] 

ose 

a 

m1 

| 

moda one!  =at means ot  za  be 

sfT 
ance die aesiasci: soiteosyRs vinty: overt ee  ain (Gye)  skithay 
pict  -ss 
7H  ent 
a  aa ford +  ch sil ee 

"20Gth  ole3 ASA’  =  | 
att.  ieuIsies — by Vo 

KOS. sere. vinanieb: 

i  ote 

er. 

hi 

rete 

hig 

; 

iis 

a 

a 
roint  s@  ano  {01.6  51]  ni  r0%s19n  we 

| 

; 

77RD  che =  ere sider @ly-  mn al 4  to  aula ia  aes 

He  eb 

|

=  oe 0) 

\ 

{' Gig) 

Tiny, 8 cisePeee {i ® 

si 

“i  } ae 

Sree 

> 

’ 
gre,  Sy 

o*(I, +  1-17/v)_ 

 
 
2 

exp) 

l 
(2a)'**  "2 

ao 
1p  +  T 

|  > 

o(1,  +  1:1'/v) 

1/2 

Ue  me  = (y  =  my  +  EV)“  =  | 

x  exp| ab  +  7) 

oe: 

; 

from  which  the factorization  in  Proposition  12.1  follows  immediately. 

®  Proof of Proposition  12.2.  The  product  of [12.1.10]  and  [12.1.11]  can  be  written  as 

with 

fly,  B|X:  0°) =  Omron  |21 

: 

I 

2 

ay 
exp  —4 a's a} 

eft 

= 

>  ee  Xp 
oM  0 

a 
(Tekpxt 
| 

As in  the  proof of Proposition  12. 1, defigenx 

+ 

.11-. 

rater”  | 0  er 

rut 

| 

i  &  aif —M i xx) n° Beef. i  Bs sob oo mwoce 
a08  aebrvery  fei. 4% 

Taal 

(T+k)x(Tt+k) 

Te 

0 

‘Cromer oi Be i ied +x%) es  (as 8 ¥ hs 

ae  Bagi 1")  “Ay 
tA oe and 

ris 

rs  or 

nf 

= 

»  Sie  a 2 - 

ree 
a =  m* 
Bh 
mA 

ha 

|  oF a  Jue -  n. jis  KS  Ay 

Ir  ia  7 

yh}  saci 
nofirrw  od  nae.  (ie}), 
eee 
sid 

mo 

fA) 

}  i 

im 4 

{xa a 
or 

Sa +  “yi 
a  we tt oa 

| 

- 

/ 
4 

—— 

: 

Define 

A*  =A  +  (y  —  Xm)'(I,  +  XMX’)-'(y  —  Xm); 
we  will  show  later  that  this  is  the  same  as  the  value  A*  described  in  the  proposition.  For 
N*  =  WN  +  T,  the  density  [12.A.11]  can  be  written  as 

{12.A.12] 

f(y.  B. a  *|X) 

m*)'(M*) 
{aati  cx |e  m*)'(M*) 

Pe 

12 

—~ 

*) 

“(B 
m* 
'(B  ~} 

— 

‘ea  YN  /2)  "(A/2)" 
————_—_———_——_  |I,_  +  XMX’| 

m™r(Niy 

| 

. 

’ 

De 

,  | Ata  |} 
'“  exp] 

- 

ae 

(12.A.13] 

=  [aoe |M*|  2  exp| [-st](8 —  m*)'(M*)  (B  is =} 

g  wm  KAI S  1 Ate? 

iw'e)  | ee le  Pes 

: 

1(N*/2)(A/2)"" 
(27r)’71'(N/2)(A*/2)"'" 

[I +  XMX'|-  nf 

The  second  term  does  not  involve  B, and  the  third  term  does  not  involve  B or  o  >. Thus, 
[12.A.13]  provides  the  factorization 

fly.  B. o  7[X)  =  {f(Blo  2. y,  XB  {f(a  Aly.  Xf  F(y|X)}. 
where  f(B|o  *, y.  X)  is  a N(m*,  o°M*)  density, f(o  2ly, X) is a  P(N*,  A*)  density,  and 
f(y|X)  can  be  written  as 

po  =  [aay 

1(N*/2)(A/2)%" 

ll, +  XMX’|  | 

= 

P(N  +  T)/2]A%2|1,  +  XMX'| 

m™?1(N/2){A  +  (y  —  Xm)'(I,  +  XMX’)  '(y  —  Xm)}(%' 7 
c:{1  +  (I/N)\(y  —  Xm)'[(A/N)(1,  +  XMX')]  "Cy  —  Xm)}  "7, 

where 

4  be :  C(N  +  T)/2](1/N)™|Q/N)(1,  + XMX’)|  '@ 
; 

ar™1(N/2) 

. 

Thus,  f(y|X)  is  a  7-dimensional  Student's  ¢ density  with  N  degrees  of  freedom,  mean 
Xm,  and  scale  matrix  (A/N)(I,  +  XM’‘X’).  Hence,  the  distributions  of (B|a~  *, y.  X) and 
(0° *|y, X) are  as  claimed  in  Proposition  12.3,  provided  that  the  magnitude  A*  defined  in 
[12.A.12]  is the  same  as  the  expression  in [12.1.28].  To  verify  that  this  is indeed  the  case, 
notice  that 

(I;  +  XMX’')°'  =  I, —  X(X’X  +  M~-')>'X’, 

[12.A.14] 

as  can  be verified  by premultiplying  [12.A.14]  by (I,  +  XMX’): 

(Il,  +  XMX’)[I,  —  X(X'X  +  M  ')  'X’] 

I,  +  XMX'  —  X(X’X  +  M  ') 'X’  —  XM(X'X)(X'X  +  M  ')  'X’  | 

I, +  x{MOX'x +M  ')-I,  -  maxx) boxx +M  ')  'Xx’ 

=  I,. 

Using  {12.A.14],  we  see  that 

(y —  Xm)'(I,;  +  XMX’)°'(y  —  Xm) 

=  (y  —  Xm)'[I,  -  X(X'X  +  M~')°'X'Ky  —  Xm) 
=  (y —  Xb  +  Xb  —  Xm)’'[I,  —  X(X’X  +  M~')  'X’](y  —  Xb  +  Xb  —  Xm) 
=  (y  —  Xb)'(y  —  Xb)  +  (b  —  m)'X'[I,  —  X(X'X  +  M-')>  'X')X(b  —  m). (12.A.15] 

368  Chapter  12 | Bayesian  Analysis 

where  cross-product  terms  have  disappeared  because  of the  OLS  orthogonality  condition 
(y —  Xb)'X = 0’.  Furthermore, 

X'[l,  —  X(X’X  +  M-')-'x‘]x 

=  [l, —  (X'X)(X'X  +  M>') \']x’x 
=  [(X’X  +  M>')(X’X  +  Mo')-!  -  (X'X)(X'X  +  M  ')” '|X'X 
=  M>'(X’X  +  M-!)°'X’X. 

This allows [12.A.15]  ta  be written  as 

(y —  Xm)‘(I,  +  XMX’)  '(y  —  Xm) 

=  (y  —  Xb)'(y  —  Xb)  +  (b —  m)’M  '(X’X  +  M  ')  'X’X(b  —  m), 

establishing  the equivalence  of [12.A.12]  and  [12. 1.28]. 

Proof of (b).  The joint posterior density of B and o  *  is given by 

f(B.  7 *\y, X) 

: 

=  f(Blo-*.  y. X)-f(o “ly, X) 

=  mts aga meet exp  [-34 ssl -» —  m*)'(M*)-(B = m*) =i} ct  saat 

or  UN" 

#/2)"" 

G 

x oa TN?) 

exp[—A*o  */2] 

ee 
path be = 

a 

mia 

1  sosbed 

ae 
: 

shes lb —o 

wm  E  Tae 5363  ; 

OR  .ysiiide .somis 
Se  eee  eowiib 

The  joint  posterior  density  of g and  a? 

is 

f(q.  o-?ly.  X)  =  f(qlo-*.  y,  X)-f(o~*|y,  X) 

oA { */(20°N*)}"2q'"-  1  exp| =ninghoe ney) 
P'(m/2) 
. 

MN)  —  A YD)  2 

x  jen 

a 
exp[—A*o  2)| 

1" tel 
a  {ce +  mq):[A*/(2N*)]} 

(12.A.17] 

r[(N*  +  m)/2] 

x  go  ln  +NV21- 

expf—(N*  +  mg)(A*/N*)o  =n] 

P(m/2)P(N*/2)(N*  +  mg)  +007! 

=  {f(o  *|9q,  y.  Xf  (aly.  X)}. 

where  f(a  ?|q.  y.  X)  isa  1((m  +  N*),  (N*  +  mq)(A*/N*))  density  and  f(q|y.  X)  is an 
F(m,  N*)  density. 

& 

a 

Chapter  12  Exercise 

12.1. 

Deduce  Proposition  12.1  as  a  special  case  of Proposition  12.2. 

Chapter  12  References 

Almon,  Shirley.  1965.  “The  Distributed  Lag  between  Capital  Appropriations  and  Expen- 
ditures."”  Econometrica  33:178-96. 
Barro,  Robert  J.  1979.  “On  the  Determination  of  the  Public  Debt.”’  Journal  of Political 
Economy  87:940-71. 
DeGroot,  Morris  H.  1970.  Optimal  Statistical  Decisions.  New  York:  McGraw-Hill. 
Diebold,  Francis  X.,  and  James  A.  Nason.  1990.  “‘Nonparametric  Exchange  Rate  Predic- 
tion?”  Journal  of International  Economics  28:315-32. 

: 

~  Doan, Thomas  A.  1990.  RATS  User’s  Manual.  VAR  Econometrics,  Suite  612,  1800 Sherman 

Ave.,  Evanston,  IL 60201. 
Fama,  Eugene  F.  1965.  ‘The  Behavior  of Stock  Market  Prices.”’  Journal  of Business  38:34- 
105. 
Geweke,  John.  1988a.  ‘‘The  Secular  and  Cyclical  Behavior  of  Real  GDP  in  19  OECD 
Countries,  1957-1983."  Journal  of Business  and  Economic  Statistics  6:479-86. 

| 

.  1988b.  “Antithetic  Acceleration  of Monte  Carlo  Integration  in Bayesian  Inference.” 

Journal  of Econometrics  38:73-89. 

.  1989.  “Bayesian  Inference  in Econometric  Models  Using Monte  Carlo  Integration.” 

Econometrica  57:1317-39. 
Hall,  Robert  E.  1978.  “Stochastic  Implications  of the  Life  Cycle-Permanent  Income  Hy- 
pothesis:  Theory  and  Evidence.”  Journal  of Political  Economy  86:971-87. 
os ame  J.  M.,  and  D.  C.  Handscomb.  1964.  Monte  Carlo  Methods,  \st  ed.  London: 

ethuen. 

Hoerl,  A.  E.,  and  R.  W.  Kennard.  1970.  “Ridge  Regression:  Biased  Estimation  for  Non- 
orthogonal  Problems.”  Technometrics  12;:55-82. 
Kloek,  T., and  H.  K. van  Dijk.  1978.  “Bayesian  Estimates  of Equation  System  Parameters: 
An  Application  of Integration  by Monte  Carlo.’  Econometrica  46:1-19. 
Leamer,  Edward  E.  1978.  Specification  Searches:  Ad  Hoc  Inference  with  Nonexperimental 
Data.  New  York:  Wiley. 

| 

370  Chapter  12  | Bayesian  Analysis 

Litterman,  Robert  B.  1986. * iodliatlig' with Bayesian  Vector  Autoregressions—Five  Years 
of Experience.”  Journal  of Business  and  Economic  Statistics  4:25-38. 
Mankiw,  N. Gregory.  1987.  “The Optimal Collection  of Seigniorage:  Theory and Evidence.” 
Journal  of Monetary  Economics  20:327-41. 
Rothenberg,  Thomas  J.  1973.  Efficient  Estimation  with  A  Priori  Information.  New  Haven, 
Conn.:  Yale  University  Press. 
Shiller,  Robert  J.  1973.  “A  Distributed  Lag Estimator  Derived  from  Smoothness  Priors.” 
Econometrica  41:775—88. 
Theil,  Henri.  1971.  Principles  of Econometrics.  New  York:  Wiley. 
Tierney,  Luke,  and  Joseph  B.  Kadane.  1986.  “Accurate  Approximations  for  Posterior 
Moments  and Marginal  Densities.” '  Journal of the American  Statistical  Association  81:82- 
86. 
Zellner,  Arnold.  1971. An  Introduction  to  Bayesian  Inference  in Econometrics.  New  York: 
Wiley.  - 

: 

Whersegku: 
ialuoretipe  abmaiey,  oaetseyb  &  eoiy: 
tof  guvinegls  Mei  tot 
tielose:btaritor  grog Au Umbiaen  bet.  OT  AGHOS{o1g  Wonil  &  gOiishqy 

cemiey.  sl  wGamnstnepa  oaqe-sinr 

yilsinoups 

’  OF?  GBH  ese}  SF) 

Sa 

S283  SILI 

GP  Ya  UESveh ay  rhe 

U8  sHiemeroesiiws  | 

eoers9O1"  AYR  isieete) YO)  Rese sf  to Ack ble 

acm  bob ity 

fi  Ts  Ye  aRkDaALE aap “ipRhtO 

aay tealization: 
mee  aio:  ©  Mi  TAsinw  od  ey  misieye  vimanyh  &  wod  zach  al [7  matiope, 
SE i ne ge comeniesteny aigatrislit  oa?  Asti ansais  kt 

ogasdo  ied?  ainsiotisos  miiw  anoize srg sIO! , 

evo 

ar  Finest: 
simi} 

r 

ach  af 2 deeivcse:- 3 a 
Ef pouse, fi  badivageb  21  yniizsssicl  ni  seat  Be  BOS 
}  ERS 9 aratomsteg notisluqog  Sd}  sisaiites  oF 
©  ENIIIGO YG 95  amy 
TOM aarais.2 ond  wor! 
arti  ~scaeaa ae 

; 

A 

, fe 

The  Kalman  Filter 

This  chapter  introduces  some  very  useful  tools  named  for  the  contributions  of 
R.  E.  Kalman  (1960,  1963).  The  idea  is to express  a dynamic  system  in a particular 
form  called  the  state-space  representation.  The  Kalman  filter  is an  algorithm  for 
‘sequentially  updating  a  linear  projection  for  the  system.  Among  other  benefits, 
this  algorithm  provides  a  way  to  calculate  exact  finite-sample  forecasts  and  the  ~ 
exact  likelihood  function  for  Gaussian  ARMA  processes,  to  factor  matrix  auto- 
covariance-generating  functions  or  spectral  densities,  and  to  estimate  vector  au- 
toregressions  with  coefficients  that  change  over  time. 

Section  13.1  describes  how  a  dynamic  system  can  be  written  in  a  form  that 
can  be  analyzed  using  the  Kalman  filter.  The  filter  itself  is derived  in Section  13.2, 
and  its  use  in  forecasting  is described  in  Section  13.3.  Section  13.4  explains  how 
to estimate  the  population  parameters  by maximum  likelihood.  Section  13.5  ana- 
lyzes  the  properties  of  the  Kalman  filter  as  the  sample  size  grows,  and  explains 
how the  Kalman  filter  is related  in the limit  to the Wold  representation  and factoring 
an  autocovariance-generating  function.  Section  13.6  develops  a  smoothing  algo- 
rithm,  which  is a  way  to  use  all  the  information  in  the  sample  to  form  the  best 
inference  about  the  unobserved  state  of the  process  at  any  historical  date.  Section 
13.7  describes  standard  errors  for  smoothed  inferences  and  forecasts.  The  use  of 
the  Kalman  filter  for  estimating  systems  with  time-varying  parameters  is investi- 
gated  in  Section  13.8. 

13.1.  The State-Space  Representation 
of a Dynamic  System 

Maintained  Assumptions 

Let  y, denote  an  (n  X  1) vector  of variables  observed  at  date  ¢.  A rich  class 
of  dynamic  models  for  y,  can  be  described  in  terms  of a  possibly  unobserved 
(r x  1) vector  €, known  as  the  state  vector.  The  state-space  representation  of the 
dynamics  of y is given  by the  following  system  of equations: 

Seo  =  FE,  +  V4: 

; 

y, =  A’x,  +  H’'E,  +  w,, 

[13.1.1] 

[13.1.2] 

where  F, A’,  and  H’  are  matrices  of parameters  of dimension  (r x  r),(n  X  k), 
and (n x  r), respectively,  and x, isa (k x  1) vector  of exogenous  or predetermined 
variables.  Equation  [13.1.1]  is known  as  the state  equation,  and  [13.1.2]  is known 

372 

as  the observation  equation.  The (r  x  1) vector  v, and  the  (n  x  1) vector  w,  are 
vector  white  noise: 

E(v,v!)  =  {2  for 4 por 
otherwise 
0 

E(w,w!)  =  A  ort 
otherwise, 
0 

[13.1.3] 

(13.1.4] 

where  Q and R are  (r x  r) and  (n  x  n) matrices,  respectively.  The  disturbances 
v, and  w, are  assumed  to  be uncorrelated  at  all  lags: 

E(v,w!)  =  0 

for  all  ¢ and  r. 

[13.1.5] 

The  statement  that  x, is predetermined  or  exogenous  means  that  x, provides  no 
information  about  &,,, or  w,,,  for  s =  0, 1, 2,.  . .  beyond  that contained in  y,_,, 
Y-2»-  ++  +Y1-  Thus, for a) x, could include lagged values  of y Or  variables 
that  are  uncorrelated  wi 
w, for all 7. 

The  system  of [13.1 ‘f arcugh (1. .1.5] is typically  used  to Lieicribe a finite 
7} for  which  assumptions  about  the  initial 
We  assume  that 6 is uncorrelated  with 
*) 

series  of observations {Y1, Yas -  a 
value of the state  vector &, are  Pah 
ee  2  te 

neksap®  nea rat 

: 
| 

| 

lt 

[13.1.6] 
Mem  E(w) = 0 iii 1. 2,.. .  =  42  Ga 

Basa  ifort = 1.22).  SF 

on ‘[13. 1.1] implies that be can dic ween aie  fnstion of 

Examples  of State-Space  Representations 
Consider  a  univariate  AR(p)  process, 

tie 

Se  p(y,  7  /)  +  $2(y,-|  _  #)  be  tchee 

+  by (Yi-p41  MK  I)  T  E419 

B(sips) 

g* 

0 

fort  =T 
otherwise. 

34.13 

This  could  be  written  in  state-space  form  as  follows: 
State  Equation  (r =  p): 

ycuy  oF 

y,  —  B 

pine!  ey! 

seek 
0 

TUL 

2 

1 

Wane pice penne 
DD  dit dececets  et 

or 
dell 

[13.1.14] 

=} 

0 

ra  ;  0  Yi-p+i  — 

0 

Observation  Equation  (n  =  1): 

eer  yO  ef 

eek 

y  —  B 

a 

(13.1.15] 

Yi-p+i 

co 

That  is, we  would  specify 

22 

8 

20 
Scheie lh 

d,  g2  ° 

the 
F={0  1+ 

Rigrd:? 

& 

P p-1  d, 

samgpic 
0 

1G 
0 

Yr-p+1  —  B 

0 

0 

1  0 

es  ae 

Bi 
0 
gee 
0 

Qe.  8 

0 
o2  0  :-: 
GO  fas  § 
=: 
S++  @ 

: 

De  Ly 
H'  =.J1  0  ---  0) 

A’  =p 

x,  =  | 
w,  =  0 

R  =  0. 

Note  that the state  equation  here  is simply the first-order  vector difference  equation 
introduced  in equation  [1.2.5];  F is the same  matrix  appearing  in equation  [1.2.3]. 
The observation  equation  here  is a trivial  identity.  Thus,  we  have  already séen  that 
the  state-space  representation  [13.1.14]  and  [13.1.15]  is just another  way  of sum- 
marizing  the AR(p)  process  [13.1.13].  The  reason  for rewriting  an  AR(p)  process 
in such  a form  was  to obtain  a convenient  summary  of the  system’s dynamics,  and 
this  is the  basic  reason  to  be  interested  in  the  state-space  representation  of any  | 
system.  The analysis  of a vector  autoregression  using equation  [10.1.11]  employed 
a similar  state-space  representation. 

As another  example,  consider  a  univariate  MA(1)  process, 

374 

Chapter 13  | The  Kalman  Filter 

y  =  mw  +  &  +  86,_). 

[13.1.16} 
| 

This  could  be written  in state-space  form  as follows: 
State Equation  (r =  2): 

Wh a  r Pale :  | a 

0  0 

Ens 

Ey 

E, 

a 

Observation  Equation  (n  =  1): 

that  is, 

i] 

y 

_  ~ 

= 

= 

Ay 

| 
‘ .~—aeee, 

“nee Oe 

— — 

ain 

(13.1.17] 

(13.1.18] 

a  | 

) 

a) 4  Y=y% A  =p  x  =  1 

wifi  4)  w,=0  R  =  0. 

There are many ways aie given system in state-space form.  For aie: 

the  MA(1)  process [13.1.16]  can  also  be represented in this ian 
State Equation (r= me . 

¢ 

amet ,oigm 
Sa 2igib  Leben  6, 

~ i's) ldaitey  2: fe Ge  ;0 bine 

7 

Tor  ) 
0  1] le, + Ge, J. + ea 7) 
J 

ee 

GE, 4) 

|  ae 3 sia  ites 

ae 

Ge, 

Mert 9) 

ae 

er  ¢ 

1  ap!  «  ved ret 

. 

tH)  Sid 7838 

¥  bi6ie  sti  = 

f  : 

$ 

f 

xe 

J 

lesiatinh  lets sae: Bess sa cus r &, +  Oe) . 
i  eae aegiaemaml 

an  TL  8, ITIL  6 anodgani > 

ion  of (13.1. a pakcok 
dt  1e second si 

ie 

—s f ay +i  ma. 

-  aS  $7 mt 

aA 

att  pat 

Taye  gaps  She  pipe  | 
ee 
ce 

that 

The  third  row  asserts  that 

Sores  =  Str 

and  in  general  the  jth  row  imphies  that 

b3.041  =  §&,=  ee 

Thus,  the  first  row  of  the  state  equation  implies  that 

€j.r+1  Pehla  "Gi 41- 

See  =  (¢,  %  gL  2  p,L’.  +e 

BGR, 

Lb’  "Ej;  +  E,4) 

or 

(POL  —  OL?  78°65  OL  bibs  =  l641- 

[13.1.24] 

The  observation  equation  states  that 

[13.1.25] 
y=pt+(1  +  OL  +  OL  +--->  +  0_,  7-6, 
Multiplying  [13.1.25]  by (1  —  $,L  —  @L?  —  ---  —  $,L’)  and  using  [13.1.24] 
gives 
(1  —  o L —  $b?  —  ---  —  $,L’)(y,  —  B) 

=(1+6L+  6,127  +--+  +  6_,L"')e,, 

which  indeed  reproduces  [13.1.21]. 

The  state-space  form  can  also  be  very  convenient  for  modeling  sums  of sto- 

chastic  processes  or  the  consequences  of measurement  error.  For  example,  Fama  ~ 
and  Gibbons  (1982)  wanted  to  study  the  behavior  of the  ex  ante  real  interest  rate 
(the  nominal  interest  rate  i, minus  the  expected  inflation  rate  7rf).  This  variable  is 
unobserved,  because  the econometrician  does  not  have  data  on  the  rate  of inflation 
anticipated  by the  bond  market.  Thus,  the  state  variable  for  this  application  was 
the  scalar  &,  =  i, —  a¢  —  pw,  where  mw  denotes  the  average  ex  ante  real  interest 
rate.  Fama  and  Gibbons  assumed  that  the  ex  ante  real  rate  follows  an  AR(1) 
process: 

Enns  SE,  +  V5 yp 

) 

[13.1.26] 

The  econometrician  has observations  on  the  ex  post  real  rate  (the nominal  interest 
rate  i, mmus  actual  inflation  7),  which  can  be  written  as 

[13.1.27] 
i, —  m7,  =  (i, —  wt)  +  (78  —  m7)  =p  +  &, +  w,, 
where  w,  =  (mf  —  7,)  is  the  error  that  people  make  in  forecasting  inflation.  If 
people  form  these  forecasts  optimally,  then  w, should  be uncorrelated  with  its own 
lagged  values  or  with  the  ex  ante  real  interest  rate.  Thus,  [13.1.26]  and  [13.1.27] 
are  the  state  equation  and  observation  equation  for  a state-space  model  with  r  = 
n=1,F = ¢,y,  =i,  —  7,  A’x,  =  w, H =  1, and w, =  (af  —  7). 

In  another  interesting  application  of the  state-space  framework,  Stock  and 
Watson  (1991)  postulated  the  existence  of an unobserved  scalar  C, that  represents 
the  state  of the  business  cycle.  A set  of n  different  observed  macroeconomic  var- 
iables  (y\,, Ya,  . 
»  Y) are  each  assumed  to  be  influenced  by the  business  cycle 
and  also  to  have  an  idiosyncratic  component  (denoted  y;,)  that  is  unrelated 
to  movements  in  y,,  for  i  #  j.  If  the  business  cycle  and  each  of  the  idiosyn- 
cratic  components  could  be  described  by univariate  AR(1)  processes,  then  the 
[(n  +  1) x  1] state  vector  would  be 

. 

» 

§ =  | Xa 

[13.1.28] 

376  Chapter  13  | The  Kalman  Filter 

with  state  equation 

Ces 

Xie 

X20+1 

- 

Pc  0 

0 

ne4 

0 

0 

?, 

0 

0 

dp» 

a9 

eae 

0 

0 

0 

"tal 

Xi 

Xx 

+ 

Vour+t 

Viorel 
Vo  r+) 

[13.1.29] 

Xnst+i 

0  0  0  a 

d,  Xn: 

yal, 

and  observation  equation 

Yur 
Yo | _ 
epee 

;  a 

My 
|e 
Fie  pry  coat 

01 

0 
O} 

C, 
i*" 

STE  E ye, |, 

(13.1.30] 

Yuu 

b,, 

Vn  0  0  i 

3i 

1 

Xn 

Thus,  y; is  a parameter  that  describes  the  sensitivity  of the  ith  series  to  the  business 
cycle.  To  allow  for pth-order  dynamics,  Stock  and  Watson  replaced  C, and  y;,  in 
[13.1.28]  with  the  (p  x  1) vectors  (C,,  C,_4,.  «. 
Xis-p+1)'  so  that  &, is an  [(n  +  1)p  x  1) vector.  The  scalars  ¢, in  [13.1.29]  are 
then  replaced  by (p x  p) matrices  F; with  the structure  of the  matrix  F in [13.1.14], 
and  [n  x  (p  —  1)] blocks  of zeros  are  added  between  the  columns  of H’  in  the 
observation  equation  [13.1.30]. 

»  C,-p+i)’  and  (Nin  Xin 

+ 

13.2.  Derivation  of the  Kalman  Filter 

Overview  of the  Kalman  Filter 

Consider  the  general  state-space  system  [13.1.1]  through  [13.1.7],  whose  key 

equations  are  reproduced  here  for convenience: 

6 
(r*  1) 

io 

y  = 

(x  1) 

PBR 
(rx<r)(rx  1) 
A’-x,° 
(n  x  k)(k  x  1) 

Shela 
(rx  1) 

+  H€,  +  w, 
(nx1) 

(1x  r)(r x  1) 

[13.2.1] 

| 

{13.2.2} 

E(v,vi) 

Q 
=  yx 
0 
R 
E(w,w,)  =  4x 
0 

fort  =T 

otherwise 
Ree 

otherwise. 

: 

: 
pe  te 

~ 

[13.2.3] 
™ 
re 

__--  [13.2.4] 
3 

- 

- 

+  Yrs  Xis  X20 + 

The  analyst  is presumed  to  have  observed  y,,  yo,  . 

+  +»  X7- 
One  of the  ultimate  objectives  may  be  to  estimate  the  values  of  any  unknown 
parameters  in the  system  on  the basis  of these  observations.  For  now,  however, 
we will assume  that  the particular  numerical  values  of F, Q, A, H, and  R are  known 
with  certainty;  Section  13.4  will  give details on  how  these  parameters  can  be es- 
timated  from  the  data. 

There  are  many  uses  of the Kalman  filter.  It is  motivated  here  as an  algorithm 
for calculating  linear  least  squares  forecasts  of the state  vector  on  the  basis  of data 
observed  through  date  f, 

id 

9 

| 

where 

Ea x  E(E,.:|9,), 

ae 

Y, ~  (y;, Yi-15  sp 

Se  Yi xX}, Xj-1  oe  x;)' 

and E(E,,,|%,)  denotes  the  linear  projection  of €,,,  on  Y, and  a constant.  The 
. 
. 
Kalman  filter  calculates  these  forecasts  recursively,  generating  &jo, 

&)1,  . 

- 

[13.2.5] 

13.2.  Derivation  of the Kalman  Filter  377 

Err  ,  in  succession.  Associated  with  each  of  these  forecasts  is  a  mean  squared 
error  (MSE)  matrix,  represented  by the  following  (r  x  r) matrix: 

Pos aye  =  E((&,+1  "  ET,  Ae  «  Ee i)’): 

[13.2.6] 

Starting  the  Recursion 
The  recursion  begins  with  Evi,  which  denotes  a  forecast  of  €,  based  on  no 

observations  of y or  x.  This  is just  the  unconditional  mean  of &,, 

with  associated  MSE 

Evo  =  E(&,), 

Pijo  =  PES  %  E(é  ME  a  E(&,)}'}. 
For  example,  for  the  state-space  representation  of  the  MA(1)  system  given  in 
[13.1.17]  and  [13.1.18],  the  state  vector  was 

for  which 

w- ]-[ 

ae 

e(|® =  cl) +  iy eh 

[13.2.8] 

Pio 

where  o?  =  E(e?), 

More  generally,  if eigenvalues  of  F are  all  inside  the  unit  circle,  then  the 
process  for  &, in  [13.2.1]  is covariance-stationary.  The  unconditional  mean  of  &, 
can  be  found  by taking  expectations  of both  sides  of [13.2.1],  producing 

or,  since  &, is covariance-stationary, 

E(&,+1)  =  F-E(€,), 

(I,  —  F)-E(é,)  =  9. 
Since  unity  is not  an  eigenvalue  of F, the  matrix  (I, —  F) is nonsingular,  and  this 
equation  has  the  unique  solution  E(&,)  =  0.  The  unconditional  variance  of — can 
similarly  be  found  by postmultiplying  [13.2.1]  by its  transpose  and  taking  expec- 
tations: 

. 

EE ,411+0)  =  E[(FE,  ft V4.1  EF’  ai Vi+1))  a  F-E(€,€;)-F’  Me E(v,4.5¥;41)- 
Cross-product  terms  have  disappeared  in  light  of  [13.1.9].  Letting  2 denote  the 
variance-covariance  matrix  of &, this  equation  implies 

whose  solution  was  seen  in [10.2.18]  to  be given  by 

x  =  FXF’  +  Q, 

vec(2)  =  [I,:  —  (F @ F)]~'-vec(Q). 
Thus,  in general,  provided  that  the  eigenvalues  of F are  inside  the  unit  circle,  the 
Kalman  filter  iterations  can  be  started  with  &,,, =  0 and  P,,, the  (r x  r) matrix 
whose  elements  expressed  as  a column  vector  are  given  by 

vec(P, 9) =  [I,:  —  (F  @ F)]~'+vec(Q). 

If instead  some  eigenvalues  of F are  on  or  outside  the  unit  circle,  or  if the 
initial  state  €, is not  regarded  as  an  arbitrary  draw  from  the  process  implied  by 
[13.2.1],  then  &,,) can  be  replaced  with  the  analyst’s  best  guess  as  to  the  initial 
value  of €,, where  Pj) is a positive  definite  matrix  summarizing  the confidence  in 
378  Chapter 13  | The  Kalman  Filter 

this  guess.  Larger  values  for  the  diagonal  elements  of P,\) register  greater  uncer- 
tainty  about  the  true  value  of &,. 

Forecasting  y, 

Given  starting  values  &,,, and  P,j,  the  next  step  is  to  calculate  analogous 
magnitudes  for  the  following  date,  €,,  and  P,,,.  The  calculations  for  ¢  =  2,  3, 
.  T all  have  the  same  basic  form,  so  we  will  describe  them  in  general  terms 

for. step (; given 

i and  P,,,_,,  the  goal is  to  calculate  é. 1,  and  P,, 1,,. 

First  note  that  since  we  have  assumed  that  x, contains  no  information  about 

£, beyond  that  contained  in Y,_,, 

Next  consider  forecasting  the  value  of y,: 

_ 

E(é,|x,,  a  bs: E(é,|Y  -)  =  E iy  42 

fA 

. 

Vir-1  -  Beaks Y,_.). 

:  Notice from  [13.2.2]  that 

and so, from  the law of iterated projections, 

bey lx. &)  =  A’ a +H . 

Gac—1  =  A’ Xx, +H:  E(é,|x,,  ¥ Y,-1)  =  A’ Xx, + H ,  a is 

[13.2.9] 

. 
From [13.2.2], the error  of this forecast is 
Wa Sun = Nik dal Got  Be  A’x,  — 
Bh - “ean 01  bea  zi{t.S.£1) 

Y  —  Yu-  a  1 

= 

* 

H'€,,-,  = H (é, - Ey) + 
adt  aes 
steve 
= 

nojssups 

, 

iy  ca os  °[H reat Pi s) + Elww! : Hei 

ye ae 

{  i eer ad 

ee 

( 

> 

- 

ra  |  b.  . 

wes 

ry 

hiati 

, 

. 

1 [13.2.10] Peat be y Beh 
ot  ae: 

aes 

oe 

7 

eo 

” 

-  equati i S| og 
tae 
fi  =| 

4 
= 

t 

LASTS 

‘p 
'  aA  % iT  _ 

en 

] 

s 

. 

_ 

ee 

Byes 

EB  :  eae  eee  es  a 

ade  at 

.*) 

hs ’ 

> 

— 

™ 

; 

EX(&,  avi  E.,-1)(Y,  “e  Vu-w} 

=  E({é,  —  &,,— MH",  - E,,-1)  +  wil} 

(13.2.14] 

-  Ele.  ss"  E.,-  (&,  7  E,,,_,)'H] 

=  P,,,-,H 

by virtue  of  [13.2.11]  and  [13.2.6].  Substituting  (13.2.14],  (13.2.12],  and  [13.2.9] 
into  [13.2.13]  gives 

é,, =  &,,-,  +  Py,-,H(H'P,,,_,H  +  R)~'(y,  —  A’x,  —  H’E,,-,)- 

(13.2.15] 
The  MSE  associated  with  this  updated  projection,  which  is denoted  P,,,,  can 

be  found  from  [4.5.31]: 

P,, by  E((&,  we  E,,,)(&,  bias €,,,)'] 

E((&  —  €,- 1,  —  4-1) 
a 
x  {E[(y,  —  Fv-W(Ye  —  Iee- 
x  {El(y,  —  Go—(E,  —  E--1)'T 

tle  E,,-)(y,  T  ¥-1)' 

=  P,,_,  —  Py,  H(H'P,,,_  H+  R)'H'P,,,-1- 

Producing  a  Forecast  of &,.,— 

Next,  the  state  equation  [13.2.1]  is used  to  forecast  &,, ;: 

Es  7  =  E(é,.1|%,) 

=  F-E(é,|%,)  +  E(v,.,|9,) 
=  Fé,  +  0. 

Substituting  [13.2.15]  into  [13.2.17], 

EF ii  +  FE, 

[13.2.16] 

[13.2.17] 

13.2.18 

+  FP,,,_  ,H(H'P,,,_  ,H  +  R)~'(y,  —  Ax,  —  HE,  det 

The  coefficient  matrix  in [13.2.18]  is known  as  the  gain  matrix  and  is denoted  K;,: 

K, =  FP,,,_ ,H(H’P,,,_,H  +  R)7', 

[13.2.19] 

allowing  [13.2.18]  to  be  written 

[13.2.20] 
The  MSE  of this  forecast  can  be  found  from  [13.2.17] and  the  state  equation 

Grote  =  Fre:  +  KiQnc  Am  =  Bey). 

[13.2.1]: 

Pi  ite  1  E[(€,+1  ti  Eid (r41  -  Eaih)') 

=~ E{(Fé,  POV 

FE,,,)(FE,  +  Veagi  Ty  Fé,,,)'] 
Fr ETE,  SG  SG  PE  Ely  eal 

I 

{13.2.21] 

FPF’  +  Q, 

with  cross-product  terms  again  clearly  zero.  Substituting  [13.2.16]  into  [132.21] 
produces 

Preis  ”  F(P,,,_  ™ 

y-H(A'P,,,_  |H ai R)~'H’P,,,_,]F'  +  Q.  [13.2.22} 

380  Chapter  13  | The  Kalman  Filter 

Summary  and  Remarks 
‘To  summarize,  the  Kalman  filter  is started  with  the  unconditional  mean  and 

variance  of &,: 

Ej =  E(é) 

Pijo  +  E{{é,  r  E(é,)j€,  _  E(&,)]'}. 
Typically,  these  are  given  by &,,) =  0 and  vec(P,,))  =  [I,-  —  (F ® F)]~'-vec(Q). 
We  then  iterate  on 

« 
Seale 

=  Fi 

: 

Sa  ! 

. 

f 

: 

(13.2.23] 

+  FP,,,_  ,H(H  P,,,_ ,H  +  R)  (y,  a  A’x,  my H’é,),- 1) 

and  (13.2.22]  for  ¢  =  1, 2,...,  T.  The  value  &,,,,, denotes  the  best  forecast  of 

 €,,,  based  on  a constant  and a linear  function  of (y,, y,_,,.  - 

. 

.  Yi.  Xp  X;-ts 

se 

x,). The matrix  P,, ;;, gives  the  MSE of this  forecast.  The  forecast  of y,,,  is given 

by 

| 

Yeo aye  -  E(y,<im.7  %,) -  A'X, 4)  -  HE, 444, 

[13.2.24] 

with  associated  MSE 

(ae  i] 

E{(y,+1  =  Vie  Ye+1  ual Vira] = HP,,\\,H  +  R. 

[13.2.25] 

____  It is worth noting  that  the  recursion  in [13.2.22]  could  be calculated  without 
ever evaluating  (13:2.23].  The  values  for P,),_,  in [13.2.2]  and  K, in [13.2.19]  are 
not functions  of the  data,  but  instead  are  determined  entirely  by the  population 
may 
of the process. 
_  parameters 
cio, 
alternative  way  of writing  the recursion for  P,,,),  is sometimes  useful. 
the Kalman  updating equation [13.2.20] from the state  equation  [13.2.1] 
Q='% 

ke 
Subtra 
produces 
ig ecug iBtusor  tal 

De  Scr 

~4* 

iti  a:  2% 

Lit): 

Pet 

0 

| 

Si+1 - 

oS. F(é, r  Einst)  =  -  K,(y, a A’x, —  H’E,,,_,)  +  V41- 

[13.2.26] 

aE; — 4-1) -  Kw,  + Vey  [13.2.27] 
as 

Seedy  Fr orn 

toe 

ae 

rar 

eRe 

v 

OA 

an 

7 

. 

‘ 

_ 

9 

= ieee Sy hl CU 
—~r ewer 

Example—  Using  the  Kalman  Filter  to  Find  Exact 
Finite-Sample  Forecasts  for an  MA(1)  Process 
Consider  again  a  state-space  representation  for  the  MA(1)  process: 

State  Equation  (r  =  2): 

Observation  Equation  (n  = 

i  oe 

4 
i 4 ay ;,  0 

Rat 
& 
1): 

y 

=u+(l  al .° 

E  =  & 

F  =  F | 

Vier  =  bah 

Q =  fs 
Ye  =  Ye 
A'  =u 
x,  =  1 
H'  =  [!t  4] 
w,  =  0 
R  =  0 

13.3.1 
[13. 

(13.3.2] 

[13.3.3] 

(13.3.4] 

[13.3.5] 

[13.3.6] 

[13.3.7] 
[13.3.8] 
[13.3.9] 
[13.3.10] 
[13.3.11] 
(13.3.12] 

The  starting  values  for  the  filter  were  described  in [13.2.7]  and  [13.2.8]: 

E51, =e | 

Pio  =  1% Si 

Thus,  from  [13.2.24],  the  period  1 forecast  is 

with  MSE  given  by [13.2.25]: 

Yiyo=  et  HE,  =  p, 

E(y,  —  Yio)?  =  H'P,)H  +  R  =  [1  al 3] +  0  =  o°A(1  +  @?). 

These,  of course,  are  just the  unconditional  mean  and  variance  of y. 

To see  the structure  of the  recursion  for ¢ =  2,3,  ...  ,  T, consider  the basic 
form  of the updating equation  [13.2.23].  Notice  that since  the first row  of F consists 
entirely  of zeros,  the  first  element  of the  vector  Braate will  always  equal  zero,  for 
all t.  We  see  why if we  recall  the  meaning  of the  state  vector  in [13.3.3]: 

é 
Eni.  =  — | 

[13.3.13] 

382  Chapter 13  | The Kalman  Filter 

Naturally,  the forecast  of the  future  white  noise,  é,, ,,,,  is always zero.  The  forecast 
of y,,,  is given  by [13.2.24]: 

You  =e  +l  al rd =p  +  6é,,. 

[13.3.14] 

ala 

The  Kalman  filter  updating  equation  for  the  MSE,  equation  [13.2.21],  for 

this example  becomes 

Piatt  = 

: 

0  0 

2 

P,,,.F  +  Q  =  \ | Pulp 4 +  i 1  [13.3.15] 

Thus,  P,,,),  is  a diagonal  matriy  of the  form 

2 
Pea tte i  if dt 

[13.3.16] 

where  the  (2,-2)  element  of P, ,  ,,, (which  we  have  denoted  by p,, ,) is the  same  as 
(1, 1) element  of P,,,.  Recalling  [13.2.6]  and  (13.3.13],  this  term  has  the  inter- 
tation  as  the  MSE  of é,,,: 

[13.3.17] 
The  (1, 1) element  of P,, ,,,  has the interpretation  as the MSE  of é,, ,),. We  have 
‘seen that 
this  forecast is always zero, and its  MSE in  [13.3.16] is o7 for all t.  The 
fact that P oe is a diagonal matrix  means hat the forecast error Cres = Geis is 

Eleptr  by  Poicoon 

Pisa 

_  The aia of the  forecast  of y,,,  is given by [13. Zz: 25}: 

(e, =  Ey). 

DTS 

A, 

EY  a 

Pras)?  =  H’ P,,1\,H  +R 

. 

es 

a 

‘ce  =  f:.0 

= 
stale plese 
castin 
eer ae 
2? +  6 p;,.;." 
=  re uition er Bile cen. st e nature of the forecast in [13.3.14]: 

sis 
13.3.18] 

C6  11 

+  0 

| 

, 

ts  Ep cima 

; 

: 

- 

+  i : 

duces  |  3.3.18].  = 
2 
4  pape  ;  Pd gi % ¥ n  {1 13. ; sank i  du  Uv ,  oF ”. ig  "7 

> 

rameters 

sehen) 

From L  |: aden 

>ries  [Or  |  le Ss  generat 

31  ott Bae 

i 

‘ 

;  ay:  7  ya =e  ;  = ; 

=  i)  a 

YJ 

Finally,  notice  from  [13.2.16]  that 

nL  sl Gta  Efe ae 2] 

a?  0 

gi» 

o” 

Of} 

1 

l 

The  (1,  1) element  of P,,, (which  we  saw  equals  p,,  ,) is thus  given  by 
Piss  = 07  ~ (Ifo?  + @pi)}-o*  = 
The  recursion  in  [13.3.21]  is started  with  p,  =  o?  and  thus  has  the  solution 

=  o? —  {I/[o? + 6%p,}}-o4  =  ——-L-. 

292p 

(13.3.21] 
13.3.21 

=  $$  13.3.22 

Pe 

T+  Ok  et  ee 

a7! 

It  is interesting  to  note  what  happens  to  the  filter  as  t becomes  large.  First 
7 

consider  the  case  when  |6| =  1. Then,  from  [13.3.22], 

lim  p,4,  =  9, 

and  so,  from  [13.3.17], 

; 
ei, 5?  Gi 
Thus,  given  a  sufficient  number  of observations  on  y,  the  Kalman  filter  inference 
é,,, converges  to  the  true  value  ¢,,  and  the  forecast  [13.3.14]  converges  to  that  of 
the  Wold  representation  for  the  process.  The  Kalman  gain  in  [13.3.20]  converges 
to  (0,  1)’. 

a consider  the  case  when  |6| >  1.  From  [13.3.22],  we  have 

Pet  =  | — p2ern 

o207(1=  07),  »  w(le=107) 
@-2%  —  @2 

5 

and 

2(]  = 

a2 

im  pia.  = a 

No  matter  how  many  observations  are  obtained,  it will  not  be  possible  to  know 
with  certainty  the  value  of the  nonfundamental  innovation  e, associated  with  date 
t on  the  basis  of (y,, y,_,,-  - 
o? 

,  y;).  The  gain  is given  by 
o? 

- 

I 
92’ 

and  the  recursion  [13.3.19]  approaches 

o  +  &p,  a  —  oA(I  —  6?) 

or 

Ent, 

(1/87)-(y,  a 

6é,_  14,-1) 

Recalling  {13.3.14],  we  thus  have 

6é,),  =  (1/8)  (yy 

6é,_ \\,-1)- 

or 

Vis aye  a 

-  aioe  (1/6)  -[(y,  kits  #1) =  (Vir  fo  #)) 

Jno =  (1/8) -(y,  ~  2)  =  (1/8)? -(y,_,  —  p)  +  (1/4)*-(y,_2  hag 
which again  is the  AR(=)  forecast  associated  with  the  invertible  MA(1)  represen- 
tation.  Indeed,  the  forecasts  of the  Kalman  filter  with  @ replaced  by @-'  and  o? 
replaced  by 670?  will  be  identical  for  any  t; see  Exercise  13.5. 

Calculating s-Period-Ahead  Forecasts 
with  the 
The forecast of y, calculated  in [13.2.24]  is an  exact  finite-sample  forecast  of 
Yin XJo4,  Bou).  +  VR) - af, 

y, on  the  basis  of x, and  Y,_,  =  (y/_),  y/o,  ..- 

Kalman  Filter 

384  Chapter 13  | The  Kalman  Filter 

is deterministic,  it is also  easy  to  use.  the  Kalman  filter  to  calculate  exact  finite- 
sample  s-period-ahead  forecasts. 

The  state  equation  [13.2.1]  can  be solved  by recursive  substitution  to  yield 

G45  =  Fé,  +  Peo  vey  *  P4e,.4  herent  oats  My)iici  +  Ves 

0  OD 
The  projection  of €,,,  on  &, and  Y, is given  by 

He Se 

E(E,..1§,.%,)  =  F'€,. 

From  the  law  of iterated  projections, 

§44, =  E(E,,,|9,)  =  PE, 

[13.3.23] 

[13.3.24] 

[13.3.25] 

Thus,  from  [13.3.23]  the  s-period-ahead  forecast  error  for  the  state  vector  is 

es.  =  rey  =  P(E,  -  Sy)  +  FP  !v,4 4  +  F742 

(13.3.26] 

with  MSE 

,  ae 

oe  ye  +  Vi+s 

Prise  =  bred Sich ee  ew SOP)?  . 1133.27] 

ee  8)  ee 

To  forecast  the Fase Sow  vector  y,,,,  recall  from  the  observation  equation 

that 

Fie  A’ Xi+s  +  H’é,,,  os Wray: 
[13.3.28] 
There are  advantages  if the  state  vector  is defined in  such  a  way that x, is deter- 
ministic,  so  that  the  dynamics  of  any  exogenous  variables  can  be  represented  | 

; 

3  a &. If x, is deterministic,  the s-period-ahead  forecast  of y is 

» 

= 

iii atau =  A' sigs +  H’ By): 

The Rediadi error  is 

ae 

[13.3.29] 
| 

ce% 

Mies  i  Giscb  (A Mit * hk, + Wis)  oT  (A’x,,,  +  HE,  ...),) 

H 

SAK}  f an =  HE...  an Esra) at Was q 

PRENOUESS 
with  MSE  — 
anges bina seh die Elon  - .  ores is $a = WP,  At - R.  .  ty 
13.4. 

Likelihood Estimation 0;  a ers 

Maximum 

Types’;  oer 

vt  ry 

>  ins  : 

'  han 

ahs 

a 

: 

eters  & 

— 

Tle  = 

|  i  ‘keli 
dl = Lik  i 

bate 

vecwor 

&,  aed wpa  aut  Oye  OL  SH(A)  . 

bd 
Rig 

_ 
AuIC 

f 

&  he 

ie” 
! st3) row HEVCHVE 

sal ee *. ey * om x 

From  [13.4.1],  it  is  a simple  matter  to  construct  the  sample  log  likelihood, 

7 

[13.4.2] 

x log Syvixsm,  (y,|x,  Ae 1) 
Expression  [13.4.2]  can  then  be  maximized  numerically  with  respect  to  the  unknown 
parameters  in  the  matrices  F, Q, A,  H,  and  R; see  Burmeister  and  Wall  (1982)  for 
an  illustrative  application. 

As stressed  by Harvey  and  Phillips  (1979),  this  representation  of the  likelihood 
is particularly  convenient  for  estimating  regressions  involving  moving  average  terms. 
Moreover,  [13.4.2]  gives  the  exact  log  likelihood  function,  regardless  of  whether 
the  moving  average  representation  is invertible. 

As  an  illustrative  example,  suppose  we  wanted  to  estimate  a  bivariate  regres- 

Sane 

sion  model  whose  equations  were 

Yu  =  aX,  +  Uy, 

Yo,  =  @2X,  +  Up,, 
where  x, is  a (k  X  1) vector  of exogenous  explanatory  variables  and  a,  and  a,  are 
(k  x  1) vectors  of coefficients;  if the  two  regressions  have  different  explanatory 
variables,  the  variables  from  both  regressions  are  included  in x, with  zeros  appro- 
priately  imposed  on  a,  and  a,.  Suppose  that  the  disturbance  vector  follows  a  bi- 
variate  MA(1)  process: 

Uy, 
U2, 

Le 4  n pa boat 
2,  922}  | E2n-1  4 

£2, 

N(0, @).  This  model  can  be  written  in state-space  form  by 

with  (€,,,  €2,)'  ~  i-i.d. 
defining 

Ei, 

0  0  0  0 

E144 

= 

Sia  eecoemiiradt,  Peay] 

0  0  0  0 

E2, 

_ 

ar 

,)  lu 

RPE 

—  | E2r+t 

Spey 
0 

Gy, 

G2  OV  O 
=  |  %22  9.0 

&  iMiGhae  & 

rome 

e_¥ 

Q 

O°  i Grre  re 

beg 

hh  Vo  Oe  Pe 

2 

-  =e E 1  62, 

65> 

n= 

where  o;,  =  E(¢;,£;,).  The  Kalman  filter  iteration  is started  from 

A 

' 

S110  r 

0 
0 

0 

0 

7,  %  O 
0 
922 
02, 

0 
0 

.¥ 

Pri 7 

0 

0 

0  Oi  O12  é 

0 

Oz,  G2 

Maximization  of [13.4.2]  is started  by making  an  initial  guess  as  to  the  nu- 
_  merical  values  of the unknown  parameters.  One  obvious  way to do this is to regress 
y,, on  the  elements  of x, that  appear  in the  first  equation  to get an  initial  guess  for 
a,.  A  similar  OLS  regression  for  y,  yields  a  guess  for  a).  Setting  @,,  =  @,.  = 
6,, =  0,. =  Oinitially,  a first guess  for  M could  be the estimated  variance-covariance 
matrix  of the residuals  from  these  two  OLS  regressions.  For these  initial  numerical 
values  for the  population  parameters,  we  could  construct  F, Q, A, H, and  R from 
the expressions  just given  and  iterate  on  [13.2.22]  through  [13.2.25]  for  ¢ =  1, 2, 
.,  T —  1. The  sequences  {€,;,_,}7.,  and  {P,,,_ ,}7., resulting  from  these  iter- 

386  Chapter  13  | The  Kalman  Filter 

ations could  then  be  used  in  [13.4.1]  and  [13.4.2]  to  calculate  the  value  for  the  log 
likelihood  function  that  results  from  these  initial  parameter  values.  The  numerical 
optimization  methods  described  in Section  5.7 can  then  be employed  to  make  better 
Buesses  as  to  the  value  of the  unknown  parameters  until  [13.4.2]  is maximized.  As 
noted  in  Section  5.9,  the  numerical  search  will  be  better  behaved  if Q  is  param- 
eterized  in  terms  of  its  Cholesky  factorization. 

As  a  second  example,  consider  a  scalar  Gaussian  ARMA(1,  1) process, 

~~ 

O(Y,-1  7!  Mt)  +t,  *  Oe,_}. 

with  ¢,  ~  i.i.d.  N(0,  0”).  This  can  be  written  in  state-space  form  as  in  [13.1.22] 
and  [13.1.23]  with  r  =  2 and 

F 

d  0 

So}  ve  =[5']  e=  [0 a] 

shat.  0 

Si 

x,  =  1 

A’  =p 
0 

Eso  -  H  Pio  =  | 2 

R=0 

H'=[1  @) 
al  —  $2)  gol  —  $7) 
gorl(1 — 6?)  ol — ¢”) 

. 

> 

2  | 

This  value  for  P,,,)  was  obtained  by recognizing  that  the  state  equation  [13.1.22] 
describes  the  behavior  of  €,  =  (z,,  Z,_),  -..,2Z,-,+1)',  where  z,  =  ,z,_;  + 
$2,  +  -->  +  $,z,_,  +  €,  follows  an  AR(r)  process.  For  this  example,  r  =  2, 
so  that  P,,,  is  the  variance-covariance  matrix  of  two  consecutive  draws  from  an 
AR(2)  process  with  parameters  ¢,  =  ¢ and  ¢,  =  0.  The  expressions  just  given 
for F, Q, A, H, and R are  then  used  in the  Kalman  filter  iterations.  Thus,  expression 
[13.4.2]  allows  easy  computation  of the  exact  likelihood  function  for an  ARMA(p,  q) 
process.  This  computation  is valid  regardless  of whether  the  moving  average  pa- 
rameters  satisfy  the  invertibility  condition.  Similarly,  expression  [13.3.29]  gives the 
exact  finite-sample  s-period-ahead  forecast  for  the  process  and  [13.3.30]  its  MSE, 
again  regardless  of whether  the  invertible  representation  is used. 

Typically,  numerical  search  procedures  for  maximizing  [13.4.2]  require  the 
derivatives  of the log likelihood.  These  can  be calculated  numerically  or analytically. 
To characterize  the  analytical  derivatives  of [13.4.2],  collect  the  unknown  param- 
eters  to  be estimated  in a  vector  @, and  write  F(®),  Q(6),  A(®),  H(®),  and  R(8). 
Implicitly,  then,  E4,-1(8)  and  P,,,_ ,(®)  will  be  functions  of  @  as  well,  and  the 
derivative  of the  log of  [13.4.1]  with  respect  to  the  ith  element  of @ will  involve 
a& ,,,_  ,(8)/98,  and  dP;,,_ ,(@)/d0;. These  derivatives  can  also  be  generated  recur- 
sively  by differentiating  the Kalman  filter  recursion,  [13.2.22]  and  [13.2.23],  with 
respect  to  6,; see Caines  (1988,  pp.  585-86)  for illustration. 

For  many  state-space  models,  the  EM  algorithm  of  Dempster,  Laird,  and 
Rubin  (1977)  offers  a  particularly. convenient  means  for  maximizing  [13.4.2],  as 
developed  by Shumway  and  Stoffer  (1982)  and  Watson  and  Engle  (1983). 

Identification 
Although  the  state-space  representation  gives  a very  convenient  way  to  cal- 
culate  the exact  likelihood  function,  a  word  of caution  should  be  given.  In  the 
absence  of restrictions  on  F, Q, A,  H,  and  R, the  parameters  of the  state-space 
representation  are  unidentified  —  more than one set  of values for the  parameters 
can give rise  to  the identical  value  of thg likelihood  function, and the data give us 
no  guide for 

choosing among  these.  A trivial  example  is the  following  system: 

13.4.  Maximum  Likelihood  Estimation  of Parameters 

387 

State  Equation  (r  =  2): 

> ae  ie eal 

[13.4.3] 

Observation  Equation  (n  = 

1): 

i 

| 

yi  STs  +  €2;- 

[13.4.4] 

: 

Here,  F  =  0, Q =  u A A'  =  0,H’  =[1 
that  y, is white  noise,  with  mean  zero  and  variance  given  by (of  +  a3).  The reader 
is invited  to  confirm  in  Exercise  13.4  that  the  log  of  the  likelihood  function  from 
[13.4.1]  and  [13.4.2]  simplifies  to 

1], andR  =  0. This  model  asserts 

2 

. 

og  fy,.v,  Nis 

a,  None  hss od 8) 

©  2h v,(Y7  Yr-1 

[13.4.5] 

=  —(T/2)  log(2m)  —  (T/2)  log(a}  +  03)  —  > y?/(2(a7  +  o%))- 

Clearly,  any  values  for  0? and  o3 that  sum  to  a  given  constant  will  produce  the 
identical  value  for  the  likelihood  function. 

The  MA(1)  process  explored  in Section  13.3  provides  a  second  example  of 
an  unidentified  state-space  representation.  As  the  reader  may  verify  in  Exercise 
13.5,  the  identical  value  for  the  log likelihood  function  [13.4.2]  would  result  if 6 
is replaced  by @~'.and  o? by 6207. 

These  two  examples  illustrate  two  basic  forms  in  which  absence  of  identifi- 
cation  can  occur.  Following  Rothenberg  (1971),  a  model  is  said  to  be  globally 
identified  at  a  particular  parameter  value  @,  if for  any  value  of  @ there  exists  a 
possible  realization  Y ;  for  which  the  value  of the  likelihood  at  @ is different  from 
the  valie  of the  likelihood  at  @,.  A  model  is said  to  be  locally  identified  at  0, if 
there  exists  a 6 >  0 such  that  for  any  value  of @ satisfying  (@  —  0,)'(®@  —  8) <  6, 
there  exists  a  possible  realization  of  Y ,  for  which  the  value  of the  likelihood  at  @ 
is different  from  the value  of the  likelihood  at @,. Thus,  global  identification  implies 
local  identification.  The  first  example,  [13.4.3]  and  [13.4.4],  is neither  globally  nor 
locally  identified,  while  the  MA(1)  example  is locally  identified  but  globally  un- 
identified. 

Local  identification  is much  easier  to test  for than  global  identification.  Roth- 
enberg  (1971)  showed  that  a  model  is  locally  identified  at  @, if and  only  if the 
information  matrix  is nonsingular  in a  neighborhood  around  @).  Thus,  a common 
symptom  of trying to estimate  an  unidentified  model  is difficulty  with  inverting  the 
matrix  of second  derivatives  of the  log likelihood  function.  One  approach  to check- 
ing for  local  identification  is to  translate  the  state-space  representation  back  into 
a  vector  ARMA  model  and  check  for  satisfaction  of  the  conditions  in  Hannan 
(1971);  see 
Hamilton  (1985)  for  an  example  of this  approach.  A second  approach 
is to  work  directly  with  the  state-space  representation,  as  is done  in  Gevers  and 
Wertz  (1984)  and  Wall  (1987).  For  an  illustration  of  the  second  approach,  see 
Burmeister,  Wall,  and  Hamilton  (1986). 

Asymptotic  Properties  of Maximum  Likelihood  Estimates 
If certain  regularity  conditions  are  satisfied,  then  Caines  (1988,  Chapter  7) 
showed  that  the  maximum  likelihood  estimate  6, based  on  a sample  of size  T is 
consistent  and  asymptotically  normal.  These  conditions  include  the following:  (1) 
the model  must  be identified;  (2) eigenvalues  of F are  all inside  the  unit  circle;  (3) 

388  Chapter  13  | The  Kalman  Filter 

apart from  a constant  term,  the  variables  x, behave  asymptotically  like  a  full-rank 
linearly  indeterministic  covariance-stationary  process;  and  (4) the  true  value  of @ 
does  not  fall on  a boundary  of the  allowable  parameter  space.  Pagan  (1980,  Theo- 
rem  4) and  Ghosh  (1989)  examined  special  cases  of state-space  models  for  which 
VT$33, (67 —  @) > N(O,  I,), 
[13.4.6] 
where a is the  number of elements  of @ and  $,,, ; is the  (a x  a) information  matrix 
for a  sample of size  T as  calculated  from  second  derivatives  of the  log likelihood 
function: 

$ 

20.T 

=  ~-——E 

1 
T  (> 

“  #  log f(y,|x,, Y,_1;  9) 
SEER  UIEEeEEEa 
70 20’ 

a 

: 

4. 
[13.4.7] 

A common  practice  is to  assume  that  the  limit  of $,, ; as  T—  ©  is the  same  as 
the  plim  of 

$  ee  ela!  VO  sal  TE  ad 

5 

1  4  #& log f(y,|x,, 

Y,—13  9)} 

| 

13.4.8 

ona 

na »> 

00 00’ 

@=67 

| 

which  can  be  calculated  analytically  or  numerically  by differentiating  [13.4.2]. 
Reported  standard  errors  for  6; are  then  square  roots  of diagonal  elements  of 
(1/T)($2p.1)~'. 

| 
| 

. 

 Quasi-Maximum  Likelihood  Estimation 
______ Even if the disturbances  v, and  w, are  non-Gaussian,  the Kalman  filter  can  _ 
-  still be pre calculate the linear projection  of y,, , on  past observables.  Moreover, 
we can  form  the function [13.4.2]  and  maximize  it with respect to  @ even for non- 
Gaussian systems.  This procedure will still yield consistent  and asymptotically  Nor- 
elements  of F, Q, A,  H, and R, with  the variance-covariance 
onstructed  as described  in equation  [5.8.7].  Watson  (1989,  Theorem 2) 
ikelihood  estimates  satisfy 
d conditions  under 

which the qua  i-maxi 

Bil“  VHOe  te 

mw 

he 

mvt 

nal 

es 

O 

5 

_— 

\/T(@.. 

— 0.)  > 

N(O. 

[g. 

Ee 

OEOFY  ti EET  2  i: 
Sits tet ia 

ee 

3 

whereas  when  |6| >  1, 

lim  Pisa,  = 

tox 

og? 
0 

0 
o?(6?  Jj  1)/6? 

euietees 
ye a  gel 

It turns  out  to be a property  of a broad  class  of state-space  models that the sequences 
{P,.,j¢/-1  and  {K,}7,  converge  to  fixed  matrices,  as  the  following  proposition 

i+ilr= 

shows. 

Let  F be  an  (r x  r) matrix  whose  eigenvalues  are  all inside the 
Proposition  13.1: 
unit  circle,  let  H'  denote  an  arbitrary  (n  X  r) matrix,  and  let  Q and  R  be positive 
semidefinite  symmetric  (r  x  r) and  (n  x  n) matrices,  respectively.  Let  {P,, iret 
be  the  sequence  of MSE  matrices  calculated  by the  Kalman  filter, 

Pig aje =  F(Pa-1  —  P,,,- ,H(H'P,,,-,H  +  R)-'H'P,,,_ JF’ +  Q. 

[13.5.1] 
where  iteration  on  [13.5.1]  is initialized  by letting  P\  be  the  positive  semidefinite 
(r x  r) matrix  satisfying 

[13.5.2] 
vec(P,),)  =  [I,:  —  (F @ F)]~'-vec(Q). 
Then  {P,,,\,}7=1  's a monotonically  nonincreasing  sequence  and  converges  as  T—>  « 
to  a  steady-state  matrix  P satisfying 

P  =  F[P  —  PH(H’PH  +  R)“'H'P|F’  +  Q. 

[13.5.3] 

Moreover,  the steady-state  value for the  Kalman  gain  matrix,  defined  by 

[13.5.4] 
K =  FPH(H’PH  +  R)~', 
has the property  that the eigenvalues  of (F —  KH’)  all lie on  or  inside  the unit  circle. 

The  claim  in  Proposition  13.1  that  P,,,,;,  =  P,,_,  means  that  for  any  real 

(r x  1) vector  h, the  scalar  inequality  h’P,,,,,h  =  h’P,,,_ ,h holds. 

* 

Proposition  13.1  assumes  that  the  Kalman  filter  is started  with  P,),) equal  to 
the  unconditional  variance-covariance  matrix  of the  state  vector  €,. Although  the 
sequence  {P,,,;,}  converges  to  a  matrix  P,  the  solution  to  [13.5.3]  need  not. be 
unique;  a different  starting  value  for P,,, might  produce  a sequence  that  converges 
to  a different  matrix  P satisfying  [13.5.3].  Under  the  slightly  stronger  assumption 
that either Q or R is strictly positive definite,  then  iteration  on  [13.5.1] will converge 
to a unique  solution  to  [13.5.3],  where  the starting  value  for  the  iteration  P,), can 
be any  positive  semidefinite  symmetric  matrix. 

Proposition  13.2: 
Let  F be an  (r X  r) matrix  whose  eigenvalues  are  all inside  the 
unit  circle,  let H'  denote  an  arbitrary  (n  X  r) matrix,  and  let Q and  R be positive 
semidefinite  symmetric  (r x  r) and  (n X  n) matrices,  respectively,  with  either  Q or 
R strictly  positive  definite.  Then  the sequence  of Kalman  MSE  matrices  {P,,. ,),}/~, 
determined  by [13.5.1] converges  to a unique positive semidefinite steady-state  matrix 
P satisfying  [13.5.3],  where  the  value  of P is the same  for any  positive  semidefinite 
symmetric  starting  value for P,\y.  Moreover,  the steady-state  value for the  Kalman 
gain matrix  K in [13.5.4]  has  the property  that  the eigenvalues  of (F —  KH’)  are  all 
Strictly  inside  the  unit  circle. 

We  next  discuss  the  relevance  of the  results  in  Propositions  13.1  and  13.2 

concerning  the  eigenvalues  of (F —  KH’), 

390  Chapter  13  | The  Kalman  Filter 

Using the  Kalman  Filter  to  Find  the  Wold  Representation 
and  Factor  an  Autocovariance-Generating  Function 

Consider  a system  in  which  the  explanatory  variables  (x,) consist  solely  of a 
constant  term.  Without  loss  of generality,  we  simplify  the  notation  by assuming 
that  A‘x,  =  0.  For  such  systems,  the  Kalman  filter  forecast  of the  state  vector  can 
be written  as  in [13.2.20]: 

East  =  FE).  +  K,(y, 

H’'é,,,_,). 

[13.5.5] 

The linear  projection  of y,,  , on  the observed  finite  sample  of its own  lagged  values 
is then  calculated  from 

os 

Peele  =  E(y,+ily,,  . 

ae  y;)  =  H’E,..1),, 

[13.5.6] 

with  MSE  given  by [13.2.25]: 

E[(y,+1  re Gee rp  Ye  “en V+ 10]  *  H'P,, ;;,H  +  R. 

[13.5.7] 
Consider  the result  from  applying the Kalman  filter  to a covariance-stationary 
process  that  started  up at a time  arbitrarily  distant in  the  past.  From  Proposition 
13.1,  the difference  equation  [13.5.5] will converge  to 

[13. 5. 8] 
with  K given by [13.5.4].  The  forecast [13.5.6] will  approach the ‘ier of y,,, 
a on  the infinite  history  of its own  lagged  values: 

=  |; ae + K(y, T  H inte 

be 

The MSE of this forecast i is given by the  limiting value  of [13.5.7], 

ier 

E(y,+ily,, A ay Oe  ah)  =H be, Hi 

| 

re  “1359 

Athy  = E(y,sily,,  Vis ae  MLY+1  - Eel, Fic te  ah 

=  H'PH  +  R,  135.1 

Fppeinsyapiss  ons: 

200 

eS  Bue  t rt ; is pe  cairo rH 

Rye Lae PRI  Bree ee ote ee 

a  KS  = 

loeeee we  gir 

te 

= 

5 

a 

ae 

' 

” 

Note  that  [13.5.14]  can  be  written  as 

[13.5.17] 
{l,  —  HI.  —  (F  —  KH’)L]-'KL}y,,1  =  €40- 
The  following  result  helps  to  rewrite  the  VAR(»)  representation  [13.5.17]  in  the 
Wold  MA()  form. 

7 

Let  F, H',  and  K be matrices  of dimension  (r X  r),(n  x  r), and 
Proposition  13.3: 
(r  x  n),  respectively,  such  that  eigenvalues  of F and  of (F  —  KH’)  are  all  inside 
the  unit  circle,  and  let  z  be a  scalar  on  the  complex  unit  circle.  Then 

(1,  +  H’(I,  —  Fz)~'Kz}{I,  —  H’{l,  —  (F  -  KH’)z]~'Kz}  =  L,. 

Applying  Proposition  13.3,  if  both  sides  of  [13.5.17]  are  premultiplied  by 

(I,  +  H'(I,  —  FL)~'KL),  the  result  is the  Wold  representation  for  y: 

¥4.=4,  +  HG,  —  FL)-'KL}e,,:. 

[13.5.18] 
To summarize,  the  Wold  representation  can  be found  by iterating  on  [13.5.1] 
until  convergence.  The  steady-state  value  for  P  is  then  used  to  construct  K  in 
[13.5.4].  If the  eigenvalues  of  (F  —  KH’)  are  all  inside  the  unit  circle,  then  the 
Wold  representation  is given  by [13.5.18]. 

The  task  of finding  the  Wold  representation  is sometimes  alternatively  posed 
as the  question  of factoring  the  autocovariance-generating  function  of y.  Applying 
result  [10.3.7]  to  [13.5.16]  and  [13.5.18],  we  would  anticipate  that  the  autocovar- 
iance-generating  function  of y can  be  written  in  the  form 

G,(z)  =  {I,  +  H'(I,  —  Fz)~'Kz}{H’PH  +  R} 

x  {L,  +  K’(I,  —  F’z~')~'Hz7‘. 

(13.5.19] 

Compare  [13.5.19]  with  the autocovariance-generating  function  that  we  would  have 
written  down  directly  from  the  structure  of the  state-space  model.  From  [10.3.5], 
the autocovariance-generating  function  of & Is given  by 

Gz)  =  [L, —  Fz]-'QUL,  —  F'z-')"'" 
while  from  [10.3.6]  the  autocovariance-generating  function  of y,  =  H’&,  +  w,  is 
[13.5.20} 
Gy(z)  =  H’[I,  -—  Fz]~'Q(L,  -—  F'z~']"'H  +  R. 
Comparing  [13.5.19] with [13.5.20]  suggests  that  the  limiting  values  for the  Kalman 
gain and  MSE  matrices  K and  P can  be used  to factor  an  autocovariance-generating 
function.  The  following  proposition  gives  a  formal  statement  of this  result. 

Proposition  13.4: 
Let F denote  an  (r X  r) matrix  whose  eigenvalues  are  all inside  the 
unit  circle;  let Q and  R denote  symmetric  positive  semidefinite  matrices  of dimension 
(rx  r) and(n  X  n), respectively;  and  let H’  denote  an  arbitrary  (n  X  r) matrix.  Let 
P be a positive  semidefinite  matrix  satisfying  [13.5.3] and  let K be given  by [13.5.4]. 
Suppose that eigenvalues  of (F —  KH’)  are  all inside  the unit  circle.  Then 
H' (I, —  Fz)-'Q{I,  —  F’z~']"'H  +R 

, 

=  {I, +  H'(I,  —  Fz)~'Kz}{H’PH  +  RXI,  +  K'(I, —  F’'z~')-'Hz-'}.  |  cake! 

_  Adirect  demonstration  of this claim  is provided in Appendix  13.A  at the end 

of this  chapter. 

% 

As  an  example  of using  these  results,  consider  observations  on  a  univariate 
AR(1)  process  subject  to white  noise  measurement  error,  such  as  the  state-space 
system  of [13.1.26]  and  [13.1.27] with  »  =  0. For  this system,  F =  ¢, Q =  o2, 
A  =  0,H  =  1, and R  =  o},.  The  conditions  of Proposition  13.2  are  satisfied  as 
long  as  ||  <  1, establishing  that  |F —  KH|  =  |@  —  K| <  1.  From  equation 

392  Chapter  13  | The  Kalman  Filter 

[13.5.14],  the AR(~)  representation  for  this  process  can  be found  from 

Yor  =  [1  —  (6  —  K)L]~'Ky,  +  €:415 

which  can  be written 

(1 *  (¢  _  K)L]y,4;  =  Ky,  +  [1  =  (>  ec K)L]e€,41 

or 

(13.5.22] 
Yio: = OY,  +  €.4,  —  (6  —  K)e,. 
This is is an ARMA(1,  1) process with  AR  parameter  given  by ¢ and  MA  parameter 
given  by  —(¢@ — K). The  variance  of the  innovation  for  this  process  can  be  cal- 
culated  from  [13.5.16]: 

The value  of P can  be found  by iterating  on  [13.5.1]: 

_E(e?,,)  =  of  +  P. 

Pisin  =  $7[ Pi  1  ad  PA Mow  +  P,,-1)]  +  oY 

[13.5.23] 

(13.5.24] 

see 

shalt 

pine 
-  ? Pi,  7 wl (Ow  9  Pri ) +  o%, 
starting from  Pio = ofl  -  7), until convergence.  The  steady-state  Kalman 
gain is given  by [13.5.4]: 

[13.5.25] 
K  =  $Pi(o2,  +  P). 
__.  Aga a second example, consider adding  an  MA(q,)  process  to an MA(q2) 
_  process $s with which the first process is uncorrelated  at all leads a 
in state-space  form  as  follows: 
sth dip, 8 36,  8-5!  Ho 

oe 
TaMit  samie®  of] 

presented 

dom 

eaw 

ene  Bye 

rok} 

is30 

> 

peste 

its  Ane 

ove 

sale  bk Lae . 

a 

ak  es  BasiGe | Heme  ft 

EOS  |  ony, 

[or oo 4: féapxs16}  2: f? 0}. et ss i  3% 

—- 1, 

3524 

[13.5.28]  takes  the  form 

Yin  =  {hit  BG  ARE  +  F?L?  +  FPL? 

fee 

ue 

athe  F¢-'La-  '\KL}e,  41 

[13.5.29] 

=  {]  +  6,L  +  AL?  +  2  45  +  6,  "SE, 41, 

where 

6, =  H'F’"'K 

ford. = hs Qenst 

-ca of > 

This  provides  a  constructive  demonstration  of  the  claim  that  an  MA(q;)  process 
plus  an  MA(q2)  process  with  which  it  is  uncorrelated  can  be  described  as  an 
MA(max{q,,  42})  process. 

The  Kalman  filter  thus  provides  a  general  algorithm  for  finding  the  Wold 
representation  or  factoring  an  autocovariance-generating  function—we  simply  it- 
erate  on  [13.5.1]  until  convergence,  and  then  use  the  steady-state  gain from  [13.5.4] 
either  in  [13.5.14]  (for  the  AR(~)  form)  or  in  [13.5.18]  (for  the  MA()  form). 

Although  the  convergent  values  provide  the  Wold  representation,  for  any 
finite  t  the  Kalman  filter  forecasts  have  the  advantage  of  calculating  the  exact 
optimal  forecast  of y,,,  based  on  a  linear  function  of Ps.  Pees 

13.6.  Smoothing 

The  Kalman  filter  was  motivated  in Section  13.2  as  an  algorithm  for  calculating  a 
forecast  of the  state  vector  &, as  a  linear  function  of previous  observations, 

where  Y,_,  =  (y;-1.  Y;-25  - 
resented  the  MSE  of this  forecast: 

+ 

acs  =  E(E,|9,_,), 
» 
©»  Yio Xp—4>  Xs-25  - 

[13.6.1] 
»  X,)'.  The  matrix  P,,,_,  rep- 

- 

P,),-1  =  E{(é,  ma  E,,—(&,  i.  E44 -1)'}- 
[13.6.2] 
For  many  uses  of the  Kalman  filter  these  are  the  natural  magnitudes  of  interest. 
In some  settings,  however,  the  state  vector  &, is given  a  structural  interpretation, 
in which  case  the  value  of this  unobserved  variable  might  be of interest  for its own. 
sake.  For  example,  in  the  model  of  the  business  cycle  by Stock  and  Watson,  it 
would  be  helpful  to  know  the  state  of the  business  cycle  at  any  historical  date  t. 
A goal  might  then  be to  form  an  inference  about  the  value  of —, based  on  the  full. 
set  of data  collected,  including  observations  on  y,,  ¥,41;,--  ++  Yrs  X:>  Xai.  +++ 
x,.  Such  an  inference  is called  the  smoothed  estimate  of —,, denoted 

Ear ~  E(E,|Y7). 
[13.6.3] 
For example,  data  on  GNP  from  1954  through  1990  might  be  used  to estimate  the 
value  that  € took  on  in  1960.  The  MSE  of this  smoothed  estimate  is denoted 

Pi -  E((&,  .  Er )(é,  es  E,,7)']. 
[13.6.4] 
In general,  P,,, denotes  the  MSE  of an  estimate  of &, that  is based  on  observations 
of y and  x  through  date  7. 

For  the reader’s  convenience,  we  reproduce  here  the  key equations  for  the 

Kalman  filter: 

E 1,  >  :  ae  +  P,,,_,H(H'P,,,.  |H  =  R)-'(y,  =  A'X,  .%  H’E,,-1) 

Ei  =  FE, 

| 
Py,  —  Py-,H(H'P,,-,H  +  R)~'H’P,,_, 

| 

Py, = 

Pra iy  =  PF’  +  Q. 

394  Chapter  13  | The  Kalman  Filter 

[13.6.5] 
[13.6.6] 
[13.6.7] 

[13.6.8] 

Consider the estimate  of , based on observations  through date /, €,,,. Suppose 
we  were  subsequently  told  the  true  value  of &,, ,.  From  the  formula  for‘ updating 
a  linear  projection,  equation  [4.5.30],  the  new  estimate  of &, could  be  expressed 

EE |E.41,  Y,)  -  Eu 7% {E[(E,  i  Eu )(Es 41  a)  65)'D 

x  {E[(E,4  =  C611  ME  es  o" FAT  | tat 

[13.6.9] 

x  (Ey 41  ?  Bs, sud 

The  first  term  in the  product  on  the  right  side  of [13.6.9]  can  be  written 

El(é,  %  Ee MEr41  %  E.+1)']  ms E((é,  be  E,), (FE,  +  Pe  Thane  Fé,,,)'], 
by virtue  of [13.2.1]  and  [13.6.6].  Furthermore,  v,,,  is uncorrelated  with  &, and 
E,,,. Thus, 

El(é,  -  Ei MEs1  od E,411,)']  “  El(é, "a: E,,,(E,  =a E,,,)'F’]  spe rae 
Substituting  [13.6.10]  and  the  definition  of P,, ,;,  into  [13.6.9]  produces 

[13.6.10] 

E(é,|&,, » ¥,) =  é,,, +  PipF Pr(Ers  7  E44141)- 

Defining 
Bifes 

J, =  Py FP), 

| 
 sabaioncs 

f.6.11) 

| 

E(E|E,41;  Y,) =  E.,, 5,3 TAL  «1  =  E411): 

[13.6.12] 

Now, the linear Projection in [13.6.12] turns  out to be the same  as 
cman baie  wit 20  BIDSAXS  ai SBRIE He = (CO 
‘that  con Knowle  a 

serps 

hy 4 

21ti 

beat 

th 

(13.6.13] 

j 

. 

2 

:  7 

UU aches  ae aaa item Sages nlite  ‘ne Ag 
ie  & 

4 

y 

+.  z.  nea  ya  4s  >a ie inion wi ei  4  | vit  a oe 

: 

s 

o 

on 

—s 

linear  function  of Y,—we  can’t  improve  on a perfect  fit!*  The  term  J, in  [13.6.11] 
is also  a  function  of  population  moments,  and  so  is again  treated  as  deterministic 
for purposes  of any  linear  projection.  The  term  é, |; is another  exact  linear  function 
of Y,.  Thus,  projecting  [13.6.15]  on  Y;, turns  out  to  be  trivial: 

or 

E(E,|Y7)  i  E,), a  SEE, 4/97)  wee  SF suds 

Er  =~  E.,  +  JA§.4ur  i  era 

[13.6.16] 

Thus,  the  sequence  of  smoothed  estimates  {Eid  is calculated  as  follows. 
First,  the  Kalman  filter,  [13.6.5]  to  [13.6.8],  is  calculated  and  the  sequences 
{E,,}7 os  Ebro of,  {Py t/-1,  and  {Pre iyhino  are  stored.  The  smoothed  estimate 
for the  final  date in  the  sample,  €7)7, is just the last entry  in {Eq37 ,-  Next,  [13.6.11] 
is used  te  generate  {J,}7—,'.  From  this,  [13.6.16] is  used  for t =  T  —  1 to  calculate 

aa  =  Er -ar-t  +  Jr_(Er7  —  Erjr-1)- 
‘Now  that  kaa  has  been  calculated,  [13.6.16]  can  be  used  for  t  =  T  —  2  to 
evaluate 

ae 

zs Ee) oie  +  Jr_(Er-17  —  €r-1)7-2)- 

Proceeding  backward  through  the  sample  in this  fashion  permits  calculation  of the 
full  set  of smoothed  estimates,  {E17} ae 

Next,  consider  the mean  squared  error  associated  with  the smoothed  estimate. 

Subtracting  both-sides  of [13.6.16]  from  &, produces 

| 

Ee  By =  €,-  E,,, =  JE. ir  +  LF  ae 

or 

E,=  6,9  Wey  pS  '8! =  pete  dibiow 

Multiplying  this  equation  by its  transpose  and  taking  expectations, 
EUE,  —  EnrME  ~  Ea]  +  FEEourbieud i 

en 

=  E((é,  33  Ed,  =  En) ] +  J ELE, +11  ead  de- 

The  cross-product  terms  have  disappeared  from  the  left  side  because  er yisa 
linear  function  of Y, and  so  is uncorrelated  with  the  projection  error  &,  —  Er 
Similarly,  on  the  right  side,  E1411 is  uncorrelated  with  —, —  E., 

Equation  [13.6.17]  states  that 

Pir Si  P,,, 53 JA=  EUG)  irs  ap)  i  E((€,+  1-824 yd)5i- 

[13.6.18] 

"The  law  of iterated  projections  states  that 

E(E,|%,)  =  E(E(E|9,)|9))- 
The  law  of  iterated  projections  thus  allows  us  to  go  from a larger  information  set  to  a  smaller.  Of 
course,  the  same  operation  does  not  work  in  reverse: 

~ 

E(E|%,)  +  ELE(E,|Y,)/%y]. 

We  cannot  go from  a smaller  information  set  to  a  larger. 

An example  may  clarify  this  point.  Let  y, be  an  i.i.d.  zero-mean  sequence  with 

: 
' 

Then 

and 

€, =  wt  Year 

E(é|y,)  = 

E(E(E 

Ly) Iy.. Yea) =  Elly  Yeoal  = 

396  Chapter 13  | The Kalman  Filter 

The  bracketed  term  in  [13.6.18]  can  be  expressed  as 
ELE,  7€)0117))  +  EE,  631) 
=  {ELE  8.)  —  Err 8). 

—  {EE  8/40)  ~  EEE) 

{E[(E, 4,  =a are  ee  cs  Eur}  =  (E((E,4  oe  He  Be  ia  E,411)')} 

=  Pi. ir  i:  Pie tle 

[13.6.19] 

The  second-to-last  equality  used  the  fact  that 

ELE, 41/4117)  =  El(€,., — Sieur +  a  ae 

=  E((é..  -  Fae  ae  +  3 | Se  ae 

=  Elbe ir8i+u7h- 

since  the  projection  error  (€,,, — é. ij)  Is  uncorrelated  with  é,, 1j7-  Similarly, 
into [13.6.18]  establishes  that 
Ets  iy  =  E(E 4  i6i. 1). Substituting [13.6.19] 
the  smoothed  estimate  E\y has  MSE  given  by 

Pir  nes P,,, +  JAP.  ay7  a2  P, 4  ijiJ;- 
[13. 6. 20) 
Again,  this  sequence  is generated  by moving  through  the  sample  backward  starting 
with ¢ = T  —  1. 

13.7.  Statistical  Inference  with  the  Kalman  Filter 
The  calculation  of the  mean  squared  error 

P.,,  =  E((é,  5  EE,  ..  Ei) | 
described  earlier  assumed  that  the  parameters  of the  matrices  F, Q, A,  H,  and  R 
were  known  with  certainty.  Section  13.4  showed  how  these  parameters  could  be 
estimated  from  the  data  by maximum  likelihood.  There  would  then  be some  sam- 
pling uncertainty  about  the  true  values  of these  parameters,  and  the  calculation  of 
P,,, would  need  to  be  modified  to  obtain  the  true  mean  squared  errors  of  the 
smoothed  estimates  and  forecasts.° 

Suppose  the  unknown  parameters  are  collected  in a  vector  8.  For  any  given 
value  of 8, the matrices  F(6), Q(@), A(®),  H(®),  and  R(@) could  be used  to construct 
E_,7(0)  and  P,,,(®)  in  the  formulas  presented  earlier;  for  r =  T,  these  are  the 
smoothed  estimate  and  MSE  given  in  [13.6.16]  and  [13.6.20],  respectively;  while 
for  r  >  T,  these  are  the  forecast  and  its  MSE  in  (13.3.25]  and  [13.3.27].  Let 
+,  X{)’ denote  the  observed  data,  and  let 
Me =(yt,  Yu  - 
.  6) denote  the true  value  of @. The  earlier derivations  assumed  that  the true  va'ue 

+  Vis X7>XP_18- 

+ 

+ 

of @ was  used  to construct  £,,,(0,)  and  P,|-(8,). 

Recall  that the formulas  for updating  a linear  projection  and  its MSE,  [4.5.30] 
and  [4.5.31],  yield the  conditional  mean  and  conditional  MSE  when  applied  to 
Gaussian  vectors;  see equation  [4.6.7]. Thus,  if {v,}, {w,}, and &, are  truly Gaussian, 
then  the  linear  projection  E .)7(8)  has  the  interpretation  as  the  expectation  of €, 
conditional  on  the  data, 

E,17(00)  =  E(E,|97); 

[13.7.1] 

while Prir(60) can  be described  as  the  conditional  MSE: 

P,;7(%)  =  E{{é, -  E.7(O  IE.  -  E_)7()]'|974- 

{13.7.2] 
Let 6 denote an estimate  of @ based on Y;, and let &,, -(6) denote  the estimate 
that results from using 6  to construct  the smoothed  inference or forecast in [13.6.16] 

SThis  discussion  is based  on  Hamilton  (1986). 

13.7.  Statistical  Inference  with the  Kalman  Filter  397 

or  [13.3.25].  The  conditional  mean  squared  error  of this  estimate  is 

E{{é,  —  &),(OE,  ~  €)7(6)]'|97} 

=  EX[é,  -  E | 7-(8)  +  E 1  (0,)  ay E_,(6)] 

~  ie.  =  E.;7(8))  +  E 1 7(8,)  ~  E47(8)}' |r} 

=  EX{é, —  & rE,  —  £27()]' 197} 

7  E{{E,;  (80)  ¥  E 1 7(8)  E17 (80)  Q  E_,(8)}'|Y  7. 

[13.7.3] 

Cross-product  terms  have  disappeared  from  [13.7.3],  since 
EX(E,,7(8o)  x  Er (O)J[E,  =  E17 (8)]'|97} 

-  [E-)7 (8)  on  E_,7(8)]  x  EX{é,  4  E.)7(8,))'|97} 
=  (€,)7(0,)  —  €,)7(6)]  x  0’.  Fags 

The  first  equality  follows  because  E_17 (0)  and  €,,;(@)  are  known  nonstochastic 
functions  of 9’,  and  the  second  equality  is implied  by [13.7.1].  Substituting  [13.7.2] 
into  [13.7.3]  results  in 
— 
ELE, —  Eur. — Err@V Ir 

its 

+  P,, 7 (8)  +  E{[E,,7(8)  =  €,)7 (8) |[E.)7(8o)  7  E,,7(8)]'|Y-t. 

Equation  [13.7.4]  decomposes  the  mean  squared  error  into  two  components. 
The  first  component,  P,,(@)),  might  be  described  as  the  “‘filter  uncertainty.”  This 
is  the  term  calculated  from  the  smoothing  iteration  [13.6.20]  or  forecast  MSE 
[13.3.27]  and  represents  uncertainty  about  &, that  would  be  present  even  if the 
true  value  6, were  known  with  certainty.  The  second  term  in  [13.7.4], 

E{{E.,7(8,)  ok  E,17 (8) ][§.,7(8o)  ru  €,;7(8)]'}, 
might be called  ‘‘parameter  uncertainty.”  It reflects  the fact that  in a typical  sample, 
6 will  differ  from  the  true  value  0). 

A simple  way  to  estimate  the  size  of each  source  of uncertainty  is by Monte 
Carlo  integration.  Suppose  we  adopt  the  Bayesian  perspective  that  @ itself  is  a 
random  variable.  From  this  perspective,  [13.7.4]  describes  the  MSE  conditional  on 
® =  8).  Suppose  that  the  posterior  distribution  of 6 conditional  on  the  data  Y; is 
known;  the  asymptotic  distribution  for  the  MLE  in  [13.4.6]  suggests  that  6|%, 
might  be regarded  as  approximately  distributed  N(@,  (1/7) 
—'), where  @ denotes 
the  MLE.  We  might  then  generate  a  large  number  of  values  of  @,  say, 
6),  0...  ,  02, drawn  from  a N(6, (1/7)-$  ~') distribution.  For each draw 
(j),  we  could  calculate  the  smoothed  estimate  or  forecast  E.,7(0).  The  devia- 
tions  of  these  estimates  across  Monte  Carlo  draws  from  the  estimate  E_,7(8) 
can  be  used  to  describe  how  sensitive  the  estimate  €,,,(@)  is to  parameter  uncer- 
tainty  about  0: 

fe" 
700  2 ir)  —  Err OME)  -  Err @!. 

BE 

3a. 

, 

p06 

(13.7.5) 

This  affords  an  estimate  of 

EX[7(8)  —  §,)r(6))€.)7(0)  —  &,)7(6)]' 197}, 
where  this  expectation  is understood  to  be  with  respect  to  the  distribution  of @ 
conditional  on  Y,.. 

For  each  Monte  Carlo  realization  0’),  we  can  also  calculate  P,, (0°)  from 

[13.6.20]  or  [13.3.27].  Its average  value  across  Monte  Carlo  draws, 

, 

2000) 

5000 p> P,) (8), 

[13.7.6] 

398  Chapter  13  | The  Kalman  Filter 

provides  an  estimate  of the  filter  uncertainty  in  [ 13.7.4], 

Again,  this  expectation  is with  respect  to  the  distribution  of 6|%,. 

The sum  of [13.7.5]  and  [13.7.6]  is then  proposed  as  an  MSE  for the  estimate 

E[P,,(8)|¥7]. 

é,, 7(®)  around  the  true  value  €,. 

Pe 

13.8.  Time-Varying  Parameters 

State-Space  Model  with  Stochastically  Varying  Coefficients 

Up to  this  point  we  have  been  assuming  that  the  matrices  F,  Q, A,  H,  and 
R were  all constant.  The  Kalman  filter  can  also  be  adapted  for  more  general  state- 
space  models  in which  the  values  of these  matrices  depend  on  the  exogenous  or 
lagged dependent  variables  included  in the  vector  x,.  Consider 

2  7  F(x, )&, +  Vea) 

y,  = a(x,)  + [H(x,)]’&,  +  w,. 

{13.8.1} 

[13.8.2] 

Here F(x ,) denotes an  (r  X r) matrix whose  element ts are functions  of x,; a(x,) 
1) vector-valued function, a  d  H(x,) pad a matrix- 
é 

Iti 1s assumed  t that conditional  « on x, and a

 on data ¢ 

et 

ilarly 

©  : : 
functi 
date ¢ — st 3 denoted 

ued 

i 
he Bik nsec 

sien a eS d

e

ties 

Care  ae  Mae Gi-wy Vea  vo  Ye Bi rsttle 20s  phys 

EY  i 

ee, 

‘=F 

(-S,5  025 

¥) 

. 

} 

+ \  Lo. 
4 

74 

d 

a 

A 

~_ f 

at 

, 

a 

: 

- 

an 

ie 8.3] 

 
where 

E ,,  *  E.,,  |  rf {Ca  LHC) 

i 

C8)  7  R(x,)]  =! 

[13.8.6] 

x  ly,  *  a(x,)  i.  (M(x),  1} 

P.,,  =  Pi  i  oot  {P-sHx) 

[13.8.7] 

, 

x  [(H(x,)]'P,,-  1) H(x,)  A  R(x,)] a  '(H(x,)]'P,,-  ! | 

It then  follows  from  [13.8.1]  and  [13.8.3]  that  &,,,|%,  ~  N(E,41),.  Pe+1),).  where 

Sr+ the  =  F(x, )é,,, 
Pra iy,  =  F(x,)P[F(x,)]'  +  Q(x,)- 

[13.8.8] 
[13.8.9] 
Equations  [13.8.6]  through  [13.8.9]  are  just the  Kalman  filter equations  [13.2.15], 
[13.2.16],  [13.2.17],  and  [13.2.21]  with  the  parameter  matrices  F, Q, A,  H,  and  R 
replaced  by their  time-varying  analogs.  Thus,  as  long as  we  are  willing  to  treat  the 
initial  state  &, as  NE:  P,,)),  the  Kalman  filter  iterations  go  through  the  same  as 
before.  The  obvious  generalization  of [13.4.1]  can  continue  to  be  used  to  evaluate 
the  likelihood  function. 

Note,  however,  that  unlike  the constant-parameter  case,  the inference  [13.8.6] 
is a  nonlinear  function  of x,.  This  means  that  although  [13.8.6]  gives  the  optimal 
inference  if the disturbances  and initial  state  are  Gaussian,  it cannot  be interpreted 
as  the  linear  projection  of €, on  Y, with  non-Gaussian  disturbances. 

Linear  Regression  Models  with  Time-Varying  Coefficients  — 

One  important  application  of the state-space  model  with stochastically  varying 
parameters  is a  regression  in which  the  coefficient  vector  changes  over  time.  Con- 
sider 

[13.8.10] 
y=  x,B, tw,  . 
where  x,  is a  (kK  x  1) vector  that  can  include  lagged  values  of y or  variables  that 
are  independent  of the  regression  disturbance  w,  for  all  r.  The  parameters  of the 
coefficient  vector  are  presumed  to  evolve  over  time  according  to 

(By.  <3 B) vr F(B,  =  B) “A Vi4t- 
(13.8.11] 
If the  eigenvalues  of the  (kK  x  k) matrix  F are  all  inside  the  unit  circle,  then  B has: 
the  interpretation  as  the  average  or  steady-state  value  for  the  coefficient  vector. 
If it is further  assumed  that 

Pajee-d-n(thl?  swan 

? 

then  [13.8.10]  to  [13.8.12]  will  be  recognized  as  a  state-space  model  of the  form 
of [13.8.1]  to  [13.8.3]  with  state  vector  &, =  B,  —  B. The  regression  in [13.8.10} 
can  be written  as 
: 
[13.8.13] 
which  is an  observation  equation  of  the  form  of  [13.8.2]  with  a(x,)  =  x/B, 
H(x,)  =  x,,  and.R(x,)'=  o7.  These  values  are  then  used  in  the  Kalman  filter 
iterations  [13.8.6] 
to [13.8.9].  A  one-period-ahead  forecast  for  [13.8.10]  can  be 
— 

Ve?  XB  +X, 

-  calculated  from  [13.8.4]  as 

Er +  Wey 

, 

| 

E(y,|x,,  Y,_;)  =  x/B si a 

400  Chapter 13  | The Kalman Filter  — 

where  {€,,,_,}/_,  is calculated  from  [13.8.6]  and  [13.8.8].  The  MSE  of this  forecast 
can  also  be  inferred  from  [13.8.4]: 

El(y,  —  xB 

x/ 8-1)? lx,,  9,1]  =  x7 Pys—i%,  +  07, 

where  {P,,,_  .}7_,  is calculated  from  [13.8.7]  and  [13.8.9].  The  sample  log likelihood 
is therefore 

T 

a log f(y,|x,.  Y,-1)  =  —(T/2)  log(2m)  —  (1/2)  > log(x/P,,,_  x,  +  0?) 

T 

- 

2 

s=1 

i  (1/2)  a (y,  si  xB  <=  KE iy —1)7/(x, Py,  1%,  +  a”). 

The  specification  in  [13.8.11]  can  easily  be  generalized  to  allow  for  a  pth- 
order  VAR  for  the coefficient  vector  B, by defining  &, =  [(B,  —  B)'. (B,-,  —  B)’. 
.  +++  (By-p+1  —  B)’] and  replacing  [13.8.11]  with 

Bre 

®, al  ®, 

ecto  to Ging 

ervetey 

®,  ®,  nihites 

Bigg 
Pet  er 
i 

ee 

V4  j 

0 
fe 
0 

Estimation  of a VAR  with  Time-Varying  Coefficients 

Section  12.2  described  Litterman’s  approach  to  Bayesian  estimation  of  an 
equation  of  a  vector  autoregression  with  constant  but  unknown  coefficients.  A 
related  approach  to estimating  a  VAR  with  time-varying  coefficients  was  developed 
by Doan,  Litterman,  and  Sims  (1984).  Although  efficiency  might  be  improved  by 
estimating  all  the  equations  of  the  VAR  jointly,  their  proposal  was  to  infer  the 
parameters  for each  equation  in isolation  from  the  others. 

Suppose  for  illustration  that  equation  [13.8.10]  describes  the  first  equation 
from  a  VAR,  so  that  the  dependent  variable  (y,)  is y,,  and  the  (kK  x  1) vector  of 
explanatory  variables  is x,  =  (1, y;_;.  Y;-2,  --  - 
Yu)’  and  k =  np  +  1. The  coefficient  vector  is 

»  ¥;-p)’,  where  y,  =  (yy,  Y2-  + 

- 

: 

Be =  (C1  PIP Didi  - 
Pin Pier  = 

= 
=»  Pin)’, 

ape 
where  $5), is the  coefficient  relating  y,,  to y;,_,.  This  coefficient  is allowed  to 
different  for each  date  ¢ in the  sample. 

Pines Ode Olde  = 

Pine  os 

Doan,  Litterman,  and  Sims  specified  a  Bayesian  prior  distribution  for  the 

initial  value  of the  coefficient  vector  at  date  1: 

[13.8.14] 
B,  ~  NOB,  Pi \0)- 
The  prior distribution  is independent  across coefficients,  so  that  Pj jo  is a diagonal 
matrix.  The  mean  of the  prior distribution,  B, is that  used  by Litterman  (1986)  for 
a  constant-coefficient  VAR.  This  prior  distribution  holds  that  changes  in y,,  are 
bly difficult  to  forecast,  so  that  the  coefficient  on  y,,-,  is likely  to  be  near 

unity and all other  coefficients  are  expected  to  be  near  zero: 

[13.8.15] 
B =  (31,0  0,44.  ,  9)’. 
As in Section  12.2,  let y characterize  the analyst’s  confidence  in the  prediction  that 
¢\\),  is near  unity: 

$4)  ~  N(I,  y’). 
Smaller  values of y imply more  confidence  in the prior conviction  that ${,’,  is near 
ities coefficient  ¢’),  relates  the  value  of variable  | at date  | to its own  value 

13.8.  Time-Varying  Parameters  401 

s  periods  earlier.  Doan,  Litterman,  and  Sims  had  more  confidence  in  the  prior 
conviction  that  »“),  is zero  the  greater  the  lag,  or  the  larger  the  value  of s.  They 
represented  this  with  a  harmonic  series  for  the  variance, 
¢*) , ~  N(0,  y7/s) 

fore  =  2.3...  .  5p. 

The  prior  distribution  for  the  coefficient  relating  variable  1 to  lags  of  other 

variables  was  taken  to  be 

$ 

te 
lj.1 

hee 

nee  j=2 

no.  me 

—_—_——_. 

 —— 

of eee 
anneal 

a] 

N Ww , 

(13.8.16] 

; 
‘ 

As  in  expression  [12.2.4],  this  includes  a  correction  (77/7?)  for  the  scale  of Vu 
relative  to  y;,,  Where  77 is the  estimated  variance  of the  residuals  for  a  univariate 
fixed-coefficient  AR(p)  process  fitted  to  series  j. The  variance  in  [13.8.16]  also 
includes  a  factor  w2  <  1 representing  the  prior  expectation  that  lagged  values  of 
y, for  j #  1 are  less  likely  to  be  of help  in forecasting  y,  than  would  be  the  lagged 
values  of y,  itself;  hence,  a  tighter  prior  is used  to  set  coefficients  on  y, to  zero. 

Finally,  let  g describe  the  variance  of  the  prior  distribution  for  the  constant 

term: 

To  summarize,  the  matrix  P,), is specified  to  be 

Cit  ees  N(0,  gti  5 

22 

' 

Pio  =  ie ae 

[13.8.17] 

where 

O 

y? 
ak 

B=  024,07, 

eed 

0 
aie 
77/3 

0 
0 
0 
: 

0 

oO 

0 

+++  yp 

© 

pr 
O  w2t?/7 
Ovi 
, 

Oes, 

stots 

0 
0 
wR? 
: 

0 
0 
0 

C=] 
(nxn) 

0 

0 

EI  ae ye 

For  typical  economic  time  series,  Doan,  Litterman,  and  Sims  recommended  using 
y’ =  0.07,  w?  =  1/74,  and g =  630.  This  last  value  ensures  that  very little  weight 
is given  to  the  prior  expectation  that  the  constant  term  is zero. 

Each  of  the  coefficients  in  the  VAR  is then  presumed  to  evolve  over  time 

according  to  a  first-order  autoregression: 

By.  a  773° B,  +  (J  7%  7) B +  Vi+a- 
[13.8.18] 
Thus,  the  same  scalar  7  is used  to  describe  a  univariate  AR(1)  process  for  each 
element  of B,; Doan,  Litterman,  and  Sims  recommended a value  of 7,  =  0.999. 
The  disturbance  v, is assumed  to  have  a  diagonal  variance-covariance  matrix: 

[13 8.19] 
E(v,v;)  =  Q. 
For  all coefficients  except  the  constant  term,  the  variance  of the  ith  element  of v, 
was  assumed  to  be  proportional  to  the  corresponding  element  of Pj).  Thus,  for 
i=  2,3,...,k,  the  row  i, column  i element  of Q is taken  to be  7, times  the  row 
1,  column  i element  of P,),.  The  (1,  1) element  of Q is taken  to  be  7,  times  the 
(2, 2) element  of P,j).  This  adjustment  is used  because  the  (1,  1) element  of Pio 
represents  an  effectively  infinite  variance  corresponding  to prior  ignorance  about 

402  Chapter  13  | The  Kalman  Filter 

the  value  for  the  constant  term.  Doan,  Litterman,  and  Sims  recommended  7,  = 
10~’ 

as  a  suitable  value  for  the  constant  of proportionality. 
Equation  [13.8.18]  can  be  viewed  as  a  state  equation  of the  form 

where the  state  vector  is given  by &, =  (B,  —  B) and  F  =  7,°I,.  The  observation 
equation  is 

Sa)  =  FE,  +  Viet 

{13.8.20] 

[13.8.21] 
Yu  =  XB  +  x/€,  +  w,,. 
The  one  parameter  yet  to  be  specified  is  the  variance  of  w,,,  the  residual  in  the 
VAR.  Doan,  Litterman,  and  Sims  suggested  taking  this to  be  0.9  times  77. 

Thus,  the  sequence  of  estimated  state  vectors  {&,,,}/_,  is  found  by  iterating 
on  [13.8.6]  through  [13.8.9]  starting  from  &,;,  =  0 and  P,,,, given  by [13.8.17],  with 
F(x,)  =  m-1,,  Q(x,)  =  77°Pyy,  a(x,)  =  x/B with  B given  by (13.8.15],  H(x,)  = 
x,,  and  R(x,)  =  0.9-7?.  The  estimated  coefficient  vector  is  then  B,,,  =  B +  E.1,. 
Optimal  one-period-ahead  forecasts  are  given  by y,,,,\),  =  x, By). 

Optimal  s-period-ahead  forecasts  are  difficult  to  calculate.  However,  Doan, 
Litterman,  and  Sims  suggested  a  simple  approximation.  The  approximation  takes 
the  optimal  one-period-ahead  forecasts  for  each  of  the  n  variables  in  the  VAR, 
¥,+1\,.  and  then  treats  these  forecasts  as  if they  were  actual  observations  on y, ,  ,. 
+  ¥1) eval- 
Then  E(y,,21Y,,  ¥s-1+  - 
uated  at  y,,,  =  E(y,;ily,  y,-1,---»  Y;)-  The  law  of iterated  expectations  does 
not  apply  here,  since  E(y,,2|y,41.  y,.  ---.»Y)  is a  nonlinear  function  of  y,, ;. 
However,  Doan,  Litterman,  and  Sims  argued  that  this  simple  approach  gives  a 
good  approximation  to  the  optimal  forecast. 

+  ¥:) is approximated  by E(y,,21¥,+1,¥  - 

- 

- 

« 

» 

APPENDIX  13.A.  Proofs  of Chapter  13 Propositions 

®  Proof  of Proposition  13.1.’ 
linear  projection  of €,,,  on  Y, =  (y;, ¥i-4.---  5  Vis Xs Mpa  ee 

Recall  that  P,,,,,  has  the  interpretation  as  the  MSE  of the 

Ka)’ 

Py  sir  x  MSE[E(é,,  ,|¥)]. 
Suppose  for some  reason  we  instead  tried  to forecast  €,,  , using only observations  2, 3,.. . , 
t,  discarding  the  observation  for  date  t¢  = 
Syelos  ,  w)yandilet 

1.  Thus,  define  Y¥*  =  (y,,  y;_,. ---.  ee 

[13.A.1] 

[13.A.2] 
P*, ,,  =  MSE[E(E,,.|¥?)]- 
Then  clearly,  [13.A.2]  cannot  be  smaller  than  [13.A.1],  since  the  linear  projection 
E(€,. ,|¥,)  made  optimal  use  of Y* along  with  the  added  information  in (y;,  x;)'.  Specif- 
ically,  if h is any  (r x  1) vector,  the  linear  projection  of z,,,  =  h’&,,,  on  Y, has  MSE  given 
by 

Pit.  =  E(z,,,|¥)P  =  Efh’é,,,  —-  h’-  E(é,, ,|%)P 

a  h’-  E{(§,.1  g  £(é,.  1M ME...  =  E(E,.,\Y))'}-h 
=  h’P,,,,,h. 
Similarly,  the  linear  projection  of z,,,  on  Y*%  has  MSE  h'P?, ,,,h, with 

{13.A.3] 
h’P,, ,,h =  h’P?, ,,,h. 
But  for a  system  of the  form  of [13.2.1]  and  [13.2.2]  with  eigenvalues  of F inside  the  unit 
circle  and  time-invariant  coefficients,  it will  be  the  case  that 

MSE[E(€E,,  ly, or-  ig de a 

Y2,  X,,  X, ae  fee  7  x,)] 

=  MSE[E(é,ly,_,,  Y,-29 

+ 

+ 

5  Yi,  Ky ys Kyidy 

+  sy  x,)], 

that  is, 

Hence,  [13.A.3]  implies  that 

Pe. 

aeile 

= 

tlre  de 

h’P,,,,h  =  h'P,,,_,h 

’The arguments  in the proofs of Propositions  13.1  and  13.2 are  adapted  from  Anderson  and  Moore 

(1979.  pp.  76-82). 

Appendix  13.A.  Proofs  of Chapter  13 Propositions 

403 

1)  vector  h.  The  sequence  of  scalars  {h’'P,,  ,,,h}7.,  is  thus  monotonically 
for  any  (r  X 
nonincreasing  and  is  bounded  below  by  zero.  It  therefore  converges  to some  fixed  non- 
negative  value.  Since  this  is true  for  any  (r x  1) vector  h and  since  the  matrix  wy  i, is  symmet- 
ric,  it follows  that  the  sequence  {P, ,  ,,,}7  ,  converges  to  some  fixed  positive  semidefinite  ma- 
ae 
trix  P. 

To  verify  the  claims  about  the  eigenvalues  of  the  matrix  (F  —  KH’),  note  that if P is 
a  fixed  point  of  [13.5.3],  then  it  must  also  be  a  fixed  point  of  the  equivalent  difference 
equation  [13.2.28]: 

Let  x  denote  an  eigenvector  of  (F  —  KH’)’  and J its  eigenvalue: 

P  =  (F  —  KH’)P(F  —  KH’)’  +  KRK’  +  Q. 

[13.A.4] 

[13.A.5] 
(F  —  KH’)'x  =  Ax. 
Although  F,  K,  and H are  all  real,  the  eigenvalue  A  and  eigenvector  x  could  be  complex. 
If x’  denotes  the  conjugate  transpose  of x,  then 

x’(F  —  KH’')P(F  —  KH’)’x  =  [(F  —  KH’)’x]”P[(F  —  KH’)’x] 

Thus,  if [13.A.4]  is premultiplied  by x/’  and  postmultiplied  by x,  the  result  is 

x  Px  =  |A|*xPx  +  x/(KRK’'  +  Q)x, 

=  [Ax]  P[Ax] 
=  Al  «Px. 

or 

(=  Jal2)x“Px  =  x4(KRK’  +  Q)x. 

[13.A.6] 
Now,  (KRK’  +  Q) is positive  semidefinite,  so  the  right side of [13.A.6]  is nonnegative. 
Likewise,  P  is  positive  semidefinite,  so  x/’Px  is  nonnegative.  Expression  [13.A.6]  then 
requires  |A| <  1, meaning  that  any  eigenvalue  of (F  —  KH’)  must  be  on  or  inside  the  unit 
circle,  as  claimed. 

& 

First  we  establish  the final  claim  of the proposition,  concerning 
®  Proof of Proposition  13.2. 
the  eigenvalues  of (F  —  KH’).  Let  P denote  any  positive  semidefinite  matrix  that  satisfies 
[13.A.4],  and  let  K be  given  by [13.5.4].  Notice  that  if Q is positive  definite,  then  the  right 
side  of [13.A.6]  is strictly  positive  for  any  nonzero  x,  meaning  from  the  left  side  of [13.A.6] 
that  any  eigenvalue  A  of (F  —  KH’)  is strictly  inside  the  unit  circle.  Alternatively,  if R  is 
positive  definite,  then  the  only  way  that  the  right  side  of  [13.A.6]  could  fail  to  be  strictly 
positive  would  be  if K’x  =  0.  But  from  [13.A.5],  this  would  imply  that  F’x  =  Ax,  that  is, 
that  x  is  an  eigenvector  and  A  is  an  eigenvalue  of  F’.  This,  in  turn,  means  that  A  is  an 
eigenvalue  of  F,  in  which  case  |A|  <  1,  by the  assumption  of  stability  of  F.  Thus.  there 
cannot  be  an  eigenvector  x  of (F  —  KH’)'  associated  with  an  eigenvalue  whose  modulus  is 
greater  than  or  equal  to  unity  if R is positive  definite. 

Turning  next  to  the  rest  of  Proposition  13.2,  let  {P,, ,,,} denote  the  sequence  that 
results  from  iterating on  [13.5.1]  starting  from  an  arbitrary  positive  semidefinite  initial  value 
P,j..  We  will  show  that  there  exist  two  other  sequences  of  matrices,  to  be  denoted 
{P,, ,,,} and  {P,, ,,,}. such  that 

_ 

where 

Pi  =  trie  SPs 
P 

for all r, 

lim  P,, ,;,  =  lim  P,,,,,  =  P 

and  where  P does  not  depend  on  P,,,.  The  conclusion  will  then  be  that  {P,, ,,,} converges 
to  P regardless  of the  value  of P,,,,. 

To  construct  the  matrix  P,,,;,  that  is  to  be  offered  as  a  lower  bound  on  P,, the 
consider  the  sequence  {P,,,j,}  that  results  from  iterating  on  [13.5.1]  starting  from  the 
initial  value  P,,,,  =  0.  This  would  correspond.  to  treating  the  intial  state  —, as  if known 
with  certainty: 

Note  that  y, and  x, are  correlated  with  €,,,  for#  =  1,2,  . 
which  means  that  we  could  equally  well  write 

Pi the  sd  MSE[E(E,,  1%,  €,)]. 

{13.A.7] 
.  only through  the  value  of €,, 

. 

where  YP  =  (y/.¥;  i...  + 
hurt  the  forecast: 

| 

P,, We  ”  MSE[E(E,,  lB?  é,)]. 

[13.A.8] 

+  YS. Xe May  +» 

&4)'.  Added  knowledge  about €, could  not 

MSE\E(E,,  |, &. &))  = MSE[E(E,, ,|9¥*.  €,)], 

[13.4.9] 

404  Chapter  13  | The  Kalman  Filter 

and  indeed,  €, is correlated  with  €,,,  for ¢ =  2,3,  . 

. 

.  only  through  the  value  of €,: 

Because coefficients are  time-invariant, 

MSE[E(E,,  |,  é., €,)]  =  MSE[E(E,,,|Y*.  €,)]. 

[13.A.10] 

MSE[E(E,,,|%*,  &)]  = MSE[E(E,|%,_.,  &))  = Bays 
(13.A.11] 
Thus,  [13.A.10]  and  [13.A.11]  establish  that  the  left  side  of [13.A.9]  is the  same  as  P,,,  ,, 
_ from  [13.A.8]  the  right  side  of [13.4.9]  is the  same  as  P 
Thus,  [13.A.9]  states 

~teile 

P.,.  1  = =P,  Ila 

so  that  {P,, ,,,} is  a monotonically  nondecreasing  sequence;  the  farther  in  the  past  is the 
perfect  information  about  &,, the  less  value  it is for  forecasting  &,, ,. 

Furthermore,  a  forecast  based  on  perfect  information  about  &,,  for  which  P,, ? 
gives the  MSE,  must  be better  than  one  based  on  imperfect  information  about  &,, for which 
P,, :;, gives  the  MSE: 

Bain 

ee 

for  all 3 

Thus, P,,,;,  puts  a  lower  bound  on  P,,,,,,  as  claimed.  Moreover,  since  the  sequence 
{P,, ,,,) is monotonically  eat and bounded from above,  it converges  to  a  fixed 
value  P satisfying  [13.5.3] and [13./ 
ph = 

|  ~ construct  an  upper bound  c vPae 1» consider a sequence  {P,, ,,,} that begins  with 
the  same  starting value that was  used to construct  { 
Recall  that  P,,  ,,, 
1 
the MSE of the sequence &,,, ,,, described  i in equation [13.2. 2 

é,, on  Fé,  1+  K,(y,  TA" — 

H’ E,,,  1).  Bo iria  rt. 

iy AS 

instead using a sequence  of suboptimal  inferences  {E,, 1; }defined  by the recursion 
[13.A.12] 

F 
Brau = FE kai K(y, 

—  A’ x,— H’ Ei i). 

a " 

7 

j 

. 

. 

whee Kis ie sar gy een 
: 
it  of tl 

ei. 

J  so must 

ee wt hich the sieady-state  value  for P is  taken 
ora ; “Note vat ite. magnitude  a  so  defined  is a 
:  deapkey  sane 
have a greater 
P43  Mbatines:  5 

bal  tne 

MS. 

P  ere  fi 

rs  GH BN= isa* Ns ~  ya  ae 
ae Seah ae EHS  Re BE 

nf  poplar = 

a  a!  be 

> 
eek 

at 

aay  baasei wes ia 

ai 

®  Proof  of  Proposition  13.3. 

Observe  that 

(1,  +  H’(L,  —  Fz)~'Kz}{l,  —  H’[I,  —  (F  —  KH’)z]~'Kz} 
I,  —  H'[I,  —  (F  —  KH’)z]"'Kz  +  H'(I,  —  Fz)"  'Kz 
—  {H'(I,  —  Fz)~'Kz}{H'[L,  —  (F  —  KH’)z]~'Kz} 

i} 

(13.A.14] 

L,  +  n'{ -U,  —  (F  —  KH’)z}-'  +  [I, -  Fz]! 

— 

[{I,  —  Fz]-'KH’2{I,  -  (F  -  Kn)2|-'  kz 

The  term  in  curly  braces  in  the  last  line  of [13.A.14]  is indeed  zero,  as  may  be  verified  by 
taking  the  identity 

and  premultiplying  by [I,  —  Fz]~'  and  postmultiplying  by [I, —  (F  —  KH’)z]  ': 

—{I,  —  Fz]  +  (I,  —  (F  —  KH’)z]  —  KH’z  =  0 

—({I, — (F —  KH’)z]~' + [I,  —  Fz]~' 

ie 
—  [I,  —  Fz]  -'KH’2{I.  —  (F  —  KH’)z]-'=0.  @ 

| nee 

(13.A.15] 

_  &  Proof  of Proposition  13.4. 

Notice  that 

| 

{L,  +  H’'(L,  —  Fz)°'Kz}{H'PH  +  RI,  +  K’(I,  —  F’z~')~'Hz~'} 

=  {H’PH  +  R}  +  H‘(I,  —  Fz)~'K{H’PH  +  R}z 

[13.A.16] 

+  {H’PH  +  R}K’(I,  —  F’z~')°'Hz"' 
+  H'(I,  —  Fz)~'K{H’PH  +  R}K’(I,  —  F’z~')-'H. 

Now  [13.5.4]  requires  that 

K{H'PH  +  R}  =  FPH  . 
{H'PH  +  R}K’  =  H’PF’ 

K{H’PH  +  R}K’  =  FPH{H’PH  +  R}-'H’PF’ 

(13.A.17] 
[13.A.18] 

with  the  last  equality  following  from  [13.5.3].  Substituting  [13.A.17]  through  [13.A.19]  into 
[13.A.16]  results  in 

- 

3 

{1, +  H’(I,  —  Fz)  'Kz}{H’PH  +  R}{I,  +  K’(I,  —  F’z-')-'Hz-"} 

_— 

=  {H’PH  +  R} +  H’(1,  —  Fz) 'FPHz  +  H'PF'(I,  —  F’z-')-'Hz~' 

+  H'(I,  —  Fz)  ‘{FPF'  —  P +  Q}(I,  —  F’z"')"'H 

=R+  np +  (I, —  Fz)~'FPz  +  PF'(I,  —  F’z~')-'z"! 

+  (I, —  Fz) {FPF’  —  P +  Q}(I,  -  Pyle 

The  result  in Proposition  13.4  follows  provided  that 
P +  (I, —  Fz)-'FPz  +  PF(I,  —  F'z-')-'z-'! 

+  (I,  —  Fz)—{FPF’  —  PI,  -—  F’z-')-'  =  0. 

To verify  that  [13.A.21]  is true,  start  from  the  identity 
(I, —  Fz)P(I,  —  F'z-')  +  FPz(I,  —  F'z~') 

(13.A.20] 

[13.A.21] 

{13.A.22] 

Premultiplying  [13.A.22]  by (I,  —  Fz)~'  and  postmultiplying  by (I,  —  F’z~")~-'  confirms 
[13.A.21].  Substituting  [13.A.21]  into  [13.A.20]  produces  the claim  in Proposition  13.4. 
@ 

+  (I,  —  Fz)PF'z-'  +  FPF’  —  P  =  0. 

Chapter 13 Exercises 

13.1.  Suppose  we  have  a noisy indicator  y on  an  underlying  unobserved  random  variable 

406  Chapter 13  | The  Kalman  Filter 

y=é+e. 

Suppose  moreover  that  the  measurement  error  (e)  is N(0,  72),  while  the  true  value  € is 
N(,  @*),  with  €  uncorrelated  with  €. Show  that ane optimal  estimate  of é is  given  by 

with  associated  MSE 

E(ély)  = 

ty  sih) 

Elf - Ely) = =——, 

Discuss  js intuition  for  these  results  as  77 —  2  te 7 
13.2.  Deduce  the state-space  representation  for an  AR(p) mode] in  [13.1.14]  and  [13.1.15] 
and  the  state-space  representation  for  an  MA(1)  model  given  in  [13.1.17]  and  [13.1.18]  as 
special  cases  of that  for  the  ARMA(r,  r  —  1) model  of [13.1.22]  and  [13.1.23]. 
13.3. 
Is the  following  a  valid  state-space  representation  of an  MA(1)  process? 
State Equation: 

0. 

Goes tao ieel cawa 

Observation  Equation: 

y-~w=[1  al. ‘| 

. 

’ 
4 

34, Dene con 1343 “analysis of the Kalman filter recursio 
ecial case of {13. 4.1) and [13.4. aes for a oe 
ns. sRcnai 
MAC) reppeseiation a form of  [13.3.1]  ~ gh (13. 33.12 
des: gastioung 
parameterized by yee win be noninvertible : etegcton dol ft mg  1 
with 7 =  1/0 and  &  =  670°.  Th € forecs sae Ms ay fe. 
ameterized by ( 

fi  te er using | ease tl representation satisfies 

Vis iev=  A'XKen  t AE,  =  +. Beat. “re = 

ad ~ 

;  wey = 68 + 6p,]}- -{y, miVEe sh OE, mee The. MSE.of this forecast is. ete 
ssisn  werd 
sazit  eT Lap  eel =RUk  Fyaypiad” 

te 
Shor  ayant forecast and MSE are. 

PL +  +O!  4 =  +  6%). 

_ i ore. Asin  X St. ; 

me reed on pence of  GUM. Sr 
5  anda Bg? ig sdianebh  axe 

!  not” SR  if  A yeedqoserit a. 
NER  a  om 

ake 

_  and  James  D.  Hamilton.  1986.  “Estimation  of Unobserved  Expected  Monthly 
Inflation  Using  Kalman  Filtering.”’  Journal  of Business  and  Economic  Statistics  4:147—60. 
Caines,  Peter  E.  1988.  Linear  Stochastic  Systems.  New  York:  Wiley. 
Dempster,  A.  P.,  N.  M.  Laird,  and  D.  B.  Rubin.  1977.  ‘Maximum  Likelihood  from  In- 
complete  Data  via  the  EM  Algorithm.”  Journal  of the  Royal  Statistical  Society  Series  B, 
39: 1-38. 
Doan,  Thomas,  Robert  B.  Litterman,  and  Christopher  A.  Sims.  1984.  “Forecasting  and 
Conditional  Projection  Using  Realistic  Prior  Distributions.”  Econometric  Reviews  3:1—100. 
Fama,  Eugene  F.,  and  Michael  R.  Gibbons.  1982.  “Inflation,  Real  Returns,  and  Capital 
Investment.”  Journal  of Monetary  Economics  9:297-323. 
Gevers,  M.,  and  V. Wertz.  1984.  “Uniquely  Identifiable  State-Space  and  ARMA  Param- 
eterizations  for  Multivariable  Linear  Systems.”’  Automatica  20:333-—47. 
Ghosh,  Damayanti.  1989.  “Maximum  Likelihood  Estimation  of the  Dynamic  Shock-Error 
Model.”  Journal  of Econometrics  41:121-43. 
Hamilton,  James  D.  1985.  ‘Uncovering  Financial  Market  Expectations  of Inflation.”’  Journal 
of Political  Economy  93:1224-41. 

.  1986.  “A  Standard  Error  for  the  Estimated  State  Vector  of a  State-Space  Model.” 

Journal  of Econometrics  33:387-97. 
Hannan,  E. J. 1971.  “The  Identification  Problem  for Multiple  Equation  Systems  with  Moving 
Average  Errors."’  Econometrica  39:751-65. 
Harvey,  Andrew,  and  G.  D. A.  Phillips.  1979.  “Maximum  Likelihood  Estimation  of Regres- 
sion  Models  with  Autoregressive—Moving  Average  Disturbances.”’  Biometrika  66:49-58. 
Kalman,  R.  E.  1960.  ““A  New  Approach  to  Linear  Filtering  and  Prediction  Problems.” 
Journal  of Basic  Engineering,  Transactions  of the ASME  Series  D,  82:35-45. 

.  1963.  “New  Methods  in Wiener  Filtering Theory,”  in John  L.  Bogdanoff  and  Frank 
Kozin,  eds.,  Proceedings  of the  First  Symposium  of Engineering  Applications  of Random 
Function  Theory  and  Probability,  270-388.  New  York:  Wiley. 
Litterman,  Robert  B.  1986.  “Forecasting  with  Bayesian  Vector  Autoregressions—Five  Years 
of Experience.”’  Journal  of Business  and  Economic  Statistics  4:25-—38. 
Meinhold,  Richard  J., and  Nozer  D.  Singpurwalla.  1983.  ‘‘Understanding  the  Kalman  Filter.” 
American  Statistician  37:123-—27. 
Nicholls,  D. F., and A.  R.  Pagan.  1985.  “Varying  Coefficient  Regression,”  in E. J. Hannan, 
P. R.  Krishnaiah,  and  M.  M.  Rao,  eds.,  Handbook  of Statistics,  Vol.  5. Amsterdam:  North- 
Holland. 
Pagan,  Adrian.  1980.  “Some  Identification  and  Estimation  Results  for  Regression  Models 
with  Stochastically  Varying  Coefficients.’  Journal  of Econometrics  13:341-63. 
ane  Thomas  J.  1971.  “Identification  in Parametric  Models.”’  Econometrica  39:577- 
}. 

Shumway,  R.  H.,  and  D.  S.  Stoffer.  1982.  ‘An  Approach  to  Time  Series  Smoothing  and 
Forecasting  Using  the  EM  Algorithm.”  Journal  of Time  Series  Analysis  3:253-64. 
Sims,  Christopher  A.  1982.  ‘Policy  Analysis  with  Econometric  Models.”  Brookings  Papers 
on  Economic  Activity  1:107-—52. 
Stock,  James  H.,  and  Mark  W.  Watson.  1991.  “A  Probability  Model  of  the  Coincident 
Economic  Indicators,”  in  Kajal  Lahiri  and  Geoffrey  H.  Moore,  eds.,  Leading  Economic 
Indicators:  New  Approaches  and  Forecasting  Records.  Cambridge,  England:  Cambridge 
University  Press. 
Tanaka,  Katsuto.  1983.  ““Non-Normality  of the  Lagrange  Multiplier  Statistic  for Testing  the 
Constancy  of Regression  Coefficients.”  Econometrica  51:1577-82. 
Wall,  Kent  D.  1987.  “Identification  Theory  for  Varying  Coefficient  Regression  Models.” 
Journal  of Time  Series  Analysis  8:359-71. 
Watson,  Mark  W.  1989.  “Recursive  Solution  Methods  for  Dynamic  Linear  Rational  Ex- 
péctations  Models."’  Journal  of Econometrics  41:65-89. 

and  Robert  F. Engle.  1983.  ‘‘Alternative  Algorithms  for the  Estimation  of Dynamic 
pains ae and  Varying  Coefficient  Regression  Models.”  Journal  of Econometrics 

White,  Halbert.  1982.  “Maximum  Likelihood  Estimation  of Misspecified  Models."’  Econ- 
ometrica  50:1—25. 

408  Chapter 13  | The  Kalman  Filter 

14 

Generalized  Method 
of Moments 

Suppose  we  have  a set  of  observations  on  a  variable  y,  whose  probability  law 
depends on  an  unknown  vector  of parameters  @. One general  approach  to estimating 
® is based  on  the  principle  of maximum  likelihood—we  choose  as  the  estimate  0 
the  value  for  which  the  data  would  be  most  likely  to  have  been  observed.  A 
drawback  of this approach  is that  it requires  us  to specify  the form  of the  likelihood 
function. 

This chapter explores  an  alternative  principle  for parameter  estimation  known 
as  generalized  method  of moments  (GMM).  Although  versions  of  this  approach 
have  been  used  for  a  long  time,  the  general  statement  of  GMM  on  which  this 
chapter is based was  only recently  developed  by Hansen  (1982).  The  key advantage 
of GMM  is that  it requires  specification  only  of certain  moment  conditions  rather 
than  the  full  density.  This  can  also  be  a  drawback,  in  that  GMM  often  does  not 
make  efficient  use  of all  the  information  in the  sample. 

Section  14.1  introduces  the  ideas  behind  GMM  estimation  and  derives  some 
of the  key results.  Section  14.2  shows  how  various  other  estimators  can  be viewed 
as  special  cases  of  GMM,  including  ordinary  least  squares,  instrumental  variable 
estimation,  two-stage  least  squares,  estimators  for  systems  of  nonlinear  simulta- 
neous  equations,  and  estimators  for dynamic  rational  expectations  models.  Exten- 
sions  and  further  discussion  are  provided  in  Section  14.3.  In  many  cases,  even 
maximum  likelihood  estimation  can  be  viewed  as  a special  case  of GMM.  Section 
14.4 explores  this  analogy  and  uses  it to derive  some  general  asymptotic  properties 
of maximum  likelihood  and  quasi-maximum  likelihood  estimation. 

14.1.  Estimation  by the  Generalized  Method 
of Moments 

Classical  Method  of Moments 
It will be helpful to introduce  the ideas  behind  GMM  with a concrete  example. 
Consider  a random  variable  Y, drawn  from a standard ¢ distribution  with  v degrees 
of freedom,  so  that  its density  is 

| 

;v)= 

fy.  v) 

Dea)  Pa 
(rv)"T(v/2)  [ 

ee 
Let  Heyes. 
(y  v)) 

eee 

14.1.1 
[ 

] 

where  I'(-) is the gamma  function.  Suppose  we  have  an  i.i.d.  sample  of size  T (y,, 
Y>,  .-  +,  Yr)  and  want  to  estimate  the  degrees  of  freedom  parameter  v.  One 
approach  is to  estimate  v  by maximum  likelihood.  This  approach  calculates  the 

409 

sample  log likelihood 

d 

£(v)  =  d log  fy,(y,5  ») 

and  chooses  as  the  estimate  v the  value  for  which  £(v)  is  largest. 

An  alternative  principle  on  which  estimation  of  v  might  be  based  reasons  as 
follows.  Provided  that  v  >  2, a  standard ¢ variable  has  population  mean  zero  and 
variance  given  by 

by  =  E(Y?)  =  vv  —  2). 

[14.1.2] 

As  the  degrees  of  freedom  parameter  (v)  goes  to  infinity,  the  variance  [14.1.2] 
approaches  unity  and  the  density  [14.1.1]  approaches  that  of  a  standard  N(0,  1) 
variable.  Let  2, 7 denote  the  average  squared  value  of  y  observed  in  the  actual 
sample: 

Pe 

fin.r  =  (1/T)  x y?. 

[14.1.3] 

For  large  7, the  sample  moment  (ji, _,) should  be close  to  the  population  moment 
(2): 

Ulsay 
M2.7  —~  #2- 
Recalling  [14.1:2],  this  suggests  that  a  consistent  estimate  of v  can  be  obtained  by 
finding  a  solution  to 

vv  —  2)  =  fine 

[14.1.4] 

or 

2:  fi 
iis  ee dee 
fz 7 —  1 

{14.1.5] 

This  estimate  exists  provided  that  4, , >  1, that  is, provided  that  the  sample  seems 
to  exhibit  more  variability  than  the  N(0,  1) distribution.  If we  instead  observed 
fi, 7 =  1, the  estimate  of  the  degrees  of  freedom  would  be  infinity—a  N(0,  1) 
distribution  fits the sample  second  moment  better  than  any  member  of the ¢ family. 
The estimator  derived  from  [14.1.4]  is known  as  a classical  method  of moments 
estimator.  A general  description  of this  approach  is as  follows.  Given  an  unknown 
(a X  1) vector  of parameters  6 that characterizes  the density of an  observed  variable 
y,,  Suppose  that  a  distinct  population  moments  of  the  random  variable  can  be 
calculated  as  functions  of @, such  as 

| 

E(Y‘)  =  u;(®) 

FOP  PE!  Hy  Ae 

Res 

[14.1.6] 

The  classical  method  of  moments  estimate  of  @ is  the  value  6, for  which  these 
population  moments  are  equated  to  the  observed  sample  moments;  that  is, 6, is 
the  value  for  which 

ples  =  (Ty  Sy) 

in 

t=1 

tori  and,  te  Vo. 

An  early  example  of this  approach  was  provided  by Pearson  (1894). 

Generalized  Method  of Moments 

In the  example  of the  ¢ distribution  just  discussed,  a  single  sample  moment 
(4,7)  was used  to estimate  a single  population  parameter  (v).  We  might  also  have 
made  use  of other  moments.  For example,  if vy >  4, the  population  fourth  moment 
of a standard ¢ variable  is 

| 

d 

3y? 

My  =  E(Y;)  =  (v —  2)(v  —  4)’ 

410  Chapter  14  | Generalized’  Method  of Moments 

and  we  might  expect  this  to  be  close  to  the  sample  fourth  moment, 

fis  =  (UT)  D yi. 

T 

r=1 

We  cannot  choose  the  single  parameter  v  so  as  to  match  both  the  sample  second 
moment  and  the  sample  fourth  moment.  However,  we  might  try to  choose  v so  as 
to  be  as  close  as  possible  to  both,  by minimizing  a  criterion  function  such  as 

O(v;  yr,  Yr-1,--->¥1)  =  g' Weg, 

[14.1.7] 

where 

ir 

4 

3v? 

é 

[14.1.8] 

fae ~  (v=  Uv  -  5| 

Here  W  is a  (2  X  2) positive  definite  symmetric  weighting  matrix  reflecting  the 
importance  given  to matching  each  of the  moments.  The  larger is the  (1, 1) element 
of W, the greater  is the importance  of being as close  as possible  to satisfying  [14.1.4]. 
An  estimate  based  on  minimization  of  an  expression  such  as  [14.1.7]  was 
called  a  “minimum  chi-square”  estimator  by  Cramér  (1946,  p.  425),  Ferguson 
(1958),  and  Rothenberg  (1973)  and a ‘‘minimum  distance  estimator’  by Malinvaud 
(1970).  Hansen  (1982)  provided  the  most  general  characterization  of this  approach 
and  derived  the  asymptotic  properties  for  serially  dependent  processes.  Most  -of 
the results  reported  in this section  were  developed  by Hansen  (1982),  who described 
this  as  estimation  by the  ‘“‘generalized  method  of moments.”’ 

Hansen’s  formulation  of the  estimation  problem  is as  follows.  Let  w,  be  an 
(h  x  1) vector  of variables  that  are  observed  at  date  ¢,  let  @ denote  an  unknown 
(a x  1) vector  of coefficients,  and  let h(@, w,) be an  (r x  1) vector-valued  function, 
h: (R*  x  R*)  —  R’.  Since  w,  is a  random  variable,  so  is h(®,  w,).  Let  @) denote 
the  true  value  of 8, and  suppose  this  true  value  is characterized  by the  property 
that 

[14.1.9] 
E{h(@,,  w,)}  =  90. 
The r rows  of the vector equation  [14.1.9]  are  sometimes  described  as orthogonality 
conditions.  Let  ¥7 =  (w7,  Wr_;,--.-,  Wj)’  be  a  (Th  X  1) vector  containing  all 
the  observations  in a  sample  of size  T, and  let  the  (r x  1) vector-valued  function 
g(8; Y,) denote  the  sample  average  of h(0,  w,): 

g(0; ¥,)  =  (1/T)  > h(@, w,). 

: 

[14.1.10] 

Notice  that  g:  R? —  R’.  The  idea  behind  GMM  is to  choose  @ so  as  to  make  the 
sample  moment  g(@; Y)  as close  as  possible  to  the  population  moment  of zero; 
that  is, the  GMM  estimator  0, is the  value  of @ that  minimizes  the  scalar  — 

[14.1.11] 
QO(8;  Y,)  =  [g(8;  Y ,))'W[e(9;  ,)), 
where  {W,}7.,  is a sequence  of (r x  r) positive  definite  weighting  matrices  which 
may  be a function  of the data Y.  Often,  this  minimization  is achieved  numerically 
using  the  methods  described  in Section  5.7. 

The  classical  method  of moments  estimator  of v given  in [14.1.5]  is a special 

. 

case  of this  formulation  with  w,  =  y,,  ®  =  v,  Wr  =  1, and 

h(8,  w,)  =  y? —  wv  —  2) 
r 
g(8; Y7)  =  (I/T) 2 y? =  uv  —  2). 

14.1.  Estimation  by the  Generalized  Method  of Moments  411 

Here, r =  a  = 

| and  the  objective  function  [14.1.11]  becomes 
2 

= 

O(0;  Y,)  =  fa) > y?  —  W(v  -  | ; 

The  smallest  value  that  can  be  achieved  for  Q(-)  is zero,  which  obtains  when vr is 
the  magnitude  given  in  [14.1.5]. 

The  estimate  of  v  obtained  by minimizing  [14.1.7]  is also  a GMM  estimator 

with  r  =  2-and 

{2 - | 

Gow 

V 

h(6,  w,)  = 

3y2 

\y _v-  Hv  | 

Here,  g(@;  Y,)  and  W; would  be  as  described  in  [14.1.7]  and  [14.1.8]. 

A  variety  of  other  estimators  can  also  be  viewed  as  examples  of  GMM, 
including  ordinary  least  squares,  instrumental  variable  estimation,  two-stage  least 
squares,  nonlinear  simultaneous  equations  estimators,  estimators  for  dynamic  ra- 
tional  expectations  models,  and  in  many  cases  even  maximum  likelihood.  These 
applications  will  be  discussed  in  Sections  14.2  through  14.4. 

If the  number  of parameters  to  be  estimated  (a) is the  same  as  the  number 
of orthogonality  conditions  (r),  then  typically  the  objective  function  [14.1.11]  will 
be  minimized  by setting 

(14.1.12] 
2(6,;%,)  =  0. 
If  a  =  r,  then  the  GMM  estimator  is the  value  6; that  satisfies  these  r equations. 
If instead  there  are  more  orthogonality  conditions  than  parameters  to  estimate 
(r >  a),  then  [14.1.12]  will  not  hold  exactly.  How  close  the  ith  element  of 
2(@,; Y)  is to zero  depends  on  how  much  weight  the  ith  orthogonality  condition 
is given  by the  weighting  matrix  W;. 

For any  value  of 0, the  magnitude  of the  (r x  1) vector  g(@; Y) is the sample 
mean  of  7 realizations  of  the  (r  x  1) random  vector  h(@,  w,).  If w,  is  strictly 
stationary  and  h(-)  is continuous,  then  it is reasonable  to  expect  the  law  of large 
numbers  to  hold: 

2(0; V7) > E{h(6,  w,)}. 
The  expression  E{h(@,  w,)}  denotes  a  population  magnitude  that  depends  on  the 
value  of @ and  on  the  probability  law of w,.  Suppose  that  this  function  is continuous 
in @ and  that  @, is the  only  value  of @ that  satisfies  [14.1.9].  Then,  under  fairly 
general  stationarity,  continuity,  and  moment  conditions,  the  value  of 6, that  min- 
imizes  (14.1.11]  offers  a  consistent  estimate  of 8); see  Hansen  (1982),  Gallant  and 
White  (1988),  and  Andrews  and  Fair  (1988)  for  details. 

Optimal  Weighting  Matrix 

Suppose  that  when  evaluated  at  the  true  value  @,, the  process  {h(@,,  w,)}*  _. 

is strictly  stationary  with  mean  zero  and  vth  autocovariance  matrix  given  by 

Assuming  that  these  autocovariances  are  absolutely  summable,  define 

r,  =  E{{h(®,,  w,)][h(8,,  w,_,)]’}. 

S=  > fii 

: 

[14.1.13] 

(14.1.14] 

412  Chapter  14  | Generalized  Method  of Moments 

Recall  from  the  discussion  in  Section  10.5  that  S is the  asymptotic  variance  of  the 
sample  mean  of h(@,,  w,): 

S  =  lim T-  EX[g(®;  Y,)][g(0,;  Y,)]'}. 

. 
The  optimal  value  for  the  weighting  matrix  W,  in  [14.1.11]  turns  out  to  be 
given  by Ss” '. the inverse  of the  asymptotic  variance  matrix.  That  is,  the  minimum 
asymptotic  variance  for  the  GMM  estimator  6, is  obtained  when  6, is  chosen  to 
minimize 

O(8;  Y,)  =  [g(6;  Y,)]'S~"[g(0;  Y ,)]. 
[14.1.15] 
To  see  the  intuition  behind  this  claim,  consider  a  simple  linear  model  in  which  we 
have  r  different  observations  (y,,  y2,  ...  ,  y,)  with  a  different  population  mean 
for  each  observation  (1,  “2,  ...  ,  w,).  For  example,  y,  might  denote  the  sample 
mean  in  a  sample  of  7, observations  on  some  variable,  y,  the  sample  mean  from 
a  second  sample,  and  so  on.  In  the  absence  of  restrictions,  the  estimates  would 
simply  be 4; =  y; fori  =  1,2,...,  7.  In the  presence  of linear  restrictions  across 
the  y’s,  the  best  estimates  that  are  linear  functions  of the  y’s would  be  obtained 
is the  value  that 
by generalized  least  squares.  Recall  that  the  GLS  estimate  of 
minimizes 

(y —  p)'Q-"y  —  p), 

[14.1.16] 

where  y  =  (yj,  Y2,---,Y,)’,  BM  =  (HM),  Mo,  ---,  B,)’,  and  | is the  variance- 
covariance  matrix  of y  —  p: 

2  =  El(y  —  py  -  p)’). 
The  optimal  weighting  matrix  to  use  with  the  quadratic  form  in  [14.1.16}  is given 
by 2~'.  Just  as  D in  [14.1.16]  is the  variance  of (y —  wr), so  S in  [14.1.15]  is the 
asymptotic  variance  of \/T-g(:). 

If the  vector  process  {h(@,,  w,)}7_ _.  were  serially  uncorrelated,  then  the  ma- 

trix  S could  be consistently  estimated  by 

| 

S;  =  (I/T)  $ [h(6,,. w)}1N(0), w,)]’ 

[14.1.17] 

Calculating  this  magnitude  requires  knowledge  of 8), though  it often  also  turns  out 
that 

S, =  (1/T)  > [h(6,,  w,)][h(6;,  w,)]’ ne 

[14.1.18] 

for 6, any consistent  estimate  of @,, assuming that h(@,, w,) is serially  uncorrelated. 
Note  that  this  description  of the  optimal  weighting  matrix  is somewhat  cir- 
cular—before  we  can  estimate  8, we  need  an  estimate  of the  matrix  S, and  before 
we  can  estimate  the  matrix  S, we  need  an  estimate  of @.  The  practical  procedure 
used  in  GMM  is  as  follows.  An  initial  estimate  6 is  obtained  by minimizing 
[14.1.11]  with  an  arbitrary  weighting  matrix  such  as  W;  =  I,. This  estimate  of 8 
is then  used  in [14.1.18]  to  produce  an  initial  estimate  S$. Expression  [14.1.11] 
8°’. This 
is then  minimized  with  W,; =  [$]-'  to arrive  at  a new  GMM  estimate 
process  can  be iterated  until  6) =  6{/*”,  though  the  estimate  based  on a single 
iteration  6 has  the  same  asymptotic  distribution  as  that  based  on  an  arbitrarily 
large  number  of iterations.  Iterating  nevertheless  offers  the  practical  advantage 
that  the  resulting  estimates  are  invariant  with  respect  to  the  scale  of the  data  and 
to the  initial  weighting  matrix  for  W,. 
On  the other  hand,  if the vector  process  {h(®,,  w,)}7— _.  is serially correlated, 

| 

14.1.  Estimation  by the Generalized  Method  of Moments 

413 

the  Newey-West  (1987)  estimate  of S could  be  used: 

§=fert 

DU  -W@  +  Wr  t+  hid. 

(141.19) 

where 

r.,=(V/T)  >  [hO, w,)][h,  w,_,)]'. 

Y i 

=v+i 

(14.1.20] 

with  6 again  an  initial  consistent  estimate  of  0).  Alternatively,  the  estimators 
proposed  by  Gallant  (1987),  Andrews  (1991),  or  Andrews  and  Monahan  (1992) 
that  were  discussed  in  Section  10.5  could  also  be  applied  in  this  context. 

Asymptotic  Distribution  of the  GMM  Estimates 
Let  6, be  the  value  that  minimizes 

[g(®;  Y,)]'Sz  '[e(@;  Y7)], 
with  S, regarded  as  fixed  with  respect  to  @  and  S,;—  S.  Assuming  an  interior 
optimum,  this  minimization  is achieved  by setting  the derivative  of  [14.1.21]  with 
respect  to  @  to  zero.  Thus,  the  GMM  estimate  0; is typically  a  solution  to  the 
following  system  of nonlinear  equations: 

(14.1.21] 

og(0;  Y 
{25:8 e 

Re 

Lae 

ss 
x  Sz!  x  [g(0;;  Y,)]  =  9. 

[14.1.22] 

@=  6; 

(axr) 

(rxr) 

(rx  1) 

(ax 1) 

Here  [ag(0; Y,\/00'||9~4,  denotes  the  (r X  a) matrix  of derivatives of the  function 
g(0; Y,-),  where  these  derivatives  are  evaluated  at  the  GMM  estimate  6,. 

Since  g(0,;  Y)  is the  sample  mean  of a  process  whose  population  mean  is 
zero,  g(:)  should  satisfy  the  central  limit  theorem  given  conditions  such  as  strict 
stationarity  of w,,  continuity  of h(@, w,), and  restrictions  on  higher  moments.  Thus, 
in many  instances  it should  be  the  case  that 

VT-8(0);  Yr) > NO, S). 
Not  much  more  than  this  is  needed  to  conclude  that  the  GMM  estimator  6, is 
asymptotically  Gaussian  and  to  calculate  its  asymptotic  variance.  The  following 
proposition,  adapted  from  Hansen  (1982),  is proved  in Appendix  14.A  at  the  end 
of this  chapter. 

€. 

Let  g(®;  Y)  be  differentiable  in ® for all Y,,  and  let 6, be  the 
Proposition  14.1: 
GMM estimator satisfying  [14.1.22]  with  r =  a.  Let {S;}>_ , be a sequence  of positive 
definite  (r x  r) matrices  such  that §,-> S, with  S positive  definite.  Suppose,  further, 
that  the fol: swing  hold: 
(2) 6;> 0); 
(b)  VT: g(8);  ¥-)  > N(O,  S); and 
(c) for any  sequence  {07-}7-_  , satisfying  07 4 0,, it is the case  that 

| 

_  { g(0; Yr) 
imi 

{2e( 7) 
9)  ——— 

= 

: 

00 

@=6; 

ia 

00 

@=8 

=  DD’, 
(r xa) 

[14.1.23 
| 

with  the  columns  of D'  linearly  independent. 
Then 

VT(6;  =  ®)— N,V), 
414  Chapter  14  | Generalized: Method  of Moments 

(14.1.24] 

where 

V  =  {DS~'D’}~'. 

Proposition  14.1  implies  that  we  can  treat  6, approximately  as 

6, ~  N(®,  V;/T), 

(14.1.25] 

where 

The  estimate  S$; can  be  constructed  as  in  [14.1.18]  or  [14.1.19],  while 

V, =  {D;$;'D;}"'. 

Testing  the  Overidentifying  Restrictions 
When  the  number  of orthogonality  conditions  exceeds  the  number  of param- 
eters  to be estimated  (r >  a), the  model  is overidentified,  in that  more  orthogonality 
conditions  were  used  than  are  needed  to  estimate  @.  In  this  case,  Hansen  (1982) 
suggested  a  test  of whether  all  of the  sample  moments  represented  by g(6,;  Y,) 
are  as  close  tozero  as  would  be expected  if the  corresponding  population  moments 
E{h(@,,,  w,)}  were  truly  zero. 

. 

From  Proposition  8.1  and  condition  (b) in Proposition  14.1,  notice  that  if the 

population  orthogonality  conditions  in  [14.1.9]  were  all  true,  then 

[14.1.26] 
(VT: 2(8);  Y7))'S~'[VT-2(80;  Y7)]  + x7(r). 
In  [14.1.26],  the  sample  moment  function  g(@;  ¥,)  is evaluated  at  the  true  value 
of @,. One’s  first  guess  might  be that  condition  [14.1.26]  also  holds  when  [14.1.26] 
is evaluated  at  the  GMM  estimate  6,. However,  this  is not  the  case.  The  reason 
is that [14.1.22]  implies  that  a  different  linear  combinations  of the  (r x  1) vector 
g(9,; Y_) are  identically  zero,  these  being the a linear  combinations  obtained  when 
g(6,;  Y,)  is premultiplied  by the  (a  x  r) matrix 

[ae Yr)  } x  Ger. 

00’ 

For example,  when  a = r, all linear  combinations  of g(6,-; Y,) are  identically  zero, 
and  if @, were  replaced  by 8,,  the  magnitude  in [14.1.26]  would  simply  equal  zero 
in all  samples. 

. 

Since  the vector  g(@,;  Y) contains  (r —  a) nondegenerate  random  variables, 
it turns  out  that  a correct  test  of the  overidentifying  restrictions  for  the  case  when 
r >  acan  be  based  on  the  fact  that 

: 

[14.1.27] 
[VT-2(67;  Y7)I'SF'[VT-8(67;  Y7r)] >  x7°(r  —  2). 
Moreover,  this  test  statistic  is trivial  to  calculate,  for  it is simply  the  sample  size  T 
times  the  value attained  for the  objective  function  [14.1.21]  at the  GMM  estimate 

tT 

Unfortunately,  Hansen’s  x? test  based  on  [14.1.27]  can  easily  fail  to  detect  a 
misspecified  model  (Newey,  1985).  It is therefore  often  advisable  to  supplement 
this  test  with  others  described  in Section  14.3. 

. 

- 

14.2.  Examples 
This section  shows  how  properties  of a variety  of different  estimators  can  be ob- 
tained  as  special  cases  of  Hansen’s  results  for  generalized  method  of  moments 

14.2.  Examples 

415 

estimation.  To  facilitate  this  discussion,  we  first  summarize  the  results  of the  pre- 
ceding  section. 

Summary  of GMM 

The  statistical  model  is assumed  to  imply  a  set  of r  orthogonality  conditions 

of the  form 

E{h(6,,  w,)}  = 
(rx  1) 

0 
(rx  1) 

[14.2.1] 

where  w,  is a  Strictly  stationary  vector  of variables  observed  at  date  ¢, 8, is the  true 
value  of  an  unknown  (a  X  1) vector  of  parameters,  and  h(-)  is  a  differentiable 
r-dimensional  vector-valued  function  with  r  =  a.  The  GMM  estimate  6;  is  the 
value  of @ that  minimizes 

[2(0; Y,)'Sz'[2(0;  Y,)]. 

(1  xr) 

(r xr) 

(7x1) 

g(0; 7)  =  (1/T)  > he,  w,) 

(rx  1) 

=) 

(rx) 

T 

where 

and  §, is an  estimate  of! 

[14.2.2] 

[14.2.3] 

S  =  lim  wr)  >, © EAth(@p, w Ith), w.-) 

[14.2.4] 

(7 xr) 

The  GMM  estimate  can  be  treated  as  if 

6,  ~  N(®,  V7/T), 

(ax) 

(axl)  (axa) 

V,  ={D,-S;'-bs)}-! 

(axa) 

(axr)  (rxr)  (rxa) 

where 

and 

P 

p;  =  2800: 3) 

6; 

Y. 

[14.2.5] 

[14.2.6] 

[14.2.7] 

We  now  explore  how  these  results  would  be applied  in various  special  cases. 

(r xa) 

00 

oo  6; 

Ordinary  Least  Squares 

Consider  the  standard  linear  regression  model, 

y,  =  %,B  +  u,, 
[14.2.8] 
for  x, a  (kK  x  1) vector  of explanatory  variables.  The  critical  assumption  needed 
to justify  OLS  regression  is that  the  regression  residual  u, is uncorrelated  with  the 
explanatory  variables: 

E(xu,)  =  0. 

[14.2.9] 

'Under  strict  stationarity,  the  magnitude 

E{(h(@,,,  w,)|[h(@,,,  w,.,)]'}  =  7. 
does  not  depend  on  1.  The  expression  in  the  text  is more  general  than  necessary  under  the  stated 
assumptions.  This  expression  is appropriate  for a characterization  of GMM  that  does  not  assume  strict 
stationarity.  The  expression  in the  text  is also  helpful  in Suggesting  estimates  of  § that  can  be used  in 
various  special  cases  described  later  in this section. 

416  Chapter  14  | Generalized  Method of Moments 

In other  words,  the  true  value  B, is assumed  to  satisfy  the  condition 

E[x,(y,  —  x/By)]  =  0. 
[14.2.10] 
Expression [14.2.10]  describes  k orthogonality  conditions  of the  form  of [14.2.1], 
in which  w,  =  (y,, x/)',  ®  =  B, and 

h(@,  w,)  =  x,(y,  —  x/B). 
[14.2.11] 
The number  of orthogonality  conditions  is the  same  as  the  number  of  unknown 
parameters  in B, so  that  r =  a  =  k.  Hence,  the  standard  regression  model  could 
be  viewed  as a just-identified  GMM  specification.  Since  it is just  identified,  the 
GMM  estimate  of B is  the  value  that  sets  the  sample  average  value  for  [14.2.11] 
equal  to zero: 

0 =  g(6;; Y,)  =  (1/T)  2 x,(y,  —  x/B7). 

(14.2.12] 

Rearranging  [14.2.12]  results  in 

T 
y i 
2 x,y,  =  (2 sir 
> 
“54 

4 

SP)  ai  Beran) 

\ 

; 

J 

| 

FO)  WT 

8) 

ae 

. 

B, = {s xxi] {> si}. 

t=]! 

t=  1 

[14.2.13] 

_which is the usual  OLS estimator.  Hence, OLS is  a  special  case  of GMM. 
nye cm  coma in acted the GMM estimator in ee 2. = we  assumed hoa the 

in 

even  in dhe  esence  of hetero-  me 

tir ate 
gene mate enn ne psn However,  recall 
AM 
s  OL = in he. presence  of heteroskedasticity 
[14.2  _— satisfied. a S yields 

‘ion 

In  this  case  the  matrix  in  [14.2.15]  should  be  consistently  estimated  by 

where 

§,'=  o2(1/T)  > x:x/, 

T 

1=1 

67  =  (1/T)  > @? 

jf 

t=1 

(14.2.16] 

for  a,  =  y,  —  xi,  the  OLS  residual.  Substituting  [14.2.14]  and  [14.2.16]  into 
[14.2.6]  produces  a  variance-covariance  matrix  for  the  OLS  estimate  B, of 
a 
wT)  (UT) Za xxi  O07 Zz xx;  | (1/T) 2 xa} 

(1/T)V; 

< 

T 

z 

T 

=  a| > “| f 

Apart  from  the  estimate  of a7,  this  is the  usual  expression  for  the  variance  of  the 
OLS  estimator  under  these  conditions. 

On  the other  hand,  suppose  that  u, is conditionally  heteroskedastic  and  serially 

correlated.  In  this  case,  the  estimate  of S proposed  in  [14.1.19]  would  be 

ee  ees  : {1  —  [v/(q  +  1I)}M,.7  +  Ty). 

where 

L 

vars  7  (1/T)  e, Adi»  2  Keo - 
f=r+l 
Under  these  assumptions,  the  GMM  approximation  for  the  variance-covariance 
matrix  of B; would  be 

: 

} 

E[(Br  —  B)\(Br  -  B)']  Ik 

win}ain > x,x/  S=! (1/T)  MP x  | 

: 

T 

-1 

® 

yr 

-1 

r 

-1 

|S «x;|  s|> xx: ; 

1=1 

t=1 

which  is the  expression  derived  earlier  in  equation  [10.5.21].  White’s  (1980)  het-  . 
eroskedasticity-consistent  standard  errors  in (8 2.35]  are  obtained  as  a special  case  _ 
when  q  =  0. 

Instrumental  Variable  Estimation 

Consider  again  a  linear  model 

y, 

[14.2.17] 
where  z, is a  (k  X  1) vector  of explanatory  variables.  Suppose  now  that  some  of 
the  explanatory  variables  are  endogenous,  so  that  E(z,u,)  #  0.  Let  x,  be  an 
(r x  1) vector  of predetermined  explanatory  variables  that  are  correlated  with  z, 
but  uncorrelated  with  u,: 

=2,B  +  u,, 

The  r orthogonality  conditions  are  now 

E(x,u,)  =  0. 

E[x,(y,  —  2/Bo)]  =  0. 
(14.2. 18] 
This  again  will  be  recognized  as  a special  case  of the  GMM  framework  in which 
_w,  =  (y,,2;,  x;)',  ® =  B,a  =  k, and 

h(6, Ww) =x(y,  -  z, B). 

[14.2.19] 

418  Chapter.14  | Generalized  Method  of Moments 

De.” 

Suppose  that  the  number  of ipoaaenctoes to  be  estimated  equals  the  number 
of orthogonality  conditions  (a  =  k =  r).  Then  the  model  is just  identified,  and 
the  GMM  estimator  satisfies 

0 =  g(6,;%,)  =  (I/T)  p> x,(y,  —  2/Br) 

T 

[14.2.20] 

or 

which  is the  usual  instrumental  variable  estimator  for this  model.  To  calculate  the 
standard  errors  implied  by Hansen’s  (1982)  general  results,  we  differentiate  [14.2.19] 
to find  © 

6=6, 

=  (1/T)  - ue 4 Ls B) 

a. |, 

B=B67 

=  arr) - x2}. 

[14.2.21] 

sells 7  2 

The requirement in  Proposition  14.1  that  the plim oththis iil aaa iusto 
re  Ppa columns is the same  condition  that was needed to establish  consistency — 

; 

nator 

in Chapter 9, namely, the  condition  that  the rows  of E(z,x’) 
it a ©  GMM variance for Bi is seen Aro [14.2.6]  to  be 

“ae 
“ 

faa 

. 

ao  d 

= 

“ 

‘ 

A 

Sag Fey 

SEE  ‘YV _~_= =n) fan ) ae 

° 
a 

r 

= 

5, Pe 
a 

Pi 

= 

qa 
> 

ae 

: 

a 

7” 

ait 

4 

. 

. 

a 

. 

a 
> 
? 

+> 

oe. 

- 
pA 

* 

. 

| 

‘ 

* 

‘) 

a 
- 

‘ 
t 
7 

ay, 
é 

u  % 

4  an 
'? 

> 

. 

; 

' 

* JI 

_ - 

where 

$=  fy  +  > (l  -  [og  +  IE  +  Pe), 

(14.2.26] 

T 

ae  a  (1/T)  >  GU,  3, Mae 
t=v+ 

Ui, stp 

ji a  z, Br. 

Two-Stage  Least  Squares 

Consider  again  the  linear  model  of [14.2.17]  and  [14.2.18],  but  suppose  now 
that  the  number  of valid  instruments  r exceeds  the  number  of explanatory  variables 
k.  For  this  overidentified  model,  GMM  will  no  longer  set  all  the  sample  orthog- 
onality conditions  to zero  as  in [14.2.20],  but  instead  will  be the solution  to [14.1.22}], 

pe  {28 V7) 

30’ 

and Ss  hm  [2(6,;  Y,)] 

[14.2.27] 

=  {-aT) > axi}87'{ IT)  > x,(y,  —  xin}. 

with  the  last  line  following  from  [14.2.21]  and  [14.2.20].  Again,  if u,  is  serially 
uncorrelated  and  homoskedastic  with  variance  o?,  a  natural  estimate  of S is given 
by [14.2.24].  Using  this  estimate,  [14.2.27]  becomes 

(1/63)  x  S2x;|  > xxi]  PHD) 5  “ibn =  0. 

t=! 

t= 

t=1 

[14.2.28] 

As  in expression  [9.2.5],  define 

5’ =  p> ai} p> xxi} } 

Thus,  5’  is a  (k  x  r) matrix  whose  ith  row  represents  the  coefficients  from  an 
OLS  regression  of z;,  on  x,.  Let 

z, =  5'x, 
be  the  (k  x  1) vector  of  fitted  values  from  these  regressions  of  z,  On  x,.  Then 
[14.2.28]  implies  that 

- 

YG 

2 2,(¥;  —  z; Br)  ai 

or 

Thus,  the  GMM  estimator  for  this  case  is simply  the  two-stage  least  squares  esti- 
mator  as  written  in [9.2.8].  The  variance  given  in [14.2.6]  would  be 

(L/T)V,  =  (1/T)  ) fave) Ss ax  ]87'{ wr)  y x  |} 

-»{lS]fi] El 

f=1 

t=1 

420  Chapter  14  | Generalized  Method  of Moments 

as earlier  derived in  expression  [9.2.25].  A test  of the  overidentifying  assumptions 
embodied in  the  model in  [14.2.17]  and  [14.2.18] is  given  by 

T[g(6,;  Y,)]' S; '[g(6,;  Y,)] 

=  T {wn 3 2 x,(y,  —  xin} {@4-(ur) > nai} 

x  {wn 2 x, (Y, -  xin} 

, 

T 
=  6y? {> axi}{> N ai} {> x, . 

als  iii 

T 

* 

(= 

r=1 

t= 

This  magnitude  will have  an  asymptotic  xy? distribution  with  (r  —  k) degrees  of 
freedom  if the  model is correctly  specified. 

‘ 

Alternatively,  to allow for heteroskedasticity  and  autocorrelation  for the  re- 
siduals u,, the  estimate  S, in [14.2.24]  would  be maphtied by [14.2.26].  Recall  the 
first-order  condition  [14.2.27]: 

{ur 3 >> ax} 7{(uir) »» x(y,  -  xib)| =  0. 

[14.2.29] 

If we now define 
i6.5.+ 

MB 

{C.2 

BOE)  So 

LAS 

i8..§  103  eos 

bidbael 

+4 

Sho  noite fon 3 Di ae ye  : KOs  | 1h  ¥ 

aiae the GMM estimator nob this case  is — ih  q 

rh} > 2 201? evi  nese  =  sey 

x  sa  ib. a  Os 

et  Shs 

é 

sy 

toh 

¢ 

auf 

ea 

h 

.isvowoH 

2»;} 8} Bane Ml  SIR  2m 

SIAR 

-in order - eat fy. ened know 

ey hen,  Mh needa  mtarh | ~  TT ~  . 

o 

Peel  >)  |  era  ge 

Wiel 

ies) 

ot doe  eovide f  sie 

amr" . oe tees aa ag E 

mes mn  (an toriitoo: ae : 

wt io. 

+: 

where  f,(0,  z,)  denotes  the  ith  element  of  f(8,  z,)  and  w,  =  (ws).  Mod  ne 
GMM  estimate  of  @ is the  value  that  minimizes 

0(0;  ¥,)  =  ae > h(0,  “| s7'{aT) p> h(@,  |.  (14.2.30} 
where  an  estimate  of S that  could  be  used  with  heteroskedasticity  and  serial  cor- 
relation  of u,  is given  by 

S; =  Tor  +  2, {1—[v(q  +  1) }(P..7  +  Py.) 

q 

A 

e 

E i.  =  (1/T)  >  (n(6,  w,)}[h(6,  w,_,)]’- 

=vt+l 

Minimization  of [14.2.30]  can  be achieved  numerically.  Again,  in order  to  evaluate 
[14.2.30],  we  first  need  an  initial  estimate  of S.  One approach  is to  first  minimize 
(14.2.30]  with  S; =  I,,  use  the  resulting  estimate  @ to  construct  a  better  estimate 
of S;,  and  recalculate  6; the  procedure  can  be  iterated  further,  if desired.  Iden- 
tification  requires  an  order  condition  (r =  a) and  the  rank  condition  that  the columns 
of the  plim  of  D;. be  linearly  independent,  where 

Dei  eae  ee 
Standard  errors  for  6, are  then  readily  calculated  from  [14.2.5]  and  [14.2.6]. 

0=  Or 

=I 

00 

i fe 

Estimation  of Dynamic  Rational  Expectation  Models 

People’s  behavior  is often  influenced  by their  expectations  about  the  future. 
Unfortunately,  we  typically  do  not  have  direct  observations  on  these  expectations. 
However,  it  is  still  possible  to  estimate  and  test  behavioral  models  if  people’s 
expectations  are  formed  rationally  in the  sense  that  the  errors  they  make  in  fore- 
casting  are  uncorrelated  with  information  they  had  available  at  the  time  of  the 
forecast.  As long as  the econometrician  observes  a subset  of the information  people 
have  actually  used,  the  rational  expectations  hypothesis  suggests  orthogonality 
conditions  that  can  be  used  in the  GMM  framework. 

For  illustration,  we  consider  the  study  of portfolio  decisions  by Hansen  and 
Singleton  (1982).  Let  c, denote  the overall  level  of spending  on  consumption  goods 
by a  particular  stockholder  during  period  ¢.  The  satisfaction  or  utility  that  the 
stockholder  receives  from  this  spending  is represented  by a  function  u(c,),  where 
it is assumed  that 

du(c,)  Ys 

d*u(c,)  Ag 

dc, 

dc? 

The  stockholder  is presumed  to  want  to  maximize 

2, BE{u(c,..,)|x?}, 

(14.2.31] 

where  x; is a  vector  representing  all  the information  available  to  the  stockholder 
at date  ¢ and B is a parameter  satisfying  0 <  6 <  1. Smaller  values  of B mean  that 
the stockholder  places a smaller  weight on  future  events.  At date t, the stockholder 
contemplates  purchasing  any of m  different  assets,  where  a dollar  invested  in asset 
i at date  ¢ will  yield  a gross  return  of (1 +  r,,,,)  at date  ¢  +  1; in general  this  rate 
of return  is not  known  for  certain  at  date  ¢.  Assuming  that  the  stockholder  takes 
a position  in each  Of these  m  assets,  the stockholder’s  optimal  portfolio  will  satisfy 
[14.2.32] 

u'(c,)  =  BE{(1  +  r,,4,)u'(c,4,)|x*}  = fori  =  1,2,....m, 

422  Chapter  14  | Generalized  Method  of Moments 

where  u  ‘(c,)  =  du/dc,.  To  see  the  intuition  behind  this  claim,  suppose  that  condition 
[14.2.32]  failed  to  hold.  Say,  for  example,  that  the  left  side  were  smaller  than  the 
right.  Suppose  the  stockholder  were  to  save  one  more  dollar  at  date  ¢ and  invest 
the dollar  in asset  /, using  the  returns  to  boost  period  ¢  +  1 consumption.  Following 
this  strategy  would  cause  consumption  at  date  ¢ to  fall  by  one  dollar  (reducing 
(14.2.31]  by an  amount  given  by the  left  side  of  [14.2.32]),  while  consumption  at 
date  ¢  +  1 would  rise  by (1  +  7;,,,,)  dollars  (increasing  [14.2.31]  by an  amount 
given  by the  right  side  of [14.2.32}).  If the  left  side  of  [14.2.32]  were  less  than  the 
right side  of [14.2.32],  then  the stockholder’s  objective  [14.2.31]  would  be  improved 
under  this  change.  Only  when  [14.2.32]  is  satisfied  is  the  stockholder  as  well  off 
as  possible.’ 

Suppose  that  the  utility  function  is parameterized  as 

u(c,)  = 

ohn 
sey 
log c, 

for  y  >  Oand y #  1 

for y = 1. 

The  parameter  y is known  as  the  coefficient  of relative  risk  aversion,  which  for  this 
class  of utility  functions  is a  constant.  For  this  function,  [14.2.32]  becomes 

cp’  =  BEX(1  +  7444,)c4%|x7}. 

[14.2.33] 

Dividing  both  sides  of [14.2.33]  by c,Y  results  in 

L=  BEX(1  ¥  Kise  enle  dt 74x}, 
[14.2.34] 
where  c, could  be  moved  inside  the  conditional  expectation  operator,  since  it rep- 
resents  a  decision  based  solely  on  the  information  contained  in  x*.  Expression 
[14.2.34]  requires  that  the  random  variable  described  by 

!  —  B{U  +  Fee  eu  MG «p/C,).  %} 
[14.2.35] 
be uncorrelated  with  any  variable  contained  in the  information  set  x* for  any  asset 
i that  the  stockholder  holds.  It should  therefore  be  the  case  that 

E{[1  %  Bi  +  ii  MGent!Go  a  Kes  =y 0, 
[14.2.36] 
where  x,  is any  subset  of the  stockholder’s  information  set  x7 that  the  econome- 
trician  is also  able  to  observe. 

Let  ® =  (8,  y)’  denote  the  unknown  parameters  that  are  to  be  estimated, 
>  "matt>  Cr41/C,  X,)’  denote  the  vector  of variables 
and  let w, =  (ry y415  20410 
that  are  observed  by  the  econometrician  for  date  ¢.  Stacking  the  equations  in 
[14.2.36]  fori  =  1, 2,  ...,  m  produces  a  set  of r  orthogonality  conditions  that 
can  be  used  to  estimate  80: 

+ 

+ 

[1 —  BAG  +  ryyor)Cr41/c,) 77x, 

: 

h(6,  w,)  = 

{1 €  B{(1  +  rae  Gre 1/6.)  VM 

(14.2.37] 

(rx  1) 

. 

‘ 

[1 ke  Bi  w  Ft  Bi  NC. +-4/Ce)  —  HX, 

The  sample  average  value  of h(9,  w,) is 

2(6; Yr)  =  (1/T)  > W®,  w,), 

T 

and  the  GMM  objective  function  is 

Q(0)  =  [g(0; Y,))'S;'[e(8;  Yr). 

[14.2.38] 

This  expression  can  then  be  minimized  numerically  with  respect  to  6. 

According  to  the  theory,  the  magnitude  in [14.2.35]  should  be  uncorrelated 
with  any  information  the  stockholder  has  available.  at  time  ?, which  would  include 

For further details, see  Sargent  (1987). 

14.2.  Examples 

423 

lagged  values  of  [14.2.35].  Hence,  the  vector  in  (14.2.37]  should  be uncorrelated 
with  its  own  lagged  values,  suggesting  that  S can  be  consistently  estimated  by 

§, =  (1/T)  > {{h(6,  w,)}[h(6,  w,)]'}, 

where 6 is an  initial  consistent  estimate.  This  initial  estimate  6 could  be  obtained 
by minimizing  [14.2.38]  with  S;  =  I,. 

Hansen  and  Singleton  (1982)  estimated  such  a  model  using  real consumption 
expenditures  for  the  aggregate  United  States  divided  by  the  U.S.  population  as 
their  measure  of c,.  For  r,,,  they  used  the  inflation-adjusted  return  that  an  investor 
would  earn  if one  dollar  was  invested  in every  stock  listed  on  the  New  York  Stock 
Exchange,  while  r,,  was  a  value-weighted  inflation-adjusted  return  corresponding 
to  the  return  an  investor  would  earn  if the  investor  owned  the  entire  stock  of each 
company  listed  on  the  exchange.  Hansen  and  Singleton’s  instruments  consisted  of 
a  constant  term,  lagged  consumption  growth  rates,  and  lagged  rates  of return: 

x,  =  (i, C,/C,-15  C,-1/C,-25  ae 

Cr-e+1!Cr—e  Tito  y-1s  ++  + 

> 

Vis-€4+1>  24>  oe  Cr)  Fe jt  es 

When  ¢ lags  are  used,  there  are  3€  +  1 elements  in  x,,  and  thus  r  =  2(3¢€  +  1) 
separate  orthogonality  conditions  are  represented  by  [14.2.37].  Since  a  =  2 pa- 
rameters  are  estimated,  the  x? statistic  in  [14.1.27]  has  6@  degrees  of freedom. 

14.3.  Extensions 

GMM  with  Nonstationary  Data 

The  maintained  assumption  throughout  this  chapter  has been  that  the  (h x  1) 
vector  of observed  variables  w,  is strictly  stationary.  Even  if the  raw  data  appear 
to  be trending  over  time,  sometimes  the  model  can  be  transformed  or  reparame- 
terized  so  that  stationarity  of the  transformed  system  is a  reasonable  assumption. 
For  example,  the  consumption  series  {c,}  used  in  Hansen  and  Singleton’s  study 
(1982)  is increasing  over  time.  However,  it was  possible  to write  the equation  to  be 
estimated  [14.2.36]  in such  a form  that  only the  consumption  growth  rate (c,, ,/c,) 
appeared,  for  which  the  stationarity  assumption  is  much  more  plausible.  Alter- 
natively,  suppose  that some  of the elements  of the observed  vector  w, are  presumed 
to  grow  deterministically  over  time  according  to 

[14.3.1] 
w,=  a+  8-+  we, 
where  a  and 6 are  (h  x  1) vectors  of constants  and  w;  is strictly  stationary  with 
mean  zero.  Suppose  that  the  orthogonality  conditions  can  be  expressed  in terms 
of wt as 

Then  Ogaki  (1993)  recommended  jointly  estimating  @, «  and  8 using 

E{f(0,,  w;*)}  =  0. 

h(6,  w,) = 
Stor 

w,  —  a  —  Of 
i  ee w,  —  a  —  ad 

to construct  the  moment  condition  in [14.2.3]. 

Testing for Structural  Stability 

Suppose  we  want  to  test  the  hypothesis  that  the  (a x  1) parameter  vector  @ 
that characterizes  the  first  7, observations  in the sample  is different  from  the value 

424  Chapter 14  | Generalized  Method  of Moments 

that  characterizes  the  last  T  —  T, observations,  where  7, is  a known  change  point. 
One  approach is  to obtain  an  estimate  6, ,,  based  solely  on  the  first  7, observations, 
minimizing 

O(8,;  a  ey en 

w,) 

-|am 3 y h(6,,  w[s 5, “i  UT Ss h(6,,  “|  [14.3.2] 

where,  for  example,  if {h(6,,  w,)}  is serially  uncorrelated, 

To 

Sim  =  (1/T))  2 (h(6,. Tu  w,)}[h(8,  7.  w,)]’. 

Proposition  14.1  implies  that 

as  T, —  ©,  where  V, can  be  estimated  from 

VT(6;.%,  —  01)  > NO,  V,) 

[14.3.3] 

for 

ae  ir  {D,.7,8 oR  eee Se 

yy 

1.7% 

= 

ah(8,,  w,) 
w,) 
(1/T 
30' 
(  o) seit  was 

To 

6; =  6), Ty 

Similarly,  a separate estimate  6, 7-7  can  be based  on  the  last  T —  Ty observations, 
with  analogous measures  S, ;_;,,  V>7_ chs ori in and 

as  T  —  T,—  ~.  Let  a  =  T,/T 
the  first  subsample.  Then  [14.3.3]  and  [14.3.4]  state  that 

VT  =  Ty (827-10  —  0) 5 N(O,  V2) 

[14.3.4] 
denote  the  fraction  of observations  contained  in 

VT (61.7,  —  91) > NO,  V\/7) 

VT (62.71  —  6) + N(O, V,/(1  —-  7) 
as  T —  ~.  Andrews  and  Fair  (1988)  suggested  using  a  Wald  test  of  the  null 
hypothesis  that  8,  =  8,, exploiting  the fact  that  under  the  stationarity  conditions 
needed  to  justify  Proposition  14.1,  6, is asymptotically  independent  of @,: 

ad T (8; 7  rs Ch  a. 

, 

x  {mo!' Vi  +  (1  —  m)7! V2.7~ 

rh  (81.7  —  92.7-7)- 

Then Figen x7(a)  under  the  null  hypothesis  that  0,  =  0. 

One  can  further  test  for  structural  change  at  a  variety  of different  possible 
dates,  repeating  the  foregoing  test  for  all  7, between,  say,  0.157  and  0.857  and 
choosing  the  largest  value  for  the  resulting  test  statistic  A;.  Andrews  (1993)  de- 
scribed  the  asymptotic  distribution  of such a test. 

Another  simple  test  associates  separate  moment  conditions  with  the obser- 
vations  before  and  after  T, and  uses  the  x? test  suggested  in [14.1.27]  to  test  the 
validity  of the  separate  sets  of conditions.  Specifically,  let 

1 
WS 
If h(@, wi) i is  an  (r X  1) vector  whose  population  mean  is Zero  at  0, define 

«forts  F 
set  >  Ap 

5 sai 
Komi 

|  ere oa =  ln 1  al  ig  +3 
The a elements of @ can  then  be estimated  by using the 27 orthogonality  conditions 
given  by Efh*(p,  w,,  d;,)}  =  O fort  =  1,2,...,  T, by simply  replacing  h(@, w,) 

h(6,  w,) d,, 

14.3.  Extensions 

425 

in  [14.2.3]  with  h*(9,  w,,  d,,)  and  minimizing  [14.2.2]  in  the  usual  way.  Hansen’s 
described  in  (14.1.27]  based  on  the  h*(-)  moment  conditions  could 
er 
, 
?  test  statistic 
. 
a y°(2r  —  a) critical  value  to  provide a test  of the  hypothesis 
then  be  compared  with 
that  0,  =  9). 

4s 

A antes of other  tests  for  structural  change  have  been  proposed  by Andrews 

and  Fair  (1988)  and  Ghysels  and  Hall  (1990a,  b). 

GMM  and  Econometric  Identification 
For  the  portfolio  decision  model  [14.2.34],  it  was  argued  that  any variable 
would  be  valid  to  include  in  the  instrument  vector  x,,  as  long  as  that  variable  was 
known  to  investors  at  date  ¢ and  their  expectations  were  formed  rationally.  Essen- 
tially,  [14.2.34]  represents  an  asset  demand  curve.  In the  light  of the  discussion  of 
simultaneous  equations  bias  in  Section  9.1,  one  might  be  troubled by the  claim 
that  it is possible  to  estimate  a  demand  curve  without  needing  to think  about  the 
way  that  variables  may  affect  the  demand  and  supply  of assets  in  different  ways. 
As  stressed  by Garber  and  King  (1984),  the  portfolio  choice  model  avoids 
simultaneous  equations  bias  because  it  postulates  that  equation  [14.2.32]  holds 
exactly,  with  no  error  term.  The  model  as  written  claims  that  if the  econometrician 
had  the  same  information  x; used  by investors,  then  investors’  behavior  could  be 
predicted  with  an  R? of unity.  If there  were  no  error  term  in the demand  for oranges 
equation  [9.1.1],  or if the  error  in the  demand  for oranges  equation  were  negligible 
compared  with  the  error  term  in the  supply  equation,  then  we  would  not  have  had 
to  worry  about  simultaneous  equations  bias  in  that  example,  either. 

It is hard  to take  seriously  the  suggestion  that  the  observed  data  are  exactly 
described  by [14.2.32]  with  no  error.  There  are  substantial  difficulties  in measuring 
aggregate  consumption,  population,  and  rates  of  return  on  assets.  Even  if these 
aggregates  could  in some  sense  be  measured  perfectly,  it is questionable  whether 
they are  the  appropriate  values  to be using to test  a theory  about  individual  investor 
preferences.  And  even  if we  had  available  a  perfect  measure  of the  consumption 
of an  individual  investor,  the  notion  that  the  investor’s  utility  could  be represented 
by a  function  of this  precise  parametric  form  with  y constant  across  time  is surely 
hard  to  defend. 

Once  we  acknowledge  that  an  error  term  reasonably  ought  to  be  included  in 
[14.2.32],  then  it is no  longer  satisfactory  to  say  that  any  variable  dated  f or  earlier 
is  a  valid  instrument.  The  difficulties  with  estimation  are  compounded  by  the 
nonlinearity  of the equations of interest.  If one  wants  to take  seriously  the possibility 
of  an  error  term  in  [14.2.32]  and  its  correlation  with  other  variables,  the  best 
approach  currently  available  appears  to  be  to  linearize  the  dynamic  rational  ex- 
pectations  model.  Any  variable  uncorrelated  with  both  the  forecast  error  people 
make  and  the  specification  error  in  the  model  could  then  be  used  as a valid  in- 
strument  for  traditional  instrumental  variable  estimation;  see  Sill  (1992)  for  an 
illustration  of this  approach. 

Optimal  Choice  of Instruments 
If one  does  subscribe  to the  view  that  any  variable  dated ¢ or  earlier  is a valid 
instrument  for estimation  of [14.2.32],  this suggests  a virtually infinite  set of possible 
variables  that  could  be  used.  One’s  first  thought  might  be  that,  the  more  orthog- 
onality  conditions  used,  the  better  the  resulting  estimates  might  be.  However, 
Monte  Carlo  simulations  by Tauchen  (1986)  and  Kocherlakota  (1990)  strongly 
suggest  that  one  should  be  quite  parsimonious  in the  selection  of x,.  Nelson  and 

426  Chapter  14  | Generalized  Method  of Moments 

Startz  (1990)  in  particular  stress  that  in  the  linear  simultaneous  equations  model 
y,  =  2,B  +  u,,  a  good  instrument  not  only  must  be  uncorrelated  with  u,,  but  must 
also  be  strongly  correlated  with  z,.  See  Bates  and  White  (1988),  Hall  (1993),  and 
Gallant  and  Tauchen  (1996)  for  further  discussion  on  instrument  selection. 

14.4.  GMM  and  Maximum  Likelihood  Estimation 

In many  cases  the  maximum  likelihood  estimate  of ® can  also  be  viewed  as  a GMM 
estimate.  This  section  explores  this  analogy  and  shows  how  asymptotic  properties 
of maximum  likelihood  estimation  and  quasi-maximum  likelihood  can  be  obtained 
from  the  previous  general  results  about  GMM  estimation. 

The  Score  and  Its  Population  Properties 

Let  y, denote  an  (m  xX  1) vector  of variables  observed  at  date  ¢, and  let  Y, = 
(y;,  yi-1.  --  +»  Yi)’  denote  the  full  set  of data  observed  through  date  ¢.  Suppose 
that  the  conditional  density  of the  ¢th  observation  is given  by 

f(y:|9,—15  8). 

(14.4.1) 

Since  [14.4.1]  is a  density,  it must  integrate  to-unity: 

I. fy  19-1;  @) dy,  =  1, 

[14.4.2] 

where  4 denotes  the  set  of possible  values  that y, could  take  on  and  f dy, denotes 
multiple  integration: 

| 

f 

| nwo  dy,  =  | [-- [Wr  Yo  see  >  Ynt) dy,,  dyz,***  AY ny- 
Since  [14.4.2]  holds  for  all  admissible  values  of 6, we  can  differentiate  both  sides 
with  respect  to  @ to  conclude  that 

af(y,|Y,—1;  9) 
PAIS)  dy, = 

0. 

4.4. 

The  conditions  under  which  the  order  of  differentiation  and  integration  can  be 
reversed  as  assumed  in arriving  at  [14.4.3]  and  the  equations  to  follow  are  known 
as  “regularity  conditions”  and  are  detailed  in Cramér  (1946).  Assuming  that  these 
hold,  we  can  multiply  and divide  the integrand  in [14.4.3] by the conditional  density 
of y,: 

df (y,|Y,-15  9) 

tt 

fly|%,-43  6) fly lM, -13  6) dy,  = 0, 

: 

‘ 

l,  “00 

or 

I, oe SR: 8) F(y:|%,- 1; ) dy,  =  0. 

[14.4.4] 

Let h(0, Y,) denote  the  derivative  of the  log of the conditional  density  of the 

_ 

tth  observation: 

no, y,)  =  2et aa J 
If there  are  a  elements  in  6,  then  [14.4.5]  describes  an  (a  x  1) vector  for  each 
date  ¢ that  is known  as  the score  of the fth observation.  Since  the score  is a function 
of ¥,, it is  a random  variable.  Moreover,  substitution  of [14.4.5] into [14.4.4] reveals 
427 

14.4.  GMM  and Maximum  Likelihood  Estimation 

[14.4.5] 

that 

[. h(6,  Y,) f(y,|%,-;  8) dy,  =  9. 

[14.4.6] 

Equation  [14.4.6]  indicates  that  if the data  were  really generated  by the density 
[14.4.1],  then  the  expected  value  of the  score  conditional  on  information  observed 
through  date  ¢  —  1 should  be  zero: 

E{h(0,  Y,)|¥,_ ,} =  0. 

[14.4.7] 

In other  words,  the  score  vectors  {h(®,  Y,)}7_,  should  form  a martingale  difference 
sequence.  This  observation  prompted  White  (1987)  to  suggest  a  general  specifi- 
cation  test  for  models  estimated  by  maximum  likelihood  based  on  whether  the 
sample  scores  appear  to  be  serially  correlated.  Expression  [14.4.7]  further  implies 
that  the  score  has  unconditional  expectation  of zero,  provided  that  the  uncondi- 
tional  first  moment  exists: 

E{h(0,  Y,)}  =  0. 

[14.4.8] 

Maximum  Likelihood  and  GMM 

Expression  [14.4.8]  can  be  viewed  as a set  of a orthogonality  conditions  that 
could  be  used  to  estimate  the  a  unknown  elements  of  @.  The  GMM  principle 
Suggests  using  as  an  estimate  of @ the  solution  to 

. 

0  =  (1/T)  > h(0,  Y,). 

T 

t=1 

[14.4.9] 

But  this  is also  the  characterization  of the  maximum  likelihood  estimate,  which  is 
based  on  maximization  of 

£(0)  =  2 log f(y,|¥,_1;  8), 

the  first-order  conditions  for  which  are 

Lame: 
x  ee fl  59) 

|Y,_,;  0 

BO)  oy 

[14.4.10] 

assuming  an  interior  maximum.  Recalling [14.4.5],  observe that [14 4310] and  [ 14, 4.9] 
are  identical  conditions—the  MLE is  the  same  as  the  GMMest timator  based  on 
the  orthogonality  conditions in  [14.4.8]. 

The  GMM  formula  [14.2.6]  suggests  that  the  varlagce-Covadlance  matrix  of 

the  MLE  can  be  approximated  by 

oe 

N 

E[(6,;  —  6,)(6,  —  ®)']  =  (/T){D,$7'D4}-', 

[14.4.11] 

where 

DD.  =  oe 
yy 
og(6;  Y,) 

lac) 

00’ 

0=6, 

=  (1/T) > 

ah(@, %,) 
2n(0, 

@=6r 

[14.4.1] 

=  (1/T)  Ss vga ct 39 

sii! 

0=0, 

428  Chapter  14  | Generalized  Method of Moments 

Moreover,  the  observation  in  [14.4. 7| that  the  scores  are  serially  uncorrelated 
Suggests  estimating  S by 

S$; =  (1/7)  2 [h(6, ,)}[h(6,  %,)]’. 

(14.4.13] 

The  Information  Matrix  Equality 
Expression  [14.4.12]  will  be  recognized  as  —1  times  the  second  derivative 
estimate  of  the  information  matrix.  Similarly,  expression  [14.4.13]  is  the  outer- 
product  estimate  of the  information  matrix.  That  these  two  expressions  are  indeed 
estimating  the  same  matrix  if the  model  is  correctly  specified  can  be  seen  from 
calculations  similar  to  those  that  produced  [14.4.6].  Differentiating  both  sides  of 
[14.4.6]  with  respect  to  6’  reveals  that 
h(O,  Y, 
0  =  [.2g.% 2 4,1;  ®) dy, + +  |. ne,  a)  L011: ®) g 
2  | BO.%) 

ah(0, %) dh, Y) ry jy - @) dy, 

|

+  I, h(6,  Y,) 

d log fly  1¥,—1;  i) 
ry 

f(y|Y,_1;  9) dy, 

or 

[, taco,  2 }10(0,  @)1°F0.1% 5 0) dy,  = 

~[,  MO ™D rye,  0) ay, 

This  equation  implies  that  if the  model  is correctly  ii me the  expected  value 
of the  outer  product  of the  vector  of first  derivatives  of the  log likelihood  is equal 
to  the  negative  of the  expected  value  of the  matrix  of second  derivatives: 

d log f(y,|Y,_ 1; 8) || 9 log f(y,|Y,_ 1; @) 

| inci sme | Gg al hee) 

0 log f(y,|Y,-  13 9) 
So  —— 

e{  a0 00’ 

ig 

4.4.14 
—— 

amy. 

Expression  [14. 4.14]  is known  as  the  information  matrix  equality.  Assuming  that 
(1/T) 37, 4,5 9,  a  positive  definite  matrix,  we  can  reasonably  expect  that  for 
many  models,  the  estimate  S$, in  [14.4. 13] converges  in  probability  to  the  infor- 
mation  matrix  ¥  and  the  estimate  D‘ in  [14.4.12]  converges  in  probability  to 
—  $. Thus,  result  [14.4.11]  suggests  that  if the data  are  stationary  and the estimates 
do not  fall on  the  boundaries  of the  allowable  parameter  space,  it will  often  be the 
case  that 

ji 

V1(6;+  —  ®) >  N(0,  $~'), 

[14.4.15] 

where  the  information  matrix  ¥ can  be estimated  consistently  from  either  —  D+ in 
(14.4.12]  or  S; in  [14.4.13]. 

In small  samples,  the  estimates  —  Dj and  §, will  differ,  though  if they  differ 
too greatly  this  suggests  that  the  model  may  be  misspecified.  White  (1982)  devel- 
oped  an  alternative  specification  test based  on  comparing  these  two  magnitudes. 

The  Wald  Test for Maximum  Likelihood  Estimates 
Result [14.4.15]  suggests  a general  approach  to  testing  hypotheses  about  the 
value of a parameter vector  @ that  has  been  estimated  by maximum  likelihood. 

14.4.  GMM and Maximum  Likelihood Estimation  429 

 
Consider  a  null  hypothesis  involving  m  restrictions  on  @ represented  as  g(8) = 
where  g:  R“‘ > R”  is  a  known  differentiable  function.  The  Wald  test  of  this  9: 
pothesis  is given  by 

riataryt'{| 

6 

agar  |]  [e(6,)], 

[14.4.16] 

(1  xm) 

(mn Xa) 

(a  Xa) 

(ax  m) 

(mx  1) 

which  converges  in  distribution  to  a  x*(m)  variable  under  the  null  hypothesis. 
Again,  the  estimate  of  the  information  matrix  $, could  be  based  on  either  —D7 
in  {14.4.12]  or  S; in  [14.4.13]. 

The  Lagrange  Multiplier  Test 

that 

We  have  seen 

the  scores 
{h(®,,  Y,)}%,  often  form  a  martingale  difference  sequence.  Expression  [14.4.14] 
indicates  that  the  conditional  variance-covariance  matrix  of the  ¢th  score  is given 
by ¥,.  Hence,  typically, 

if  the  model  is  correctly  specified, 

T 

a 

T 

. 

ran Sy h(O,  Y,)  | $7]  (1/7)  2 h(@,,  Y,)  | >  x7(a). 

-[14.4.17] 

1=1 

t= 

Expression  [14.4.17]  does  not  hold  when  @, is replaced  by 6,, since,  from  {14 4.9], 
this  would  cause  [14.4.17]  to  be  identically  zero. 

Suppose,  however,  that  the  likelihood  function  is  maximized  subject  to  m 
constraints  on  @, and  let  @, denote  the  restricted  estimate  of @.  Then,  as  in  the 
GMM  test  for  overidentifying  restrictions  [14.1.27],  we  would  expect  that 

run . h(6,, a) 7! jun > h(@,, 2  | > x7(m). 

 [14.4.18] 

The  magnitude  in  [14.4.18]  was  called  the  efficient  score  statistic  by Rao  (1948) 
and  the  Lagrange  multiplier  test  by Aitchison  and  Silvey  (1958).  It  provides  an 
extremely  useful  class  of  diagnostic  tests,  enabling  one  to  estimate  a  restricted 
model  and  test  it against  a more  general  specification  without  having  to  estimate 
the  more  general  model.  Breusch  and  Pagan  (1980),  Engle  (1984),  and  Godfrey 
(1988) illustrated  applications  of the usefulness  of the Lagrange  multiplier principle. 

Quasi-Maximum  Likelihood  Estimation 

Even  if the  data  were  not  generated  by the  density  f(y,|%,_,;  @),  the  or- 
thogonality  conditions  [14.4.8]  might  still  provide  a  useful  description  of the  pa- 
rameter  vector  of interest.  For  example,  suppose  that  we  incorrectly  specified  that 
a scalar  series  y, came  from  a  Gaussian  AR(1)  process: 

log f(y,|Y, 1; 8)  =  —4 log(2m)  —  + log(o*)  —  (y, —  dy,-1)/(2e”), 

with  6 =  (¢, o”)’.  The  score  vector  is then 

h(@, Y,)  =  patios +  (y,-  Autrey : 

(y, vm PY 1-1) Yr -  lo? 

which  has  expectation  zero  whenever 

El(y,  —  ¢y,-1)¥-1]  =  0 

E{(y,  he dy,-1)"]  =  9%, 

[14.4.19] 

[14.4.20} 

430  Chapter  14  | Generalized  Method  of Moments 

The  value  of the  parameter ¢ that  satisfies  [14.4.19]  corresponds  to the  coefficient 
of a  linear  projection  of y, on  y,_,  regardless  of the  time  series  process  followed 
by y,, while  a? in  [14.4.20] is  a general  characterization  of the  mean  squared  error 
of this linear projection.  Hence,  the  moment  conditions in  [14.4.8]  hold  for a broad 
class  of possible  processes,  and  the  estimates  obtained  by maximizing  a  Gaussian 
likelihood  function  (that  is, the  values  satisfying  [14.4.9])  should  give  reasonable 
estimates  of the linear  projection  coefficient  and  its mean  squared  error  for a fairly 
general  class  of possible  data-generating  mechanisms. 

However,  if the  data  were  not  generated  by a  Gaussian  AR(1)  process,  then 
the  information  matrix  equality  no  longer  need  hold.  As  long  as  the  score  vector 
is serially  uncorrelated,  the  variance-covariance  matrix  of the  resulting  estimates 
could  be obtained  from  [14.4.11].  Proceeding  in this fashion—maximizing  the  like- 
lihood  function  in the  usual  way,  but  using  [14.4.11]  rather  than  [14.4.15]  to  cal- 
culate  standard  errors—was  first  proposed  by White  (1982),  who  described  this 
approach  as  quasi-maximum  likelihood  estimation.’ 

APPENDIX 14.A. Foe of Chapter 14 Proposition 

® Proof  of Proposition  14.1. 
RR R'.  By the mean-value  theorem, 

‘ye  ae a) anaate  the ith seca of 2g(9;  Y,),  so that 

ai 

oS 

(67; Yr)  =  g{8;  7)  +  [40+ T ayy (6, = 6).  ns  4A. 1] 

‘  tae  om  en  NR  oe  Se  —  dg,(0;  Y;)  ~ 
Brewch, 
5.200  A. 
Pagad(Ofys  Vheiers 
io.  Mente!  Spevificatnon 

8. 

| 

te 

me 

ee  eee  Rit 

Syieen  Bl  ISORD 

~ 

a 

Mets  Die  toss 

Ls * 

~* 

° 

> 

> 

i  hehe ave a 
ot  4%}  20 booritizad 
i fs Sand Peabe  a ey 

But  equation  [14.1.22]  implies  that  the  left  side  of  [14.A.4]  is zero,  so  that 

“ 
Sk 
CR  re Sets  i{ a0” 

og(9;  Y,) 

| 

o;| 

ded Be  AOC 

«  fO8]  Yo 85+ x fale 

00’ 

@=67 

(14.A.5] 

Now,  @*,  in  [14.A.1]  is  between  0, and  6,,  so  that  0*,—  8, for  each  .  Thus,  condition 
(c)  ensures  that  éach  row  of  D{, converges  in  probability  to  the  corresponding  row  of  D’. 
Then  [14.A.5]  implies  that 

V7(6,  —  0,) > —{DS-'D'}-'  x  {DS-'VT-g(8,;  Y7)}.- 

[14.A.6] 

Define 

so  that  [14.A.6]  becomes 

C  =  —{DS“'D'}-'  x  DS“', 

V7(6,  —  ®,) > CVT-g(0,;  97). 

Recall  from  condition  (b) of the  proposition  that 

VT-9(0,;  Y7) > N(@,  S). 

It follows  as  in  Example  7.5  of Chapter  7 that 

V7(6;  —  )  > N(,  Y), 

[14.4.7] 

where 

V  =  CSC’  =  {DS ''D’}-'DS~-'  x  S  x  S-'D‘{DS  'D’}-'  =  {DS~'D’}  |," 

as  claimed. 

@ 

Chapter  14 Exercise 

14.1. 

Consider  the  Gaussian  linear  regression  model, 

y, 

x/B  +  U,, 

with  u,  ~  1.i.d.  N(0,  o*) and  u, independent  of x,  for all  ¢ and  7.  Define  6 =  (B’,  a*)’.  The 
log of the  likelihood  of (y,, y2,  . 

, Yr) conditional  on  (x,, x»,  . 

,  Xz) is given  by 

. 

. 

. 

. 

£(0)  =  —(T/2)  log(2m)  —  (T/2)  log(o*)  -  > (y,  —  x/B)*/(20°). 

(a) Show  that  the  estimate  D; in [14.4.12]  is given  by 

D;  = 

. 

iSat-F  > a? 
+S (a z 
GF 

r 

where  4, =  (y,  —  x/B,) and  B, and  &3 denote  the  maximum  likelihood  estimates. 

(b) Show  that  the  estimate  $; in [14.4.13]  is given  by 
Be: 
1S  farx 
ot 
ty 
yl  / Ad 
id in| 
hal 
7  DMM, 
ED (a 
d 
1S {2x  4S | ie aya) 
TH  2  Hh 
TA  268)” 

S$,  = 
: 

(c) Show  that  plim(S,)  =  —plim(D,)  =  $, where 

oS  ‘ae  0 
0 

1/26) 

for Q =  plim(1/T)  &/,, x,x!. 

432  Chapter  14  | Generalized  Method  of Moments 

(d)  Consider  a  set  of  m  linear  restrictions  on  B of  the  form  RB  =_r 

for  R  a  known 
(m  x ‘A) matrix  and  r  a  known  (m  x  1)  vector.  Show  that  for  9,  =  —D,,  the  Wald  test 
Statistic  given  in  [14.4.16)  is  identical  to  the  Wald  form  of  the  OLS  y? test  in  [8.2.23]  with 
the  OLS  estimate  of  the  variance  57 in  [8.2.23]  replaced  by  the  MLE  a3. 

(e)  Show  that  when  the  lower  left  and  upper  right  blocks  of  S, are  set  to  their  plim 
of zero,  then the  quasi-maximum  likelihood  Wald  test  of  RB  =  r is  identical  to  the  hetero- 
skedasticity-consistent  form  of  the  OLS  y? test  given  in  [8.2.36] 

Chapter  14 References 

Aitchison,  J.,  and  S.  D.  Silvey.  1958.  “Maximum  Likelihood  Estimation  of  Parameters 
Subject  to  Restraints.""  Annals  of Mathematical  Statistics  29:813-28. 
Amemiya,  Takeshi.  1974.  “The  Nonlinear  Two-Stage  Least-Squares  Estimator.”  Journal  of 
Econometrics  2:105—10. 
Andrews,  Donald  W.  K.  1991.  ‘Heteroskedasticity  and  Autocorrelation  Consistent  Co- 
variance  Matrix  Estimation."  Econometrica  59:817-58. 

.  1993.  “Tests  for  Parameter  Instability  and  Structural  Change  with  Unknown  Change 

Point."  Econometrica  61:821-56. 

and  Ray C.  Fair.  1988.  “Inference  in Nonlinear  Econometric  Models  with  Structural 

Change.”’  Review  of Economic  Studies  55:615-40. 

and  J.  Christopher  Monahan,  1992.  “An  Improved  Heteroskedasticity  and  Auto- 

correlation  Consistent  Covariance  Matrix  Estimator.”  Econometrica  60:953-66. 
Bates,  Charles,  and  Halbert  White.  1988.  “Efficient  Instrumental  Variables  Estimation  of 
Systems  of  Implicit  Heterogeneous  Nonlinear  Dynamic  Equations  with  Nonspherical  Er- 
rors,’  in  William  A-.  Barnett,  Ernst  R.  Berndt,  and  Halbert  White,  eds.,  Dynamic  Econ- 
ometric  Modeling.  Cambridge.  England:  Cambridge  University  Press. 
Breusch,  T.  S.,and  A.  R.  Pagan.  1980.  ‘“‘The  Lagrange  Multiplier  Test  and  Its  Applications 
to  Model  Specification  in  Econometrics."*  Review  of Economic  Studies  47:239-53. 
Cramér,  H.  1946.  Mathematical  Methods  of Statistics.  Princeton,  N.J.:  Princeton  University 
Press. 
Engle,  Robert  F.  1984.  “Wald,  Likelihood  Ratio,  and  Lagrange  Multiplier  Tests  in  Econ- 
ometrics,”’  in  Zvi  Griliches  and  Michael!  D.  Intriligator,  eds.,  Handbook  of Econometrics, 
Vol.  2. Amsterdam:  North-Holland. 
Ferguson,  T.  S.  1958.  “A  Method  of  Generating  Best  Asymptotically  Normal  Estimates 
with  Application  to the  Estimation  of Bacterial  Densities.”  Annals  of Mathematical  Statistics 
29:1046-62. 
Gallant,  A.  Ronald.  1977.  ““Three-Stage  Least-Squares  Estimation  for  a  System  of Simul- 
taneous,  Nonlinear,  [Implicit  Equations.”  Journal  of Econometrics  5:71-88. 

.  1987.  Nonlinear  Statistical  Models.  New  York:  Wiley. 

—  and George Tauchen.  1996. “Which Moments to Match?” Econometric  Theory 12:657-81. 
and Halbert  White.  1988.  A Unified  Theory of Estimation  and Inference for Nonlinear 

Dynamic  Models.  Oxford:  Blackwell. 
Garber,  Peter  M.,  and  Robert  G.  King.  1984.  “Deep  Structural  Excavation?  A Critique  of 
Euler  Equation  Methods.”  University  of Rochester.  Mimeo. 
Ghysels,  Eric,  and  Alastair  Hall.  1990a.  ‘A Test  for Structural  Stability  of Euler  Conditions 
Parameters  Estimated  via  the  Generalized  Method  of  Moments  Estimator.”  /nternational 
Economic  Review  31:355-64. 

and 

.  1990b.  “Are  Consumption-Based  Intertemporal  Capital  Asset  Pricing 

Models  Structural?”’  Journal  of Econometrics  45:121-39. 
Godfrey,  L. G.  1988.  Misspecification  Tests  in Econometrics:  The  Lagrange  Multiplier  Prin- 
ciple and Other  Approaches.  Cambridge,  England:  Cambridge  University  Press. 
Gourieroux,  C.,  A.  Monfort.  and  A.  Trognon.  1984.  “‘Pseudo  Maximum  Likelihood  Meth- 
ods:  Theory.”  Econometrica  52:681-700. 
Hall,  Alastair.  1993.  “Some  Aspects  of Generalized  Method  of Moments  Estimation,”  in 
C.  R.  Rao,  G.  S.  Maddala,  and  H.  D.  Vinod,  eds.,  Handbook  of Statistics,  Vol.  11, 
Econometrics.  Amsterdam:  North-Holland. 

Chapter  14  References 

433 

4A, 

Hansen,  Lars  P.  1982.  “Large  Sample  Properties  of  Generalized  Method  of  Moments  Es- 
timators.””  Econometrica  50:1029-54.~ 

and  Kenneth  J.  Singleton.  1982.  “Generalized  Instrumental  Variables  Estimation  of 
Nonlinear  Rational  Expectations  Models.””  Econometrica  50:1269-86.  Errata:  Econometrica 
52:267-68. 
Jorgenson,  D.  W.,  and  J.  Laffont.  1974.  “Efficient  Estimation  of  Nonlinear  Simultaneous 
Equations  with  Additive  Disturbances."’  Annals  of Economic  and Social  Measurement  3:615- 
40. 
Kocherlakota,  Narayana  R.  1990.  “‘On  Tests  of  Representative  Consumer  Asset  Pricing 
Models.”  Journal  of Monetary  Economics  26:285-304. 
Malinvaud,  E.  1970.  Statistical  Methods  of Econometrics.  Amsterdam:  North-Holland. 
Nelson,  Charles  R.,  and  Richard  Startz.  1990.  “‘Some  Further  Results  on  the  Exact  Small 
Sample  Properties  of  the  Instrumental  Variable  Estimator.’’  Econometrica  58:967-—76. 
Newey,  Whitney  K.  1985.  ‘Generalized  Method  of Moments  Specification  Testing.”  Journal 
of Econometrics  29:229-S6. 

: 

. 

| 

and  Kenneth  D.  West.  1987.  ‘‘A  Simple  Positive  Semi-Definite,  Heteroskedasticity 

and  Autocorrelation  Consistent  Covariance  Matrix.”  Econometrica  55:703-8. 
Ogaki,  Masao.  1993.  ‘‘Generalized  Method  of  Moments:  Econometric  Applications,”  in 
G.  S.  Maddala,  C.  R.  Rao,  and  H.  D.  Vinod,  eds.,  Handbook  of Statistics,  Vol.  11, 
Econometrics.  Amsterdam:  North-Holland. 
Pearson,  Karl.  1894.  ‘“‘Contribution  to the Mathematical  Theory of Evolution.’’  Philosophical 
Transactions  of the  Royal  Society  of London,  Series  A  185:71-110. 
Rao,  C.  R.  1948.  “Large  Sample  Tests  of  Statistical  Hypotheses  Concerning  Several  Pa- 
rameters  with  Application  to  Problems  of Estimation.”  Proceedings  of the  Cambridge  Phil- 
osophical  Society  44:50-—57. 
Rothenberg,  Thomas  J.  1973.  Efficient  Estimation  with  A  Priori  Information.  New  Haven, 
Conn.:  Yale  University  Press. 
Sargent,  Thomas  J.  1987.  Dynamic  Macroeconomic  Theory.  Cambridge,  Mass.:  Harvard 
University  Press. 
Sill,  Keith.  1992.  Money  in  the  Cash-in-Advance  Model:  An  Empirical  Implementation. 
Unpublished  Ph.D.  dissertation,  University  of Virginia. 
Tauchen,  George.  1986.  ‘‘Statistical  Properties  of  Generalized  Method-of-Moments  Esti- 
mators  of Structural  Parameters  Obtained  from  Financial  Market  Data.”  Journal  of Business 
and  Economic  Statistics  4:397—416. 
White,  Halbert.  1980.  ‘A  Heteroskedasticity-Consistent  Covariance  Matrix  Estimator  and 
a  Direct  Test  for  Heteroskedasticity.’’  Econometrica  48:817-38. 

) 

.  1982.  “Maximum  Likelihood  Estimation  of  Misspecified  Models.’’  Econometrica 

50:1-25. 

.  1987.  “Specification  Testing in Dynamic  Models,”  in  Truman  F.  Bewley,  ed., 
Advances  in Econometrics,  Fifth  World  Congress,  Vol.  Il.  Cambridge,  England:  Cambridge 
University  Press. 
: 
Wooldridge,  Jeffrey  M.  1991a.  “On  the  Application  of  Robust,  Regression-Based  Diag- 
nostics  to  Models  of  Conditional  Means  and  Conditional  Variances.”  Journal  of Econo- 
metrics  47:5-46. 

.  1991b.  “Specification  Testing and Quasi-Maximum  Likelihood  Estimation.”  Journal 

of Econometrics  48:29-55. 

434  Chapter  14  | Generalized  Method  of Moments 

15 

Models 
of Nonstationary 
Time  Series 

Up to  this  point  our  analysis  has  typically  been  confined  to  stationary  processes. 
This chapter  introduces  several  approaches  to  modeling  nonstationary  time  series 
and analyzes  the  dynamic  properties  of different  models  of nonstationarity.  Con- 
sequences  of nonstationarity  for statistical  inference  are  investigated  in subsequent 

15.1. Introduction 
Chapters 3 and 4 tecucsed univariate time series  models  that can be e  write in the ae 

| 

| 

55 po pat “pn  ini! 

i 

a  -- 

) yar ary 

‘ 
ae to  op  od A  Bears 
e.  Firs 

eat ting 

OTe, Wa ral wLen  isa 
ose  é  Bile ttn and spe isa 

e aaa eeiton of weit picts tig is 

r  Jepende dent f the pee, the observation: 
ped 

| 

|  -— ts aun msn  it : 

S000 

4000 

3000 

2000 

1000 

0 

47 
FIGURE  15.1 

S! 

SS 

$9 
U.S.  nominal  GNP,  1947-87. 

63 

67 

7\ 

7S 

79 

* 

83 

87 

where  #(1)  #  0.  For  a  unit  root  process,  a  stationary  representation  of  the  form 
of  [15.1.1]  describes  changes  in  the  series.  For  reasons  that  will  become  clear 
shortly,  the  mean  of (1  —  L)y,  is denoted  6 rather  than  pw. 

The  first-difference  operator  (1  —  L) will  come  up  sufficiently  often  that  a 

special  symbol  (the  Greek  letter  A) is reserved  for  it: 

Ay,  =  Me  —  yyy. 

The  prototypical  example  of a  unit  root  process  is obtained  by setting  y(L) 

equal  to 1 in  [15.1.3]: 

: 

yr =  Yi-1  +  O + &,. 
This  process  is known  as  a  random  walk  with  drift  6. 

[15.1.4] 

In the  definition  of the  unit  root  process  in [15.1.3],  it was  assumed  that  (1) 

is nonzero,  where  #/(1)  denotes  the  polynomial 

W(z)  =  1+  Wz!  +  Yozr+--- 
evaluated  at  z  =  1.  To  see  why  such a restriction  must  be  part  of the  definition 
of a  unit  root  process,  suppose  that  the  original  series  y, is in fact  stationary  with 
a representation  of the  form 

If such  a  stationary  series  is differenced,  the  result  is 

y=  mt  x(L)e,. 

(1  —  L)y,  =  (1  —  L)x(L)e,  =  W(L)e,, 
where  (L)  =  (1: —  L)y(L).  This  representation  is in the  form  of [15.1  3]—if the 
original  series y, is stationary,  then  so is Ay,. However,  the moving average  operator 
W(L) that  characterizes  Ay, has  the  property  that  ¥(1)  =  (1 -  1)-y(1)  =  0. When 
we  stipulated  that  #(1)  #  0 in  [15.1.3],  we  were  thus  tuling  out  the  possibility 
that  the  original  series  y, is stationary. 

It is sometimes  convenient  to work  with  a slightly  different  representation  of 

436  Chapter  15  | Models  of Nonstationary  Time  Series 

the  unit  root  process  [15.1.3].  Consider  the  following  specification: 

y,  =  ar  6  +  <u, 

where  u, follows  a zero-mean  ARMA  process: 
P="¢,L  - 
&bL?  +++.  - 
( 
p, 

, 

P 
$,L”)u, 

=  (1+  OL  +  612  +--+  +  OL %e, 

[15.1.5] 

[15.1.6] 

and  where  the  moving  average  operator  (1  +  Ob  +L?  Fo.  +  CLO  e 
invertible.  Suppose  that  the  autoregressive  operator  in  [15.1.6]  is  factored  as  in 
equation  [2.4.3]: 

(PSOE  -—  Gb 

If all of the  eigenvalues  A,, A3,..  . 
be expressed  as 

=p  Ery'=  (1  =  AL)  -  aL)  -  «(1  —  A, L). 
,  A, are  inside  the  unit  circle,  then  [15.1.6]  can 

bt  Abt) Ob? +252: +.0,L4 

u, = 

(id -—A,L)(Qi  -  eh:  hiharer 

Ay mH  el 

=  W(L)e,, 

with  >*,| | <  ©  and  roots  of  #(z) = 0 outside  the  unit  circle.  Thus,  when 
lAi| <  1 for all  i, the  process  [15.1.5]  would  just be  a  special  c case  of the  trend- 
stationary  process  of [15.1.2]. 

Suppose  instead  that  A,  =  1 and|A,|  <  1 fori  =  2,3,..., 1pTtien [15.1.6] 
: 

would state  that 

(i -  L)a  -—  A,L)(1  ai A;L) -  call  TE  A,L)u, 

PIS 

[15.1.7] 

-( + fe POL  ae  ee 

implying that 
ee  er inna  94 OL + parte ¥  to sphyagpib 
racing hen Syne EES 
with 3. ee = and roots of *(z) = 0 outside the unit circle.  Thus, if [15.1.5] 
is first-  liifere nce ‘  the  result is _ a)  ta t}gol  % 
5s si tx Steeda [or — a  - yea -  Lyu, = 0+ 6 +  W(Lye,, 

Pai 
= (Lye. 

san 

rat 

Ax 

7 

ms 

OT  aclplalain  seed  neers iat oat Sgn oved  s aw sate 

15.2.  Why  Linear  Time  Trends  and  Unit  Roots? 
One  might  wonder  why,  for  the  trend-stationary  specification  [15.1.2],  the  trend 
is  specified  to  be  a  linear  function  of  time  (6¢)  rather  than  a  quadratic  function 
(5¢  +  yt?)  or  exponential  (e*’).  Indeed,  the  GNP  series  in  Figure  15.1,  like  many 
economic  and  financial  time  series,  seems  better  characterized  by an  exponential 
trend  than  a  linear  trend.  An  exponential  trend  exhibits  constant  proportional 
growth;  that  is,  if 

[15.2.1] 

yee, 
then  dy/dt  =  5-y,.  Proportional  growth  in the  population  would  arise  if the  number 
of children  born  were  a  constant  fraction  of the  current  population.  Proportional 
growth  in  prices  (or constant  inflation)  would  arise  if the  government  were  trying 
to  collect  a  constant  level  of  real  revenues  from  printing  money.  Such  stories  are 
often  an  appealing  starting  point  for  thinking  about  the  sources  of time  trends,  and 
exponential  growth  is often  confirmed  by the  visual  appearance  of the  series  as  in 
Figure  15.1.  For  this  reason,  many  economists  simply  assume  that  growth  is of the 
exponential  form. 

Notice  that  if we  take  the  natural  log of the  exponential  trend  [15.2.1],  the 

result  is a  linear  trend, 

log(y,)  =  4t. 

Thus,  it is common  to  take  logs  of  the  data  before  attempting  to  describe  them 
with  the  model  in  [15.1.2]. 

Similar  arguments  suggest  taking  natural  logs  before  applying  [15.1.3].  For 
small  changes,  the  first  difference  of the  log of a variable  is approximately  the  same 
as  the  percentage  change  in the  variable: 

(1 —  L) log(y,)  = log(y,/y,-1) 

-  log{1  +  [(y,  at  yr-1)/Y,-1}3 
IR 

(945  —  Yura)  Yea 

where  we  have  used  the  fact  that  for x  close  to zero,  log(1  +  x) =  x.? Thus,  if the 
logs of a variable  are  specified  to follow  a unit  root  process,  the  assumption  is that 
the  rate  of growth  of the  series  is a  stationary  stochastic  process.  The  same  argu- 
ments  used  to  justify  taking  logs before  applying  [15.1.2]  also  suggest  taking  logs 
before  applying  [15.1.3]. 

Often  the  units  are  slightly  more  convenient  if log(y,)  is multiplied  by 100. 
Then  changes  are  measured  directly  in  units  of percentage  change.  For  example, 
if (1 —  L)[100  x  log(y,)]  =  1.0,  then  y, is 1%  higher  than  y,_. 

15.3.  Comparison  of Trend-Stationary 
and  Unit  Root  Processes 
This section compares  a trend-stationary  process  [15.1.2]  with  a unit  root  process 
| 15.1.3] in  terms  of forecasts  of the  series,  variance  of the  forecast  error,  dynamic 
multipliers,  and  transformations  needed  to  achieve  stationarity. 

*See  result  [A.3.36]  in the  Mathematical  Review  (Appendix  A) at  the end  of the  book. 

438  Chapter  15  | Models  of Nonstationary  Time  Series 

Comparison  of Forecasts 
To forecast  a trend-stationary  process  [15.1.2],  the  known  deterministic  com- 
ponent  (a  +  6f) is simply  added  to  the  forecast  of the  stationary  stochastic  com- 
ponent: 

2 

Vixste  =a  +  Gg  +  5)  +  W,€,  ¥  934182)  +  W, + 2€,-2  dle  [15.3.1] 

Here  y,,,;,  denotes  the  linear  projection  of y,,,  on  a  constant  and  y,,  y,_;,...  . 
Note  that  for nonstationary  processes,  we  will  follow  the  convention  that  the  ‘‘con- 
stant”  term  in a  linear  projection,  in this  case  a  +  S(t  +  s), can  be  different  for 
each  date ¢ +  s.  As  the  forecast  horizon  (s) grows  large,  absolute  summability  of 
{w,} implies  that  this  forecast  converges  in mean  square  to  the  time  trend: 

Ely...  — 

—  8  +5)  RO  . as..s->™, 

To  forecast  the  unit  root  process  [15.1.3],  recall  that  the  change  Ay,  is  a 

stationary  process  that  can  be forecast  using  the  standard  formula: 

pede = E((i+s  <  Pace,  Vi-ts  e+  J 

[15.3.2] 

en  ron 

8  tahee + Weert  Ft  Wee 2hinen tote 

The lt ot Sul date 1 tts is cL  So sum othe changes between t 

ft  x 

¥ 

er Saar =  Bye + Byer to 

facise AO}  eQnyg = Om - eave oi + sonar Yona) au 
eo Sse pete  =i seer  ee  it 8 

botor 
(1533) 
+ Aer +e = nb  8 areisihte 
inear  projectio  — ebsites Ves Vr ss bs  IB and aot 
to 
es  +  eee iO mfp  art  S8eRoSI0}  oA!  son 
f i> t 4) VAMISLA ne seed Sa 9151  oth Ia worg  03  bsioegrs 

: 

in 

sae 

be  test? 

F155 

vino 

STA toh; 

Bere 
s+ 

i  rae  Pe 

Lode  ae alah 

: 

pots  enh ie  aaa Barve  As 

a 

Notice  that  ¢,  is the  one-period-ahead  forecast  error: 

It follows  from  [15.3.5]  that  for  6 =  O ands  =  1, 

E,  = 

=  Vyr- 1 

Vrs  =  Ve 

+O,  —  Ine-1) 

[15.3.6] 

or 

Year  =  (+  Oy,  —  Wyr-1- 
Equation  [15.3.7]  takes  the  form  of a simple  first-order  difference  equation, relating 
Y,+1),  to  its  own  lagged  value  and  to  the  input  variable  (1  +  6)y,.  Provided  that 
|@| <  1, expression  [15.3.7]  can  be  written  using  result  [2.2.9]  as 

[15.3.7] 

Viste  =  (1  +  Oy]  +  (— OC  +  4) y%-1] 

+  (-6)[(1  +  A)y,-2]  +  (— OPC  +  A)y,-3]  + 

°° 

[15.3.8] 

=  (1  ss 6) 2, (-0/y,-;, 

Expression  [15.3.7]  is sometimes  described  as adaptive  expectations,  and  its impli- 
cation  [15.3.8]  is referred  to  as  exponential  smoothing;  typical  applications  assume 
that  —1  <  @ <  0.  Letting  y,  denote  income,  Friedman  (1957)  used  exponential 
smoothing  to  construct  one  of  his  measures  of permanent  income.  Muth  (1960) 
noted  that adaptive expectations  or exponential  smoothing corresponds  to a rational 
forecast  of  future  income  only  if y,  follows  an  ARIMA(0,  1,  1) process  and  the 
smoothing  weight  (— 6) is  chosen  to  equal  the  negative  of  the  moving  average 
coefficient  of the  differenced  data  (6). 

For  an  ARIMA(0,  1, q) process,  the  value  of y, and  the  q most  recent  values 
»  Yr+qie  Dut  thereafter  the  series  is 
- 
of e,  influence  the  forecasts  Y,, 1),,  Yi+2\1  - 
expected  to  grow  at  the  rate  6.  For  an  ARIMA(p,  1, q), the  forecast  growth  rate 
asymptotically  approaches  6. 

- 

Thus,  the  parameter 6 in the  unit  root  process  [15.1.3]  plays  a similar  role  to 
that  of 5 in  the  deterministic  time  trend  [15.1.2].  With  either  specification,  the 
forecast  y,,,),  in [15.3.1]  or  [15.3.4]  converges  to  a  linear  function  of the  forecast 
horizon  s with  slope  5; see  Figure  15.2.  The  key difference  is in  the  intercept  of 
the  line.  For  a  trend-stationary  process,  the  forecast  converges  to  a  line  whose 
intercept  is the  same  regardless  of the  value  of y,.  By contrast,  the  intercept  of the 
limiting  forecast  for  a  unit  root  process  is  continually  changing  with  each  new 
observation  on  y. 

Comparison  of Forecast  Errors 

The  trend-stationary  and  unit  root  specifications  are  also  very  different  in 
their  implications  for  the  variance  of the  forecast  error.  For  the  trend-stationary 
process  [15.1.2],  the s-period-ahead  forecast  error  is 

Yios  ~  Vash  =  {a  +  Oe  t's)  +  "Gay 

WiErase1  +  W2645-2  +  °°" 

+  W,_ 1&4;  +  We,  +  Pe in  ee  | 

—  {a +  5(t  +  s) +  Web,  +  Wy  6p  +  Wea 2Epa2a +  >>  +} 

=  E45  +  Wybas-)  +  a aa as  inal 

2 Ws  18141: 

440  Chapter  15  | Models  of Nonstationary  Time  Series 

The  mean  squared  error  (MSE)  of this forecast  is 

Elyiss  —  Devs?  =  {1  +  o2 +  3  +  °° 

+  YP  ho? 

The  MSE increases  with  the  forecasting  horizon  s, though  as  s becomes  large,  the 
added  uncertainty  from  forecasting  farther  into  the  future  becomes  negligible: 

lim  Ely,+;  =  1 eM 

=  {1  +  yi  +  Wi  +  **  ‘}o?. 

Note  that  the  limiting  MSE  is just  the  unconditional  variance  of  the  stationary 
component  (L)e,. 

ma contrast,  for  the  unit  root  process  [15.1.3],  the  s-period-ahead  forecast 

error is” 

DAUD ST  STS a2W  Sa? one ES er s08%  § 
GM  on 2299014  toor  Fay  6  Yo 

T niseet  7)  twen  ait cS  ior a  Se Toa ae ne 

ay  as a  20  nosed dacinbabih. a  26 

Ssnenuast  G | 

& testo §ee93 
,  i oe,  2 hie  Stil  Sthiw  ewor a MOTiS  Ieeo7t01 

geet  FTissci  . 

Ad) 

Tt}  1D  noiseiva bisbnaye 
| choad ’ ea as Yiseail  dworg  Nszii:  tegos.0%  sd}  asi:  O  <  &  WH bn  verdtc 
Beer"  a  kk 
Nom) asd  ylwole  stom  ebsagxs  .»i¢  10?  levistai  sonshtines 
big  ihe 2eso6nG  1002  tian  6 mow?  sich  isd}  sringom 
+b phvag  wa ot  ylinsioiue  s 10} bovieado ti-beew brs we G8 

Gt  fi 

ie 

puke snes ;  —— meee ey  pose  ee eS  *Q-  reg 
baad  sia  es iia  aba  ala: 

*< 

a 

= 

a 

ty 

vite  Yr+sie  +  {Ayi+5  +  AYi+s-1  > 

em 

AY, 41  t yt 

=  {AY  4541  +  AVie 5-140  ee: 

hS  ay  v  y) 

m  {845  1  ViGiss—) 

oe 

Ws-1Er4ah 

+  f6/\g0F  Wrlcased  Gilevtok  Medationd  Ansett  {e,,4} 

ail  Pe  he  {1  +P  Je,4,  27%  (let  Gye  Whey  ear  rot 

+  {1  ty  tbe  tect  +  Wy-sherei, 

with  MSE 

El. +s  va Texel  ={1+  (1+  y,)?  +  (1+  ob,  +  &)  +->> 

+(1+ 

+ 

+--+  w,_,)7o”. 

The  MSE  again  increases  with  the  length  of  the  forecasting  horizon  s,  though  in 
contrast  to  the  trend-stationary  case,  the  MSE  does  not  converge  to any  fixed  value 
as  $  goes  to  infinity.  Instead,  it  asymptotically  approaches  a  linear  function  of s 
with  slope  (1 +  w,  +  #2.  +  °°  -)?a?.  For  example,  for  an  ARJMA(0,  1, 1) process, 

E[Yixs  +,  Bie cick  =  {1  ¥- (s vas  1)(1  +  6)}07. 

[15.3.9] 

To summarize,  for a trend-stationary  process  the  MSE  reaches a finite  bound 
as  the  forecast  horizon  becomes  large,  whereas  for  a  unit  root  process  the  MSE 
eventually  grows linearly  with  the  forecast  horizon.  This  result  is again  illustrated 
in Figure  15.2. 

: 

| 

Note  that  since  the  MSE  grows  linearly  with  the  forecast  horizon  s,  the 
standard  deviation  of the  forecast  error  grows  with  the  square  root  of s.  On  the 
other  hand,  if 5  >  0,  then  the  forecast  itself  grows  linearly  in  s.  Thus,  a  95% 
confidence  interval  for  y,,,  expands  more  slowly  than  the  level  of  the  series, 
meaning  that  data  from  a unit  root  process  with  positive  drift  are  certain  to  exhibit 
an  upward  trend  if observed  for  a  sufficiently  long  period.  In  this  sense  the  trend 
introduced  by a nonzero  drift  6 asymptotically  dominates  the  increasing  variability 
arising  over  time  due  to  the  unit  root  component.  This  result  is very  important  for 
understanding  the  asymptotic  statistical  results  to  be presented  in Chapters  17 and 
18. 

Figure  15.3  plots  realizations  of a  Gaussian  random  walk  without  drift  and 
with  drift.  The  random  walk  without  drift,  shown  in panel  (a), shows  no  tendency 
to  return  to  its  starting  value  or  any  unconditional  mean.  The  random  walk  with 
drift,  shown  in  panel  (b),  shows  no  tendency  to  return  to  a  fixed  deterministic 
trend  line,  though  the series  is asymptotically  dominated  by the  positive  drift  term. 

Comparison  of Dynamic  Multipliers 
Another  difference  between  trend-stationary  and  unit  root  processes  is the 
_  persistence  of innovations.  Consider  the consequences  for y,, , if  , were  to increase 
by one  unit  with  e’s for all other  dates  unaffected.  For the  trend-stationary  process 
[15.1.2],  this  dynamic  multiplier  is given  by 

OVi45  _ 
iden  W. 

For a trend-stationary  process,  then,  the  effect  of any stochastic  disturbance  even- 
tually  wears  off: 

lim  Ores  -  0. 

sex 

€, 

442  Chapter  15 

Models  of Nonstationary  Time  Series 

1 

10 

19 

2B%i2  37 

46 

55 

64 

73 

82 

91 

100 

(a) 

Random walk without drift 

ub > 

> 

< 

lw 

e 

4 

tm 

~ 

se ~  ~~ 

» en 4  ey 4  “ ~ I 

z 

. 

“/ —- - 

+ 

o 

Sins  ond 

itive 

22320 

eworg 
GT 

7*us 

21 
href 

2i  ot  Taupiesy’s st 7 -  soneingy  oi 
Pais 8  gree  dur  | 
BShap  anit Visa  oy és 

| 

a?  Sates  Beet 

MERE)  Yd.bediisesh  & 

70)  SAIS  5  ve  yee 
tng  s  KOREA 
ene li isi?  asvowod  -si0K  2a»  ong “Gainey 
J  tiksciwsieder  os) 1. $24 aR  Wh att  ite%e 

; 

<a > 

; 

.  iy f Brea —— fete Foot A  : 

et  ae  Bence  ;  at Oy  Pits 

Lee 

eins 

ete > _- ue >.  - 

ci  _  -  ay  -s 

—s 

led 

ae  CaaS Io  ate 

As  an  illustration  of  calculating  such  a  multiplier,  the  following  ARIMA(4, 
1, 0) model  was  estimated  for  y,  equal  to  100  times  the  log  of  quarterly  U.S.  real 
GNP  (¢  =  1952:I1  to  1984:1V): 
Ay,  =  0.555  +  0.312  Ay,_,  +  0.122  Ay,,  —  0.116  Ay,_;  —  0.081  Ay,.4  +  €,. 
For  this  specification,  the  permanent  effect  of  a one-unit  change  in  e, on  the  level 
of  real  GNP  is estimated  to  be 

(1)  =  1/o(1)  =  WC  —  0.312  —  0.122  +  0.116  +  0.081)  =  1.31. 

Transformations  to Achieve  Stationarity 

A  final  difference  between  trend-stationary  and  unit  root  processes  that  de- 
serves  comment  is the  transformation  of the data  needed  to  generate  a  stationary 
time  series.  If the  process  is really  trend  stationary  as  in  [15.1.2],  the  appropriate 
treatment  is  to  subtract  dt  from  y,  to  produce  a  stationary  representation  of  the 
form  of  [15.1.1].  By  contrast,  if the  data  were  really  generated  by the  unit  root 
process  [15.1.3],  subtracting  d¢  from  y,  would  succeed  in  removing  the  time-de- 
pendence  of the  mean  but  not  the  variance.  For example,  if the  data  were  generated 
by [15.1.4],  the  random  walk  with  drift,  then 

Vy  pe Ohe =  Yoel  okregst  or  eh. 6)  =  You#  u,- 
The  variance  of  the  residual  u,  is to”;  it grows  with  the  date  of  the  observation. 
Thus,  subtracting  a time  trend  from  a  unit  root  process  is not  sufficient  to  produce 
a  stationary  time  series. 

; 

The  correct  treatment  for  a  unit  root  process  is to  difference  the  series,  and 
for  this  reason  a  process  described  by [15.1.3]  is  sometimes  called  a  difference- 
stationary  process.  Note,  however,  that  if one  were  to  try  to  difference  a  trend- 
stationary  process  [15.1.2],  the  result  would  be 

Ay,  =  6 +  (1  —  L)b(L)e,. 

This  is a stationary  time  series,  but  a unit  rout  has  been  introduced  into  the  moving 
average  representation.  Thus,  the  result  would  be  a  noninvertible  process  subject 
to  the  potential  difficulties  discussed  in Chapters  3 through  5. 

15.4.  The  Meaning  of Tests for  Unit  Roots 
Knowing  whether  nonstationarity  in  the  data  is due  to  a  deterministic  time  trend 
or  a  unit  root  would  seem  to  be  a  very  important  question.  For  example,  mac- 
roeconomists  are  very  interested  in  knowing  whether  economic  recessions  have 
permanent  consequences  for  the  level  of future  GNP,  or  instead  represent  tem- 
porary  downturns  with  the  lost  output  eventually  made  up  during  the  recovery. 
Nelson  and  Plosser  (1982)  argued  that  many  economic  series  are  better  character- 
ized  by unit  roots  than  by deterministic  time  trends.  A number  of economists  have 
tried  to  measure  the  size  of  the  permanent  consequences  by estimating  W(1)  for 
various  time  series  representations  of GNP  growth.4 

Although  it might  be  very  interesting  to  know  whether  a  time  series  has  a 
unit  root,  several  recent  papers  have  argued  that  the  question  is inherently  un- 

‘See,  for example,  Watson  (1986),  Clark  (1987), Campbell  and  Mankiw  (1987a,  b), Cochrane  (1988), 

Gagnon  (1988),  Stock  and  Watson  (1988),  Durlauf  (1989),  and  Hamilton  (1989). 
444  Chapter  15  | Models  of Nonstationary  Time  Series 

answerable  on  the  basis  of a  finite  sample  of observations.‘  The  argument  takes 
the  form  of two  observations. 

The first  observation is  that  for any  unit  root  process  there  exists  a stationary 
Process  that  will  be  impossible  to  distinguish  from  the  unit  root  representation  for 
any given sample  size  T. Such a stationary  process  is found  easily enough  by setting 
One  of the eigenvalues  close  to  but  not  quite  equal  to  unity.  For example,  suppose 
the  sample  consists  of  T  =  10,000  observations  that  were  really  generated  by a 
driftless  random  walk: 

y=  yY-1  +  & 

‘true  model  (unit  root). 

[15.4.1] 

Consider  trying  to  distinguish  this  from  the  following  stationary  process: 
|| <1 

false  model  (stationary). 

y=  OY-1  +  & 
The s-period-ahead  forecast  of [15.4.1]  is 

with MSE 

Vex sir  wall 

“tte 

ach cserddejetibiihalibelas is 

BEMIOg 

és 

E(yra5 

Dus su? =  so’. 

[15.4.2] 

[15.4.3] 

[15.4.4] 

eel 

with MSE 

mes 
Ma  oh  thabin Bie  ns  oe 
os there exists a value  of ¢ sufficiently  close to unity such that the observable 
lications  of the stationary  representation  ({15.4.5]  and (15. 4. 6]) are arbitrarily 
S  creme unit Toot aga 4. es and ae .4.4]) in  a sample of size 

Pnaiedemtee 

aan wnt oo ssa with 
yperty  that 

ae os  i  | pre ee vill be impc toate wh 
FAP ARAN tts erm  sean 

feet Wowie  ae mere sam 

The  forecast  of  [15.4.8]  is obtained  from  [15.3.5]: 

Vresle  =  Yr  +  Oe, 

eiAy.  +  Avg  Tae  Ay,  +  y,}  +  Oe, 
=  {(e,  +  Oe,-,)  +  (&-1  +  Oc,_.) 
=  (1+  O}fe,  Hep  ctleps 

€,}. 

+  +--+  +  (€>  +  Oe,)  +  (€,)}  +  Ge, 

From  [15.3.9],  the  MSE  of the  s-period-ahead  forecast  is 

E(\:2s  —  Jers)?  =  {1  +  (s  —  1)  +  4)?}o”. 
Again,  clearly,  given  any  fixed  sample  size  7,  there  exists  a  value  of  0 sufficiently 
close  to  —1  that  the  unit  root  process  [15.4.8]  will  have  virtually  the  identical 
observable  implications  to  those  of the  stationary  process  [15.4.7]. 

Unit  root  and  stationary  processes  differ  in their  implications  at  infinite  time 
horizons,  but  for  any  given  finite  number  of observations  on  the  time  series,  there 
is  a representative  from  either  class of models  that could  account  for all the observed 
features  of the  data.  We  therefore  need  to  be careful  with  our  choice  of wording — 
testing  whether  a  particular  time  series  “contains  a  unit  root,”  or  testing  whether 
innovations  ‘“‘have  a  permanent  effect  on  the  level  of  the  series,’’  however  inter- 
esting,  is simply  impossible  to  do. 

Another  way  to  express  this  is as  follows.  For  a  unit  root  process  given  by 

[15.1.3],  the  autocovariance-generating  function  of (1 oe L)y,  is 

The  autocovariance-generating  function  evaluated  at  z  =  | is then 

Bar(z)  =  (z)o7*h(z~'). 

Recalling  that  the  population  spectrum  of Ay at  frequency  w  is defined  by 

Bay(1)  =  (W(1))?o’. 

[15.4.9] 

1 
Say(w)  =  on Bay(e™), 

Wee 

expression  [15.4.9]  can  alternatively  be  described  as  27  times  the  spectrum  at 
frequency  zero: 

suy(0)  =  5— (Y()Po?. 

By contrast,  if the  true  process  is the  trend-stationary  specification  [15.1.2], 

the  autocovariance-generating  function  of Ay can  be  calculated  from  [3.6.15]  as 

Bay(z)  =  (1  —  z)W(z)o*y(z> "(1 —  27"), 

which  evaluated  at z  =  1 is zero.  Thus,  if the  true  process  is trend-stationary,  the 
population  spectrum  of Ay at  frequency  zero  is zero,  whereas  if the  true  process 
is characterized  by a  unit  root,  the  population  spectrum  of Ay at  frequency  zero 
is positive. 

The  question  of whether  y, follows  a  unit  root  process  can  thus  equivalently 
be expressed  as  a question  of whether  the  population  spectrum  of Ay at frequency 
zero  is zero.  However,  there  is no  information  in a:sample  of size  T about  cycles 
with  period  greater  than  7, just  as  there  is no  information  in  a  sample  of size  T 
about  the  dynamic  multiplier  for  a  horizon  s  >  T. 

These observations  notwithstanding,  there  are  several  closely related  and very 
interesting  questions  that  are  answerable.  Given  enough  data,  we certainly can  ask 

446  Chapter  15  | Models  of Nonstationary  Time  Series 

whether  innovations  have  a significant  effect  on  the  level  of  the  series  over  a 
specified  finite  horizon.  For  a  fixed  time  horizon  (say,  s  =  3 years),  there  exists 
a  sample  size  (say,  the  half  century  of observations  since  World  War  II) such  that 
we  can  meaningfully  inquire  whether  dy,,,/de,  is  close  to  zero.  We  cannot  tell 
whether  the  data  were  really  generated  by [15.4.1]  or  a  close  relative  of the  form 
of [15.4.2],  but  we  can  measure  whether  innovations  have  much  persistence  over 
a fixed  interval  (as in [15.4.1]  or  [15.4.2])  or  very  little  persistence  over  that  interval 
(as in  [15.4.7]  or  [15.4.8}). 

We  can  also  arrive  at  a  testable  hypothesis  if we  are  willing  to  restrict  further 
the  class  of  processes  considered.  Suppose  the  dynamics  of  a  given  sample  {y,, 
»  Yr} are  to  be  modeled  using  an  autoregression  of fixed,  known  order  p.  For 

. 

example,  suppose  we  are  committed  to  using  an  AR(1)  process: 

Within  this  class  of models,  the  restriction 

y,  =  oy,_,  +  &,. 

Hy:  ¢ =  1 

| 

[15.4.10] 

is certainly  testable.  While  it  is  true  that  there  exist  local  alternatives  (such  as 
¢@ =  0.99999)  against  which a test  would  have  essentially  no  power,  this  is true  of 
most  hypothesis  tests.  There  are  also  alternatives  (such  as  ¢ =  0.3)  that  would 
lead  to  certain  rejection  of Hy given  enough  observations.  The  hypothesis  ‘‘{y,}  is 
an  AR(1)  process  with  a  unit  root”  is potentially  refutable;  the  hypothesis  ‘‘{y,}  is 
a  general  unit  root  process  of the  form  [15.1.3]”  is not. 

There  may  be  good  reasons  to  restrict  ourselves  to  consider  only  low-order 
autoregressive  representations.  Parsimonious  models  often  perform  best,  and  au- 
toregressions  are  much  easier  to  estimate  and  forecast  than  moving  average  proc- 
esses,  particularly  moving  average  processes  with  a  root  near  unity. 

If we  are  indeed  committed  to  describing  the  data  with  a  low-order  auto- 
regression,  knowing  whether  the further  restriction  of a unit root  should  be imposed 
can  clearly  be  important  for  two  reasons.  The  first  involves  a  familiar  trade-off 
between  efficiency  and  consistency.  If a  restriction  (in  this  case,  .a  unit  root)  is 
true,  more  efficient  estimates  result  from  imposing  it.  Estimates  of the  other  coef- 
ficients  and  dynamic  multipliers  will  be more  accurate,  and forecasts  will  be better. 
If the  restriction  is  false,  the  estimates  are  unreliable  no  matter  how  large  the 
sample.  Researchers  differ  in their  advice  on  how  to deal  with  this  trade-off.  One 
practical guide is to estimate  the model  both  with  and without  the unit root  imposed. 
If the  key inferences  are  similar,  so  much  the  better.  If the  inferences  differ,  some 
attempt  at  explaining  the  conflicting  findings  (as  in  Christiano  and  Ljunggqvist, 
1988,  or  Stock  and  Watson,  1989)  may  be  desirable. 

In addition  to  the  familiar  trade-off  between  efficiency  and  consistency,  the 
decision  whether  or  not  to impose  unit  roots  on  an  autoregression  also  raises  issues 
involving  the  asymptotic  distribution  theory  one  uses  to  test  hypotheses  about  the 
process.  This  issue  is explored  in detail  in  later  chapters. 

_ 

15.5.  Other  Approaches  to  Trended  Time  Series 
Although  most  of the  analysis  of nonstationarity  in  this  book  will  be  devoted  to 
unit roots  and time  trends,  this section  briefly discusses two alternative  approaches 
to  modeling  nonstationarity:  fractionally  integrated  processes  and  processes  with 
occasional,  discrete  shifts  in the  time  trend. 

15.5.  Other Approaches  to  Trended  Time  Series 

447 

“7 

; 

Fractional  Integration 

Recall  that  an  integrated  process  of order  d can  be  represented  in  the  form 

(1  —  L)ty,  =  W(L)e,, 

[15.5.1] 

with  27. |,| <  ©.  The  normal  assumption  is that  d  =  1, or  that  the  first  difference 
of  the  series  is stationary.  Occasionally  one  finds  a  series  for  which  d =  2 might 
be  a  better  choice. 

Granger  and  Joyeux  (1980)  and  Hosking  (1981)  suggested  that  noninteger 
values  of d in [15.5.1]  might  also  be  useful.  To  understand  the  meaning  of [15.5.1] 
for  noninteger  d,  consider  the  MA(*)  representation  implied  by [15.5.1].  It  will 
be  shown  shortly  that  the  inverse  of  the  operator  (1  —  L)*@  exists  provided  that 
d <  3. Multiplying  both  sides  of [15.5.1]  by (1  —  L)~¢  results  in 

y,  =  1  —  Lire. 

[15.5.2] 

For  z  a  scalar,  define  the  function 

This  function  has  derivatives  given  by 

f(z)  =  (1  -  27% 

re  J 
az =d  (1 

“ 

—d-1 

z) 

of.  d  4  1)-a-(1  —  2) 
az? 
wd =  (d  +  2)-(d.+  1)-d-(1  —  z)-4-3 

“t =(d  +j-1)-(d  +f  -2)+--  (d+  1d  —  2-4, 

A power  series  expansion  for f(z)  around  z  =  0 is thus  given  by 

br 

hs 

2) 

f(0) 

0  +. 

af 
Fig  ear’  f 

1  ef 
+  —  —* 
2! dz? 

>=( 
<a 

1  af 
pig  EL 
3! az 

2=0 

de  ie ee 

=  1  +  dz  +  (12!)(d  +  1)dz?  +  (1/3!)(d  +  2)(d  +  1)dz>  +--+, 

This  suggests  that  the  operator  (1  —  L)~“  might  be  represented  by the  filter 

(l  -  L)-¢  =  1+  dl  +  (1/2!)(d  +  1)dL? 

+  (1/3!)(d  +  2)(d  +  1)dL?  +  --- 

=  Sab 

y=0 

[15.5.3] 

where  A, =  1 and 

h, =  (Wj!(d  +  j ~  1)(d  +  j ~  20d  +  j —  3)-+-  (a +  1d). 
[15.5.4] 
Appendix  15.A  to  this  chapter  establishes  that  if d <  1, h; can  be  approximated 
for  large j by 

hy =  (j +  Ae! 

[15.5.5] 

448  Chapter  15  | Models  of Nonstationary  Time Series 

Thus,  the  time  series  model 

y=  (1  =  L)-“e,  =  hye,  +  hye, 

+  hotna  +  °°: 

[15.5.6] 

describes  an  MA(*)  representation  in  which  the  impulse-response  coefficient  h, 
behaves for large j like (j +  1)~'.  For comparison,  recall  that  the  impulse-response 
coefficient  associated  with  the  AR(1)  process  y,  =  (1  —  oL)~'e,  is given  by ¢/. 
The impulse-response  coefficients  for  a  stationary  ARMA  process  decay  geomet- 
rically,  in contrast  to  the  slower  decay  implied  by [15.5.5].  Because  of this  slower 
rate  of decay,  Granger  and  Joyeux  proposed  the  fractionally  integrated  process  as 
an  approach  to  modeling  long  memories  in a  time  series. 

In a  finite  sample,  this  long  memory  could  be  approximated  arbitrarily  well 
with  a  suitably  large-order  ARMA  representation.  The  goal  of the  fractional-dif- 
ference  specification  is to  capture  parsimoniously  long-run  multipliers  that  decay 
very  slowly. 

The  sequence  of limiting  moving  average  coefficients  {h,}7-» given  in [15.5.4] 

can  be shown  to  be square-summable  provided  that  d <  4:° 

DSh?<e%  § ford<}. 
je 

Thus,  [15.5.6]  defines  a covariance-stationary  process  provided  that  d <  3. If d > 
4, the  proposal  is to  difference  the  process  before  describing  it by [15.5.2].  For 
example,  if  d =  0.7,  the  process  of [15.5.1]  implies 

(i.=  E)  1  an L)y,  =  (L)e,; 

that  is, Ay, is fractionally  integrated  with  parameter  d  =  —0.3  <  3. 

Conditions  under  which  fractional  integration  could  arise  from  aggregation 
of other  processes  were  described  by Granger  (1980).  Geweke  and  Porter-Hudak 
(1983)  and  Sowell  (1992)  proposed  techniques  for  estimating  d.  Diebold  and  Ru- 
debusch  (1989) analyzed  GNP data and the persistence  of business  cycle fluctuations 
using  this  approach,  while  Lo  (1991)  provided  an  interesting  investigation  of the 
persistence  of movements  in stock  prices. 

Occasional  Breaks  in  Trend 
According  to the  unit  root  specification  [15.1.3],  events  are  occurring  all the 
time  that  permanently  affect  the  course  of y.  Perron  (1989)  and  Rappoport  and 
Reichlin  (1989) have argued that economic  events  that have large permanent  effects 

Reasoning  as in Appendix 3.A  to Chapter 3. 

by ¢] +  Lael  =  dre 

<1+  [" eo  ae 
=  1 +  [12d  -  1)]xo"|M, 
=  1 +  [12d  -  1)):[N*-'  -  4], 

which cunverges  to 1 -  [1/(2d - 1)] as N—* ©,  provided that d <  4. 

| 

15.5.  Other Approaches 

to Trended Time Series  449 

are  relatively  rare.  The  idea  can  be  illustrated  with  the  following  model,  in  which 
y,  is stationary  around  a  trend  with  a  single  break: 

_ 

J%  +  dt  +  «, 

fort < Ty 

[15.5.7] 

yt 

a,  +  dt  +  e&, 

for  f'="T7,. 

The  finding  is  that  such  series  would  appear  to  exhibit  unit  root  nonstationarity 
on  the  basis  of the  tests  to  be  discussed  in  Chapter  17. 
Another  way  of thinking  about  the  process  in  [15.5.7]  is as  follows: 

1  Ee 

ae  ee  ap 

[15.5.8] 

where  & =  (a)  —  a)  whent  =  Ty and  is zero  otherwise.  Suppose  €, is viewed  as 
a  random  variable  with  some  probability  distribution—say, 

a,  —  @ 

$=  lo 

with  probability  p 
with  probability  1 —  p. 

Evidently,  p must  be  quite  small  to  represent  the  idea  that  this  is a  relatively  rare 
event.  Equation  [15.5.8]  could  then  be  rewritten  as 

Ay,  = 

+  1 

[15.5.9] 

where 

w=  pla,  —  a)  +  6 

L  io  & 7  p(a  =  a)  +  €  —  &_}- 

But  7,  is the  sum  of  a  zero-mean  white  noise  process  [é,  —  p(a@,  —  @)]  and an 
independent  MA(1)  process [e, —  €,-,].  Therefore,  an  MA(1)  representation  for 
n,  exists.  From  this  perspective,  [15.5.9]  could  be  viewed  as  an  ARJMA(O,  1,  1) 
process, 

Ay,  =  m+,  +  B,_1, 

with  a  non-Gaussian  distribution  for  the  innovations  v,: 

ety,  =  BO  yeh  Jina...  2d. 

The  optimal  linear  forecasting  rule, 

E( yas] Yn Yeniy  +)  =  MS  +  YY  +  OV, 

puts  a  nonvanishing  weight  on  each  date’s  innovation.  This  weight  does  not  dis- 
appear  as  s  —  ~*,  because  each  period  essentially  provides  a  new  observation  on 
the  variable  é, and  the  realization  of & has  permanent  consequences  for  the  level 
of  the  series.  From  this  perspective,  a  time  series  satisfying  [15.5.7]  could  be 
described  as  a  unit  root  process  with  non-Gaussian  innovations. 

Lam  (1990)  estimated  a  model  closely  related  to  [15.5.7]  where  shifts  in the 
slope  of the  trend  line  were  assumed  to  follow  a  Markov  chain  and  where  U.S. 
real  GNP  was  allowed  to follow a stationary  third-order  autoregression  around  this 
trend.  Results  of his  maximum  likelihood  estimation  are  reported  in Figure  15.4. 
These  findings  are  very  interesting  for the  question  of the  long-run  consequences 
of economic  recessions.  According  to  this  specification,  events  that  permanently 
changed  the  level  of GNP  coincided  with  the  recessions  of 1957,  1973,  and  1980. 

450  Chapter  15  | Models  of Nonstationary  Time  Series 

aasileM rr eal! wet Cong)  sW oo 

,  Ge 
i 

dat, ban 4 = 

baaliot acl “sa 

for  some  & between  zero  and  x.  For  x  >  —1  and  d <  1, equation  [15.A.3]  implies  that 
(d — lp). 
(1 +  x)°'21+ 

Letting  x  =  1// gives 

d-| 

5 

d-\ 

1  >  d-1 =  E +  1]  =  4] 
J 

J 

J 

[15.A.4] 

for  all  j >  0 and  d <  1, with  the  approximation  [15.A.2]  improving  as j —  ©.  Substituting 

[15.A.4]  into  [15.A.1]  implies  that ETT TED BT  UD sao  as 

j-1],lj-2 

2 

j 

i 

Chapter  15 References 

Blough,  Stephen  R,  1992a.  “The  Relationship  between  Power  and  Level  for  Generic  Unit 
Root  Tests  in Finite  Samples."  Journal  of Applied  Econometrics  7:295—308. 

.  1992b.  "Near  Observational  Equivalence  of Unit  Root  and  Stationary  Processes: 

Theory  and  Implications."’  Johns  Hopkins  University.  Mimeo. 
Box, G. E. P., and Gwilym  M.  Jenkins.  1976.  Time Series  Analysis:  Forecasting  and Control, 
rev.  ed.  San  Francisco:  Holden-Day. 
Campbell,  John  Y.,  and  N.  Gregory  Mankiw,  1987a.  Permanent  and  Transitory  Compo- 
_.  hents  in Macroeconomic  Fluctuations.""  American  Economic  Review  Papers and Proceedings 

77:111-17. 
and 

.  1987b.  “Are  Output  Fluctuations  Transitory?"’  Quarterly  Journal  of 

3 

Economics  102:857-80. 
Christiano,  Lawrence  J., and  Martin  Eichenbaum.  1990.  “Unit  Roots  in Real  GNP:  Do We 
Know  and  Do  We  Care?”  in Allan  H.  Meltzer,  ed.,  Unit  Roots,  Investment  Measures,  and 
Other  Essays,  7-61,  Carnegie-Rochester  Conference  Series  on  Public  Policy,  Vol.  32.  Am- 
sterdam:  North-Holland. 

and  Lars  Ljungqvist.  1988.  ‘Money  Does  Granger-Cause  Output  in  the  Bivariate 

Money-Output  Relation.”  Journal  of Monetary  Economics  22:217-35. 
Clark,  Peter  K.  1987.  “The  Cyclical  Component  of  U.S.  Economic  Activity.”  Quarterly 
Journal  of Economics  102:797-814, 
Cochrane,  John  H.  1988.  “How  Big. Is the  Random  Walk  in  GNP?"  Journal  of Political 
Economy  96:893-920, 

.  1991.  “A  Critique  of the  Application  of Unit  Root  Tests.”  Journal  of Economic 

Dynamics  and  Control  15:275-84., 
Diebold,  Francis  X.,  and  Glenn  D.  Rudebusch.  1989.  “Long  Memory  and  Persistence  in 
Aggregate  Output.”  Journal  of Monetary  Economics  24:189-209. 
Durlauf,  Steven  N.  1989.  “Output  Persistence,  Economic  Structure,  and  Choice  of Stabi- 
lization  Policy."  Brookings  Papers  on  Economic  Activity  2:1989,  69-116. 
Friedman,  Milton.  1957.  A Theory of the Consumption  Function.  Princeton,  N.J.:  Princeton 
University  Press. 
Gagnon,  Joseph  E.  1988.  “Short-Run  Models  and  Long-Run  Forecasts:  A  Note  on  the 
Permanence  of Output  Fluctuations."  Quarterly  Journal  of Economics  103:415-24, 
Geweke,  John,  and  Susan  Porter-Hudak,  1983,  “The  Estimation  and  Application  of Long 
Memory i: Series  Models,"  Journal  of Time Series Analysis  4:221-38 
Granger,  C.  W. J, 1980.  “Long  Memory  Relationships  and  the Aggregation  of 
Dynamic 
Models."  Journal of Econometrics  14:227-38. 
———  and Roselyne Joyeux.  1980.  “An  Introduction  to Long-Memory  Time  Series Models 
and  Fractional  Differencing.”  Journal  of Time  Series  Analysis  1:15-29. 
Hamilton,  James  D,  1989.  “A  New  Approach  to the Economic  Anal 
Time  Series  and  the  Business  Cycle."  Toolensieeki 57:357-84, 
452  Chapter 15  | Models  of Nonstationary  Time Series 

of Nonstationa 

oAe 

- 

' 

‘ 

Hosking, J. R.  M.  198).  “Fractional  Differencing.”  Biometrika  68:165-76. 
Lam,  Pok-sang.  1990.  “The  Hamilton  Model  with  a  General  Autoregressive  Component: 
Estimation  and  Comparison  with  Other  Models  of  Economic  Time  Series.”  Journal  of 
Monetary  Economics  26:409-32. 
he Andrew  W.  1991.  “Long-Term  Memory  in Stock  Market  Prices.”  Econometrica  59:1279- 

Muth,  John  F.  1960.  “Optimal  Properties  of Exponentially  Weighted  Forecasts.”  Journal 
of the American  Statistical  Association  55:299-306. 
Nelson,  Charles  R., and  Charles  |. Plosser.  1982.  “Trends  and  Random  Walks  in  Macro- 
ree, Time  Series: Some  Evidence  and  kanpnications. **  Journal  of Monetary  Economics 

Perron,  Pierre. 1989.  “The  Great  Crash,  the Oil Price Shock,  and the Unit  Root  Hypothesis.” 
Econometrica  57:1361-1401. 
Rappoport,  Peter, and Lucrezia  Reichlin.  1989.  “Segmented  Trends and Nonstationary  Time 
Series.”  Economic  Journal  supplement  99:168-77. 

Christopher  A.  1989. “Modeling  Trends.”  Yale  University.  Mimeo. 

well, 

Fallaw. 1992. “Maximum  Likelihood Estimation  of Stationa  Univariate Fraction- 

ntegrated  Time Series Models.’ 

Journal  of Econometrics 

53:165-88. 

. 

James H. 1990. “*Unit Roots in Real GNP:  Do  We Know and  Do We  Care?’ A 
ent.”  in Allan H. Meltzer, ed., Unit Roots, Investment Measures, and Other ree: 
Seg Genii Rechte Capteanen finned Pets BN (Nt ti ARPA  He 
1 
Holland. 
ruct  ¢ Trends in Econom Tine Seri,” Journal 
Journ 
arin nthe Eine on Mone foe Cu wusality.”* 

te  asb. ott  ssquborini 

: 

te Detending Methods with se  ef 

2  lewey 

> 
ro, 

my SHGIGMY2e  oti qoisveb  oi  beeu  sd  ozis  Mews  sass nth 
_  BI  bas  = pemante. hi 2tom  tine  gmbul 
wert aie Feo  , ai f =e  fitv  2nised  reiqads gE. 

in  GUS  BNtiii  Seow 

» Bee’  mas  —, 

oni?  SMeinisrsstob 
ae 
whine v8 ease  ye  cl ot 9 eae  eda 

@ 

—_— 

5) 

ed  ch mens peti. pinged  Soom, 

. 

mis 9d pee wfinsy  I 

et eld 

*. 

16 

Processes 
with  Deterministic 
Time  Trends 

The  coefficients  of  regression  models  involving  unit  roots  or  deterministic  time 
trends  are  typically  estimated  by ordinary  least  squares.  However,  the  asymptotic 
distributions  of the  coefficient  estimates  cannot  be  calculated  in  the  same  way  as 
are  those  for  regression  models  involving  stationary  variables.  Among  other  dif- 
ficulties,  the estimates  of different  parameters  will  in general  have  different  asymp- 
totic  rates  of convergence.  This  chapter  introduces  the  idea  of  different  rates  of 
convergence  and  develops  a general  approach  to obtaining  asymptotic  distributions 
suggested  by Sims,  Stock,  and  Watson  (1990).'  This  chapter  deals  exclusively  with 
processes  involving  deterministic  time  trends  but  no  unit  roots.  One  of the  results 
for  such  processes  will  be  that  the  usual  OLS ¢ and F statistics,  calculated  in  the 
usual  way,  have  the  same  asymptotic  distributions  as  they do for stationary  regres- 
sions.  Although  the limiting  distributions  are  standard,  the techniques  used  to verify 
these  limiting  distributions  are  different  from  those  used  in  Chapter  8.  These 
techniques  will  also  be  used  to  develop  the  asymptotic  distributions  for  processes 
including  unit  roots  in Chapters  17  and  18. 

This  chapter  begins  with  the  simplest  example  of i.i.d.  innovations  around  a 
deterministic  time  trend.  Section  16.1  derives  the  asymptotic  distributions  of the 
coefficient  estimates  for  this  model  and  illustrates  a  rescaling  of variables  that  is 
necessary  to  accommodate  different  asymptotic  rates  of convergence.  Section  16.2 
shows  that despite  the different  asymptotic  rates  of convergence,  the standard  OLS 
t and F statistics  have  the  usual  limiting  distributions  for  this  model.  Section  16.3 
develops  analogous  results  for  a  covariance-stationary  autoregression  around  a 
deterministic  time  trend.  That  section  also  introduces  the  Sims,  Stock,  and  Watson 
technique  of transforming  the  regression  model  into  a  canonical  form  for  which 
the  asymptotic  distribution  is simpler  to  describe. 

16.1.  Asymptotic  Distribution  of OLS  Estimates 
of the  Simple  Time  Trend  Model 

This  section  considers  OLS  estimation  of the  parameters of a  simple  time  trend, 

[16.1.1] 
for €,  a white  noise  process.  If «, ~  N(0,  7”), then  the  model  [16.1.1]  satisfies  the 
classical  regression  assumptions?  and  the standard  OLS ¢ or F statistics  in equations 

AH  Bt  Hb, 

Vo 

'A simpler  version  of this  theme  appeared  in the  analysis of a univariate  process  with  unit  roots by 

Fuller  (1976). 

*See  Assumption  8.1  in Chapter  8. 

454 

[8.1.26]  and  [8.1.32]  would  have  exact  small-sample  ¢ or  F distributions.  On  the 
other  hand,  if e, is non-Gaussian,  then a slightly  different  technique  for finding  the 
asymptotic  distributions  of the  OLS  estimates  of a  and  5 would  have  to  be  used 
from that employed  for stationary  regressions  in Chapter 8. This chapter introduces 
_  this technique,  which  will  prove  useful  not  only  for  studying  time  trends  but  also 
for  analyzing  estimators  for  a  variety  of  nonstationary  processes  in  Chapters  17 
and  18.° 

Recall  the approach  used  to find  asymptotic  distributions  for regressions  with 
stationary  explanatory  variables  in  Chapter  8.  Write  [16.1.1]  in  the  form  of  the 
standard  regression  model, 

where 

y= 

+  €,, 

x,-  =(1  2] 
(1x2) 

| 

7 

[16.1.2] 

[16.1.3] 

B  =  /*] 2 
B54 
(2x1) 

| 

16.1.4] 
i 

Se 

PRBS 

é 

AGE ne OLS eatitnare UF Based ‘on’  sample of size T: 

: 

a  3 

? 

fE1.4.01) — 

rau.  27. 

” 

b=  [57] hi > xx] > x} 

16, 

: 

ae 

t 
ee 

3 

: 

[16.1.5] 

3 Recall from quation  [8.2.3]  that the  deviation  of the  OLS estimate from the true 

~  o he > 

a 

= 

a 

*  . 
Foe  r 

Pe  aageY  2s 

> 
“ 

: 

‘ 

where  S denotes  summation  for  ¢  =  1 through  T.  It is straightforward  to  show  by 

.  induction  that* 

tt 

SY t=  T(T  +  12 

ol | 

y (2  =  T(T  +  1)(2T  +  14. 

am | 

[16.1.9] 

; 

[16.1.10] 

Thus,  the  leading  term  in  27_, ¢ is  77/2;  that  is, 

(1/T?) > t =  (1/T?){(72/2)  +  (7/2)]  =  1/2  +  (2T)—>  12. 

& 

[16.1.1] 

Similarly,  the  leading  term  in 27. , ¢? is  7°/3; 

(1/T*)  me t?  =  (1/T°)[(2T°/6)  +  (377/6)  +  T/6) 

=  1/3  +  1(2T)  +  1(6T?) 
—  1/3. 

(16.1.12] 

For  future  reference,  we  note  here  the  general  pattern—the  leading  term  in 
r,t"  is  T’* "(vy +  1): 

7 

T 

(U/T**")  > ev  Mv  +  1). 

t=] 

To verify  [16.1.13],  note  that 

T 

T 

(/T"*')  >t”  =  (/T)  2 (t/T)’. 

[16.1.13] 

[16.1.14] 

| 

(= 

The  right  side  of [16.1.14]  can  be viewed  as  an  approximation  to  the  area  under 
the curve 

fy  =r 

for  r between  zero  and  unity.  To  see  this,  notice  that  (1/7): (t/T)"  represents 
the area  of a rectangle  with  width  (1/7)  and  height  r" evaluated  at r  =  ¢/T (see 
Figure  16.1).  Thus,  [16.1.14]  is the sum  of the area  of these  rectangles  evaluated 

‘Clearly,'[16.1.9]  and (16.1.10]  hold  for 7 =  1. Given  that  [16.1.9]  holds  for 7, 

Vel 

7 

2s  hit(r t+ I) =  T(T + 12  +  (T + 1) =  (T + I(T?) + 1) =  (T + I(T + 292. 

establishing  that  [16.1.9]  holds  for  7 +  1. Similarly,  given  that  [16.1.10}  holds  for  7, 

Ve 

‘ 

& PETE s NUT gave eT +  Os ges  os  weeps 

=  (T +  1){(7(2T7  +  1/6)  + (T +  1) 
=  (7 +  1)(2T?  +  77  +  6) 
=  (7 +  I(T  +  2)(2(T  +  1) +  1), 

establishing  that  [16.1.10]  holds  for  7 +  1, 

456  Chapter 16 | Processes with Deterministic 
Time Trends 

Te Cae sae 
i;  Al, 

re  yaaa 
ie 

FIGURE  16.1 

Demonstration  that  (1/7)  2/7. , (t/T)" > fir" dr  =  iv  +  1). 

atr  =  1/7,  2/T,...,1.  As  T—  &,  this  sum  converges  to  the  area  under  the 
curve  f(r): 

(1/T)  3 (Ty =  [ r’ dr  =  r°*"“W(v  +  1) tg  =  Wy  +  1). 

[16.1.15] 

For x, given  in [16.1.3],  results  [16.1.9]  and  (16.1.10]  imply  that 

Ba  fBEct} 
2 aH;  \ 

| 

T 

T(T + 1)/2 
Latte  Pf  i  aie,  ‘sti 

In contrast  to the usual  result  for stationary  regressions, for the matrix  in (16.1.16], 
(1/7) Z7_,x,x;  diverges.  To  obtain  a convergent  matrix,  [16.1.16]  would  have  to 
be divided  by 7° rather  than  T: 

Unfortunately,  this  limiting  matrix  cannot  be inverted,  as  (1/7) 27_,x,x;  can  be 
in the usual case.  Hence,  a different  approach  from  that in the stationary  case  will 
| 
be needed  to calculate  the  asymptotic  distribution  of b;. 
It turns  out  that  the  OLS estimates  4, and 8, have different  asymptotic  rates 
of convergence.  To arrive  at nondegenerate  limiting  distributions,  4; is multiplied 

by. 771 We can. think of this adjustment 

as premultiplying  [16.1.6] or  [16.1.8]  by the  matrix 

Y;#*  a4 Jat 

(16.1.17]} 

16.1.  OLS Estimates of the Simple  Time  Trend Model  457 

resulting  in 

box ’  md :  wap» xxi] [3 «| 

T*?  (87  —  5) 

II  me 

“= 

~J 

T 
» «| vi¥F| 3 «| 
i 

T 

(16.1.18] 

Consider  the  first  term  in  the  last  expression  of  [16.1.18].  Substituting  from 

[16.1.17]  and  [16.1.16], 

d 

{vr| 3 xx;  |v7'| =)1 

rt  1 30 

4  Sie 
r-s4lsr  sejl 

o 

0 
7T-# 

0 

FP Uytiek ese 
A  TBE  ADA  | 

Thus,  it follows  from  [16.1.11]  and  [16.1.12]  that 

where 

ii 

{ve| 3 xxi  fye'| —  Q, 

r=1 

. 

e =  é 4 

[16.1.19] 

[16.1.20] 

Turning  next  to  the  second  term  in  [16.1.18}], 

lx 

Le,] 
ha  3 x R  | 0  posite |  Fee  ee  igs 

j-th be ~0 

(V/VT)ze, 

[ 

Under  standard  assumptions  about  e¢,,  this  vector  will  be asymptotically  Gaussian. 
For example,  suppose  that  ¢, is i.i.d.  with  mean  zero,  variance  a, and  finite  fourth 
moment.  Then  the  first  element  of the  vector  in [16.1.21]  satisfies 

(1/VT) ; e, > N(0,  0), 

by the  central  limit  theorem. 

For  the  second  element  of the  vector  in [16.1.21],  observe  that  {(¢/T)e,} is  a 
martingale  difference  sequence  that  satisfies  the  conditions  of  Proposition  7.8. 
Specifically,  its variance  is 

=  E[(t/T)e,?  =  o7-(t?/T?), 

where 

Z. 

: Si 

(/T)  So?  =  o(WV/T?)  St? 

> 0/3. 

t=) 

t=] 

458  Chapter  16  | Processes  with  Deterministic  Time  Trends 

Furthermore,  (1/7) 7. [W/T)eP  & o*/3.  To verify  this  last  claim,  notice  that 

E(w) 3 p> [(t/T)e,?  -  air)  > v3) 

T 

2 

=  (cur) > ((/T)e?  —  (1/7)  s (ro?) 

; 

(16.1.22] 

=  E(w) S (/T)P(e2  -  o*)) 

T 

r=1 

T 

=  (/T)?  > (t/T)*E(e?  -  o)?. 

1=1 

But  from  [16.1.13],  T times  the  magnitude  in [16.1.22]  converges  to 

J 

| 

(1/T)  >> (W/T)*E(E?  —  0)? > (1/5):  E(e?  —  o?)?, 

Herein that Ehi6, Arith itself converges to zero: 

: 

“wn  > (TF - _ (WT) P Mis 

|  ‘Bat this implies that 

,  RP Satss6  +  $j ye 

: 

| 

; 

| 

ai r 

2 

bes  : 7 nen  |  - 

2 

FORE  ~"Y).0  99  0) 
as 

* 

bi  (UT) > [WTP > o 1, 

ot 

_ 

CHAM 

ay 4 

claimec .  Hence, —_ Mia es 8, avn “A ale 2 aattifies the central 

| 

' 

Be 

| 

AG  ah ha 

hey 0,  03). pam “i  + = “a ; 

Seg’ ak pea SqmnExs WA Eh 
e  joint  distribution  of the two elements in the x sh 
combination  of these elements  takes  the 

linear 

Any 

ek  es  Gabee3) 
lub wc) ond 
1  .T\» 

aydad) 

tribution: 

16.1.24 
ees: 
From  (16.1.19]  and  (16.1.24],  the  asymptotic  distribution  of [16.1.18]  can  be 

2 
(UVT)Ze,  | 4 
enamels  NOT 

calculated  as  in  Example  7.5  of Chapter  7: 

4 
Sade  N(O,  [(Q-':07Q:Q 
Naies: = 

rik 

‘Q-'/) 

})  = NO,  o*Q  »  | 

70°"). 

= 

[16.1.25 

J 

These  results  can  be  summarized  as  follows. 

Proposition  16.1: 
trend  [16.1.1]  where  ¢, is i.i.d.  with  E(e?)  =  o? and  E(e7)  <  ©.  Then 

Let  y,  be  generated  according  to  the  simple  deterministic  time 

VT(ar  -  a)| 

Los 

[|  0 

i  : 
« 
a  i  "ye  Oe 

-1 

‘ 

16.1.26 

Note  that  the  resulting  estimate  of the  coefficient  on  the  time  trend  (8,) is 
superconsistent—not  only  does  §,—>  5, but  even  when  multiplied  by  7,  we  still 
have 

(8,  —  6) > 0; 

(16.1.27] 

see  Exercise  16.2. 

Different  rates  of convergence  are  sometimes  described  in terms  of order  in 
probability.  A sequence  of random  variables  {X7}7..,  is said  to  be  O,(7 ~ ') if for 
every  ¢  >  0, there  exists  an  M  >  0 such  that 

P{|X7|  >  MIVT}  <  € 
[16.1.28] 
for all  T; in other  words,  the  random  variable  VT: X; is almost  certain  to  fall  within 
+M  for  any  ‘T.  Most  of  the  estimators  encountered  for  stationary  time  series  are 
O,(T~ '?). For example,  suppose  that  X  represents  the  mean  of a sample  of size  7, 

X; =  (1/T)  > Yn 

T 

where  {y,}  is i.i.d.  with  mean  zero  and  variance  a2.  Then  the  variance  of X; is 
o?/T.  But  Chebyshev’s  inequality  implies  that 

. 
P{|X,|  >  M/V/T}  s  Mar  =  (a/M) 

o7/T 

for any  M.  By choosing  M so  that  (o/M)?  <  «,  condition  [16.1.28]  is guaranteed. 
Since  the  standard  deviation  of the  estimator  is o/V7,  by choosing  M  to  be  a 
suitable  multiple  of a,  the  band  X;  +  M/VT can  include  as  much  of the  density 
as  desired. 

As  another  example,  the  estimator  @, in  [16.1.26]  would  also  be  said  to  be 
O,(T~'2).  Since  VT times  (& —  a) is asymptotically  Gaussian,  there  exists  a band 
+MIVT around  4; that  contains  as  much  of the  probability  distribution  as  desired. 

In general,  a sequence  of random  variables  {X7}7..,  is said  to be  O,(T ~*) if 

for every  e  >  0 there  exists  an  M >  0 such  that 

P(|X,|  > MUT*)}  < €. 
(16.1.29] 
Thus,  for example,  the estimator  5; in [16.1.26] is O,(7-*2),  since  there exists a band 
is M _— T**(8,,  —  8) that  contains  as  much  of the  probability  distribution  as 

ired, 

460  Chapter  16  | Processes  with  Deterministic  Time  Trends  . 

16.2.  Hypothesis  Testing for the  Simple  Time 
Trend  Model 

If the  innovations  e, for the  simple  time  trend  [16.1.1]  are  Gaussian,  then  the  OLS 
estimates  &, and  5, are  Gaussian  and  the  usual  OLS  ¢ and  F tests  have  exact  small- 
sample  ¢ and  F distributions  for  all  sample  sizes  7.  Thus,  despite  the  fact  that  a, 
and  8, have  different  asymptotic  rates  of convergence,  the  standard  errors  4, and 
6%, evidently  have  offsetting  asymptotic  behavior  so  that  the  statistics  such  as 
(6, —  8,)/6% ,  are  asymptotically  N(0,  1) when  the  innovations  are  Gaussian.  We 
might  thus  conjecture  that  the  usual  ¢ and F tests  are  asymptotically  valid  for  non- 
Gaussian  innovations  as  well.  This  conjecture  is indeed  correct,  as  we  now  verify. 
First  consider  the  OLS ¢ test  of  the  null  hypothesis  a  =  a,,  which  can  be 

written  as 

‘ 
5 

fan  cee  [i]}” 

16.2.1] 

Here  s?- denotes  the  OLS  estimate  of o?: 

si =  [1(T  —  2)]  d (y, -  @r —  870); 

zr 

[16.2.2] 

t=1 
and  (X;X,)  =  =/_, x,x;  denotes  the  matrix  in equation  [16.1.16].  The  numerator 
and  denominator  of [16.2.1]  can  further  be  multiplied  by 7,  resulting  in 

VI(4 rT  ~  %) 

oar  | FY BAAD 2173 FS 
{suivt o1xsx|  "|| 

0 

Note  further  from  [16.1.17]  that 

[VT  0] =[1  O]Y;z. 

Substituting  [16.2.4]  into  [16.2.3], 

VT (ar  —  %) 

[16.2.3] 

[16.2.4] 

| 

[16.2.5] 

es 

{e911 o1vnexixn-%{9]| 

1 

2 

But  recall  from  [16.1.19]  that 

[16.2.6] 
Y7(XpX7)-'V7  =  [VF'(KrX7)Yz']"'  >  Qu’. 
It is straightforward  to  show  that  s3.—>  o*.  Recall  further  that  V7(4@7  —  a) + 
N(0,  o7q!')  for q'! the  (1,  1) element  of Q~'.  Hence,  from  [16.2.5], 
VT(Gr  -  %) 

VT(ar  —  %) 
,  ———  =  ——_———. 
oVq" 
mW 

16.2.7 
[ 

sad 

] 

o{1  OjQ-'  0 

But  this  is an  asymptotically  Gaussian  variable  divided  by the  square  root  of its 
variance,  and  so  asymptotically  it has a N(0,  1) distribution.  Thus,  the  usual  OLS 
t test of  a  =  a will give an  asymptotically  valid  inference. 
Similarly,  consider  the  usual  OLS  ¢ test  of 6 =  8): 

l; = 

{ s 4   n e x e x e y [ ] }  

16.2.  Hypothesis 

Testing for the Simple Time  Trend Model  461 

Multiplying  numerator  and  denominator  by  T*”, 

= 

T7715  ae  5y) 
Se  ets  Ee 

{s#10 P2|0%;%,)"| 4] 

T*?(5,  —  5y) 

9])'2 
{5410 LY 7(X7X  7)- vo} 

cc  T*2(5,  a  do) 
; 

ov qu 

which  again  is asymptotically  a N(0,  1) variable.  Thus,  although  @, and  5, converge 
at  different  rates,  the  corresponding  standard  errors  6, and  63, also  incorporate 
different  orders  of  T, with  the  result  that  the  usual  OLS  ¢ tests  are  asymptotically 
valid. 

It is interesting  also  to  consider  a  test  of a  single  hypothesis  involving  both 

a  and  6, 

Hy:  r,\@  i  r,6  =  7, 

where  r,,  7,,  and  r are  parameters  that  describe  the  hypothesis.  A ¢ test  of Hy can 
be obtained  from  the  square  root  of the  OLS F test  (expression  [8.1.32]):° 

Be 
t; = 

(rar  +  57  —  1) 

: 

=) 

{stn r2\(X7X7)~' | 

In  this  case  we  multiply  numerator  and  denominator  by \/T,  the  slower  rate  of 
convergence  among  the  two  estimators  &, and  6;: 

3 

= 

Vina  +  rb —  17) 
SEVTIr  ra\(KrX,)- {" vr} % 
VT (rar  +  rd7 —  1) 
{stVTIn IVE  otex)-VVG'f  |vTh 

12 

_  VT (ar  +  rb,  —  1) 

where 

{sor 7[¥7(X7X7)~'Y7]r7}!?’ 

veveft}ee[S]-[ep 

veil 

——  La 

mio 

was 

Similarly,  recall  from  [16.1.27]  that  8, is superconsistent,  implying  that 

VI (nar  +  157 —  1) >  VT (raz  +  8  =  1), 

[16.2.9] 

“With  a  single  linear  restriction  as  here,  m  =  | and  expression  [8.1.32]  describes  an  F(1,  T —  k) 
variable  when  the  innovations  are  Gaussian.  But  an  F(1,  T —  k) variable  is the square  of a1(T  —  k) 
variable.  The  test  is described  here  in terms  of  a  ¢ test  rather  than  an  F test  in order  to  facilitate 
comparison  with  the  earlier  results  in this  section. 

462  Chapter 16  | Processes  with  Deterministic  Time Trends 

where 6 is the  true  population  value  for the  time  trend  parameter.  Again  applying 
[16.2.6],  it follows  that 

»  VT(riar  +  r,5  —  r) 

VT(rja@r  +  128  -  r) 

{or ajQ-| ")} 

(16.2. 10] 

But  notice  that 

VT (raz  +  7,6  —  r)  =  VT[r\(a;  —  a)  +  ra  +  7,6  —  7] 

=  VT[r(4ar  —-  a)] 

: 

under  the  null  hypothesis.  Hence,  under  the  null, 

2  MT lri(ér  =  @))  _  VT(Gr  =  2) 

T 

{r?02q!!}!2 

a 

{o7q"}'?  ’ 

_  which  asymptotically  has a  N(0,  1) distribution.  Thus,  again,  the usual  OLS 1 test 
. 

of H, is valid asymptotically. 

ee 

an 

This last example  illustrates the following general principle:  A test involving 
a single restriction 
across parameters  with different  rates  of convergence  is dom- 
nay asymptotically  by the  parameters  with the slowest  rates of convergence. 

a 

i 

s that  a test involving  both a and 6 that employs  the estimated value  of 
8 would have  the same  asymptotic  properties  under  the null as a test that employs  — 
the true value of  65. 
jee  Finally, consider  a joint  test  of separate  hypotheses  about  a and  6, 

ae  gee  ee 

| 

Bie 

ON 

*& 

+ 

4 

ee 

$s 

= 

ae  oe? oe Gs 

or, in vector form, 

7 

‘ 

a 

. 

7 

J 

ie 
2 

‘ 

s 

testes.  Me  pon’  *  | 

¥e 

. 

bee 

th 

PuIsIGS  oft © {UCTl)  qneiaW  bag  ood  .2mi2  oj 

. 

: 

}-F1-4(  bh.  — 

a 

ar 

. 

. 

2 

i 

Me, 

gh 

I 

lal  rs,  a ae  a 

Sa 

” 

7 

7 
is 

z 

-  or 

ES 

ero  eeearee 
PRCT 

ie  epee, tw 

> 

~>- 

“+ 

[t is assumed  throughout  this  section  that  e,  is i.i.d.  with  mean  zero,  variance  o°, 
and  finite  fourth  moment,  and  that  roots  of 
1  —  bz — hz?  78 

—  Gz” = 0 

lie outside  the  unit  circle.  Consider  a  sample  of  T +  p observations  ony,  {y_» +1, 
Soh and  let ay,  by. irs  6  os  ¢,.1 denote  coefficient  estimates  based 
Yparr  eee 
on  ordinary  least  squares  estimation  of [16.3.1]  for¢  =  1,2,....  7. 

A  Useful  Transformation  of the  Regressors 
Define 

: 
te 
; 

) 
NE  Ne  ds FPS 
a 

_  (br  +  2b2 + 222  +  PH) 

Multiplying  these  equations  by (1  -  ¢,  —  $2  — 
resulting  expressions  for  a  and  6 into  [16.3.1]  produces 

°°  *  —  %) and  substituting  the 

y  =  (1  -  ,  —  2  -  °°: 

8 
+  (1  =  b)  —  br  — 
+--+:  +  OY,»  +  & 

—  b)a®  +  (gb  +  2b.  +  °°  +  Ph, )e 
—  GB  +  HY)  +  O22 

[16.3.2] 

or 

where 

and 

y  =a  +  8  +  hfy?,  t  O2yr-2  + 

°°  +  OPyi_p  +  &  [16.3.3] 

$;  =  $; 

forj=1,2,....p 

yup PY  ~*~  8° 

HS 

forj  =  1.2,....p. 

[16.3.4] 

The  idea  of transforming  the  regression  into  a  form  such  as  [16.3.3]  is due 
to  Sims,  Stock,  and  Watson  (1990).”  The  objective  is to  rewrite  the  texzressors  of 
[16.3.1]  in terms  of zero-mean  covariance-stationary  random  variables  (the  terms 
y;.; forj  =  1,2,....p),  a  constant  term,  and  a  time  trend,  Transtorming  the 
regressors  in  this  way  isolates  components  of  the  OLS  coefficient  vector  with 
different  rates  of convergence  and  provides  a  general  technique  for  finding  the 
asymptotic  distribution  of regressions  involving  nonstationary  variables,  A general 
result  is that,  if such  a transformed  equation  were  estimated  by OLS,  the coefficients 
On  zero-mean  covariance-stationary  random  variables  (in  this  case.  OF 5.  O29. 
...  «@j.7)  would  converge  at rate  VT to a Gaussian  distribution.  The coefficients 
—&%  and  67 from  OLS  estimation  of  [16.3.3]  turn  out  to  behave  asymptotically 
exactly  like  @,  and  8, for  the  simple  time  trend  model  analyzed  in  Section  16.1 
and  are  asymptotically  independent  of the  °'s. 

It is helpful  to describe  this transformation  in more  general  notation  that  will 
also  apply  to  more  complicated  models  in the  chapters  that  follow,  The  original 
regression  model  [16.3.1]  can  be written= 

y=  XB  +  &, 

[16.3.5] 

“A simpler  version  of this  theme  appeared  in the analysis  of a univariate  process  with  unit  roots  by 

Fuller  (1976). 

464  Chapter  16  | Processes  with Deterministic  Time  Trends 

Yi-2 
Bet 
Vi-p 
l 
t 

0 
] 

: 
0 

where 

} 

bm  anata 

ext 

(p+2)xit 

WED  - 

d, 

x 
[6 ; 6] 

$, 
p> 

) 

The  algebraic  transformation  in  arriving  at  [16.3.3]  could  then  be  described  as 
rewriting [16.3.5]  in the  form 

y,  =  x/G[G']-'B  +  «,  =  (x7*]'B*  +  e,. 

[16.3.7] 

where 
. 

“ 
fe 
(p+2)x(p  12) 

= 

| 

(pw 

| 
Panay. 
meiG’}-' 

3: 

<= 
#2y xp 42) 

fei:é.a1 

1 
0 
? 
: 
0 

ebiar 
0 
: 
— 
0 

tee 
oe  3 
ia 
v° 
--- 
ee 

0) 
0 
; 
A 
l 
~-a*  +  pé* 
—  §*  | 

0  0 
0  0 
a0 
3 
: 
0 
0 
1 

[16.3.8] 

—a*  +  &*  -a*  +  25* 

—  §* 

—  §* 

9 

iy 

0 
0 

0 0  ated  at 
00 
fae 

- 

tet 

a  -  &  a* <8) 

| 
a: -—ps* 

0 
I  0 

Ot 

ehere 

AS 

St  Oge  OM.  (8  erid). 

Be  vargb 
Tv 

wah 

Siowesi 

, 

ee  2  i  en*) 

ow 

rate  oF 
[16.3.9] 

hee 

$55... 

where  b denotes  the  estimated  coefficient  vector  from  an  OLS  regression  of y, on 
x.  Thus,  the  coefficient  estimate  for  the  transformed  regression  (b*)  is  a  simple 
linear  transformation  of  the  coefficient  estimate  for  the  original  system  (b).  The 
fitted  value  for  date  ¢ associated  with  the  transformed  regression  is 

[x]’b*  =  [Gx,]'[G’]-'b  =  x;b. 

Thus,  the  fitted  values  for  the  transformed  regression  are  numerically  identical  to 
the  fitted  values  from  the  original  regression. 

Of course,  given  data  only  on  { y,},  we  could  not  actually  estimate  the  trans- 
formed  regression  by OLS,  because  construction  of x* from  x, requires  knowledge 
of the  true  values  of the  parameters  a@  and  6.  It is nevertheless  helpful  to  summarize 
the properties  of hypothetical  OLS  estimation  of [16.3.7],  because  [16.3.7]  is easier 
to analyze  than  [16.3.5].  Moreover,  once  we  find  the  asymptotic  distribution  of b*, 
the  asymptotic  distribution  of b can  be  inferred  by inverting  [16.3.11]: 

b  =  G'b*. 

(16.3. 12] 

The  Asymptotic  Distribution  of OLS  Estimates 
for the  Transformed  Regression 

Appendix  16.A  to  this  chapter  demonstrates  that 

Y;(b}  -  B*) > NO,  o[Q*]-'), 

[16.3.13] 

where 

Y 

as 

. 

° 

e 

2  ee 

beeen 

* 

- 

nt  pane 

CFD” 

VF. 
0  TF  O  060 

=-Fip 

0 

0 

0 

a  Ly 
0 
O 
0 

0 

0 

: 

VT  0 

0 

0 

0 

0 

: 

: 

0 
Sobol 
a 

Yo 

vr 
; 

yi 

Yo 
: 

Y2 

Yr 
: 

eee 

Vous  spe  Yp-3  ane 

0 
0 

0 
0 

0 
0 

yp-1  0  0 
Yp-2  0  0 

: 

:  : 

yi  | 
oo 
OT 

0  0 
7  S 
Spory 

[16.3.14] 

[16.3.15] 

for y' =  E(y*y?_;).  In other  words,  the  OLS  estimate  b* is asymptotically  Gauss- 
ian, with  the coefficient  on  the time  trend  (5*) converging  at rate  7*? and all other 
coefficients  converging  at  rate  VT.  The  earlier  result  [16.1.26]  is a special  case  of 
[16.3.13]  with p  =  0. 

The Asymptotic  Distribution  of OLS Estimates 
for the  Original  Regression 

What does this  result  imply  about  the  asymptotic  distribution  of b, the  esti- 
mated  coefficient  vector  for the OLS  regression  that is actually  estimated?  Writing 
466  Chapter 16  | Processes  with  Deterministic  Time  Trends 

out  (16.3.12]  explicitly  using  [16.3.8],  we  have 

é, 
d, 

b, 

a 
) 

| 
0 

0 

() 
| 

0 

As 
ve 

ve 

0 
( 

i 

0  0 
¢; 
0  0}  | df 

0  O}  | dt]  Py3¢ 

eat)  6  sa”  +26  «+  |) -a*  +  pe?  st: 
0  1) 

-8* 

6" 

—6* 

a 

a* 
Lé 

The  OLS  estimates  d; of the  untransformed  regression  are  identical  to  the  corre- 
sponding  coefficients  of the  transformed  regression  $*, so  the  asymptotic  distri- 
bution  of ¢; is given  immediately  by [16.3.13].  The  estimate  4; is a  linear  com- 
bination  of variables  that  converge  to  a  Gaussian distribution  at  rate  \/7T,  and  so 
a, behaves  the  same  way.  Specifically,  a, =  &..b7,  where 

8. =  [-a*.+.6*  -a*  +  106" wer) + pe.  1:  Oj, 

and  so, from  [16.3. 13], 

oo 
(ay - a)  > NO, o7¢.1Q*]'8.). 

Fo) 

| 

[16.3.17] 
Finally,  the estimate  8, is a linear combination of variables converging at different 
rates: 

VT 

2 

, 

th, 
ree 

fs 

4 

att ~ 397 COYOR) 

F 

o 

Rd  o.  | 
" 
\ 

ee  Xd  -  Cope. 

: 

: 

’  S641) feUuTIeeH  ate 5  a  an  =  0  0). 

Its aes distribution  is a  pee by the gig with the slowest 7a Xe of 
Col 

gence. 

S ¢ 

. 

— 

| 

aot 2 - 

- Sebi 

ms te  iste? 

| 

| 

3 

Re 

ere 

ity 

Pie  PE  ee a 

ea 

oes 

<—— > 

that  knowledge  of the  transformation  matrix  G  in  [16.3.8]  is necessary  in  order  to 
conduct  hypothesis  tests.  Fortunately,  this  is  not  the  case.  The  results  of  Section 
16.2  turn  out  to  apply  equally  well  to  the  general  model  [16.3.1]—the  usual  ¢ and 
F tests  about  B calculated  in  the  usual  way  on  the  untransformed  system  are  all 
asymptotically  valid. 
ll hypothesis  about  the  parameters  of the  untrans- 

Consider  the  following  nu 

formed  system: 

(16.3.18] 
Hy:  RB  = r. 
Here  R  is  a known  [m  x  (p  +  2)] matrix,  r  is  a  known  (m  x  1) vector,  and  m 
is the  number  of restrictions.  The  Wald  form  of the  OLS  x? test  of Hy (expression 
(8.2.23])  is 

r 
x;  =  (Rb;  -  n'|saR( > “x;] R’| (Rb;  —  r). 

-1 

-1 

[16.3.19] 

Here  b; is the  OLS  estimate  of B based  on  observation  of {y_,41+  Y-p+2>---> 
You  Yis  = 

+  Yr} and  sz =  [1(T  —  p  —  2))  Zi (y,  —  xrbr/’. 

Under  the  null  hypothesis  [16.3.18],  expression  [16.3.19]  can  be  rewritten 

+ 

1=1 

x7 =  [R(br  -  p!'|san( > xxi] R| [R(b;  —  B)] 

T 

r=1 

=  [RG'(G’)~'(b;  —  B)]’ 

x  [sencwoy"(> x) (@)'6R | [RG’(G’)-'(b;  —  B)]. 

(16.3.20] 

Notice  that 

for x* given  by [16.3.9].  Similarly,  from  [16.3.10]  and  [16.3.11], 

t=1 

Defining 

(b;  —  B*)  =  (G’)"  ‘(br  —  B). 

R*  =  RG’, 

expression  [16.3.20]  can  be  written 

x} =  [R*(bs  -  pyr |seRe(> ste) ;  wey  | 

x  [R*(bt  —  B*)]. 

[16.3.21] 

Expression  [16.3.21]  will  be recognized  as  the x? test  that would  be calculated 
if we  had estimated  the  transformed  system  and  wanted  to test  the  hypothesis  that 
R*B*  =  r (recall  that  the  fitted  values  for  the  transformed  and  untransformed 
regressions are  identical,  so that s  will be the same  value for either representation). 
Observe  that  the  transformed  regression  does  not  actually  have  to be estimated  in 
order  to  calculate  this  statistic,  since  [16.3.21]  is numerically  identical  to  the  y? 
Statistic  [16.3.20]  that  is calculated  from  the  untransformed  system  in  the  usual 
way. Nevertheless,  expression  [16.3.21]  gives us another  way of thinking about  the 
distribution  of the  statistic  as  actually  calculated  in [16.3.20]. 

468  Chapter 16  | Processes  with  Deterministic  Time  Trends 

Expression  [16.3.21]  can  be further  rewritten  as 

x7 =  [R*Y;'Y,(b7  —  B*))’ 

T 

-1 

x  [sinrve'y,(3 xbe'!)  vive  tey |  [16.3.2] 
x  [R*Y;'Y;(b;  —  B*)] 

(= 

1 

for Y; the matrix  in [16.3.14].  Recall  the  insight  from  Section  16.2  that  hypothesis 
tests  involving  coefficients  with  different  rates  of convergence  will  be  dominated 
by the variables  with  the slowest  rate  of convergence.  This  means  that  some  of the 
elements  of R*  may  be irrelevant  asymptotically,  so  that  [16.3.22]  has  the  same 
asymptotic  distribution  as  a simpler  expression.  To  describe  this  expression,  con- 
sider  two  possibilities. 

| 

Case 1 .  Each of the m  Hypotheses  Represented 
by R*B*  = r Involves  a  Parameter  that  Converges 

: 

bin  Of course, we could trivially rewrite  any system of restrictions SO as to involve 

O,(T~?) parameters  in every  equation.  For example,  the null hypothesis 
Me  Maction 

He: of =  0,  BPG  wow  oo  Tapes 
SAY  =  5K 

“could 

be rewritten 
Gia?  One  0  thy  Hipiallaces  baseline  Oil 

ig  ane 

no 

as 

| 

——- > oe 

=  _, le  Tiene  iv7end  He $= 

7 

eis  — == xiiblf 

— 

* 

%  Ww!  wu  — 

rit 

, 

; 

: 

=?  ©oul 

_ 

= 

te 

H,. 

1t  does  | 

t 

matter  v  T  Cc 

} 

SF 

Re 

oe 

? 

EE 

A  be ae 

{ 

scl 

: 

3]  rather  than  [16.3.24]. 

~ 

LJ 

> 

=  oe  = 

ae 
4 
; 

: 

as  4 

a 

Lf  a 

_~ 

i 

a 

“ 

~'s 

Be.) 

" 

he 

od 

5 
—  = 
< 

= 
J 
a  es, 

4 

= 

i 

~  >t 
; 

‘ 

7 

vis 

i 

% 

~ 

; 

“ 
alc. 
an  OP 
7 

‘i 

= 

} 

In  general  terms,  this  means  that  R*  is “upper  triangular.”’?  “‘Case  1”  describes 
the  situation  in  which  the  first  p  +  1 elements  of  the  last  row  of  R*  are  not  all 
zero. 
For  case  1, even  though  some  of the  hypotheses  may  involve  67,  a  test  of the 
null  hypothesis  will  be asymptotically  equivalent  to a test  that  treated  5”  as  if known 
with  certainty.  This  is  a consequence  of 57 being  superconsistent.  To  develop  this 
result  rigorously,  notice  that 

: 

. 

rt IVT  r5IVT  . 

rt (VT  r,/VT  elie 
oe 

; 
- 

:! 
4 

pee  eM  MMO  Rody 
3/2 

PS  aig  wie  3 ps2/T 

* 

4 

A 

- 

ma 

; 

ESTE 

re olVT  rete  ay  eee 

ene 

RE  oe 

and  define 

Y,  =VTI, 

(m  xm) 

* 
Ris 
* 
lar 

R*  = 

Ca 

. 

* 

* 

“2h: 

ee 

* 
Ix 

. 

* 

ee 

2h 

Pmi  lm 

* 

liip+i 

* 

lo  p+ 

* 

ri  p+2/T 

* 

"3  p+2/T 

. 

. 

* 

* 

lin.p+  I  rnp+2! 

_These  matrices  were  chosen  so  that 

Rey}  =  77's: 

| 

[16.3.25] 

The  matrix  R* has  the  further  property  that 

R*—  R*, 

[16.3.26] 

where  R* involves  only  those  restrictions  that  affect  the  asymptotic  distribution: 

p* 

de |  ail & 
56)  a  ee 

* 

R* 

* 

* 

eee 

eee 

* 

ripen 
r2p+i 

* 

(0 
9 

acs 
mi  'm2 

(ee x ca  atthe 

Timn.p+l  0 

**  Upper  triangular”  means  that  if the  set  of restrictions  in H,, involves  parameters  B7, By, ..  . 
, 
BF with  i, <  i,  < +++  <i,,  then  elements  of R*  in rows  2 through  m  and  columns  | through  i, are  all 
zero.  This  is simply  a  normalization—any  hypothesis  R*B*  =  r  can  be  written  in  such  a  form  by 
selecting  a  restriction  involving  8B to  be  the  first  row  of R*  and  then  multiplying  the  first  row  of this 
system  of equations  by a suitable  constant  and  subtracting  it from  each  of the  following  rows.  If the 
system  of  restrictions  represented  by rows  2 through  m  of  the  resulting  matrix  involves  parameters’ 
B;. By...  . 
,  Bj with  j, <j,  <  ++:  <j),  then  it is assumed  that  the  elements  in rows  3 through  m  and 
columns  | through  j, are  all zero.  An  example  of an  upper  triangular  system  is 

0  rea,  This 

pe  | OMMO 

0 

eS 

IMO:  | A  yong 

0 

Th in 

eA 

0  O 

0  Cer  Pe;  Vande 

470  Chapter  16  | Processes  with  Deterministic  Time  Trends 

Substituting  [16.3.25]  into  [16.3.22], 

xr =  [Yr'RFY,(b;  —  B*))' 

«  | site iv, (3 acterr)  valve Ray | Ve REY (OF —  BI 

=  [RFY,(b}-  —  B*))'Y7' 

= 

# 

=) 

+t 

x  vf  siniv,(3 xiIx:')  a(R  | Y7Y;7'[R7Y7(b}  -  B*)] 

= sAmars(P?  -  B*))’ 

x fami (Sacto) tear]  weer -  on 

ey een onl B*))'[o?R  *[Q*}-"[R*]'] “RY; (bF  od B*)] 
a 
by virtue  of [16.3.26]  and [16.4.4]. 
Now  [16.3.13]  implies  that 
seq) 

Poe  ie  es  ranaas 
ae so [ 16.3. 27] is is a auadrate form in an pans Cikenas rata a 

ey 

e 

(16.3.27] 

2red in Proposition  8.1. It is therefore  asymptotically 

[16. 
ab) ‘oa [16.3.19], the Wald or of the payee by calcu 

y’ 

ce 

the eer Tree id ae 16.3 3.1], has the  usual ae 

th " ssua way from 

ashes  Sart tye Sri  -sogy 

Notice  that  these  matrices  again  satisfy  [16.3.25]  and  [16.3.26]  with 

R*  i 

* 

ri 
’ 
2) 
: 
+ 
Pm-1.1 

a 

oe 

eet 
ae 

ri2 
* 
'22 
. 

lip+t 
T2.p+l 
: 
ye 
.  we 

. 

“m-1.2 

Tin-\.p+) 

0 
0 
y 

0 

ike 
The analysis of [16.3.27]  thus goes  through  for this case  as well  with  no change. 

a 

0 

0 

0 

Summary 

Any  standard  OLS  ,? test  of the  null  hypothesis  RB  =  r for  the  regression 
model  [16.3.1]  can  be  calculated  and  interpreted  in  the  usual  way.  The  test  is 
asymptotically  valid  for  any  hypothesis  about  any  subset  of the  parameters  in  B. 
The  elements  of R do  not  have  to  be  ordered  or  expressed  in any  particular  form 
for this  to  be  true. 

APPENDIX  16.A.  Derivation  of Selected  Equations for Chapter  16 

®  Derivation  of (16.3.13]. 

As  in [16.1.6], 

b>;  —  B*  =  b ect] [> xe 

{16.A.1] 

since  the  population  residuals  e, are  identical  for  the  transformed  and  untransformed  rep- 
resentations.  As  in [16.1.18],  premultiply  by Y; to  write 

Y7(b;  —  B*)  =  {ve > epryye'}  {ve Sx7e 

ee  | 

‘=i 

[16.A.2] 

From  [16.3.9], 

T 
> x? [x*]’  ie 
rt 

zy 
2ty?, 

and 

T 

Y7'  D> xP [xt)'¥;! 

‘ 

Lye,  Syhayes  - 
Sy 2 int,  -eAVi ay 
Ss 

yt ye,  Bye,  Bye, 
Zyr-sy?,  1 Sy2..  Uys; 

. 
; 

a 
ZYi~  Vint, WinpYr-2  *<" 

. 
: 

. 
: 

. 
: 

: 
: 

Wyrzy  | Sys,  Bye, 
Eyrig 
°°  oe Bin, 
21 
My  "as  =t 

>a 
=r 

=] 

To  'X(y ry?  ah? > Pah > 
Po'Zyreayiey 
TOUAyrsP 

To  Zyl  WVemp  thet A  ie?” al 
To Syfayh,  To Bye,  T-ktyh, 

ov 

T-'Sy?  py?  To eyipiaa  8 

T'S  yeayh  ToS,  T-keye, 

T -'Sy?., 
T-Xty?, 

T~'Syf.g  0; +3  gh, 
T“Lyfs'-o)  TB)  Peay 

TWF  = + H-2.% 

OT -3.  ge 

[16.A.3] 

472  Chapter 16  | Processes  with  Deterministic  Time  Trends 

For the first  p rows  and columns,  the row  i, column j element  of this matrix  is 

Se  drt, 
But y? follows a zero-mean  stationary AR(p)  process satisfying  the conditions  of Exercise 
7,7, Thus,  these  terms  converge  in probability  to y*_,,.  The  first  p elements  of row  p +  1 
(or the first p elements  of column p  +  1) are  of the 

form 

t 

of column  p +  2) are of the form 

| as  > Yori. 
ol 
which  converge  in probability  to  zero.  The  first  p elements  of row  p  + 2 (or the  first  p 
elements 
ela 

T-! > (T)yt,, 
which  can  be  shown  to  converge  in  probability  to  zero  with  a  ready adaptation  of  the 
1 7 (see Exercise  16.3).  Finally,  the (2 x  2) matrix  in the bottom  right 
techniques 
comer of [16.A.3 converges  to 
| 

SIOIBS  AOS eG?   Viecwicerts  bas  once  nb 4] wea, 

Sona 

in 

7  os 

erar 

4 
= 

e  34 

ca. Ovi 

Xe 

<  7}  ~?  a  ‘ 

‘et 

, 

por sth  ues ve  merry Bi 545 gniigebA  1 Rie  (Ash 

and 

: 

” 

i 

| 

(1/T)  2a ~»  Q*. 

ade  act 

Applying  the  arguments  used in  Exercise  8.3  and in  [16.1.24],  it can  be shown that. 

oi 

Y;!  > x* as N(0,  o?Q*). 

| 

116. A 6 

It follows  from  [16.A.4],  [16. A. 6], and  [16.A.2]  that 

Y7(b;  -  of + N(O,  [Q*]- #0710") i 

mee tS ), 

as claimed  in (16.3.13]. 

~=eunee 

dose 

Chapter 16  Exercises 

; 

nai 

16.1. ‘Verify result 16. 1. 1%  . 
fy 
Verify expression  [16.1.27]. 
16.2. 
16.3.  Let  y,  be  covariance-stationary  with mean zero  and  absolutely  summable  autoco- 
variances: 

aa  fm.  .  SVD  Ul  GWOR  So  nes  a-nai« 
AS  CF  BEAGLE  S52)"  yD WW  Phupindes 
ATO  ERO  PeuwaOl  Pio kee 

(SEEPS 

‘ 

. 

—  Pe : gor _ P os.  St 

trea 

Pa Eh 

tt 

gin 

, Il <  o 

. 

3  =:  “;  comet 

ust 

: 

= 

eesris  gue 

E  | 

for % = E(yy,-))- Ader the i 
ere we pal sx  T2é. = Si 

my eT: 

. 

« 

; 

6 References 

rf ir sunckiq oo  Bee 

Faller, Wayne i 1976. ji  he to Sica Ti eM 

r7 

Univariate  Processes 
with  Unit  Roots 

This  chapter  discusses  statistical  inference  for  univariate  processes  containing  a 
unit  root.  Section  17.1  gives a  brief  explanation  of why the  asymptotic  distributions 
and  rates  of convergence  for  the  estimated  coefficients  of unit  root  processes  differ 
from  those  for  stationary  processes.  The  asymptotic  distributions  for  unit  root 
processes  can  be  described  in terms  of functionals  on  Brownian  motion.  The  basic 
idea  behind  Brownian  motion  is  introduced  in  Section  17.2.  The  technical  tools 
used  to establish  that  the  asymptotic  distributions  of certain  statistics  involving  unit 
root  processes  can  be  represented  in  terms  of  such  functionals  are  developed  in 
Section  17.3,  though  it  is  not  necessary  to  master  these  tools  in  order  to  read 
Sections  17.4  through  17.9.  Section  17.4  derives  the  asymptotic  distribution  of the 
estimated  coefficient  for  a  first-order  autoregression  when  the  true  process  is  a 
random  walk.  This  distribution  turns  out  to  depend  on  whether  a constant  or  time 
trend  is included  in the  estimated  regression  and  whether  the  true  random  walk  is 
characterized  by nonzero  drift. 

Section  17.5  extends  the  results  of Section  17.3  to  cover  unit  root  processes 
whose  differences  exhibit  general  serial  correlation.  These  results  can  be  used  to 
develop  two  different  classes  of tests  for unit  roots.  One  approach,  due  to  Phillips 
and  Perron  (1988),  adjusts  the  statistics  calculated  from  a  simple  first-order  au- 
toregression  to  account  for  serial  correlation  of the  differenced  data.  The  second 
approach,  due  to  Dickey  and  Fuller  (1979),  adds  lags to  the  autoregression.  These 
approaches are  reviewed  in Sections  17.6 and  17.7, respectively.  Section  17.7 further 
derives  the  properties  of  all  of  the  estimated  coefficients  for  a  pth-order  auto- 
regression  when  one  of the  roots  is unity. 

Readers  interested  solely  in  how  these  results  are  applied  in  practice  may 
want  to begin with  the summaries  in Table  17.2 or Table  17.3  and with  the empirical 
applications  described  in Examples  17.6  through  17.9. 

17.1.  Introductio 
Consider  OLS  estimation  of  a Gaussian  AR(1)  process, 

where  u,  ~  i.i.d.  N(0,  a2),  and  y,  =  0.  The  OLS  estimate  of p is given  by 

yi  oa PY1-1  el u,, 

[17.1.1] 

T 
>» Yi-1)1 
. 

bra  OF 

[17.1.2] 

475 

We  saw  in  Chapter 8 that  if the  true  value  of p  is  less  than  1 in  absolute  value, 
then 

[17.1.3] 
VT(6r —  p) > N(0, (1  -  p?)). 
If [17.1.3]  were  also  valid  for  the  case  when  p  =  1,  it would  seem  to  claim  that 
VT(p;  —  p) has  zero  variance,  or  that  the  distribution  collapses  to  a  point  mass 
at  zero: 

VT (67  —  1)> 0. 

[17.1.4] 

As we  shall  see  shortly,  [17.1.4]  is indeed  a valid  statement  for unit  root  processes, 
but  it obviously  is not  very  helpful  for  hypothesis  tests.  To  obtain  a  nondegenerate 
asymptotic  distribution  for  67 in  the  unit  root  case,  it turns  out  that  we  have  to 
multiply  6; by  T rather  than  by \/T.  Thus,  the  unit  root  coefficient  converges  at 
a faster  rate  (7)  than  a coefficient  for  a  stationary  regression  (which  converges  at 
VT),  but  at  a  slower  rate  than  the  coefficient  on  a  time  trend  in  the  regressions 
analyzed  in the  previous.  chapter  (which  converged  at  T°”). 

To  get  a  better  sense  of why  scaling  by T is necessary  when  the  true  value 
of p is unity,  recall  that  the  difference  between  the  estimate  6, and  the  true  value 
can  be expressed  as  in equation  [8.2.3]:' 

(@r-)N  => 

' 

»> yey 

(1/7) > ye— set 
T(6;  -  1)  =  ——*}——_ 
= 
, 
(1/77) & y?-s 

T 

r=1 

[17.1.5] 

[17.1.6] 

so  that 

: 

Consider  first  the numerator  in [17.1.6].  When  the true  value  of pis unity, equation 
[17.1.1]  describes  a  random  walk  with 

Pere  Ry  otf,  Cb) 

26, 

hy, 

since yy  =  0.  It follows  from  [17.1.7]  that 

Note  further  that  for  a  random  walk, 

y, ~ N(O,  o7t). 

implying  that 

y? aa (-1  +  u,)?  a7 yr)  +  2y,—1u,  +  u;, 

y,-\4,  =  (1/2){y?  —  yi  —  wry 
If [17.1.9]  is summed  over  ¢ =  1,2,...  ,  T, the  result  is 
T 

T 
2 Yr)  =  (2M y% —  y3} —  (12)  D u?. 

Recalling  that y,  =  0, equation  [17.1.10]  establishes  that 

(= 

: 

T 

t= 

“ 

(1/T)  2 YM,  =  (1/2)-(W/T)y}  =  (12)-(V/T)  S w?, 

'This  discussion  is based  on  Fuller  (1976,  p. 369). 

476  Chapter  17  | Univariate  Processes  with  Unit Roots 

t=) 

: 

T 

t=) 

[17.1.7] 

[17.1.8] 

{17.1.9} 

[17.1.10] 

[17.1.1] 

and  if each  side  of [17.1.11]  is divided  by a7,  the  result  is 

(3) > a  (3) (2%) -  (4)(#)-3 u2. 

—(17.1.12] 

But  [17.1.8]  implies  that y;/(a0VT)  is a N(0,  1) variable,  so  that  its square  is x?(1): 
[yr(oVT)P  ~  x?(1). 
(17.1.13] 
Also,  2/_,  u; is the  sum  of  Ti.i.d.  random  variables,  each  with  mean  o2.  and  sO, 
by the  law  of large  numbers, 

Using  [17.1.13]  and  [17.1.14],  it follows  from  [17.1.12]  that 

/ 7 

(1/T)- > u?  > o?. 

i=! 

[1(o?T)]  > y,_,u, > (1/2):(X  -  1), 

x 

L 

where  X ~  y?(1). 

t=1 

Turning  next  to  the  denominator  of [17.1.6],  consider 

T 

2 vin. 

[17.1.14] 

[17.1.15] 

[17.1.16] 

Recall  from  [17.1.8] that y,_,  ~  N(0, 07(t  —  1)),so E(y2_,)  =  o(t —  1). Consider 
the  mean  of [17.1.16], 

|S vi] =o  > (¢ —  1) =  o°A(T  -  1)772. 

In order  to construct  a random  variable  that  could  have  a convergent  distribution, 
the quantity  in [17.1.16]  will  have  to  be divided  by 7, as  was  done  in the  denom- 
inator  of [17.1.6]. 

To  summarize,  if the  true  process  is a  random  walk,  then  the  deviation  of 
the  OLS  estimate  from  the  true  value  (6,  —  1) must  be  multiplied  by  T rather 
than  VT to obtain  a variable  with  a useful  asymptotic  distribution.  Moreover,  this 
asymptotic  distribution  is not  the  usual  Gaussian  distribution  but  instead  is a ratio 
involving a y2(1) variable  in the numerator  and a separate,  nonstandard  distribution 
in the  denominator. 

The  asymptotic  distribution  of T(6;  —  1) will  be  fully characterized  in Sec- 
tion  17.4.  In  preparation  for  this,  the  idea  of Brownian  motion  is introduced  in 
Section  17.2,  followed  by a  discussion  of the  functional  central  limit  theorem  in 
Section  17.3. 

17.2.  Brownian  Motion 

Consider  a random  walk, 

yi  28 Yr-1  +  E,, 

[17.2.1] 

in which  the  innovations  are  standard  Normal  variables: 

€, ~  i.i.d  N(O,  1). 
If the process  is started  with y,  =  0, then  it follows  as  in [17.1.7]  and  [17.1.8]  that 
y  8)  F  yet FR, 

y, ~  N(O,  ¢). 

17.2.  Brownian  Motion 

477 

Moreover,  the  change  in  the  value  of y between  dates  ¢ and  s, 

Ys  r=! Gra   Peeeree hr  ie  +  €;, 
is itself  N(0,  (s  —  ¢)) and  is independent  of the  change  between  dates  r  and  q for 
any  datest¢<s<r<q. 

Consider  the  change  between  y,_,  and  y,.  This  innovation  €,  was taken to  be 

N(0,  1). Suppose  we  view  e,  as  the  sum  of two  independent  Gaussian  variables: 

. 

with  e,,  ~  i.i.d.  N(O,  4). We  might  then  associate  e,,  with  the  change  between  y,_, 
and  the  value  of y at  some  interim  point  (say,  y,— (1/2); 

c,  *  ty  *  C24) 

and  e,,  with  the  change  between y,_ (jz)  and  y,: 

Yr-(vzy  7  Yr-1  FF  Ew 

Y,  —  Vr-any  =  €a- 

(1722) 

[17.2.3] 

Sampled  at  integer  dates  ¢ =  1, 2,  . 
have  exactly  the  same  properties  as  [17.2.1],  since 

. 

. 

,  the  process  of [17.2.2]  and  [17.2.3]  will 

Y,  —  Yn  =  Cu  +  €y  Vii.d.  NO,  1). 
In  addition,  the  process  of [17.2.2]  and  [17.2.3]  is defined  also  at  the  noninteger 
dates  {t +  4}*., and  retains  the  property  for  both  integer  and  noninteger  dates  that 
y,  —  y,  ~  N(O,  s  —  ¢) with  y,  —  y,  independent  of  the  change  over  any  other 
nonoverlapping  interval. 

By the  same  reasoning,  we  could  imagine  partitioning  the  change  between 

t  —  1 and  ¢t into  N separate  subperiods: 

Vp  Dien  E  Cit  Cx  FT  ems 

with e,, ~  i.i.d.  N(0,  1/N). The  result  would  be a process  with  all the same  properties 
as  [17.2.1],  defined  at  a  finer  and  finer  grid  of dates  as  we  increase  N.  The  limit 
as  N  —  < is a  continuous‘time  process  known  as  standard  Brownian  motion.  The 
value  of  this  process  at  date  ¢ is denoted  W(t).?  A  continuous-time  process  is a 
random  variable  that  takes  on  a value  for any  nonnegative  real  number  f, as  distinct 
from  a  discrete-time  process,  which  is  only  defined  at  integer  values  of  ¢.  To 
emphasize  the  distinction,  we  will put  the  date  in parentheses  when  describing  the 
value  of a continuous-time  variable  at  date  ¢ (as in  W(t))  and  use  subscripts  for  a 
discrete-time  variable  (as  in  y,).  A  discrete-time  process  was  represented  as  a 
countable  sequence  of random  variables,  denoted  {y,}*,.  A  realization  of a  con- 
tinuous-time  process  can  be viewed  as  a stochastic  function,  denoted  W(-),  where 
W:  + €  [0,  ©)  >  R’. 

A  particular  realization  of  Brownian  motion  turns  out  to  be  a  continuous 
function  of t.  To  see  why  it would  be  continuous,  recall  that  the  change  between 
t and  ¢  +  A is  distributed  N(0,  A).  Such  a  change  is essentially  certain  to  be 
arbitrarily  small  as  the  interval  A goes  to  zero. 

Definition:  Standard  Brownian  motion  W(-) is a continuous-time  stochastic  proc- 
ess,  associating  each  date  t €  [0, 1] with  the scalar  W(t) such  that: 

(a)  W(0)  =  0; 

- 
“Brownian  motion  is  sometimes  also  referred  to  as  a  Wiener process. 

Megs 

oo 

: 

: 

478  Chapter  17  | Univariate  Processes  with  Unit  Roots 

(6)  For  any  dates  0  =  t,  <1,  <---<  t,  =  1,  the  changes  (W(t.)  —  W(t,)], 
(W (és) —  W(t)],...,[W(e,)  -  W  (1, _,)] are  independent  multivariate  Gauss- 
lan  with  [W(s)  —  W(t)]  ~  N(O,s  —  ¢); 

(c)  For  any  given  realization,  W(t)  is continuous  in  t  with  probability  1. 

There  are  advantages  to  restricting  the  analysis  to  dates  ¢  within  a  closed 
interval.  All  of the  results  in  this  text  relate  to  the  behavior  of  Brownian  motion 
for  dates  within  the  unit  interval  (¢ €  [0,  1}),  and  in  anticipation  of  this  we  have 
simply  defined  W(-)  to  be  a  function  mapping  ¢ €  (0, 1] into  R'. 

Other  continuous-time  processes  can  be  generated  from  standard  Brownian 

motion.  For  example,  the  process 

Z(t)  = 

a  W(t) 

has  independent  increments  and  is distributed  N(0,  ot)  across  realizations.  Such 
a process  is described  as  Brownian  motion  with  variance  a7. Thus,  standard  Brown- 
ian  motion  could  also  be  described  as  Brownian  motion  with  unit  variance. 

As  another  example, 

Z(t)  =  [W(t)P 

(17.2.4] 

would  be  distributed  as  ¢ times  a y?(1)  variable  across  realizations. 

Although  W(f)  is continuous  in ¢, it cannot  be  differentiated  using  standard 
calculus;  the  direction  of change  at  ¢ is likely  to  be  completely  different  from  that 
at  ¢ +  A, no  matter  how  small  we  make  A.* 

17.3.  The  Functional  Central  Limit  Theorem 

One  of the  uses  of Brownian  motion  is to  permit  more  general  statements  of the 
central  limit  theorem  than  those  in  Chapter  7.  Recall  the  simplest  version  of the 
central  limit  theorem:  if u, ~  i.i.d.  with  mean  zero  and  variance  a”, then  the sample 
mean  i, =  (1/T)2/_,  u, satisfies 

VTi; 

N(0,  o?). 

| 

Consider  now  an  estimator  based  on  the  following  principle:  When  given  a 
sample  of size  T, we  calculate  the  mean  of the  first  half  of the  sample  and  throw 
out  the  rest  of the  observations: 

tray  =  (W{TI2*)  2 a, 

[7/2)}* 

{= 

Here  [7/2]*  denotes  the  largest  integer  that  is less  than  or  equal  to 7/2;  that  is, 
[7/2]*  =  T/2 for  T even  and  [7/2]*  =  (T —  1)/2 for  T odd.  This  strange  estimator 
would  also  satisfy  the  central  limit  theorem: 

V[7/2]*  472) — N(0,  a). 

[17.3.1] 

Moreover,  this  estimator  would  be independent  of an  estimator  that  uses  only the 
second  half of the  sample. 

More  generally,  we can  construct  a variable  X(r)  from  the  sample  mean  of 

For an introduction to differentiation  and  integration  of Brownian  motion,  see  Malliaris  and  Brock 
(1982,  Chapter  2). 

17.3.  The Functional  Central  Limit  Theorem 

479 

the  first  rth  fraction  of observations,  r  €  [0, 1], defined  by 

X,(r)  =  (/T)  > u,. 

{Tr}° 

t= 

[17.3.2] 

For  any  given  realization,  X_(r)  is a  step  function  in  r,  with 

0 
u,/T 

X7(r)  = 

(u,  +  u,)/T 

forO0  =  r<  1I/T 
for  UT  =r  <  2T 
for  2/T  =r  <3/T 

[17.3.3] 

er +  Uz  tree  +  ur)/T 

for r  ad  } 

VT-X7(r)  =  (1/VT)  2 u,  =  (V[Tr  */V/T)(/V{Tr)*)  > U,- 

[17.3.4] 

|7r]° 

(7r}* 

Then 

But 

{Tr\" 

(1/V[Tr]*)  » u,  > N(0,  o?), 

by the  central  limit  theorem  as  in [17.3.1],  while  (V{Tr}*/VT)  >  Vr.  Hence,  the 
asymptotic  distribution  of  VT-X-;(r)  in  [17.3.4]  is that  of  Vr times  a  N(0,  o?) 
random  variable,  or 

VT-X,(r)  > NO, ro?) . 

and 

VT-[X7(r)/o]  > N(O,  r). 

[17.3.5] 

If we  were  similarly  to  consider  the  behavior  of  a  sample  mean  based  on 
observations  [7r,]*  through  [7r,]*  for  r,  >  r,,  we  would  conclude  that  this  too  is 
asymptotically  Normal, 

VT-[Xr(r2)  —  Xr(r Vo  NO. 2  -  1), 
and  is independent  of the  estimator  in [17.3.5],  provided  that  r <  r,.  It thus  should 
not  be  surprising  that  the  sequence  of  stochastic  functions  {VT-X r(- Woh}, 
has  an  asymptotic  probability  law  that  is described  by standard  Brownian  motion 
W(-): 

VT: X;7(-)lo  > W(-). 

[17.3.6] 

Note.the  difference  between  the claims in [17.3.5] and [17.3.6]. The expression 
X7(-)  denotes  a  random  function  while  X;(r)  denotes  the  value  that  function 
assumes  at  date  r;  thus,  X,(-)  is a function,  while  X;(r)  is a random  variable. 

Result  [17.3.6] is known  as the functional central limit theorem.  The derivation 
here assumed  that u, was  i.i.d.  A more  general statement  will be provided in Section 
Fred 

; 

Evaluated  at  r  =  1, the  function  X;(r)  in [17.3.2]  is just the sample  mean: 

X;(1)  =  (WT)  > ,. 

Thus,  when  the  functions  in  [17.3.6]  are  evaluated  at  r  =  1, the  conventional 
central  limit  theorem  [7.1.6]  obtains  as  a special  case  of [17.3.6): 

VTX;,(Ilo  =  [Io  VT)] 2 u,—> W(1)  ~  N(0,  1). 

T 

[17.3.7] 

480  Chapter  17  | Univariate  Processes  with  Unit Roots 

We  earlier  defined  convergence  in  law  for  random  variables,  and  now  we 
need  to  extend  the  definition  to  cover  random  functions.  Let  § (-)  represent  a 
continuous-time  stochastic  process  with  S(r)  representing  its  value  at  some  date  r 
for r € [0, 1]. Suppose,  further,  that  for  any  given  realization,  §(-) is  a continuous 
function  of  r  with  probability 1.  For  {$;(-)}#.,  a  sequence  of  such’ continuous 
functions,  we  say  that  S;(-)  > S$ (-)  if all  of the  following  hold:4 

(a)  For  any  finite  collection  of k particular  dates, 

OS  Ff,  <3,  <<: -  <r,  S'1, 

the  sequence  of k-dimensional  random  vectors  { Yr}r=1  converges  in  distri- 
bution  to  the  vector  y,  where 

yr 

S7(r ) 
Sr(%) 
S(r) 

S(r,) 
y =  St) 
Sr) 

(b)  For  each  e  >  0, the  probability  that  S;(r,)  differs from  S,(r,)  for  any 
dates  r, and  r, within  5 of each  other  goes  to zero  uniformly in  T as  6 —  0; 
(c)  P{|S;(0)| >  A}—  0 uniformly in  T as  A >  ~. 

: 

5 

_  This definition  applies to sequences. ofc continuous  functions, though the func- 
is a discontinuous  step  function.  Fortunately,  the  discontinuities 
tion in [17.3.2] 
occur at a countable  set of points.  Formally,  S;(-)  can  be replaced with  a similar 
einssiaucnd function,  interpolating  between  the steps (as in  Hall and Heyde,  1980). 
nativels  the definition of convergence of random functions can be generalized 
discon  inuities of the type in [17.3.2] (as in ee Billingsley, 
ow 

for 

; 

_ 
-. 
, 

, 

es its will ao = —  to etedd the earlier definition af carveegenee in prob- 
2s  of Fandom functions.  Let {S;(- Nia and {V7(- rer denote 

qu 

; 

ontine ous functions  with S;: r  € [0, 1]  > R' and V;: r € 
a!  esac Spohereestarenatlis which saison  Ss 

Or  ae 
»  aS 

bamtse ras  5 

to  sequences  of functions.  Specifically,  if {S,(-)}7-,  and  {Vr -)}7_,  are  sequences 
of  continuous  functions  with  V;(-)  &  S;(-)  and  S7(-)  ie S(-)  for  S(-)  a  con- 
tinuous  function,  then  V;(-)  +  S(-);  see,  for  example,  Stinchcombe  and  White 
(1993). 

Example  17.1 
Let  {x;}7_,  be  a  sequence  of  random  scalars  with  x;  0,  and  let  {Sr  hr 
be  a  sequence  of  random  continuous  functions,  S7:  r  ©  (0,  1] —  R'  with 
S7(-)  +S (-).  Then  the  sequence  of  functions  {V7(-)}  <1  defined  by  V;(r)  = 
S,(r)  +  x;  has  the  property  that  V;(-)  >  S(-).  To  see  this,  note  that 
V,(r)  —  S,(r)  =  xr  for  all  r,  so  that 

sup  15767) —  V,(r)|  =  |xzl, 
re|0.1 

which converges  in  probability  to  zero:  Hence,  V;(-)  “,  S;(-),  and  therefore 
V(-)  > S(+). 

|

Example  17.2 
Let  7,  be a strictly  stationary  time  series  with  finite  fourth  moment,  and  let 
Sr(r)  =  (VT): nr,\-  Then  S7(-)  %  0.  To  see  this,  note  that 

P| sup  [Sx(71  >  8 

re|0.1} 

P{IQ/VT)-m|  >  8)  or  [1G/VT)-m|  >  6]  oF 
or  [|(1/VT)-nz| > 3] 
T-P{|\(Q/VT)-7,|  >  5} 

lA 

EX(1/VT)-n}4 
<  | Sora ear ea 

__  E(n;) 
aaa 

where  the  next-to-last  line  follows  from  Chebyshev’s  inequality.  Since  E (7?) 
is finite,  this  probability  goes  to  zero  as  T —  &,  establishing  that  S;(-)  >  0, 
as  claimed. 

Continuous  Mapping  Theorem 

In  Chapter  7 we  saw  that  if {x,;}7_,  is  a sequence  of random  variables  with 
x;-> x  and  if g:  R'  >  R'  is a  continuous  function,  then  g(x7)  > g(x).  A similar 
result  holds  for  sequences  of random  functions.  Here,  the  analog  to  the  function 
g(-)  is a  continuous  functional,  which  could  associate  a  real  random  variable  y 
with  the  stochastic  function  S(-).  For  example,  y =  fj S(r) dr and y  =  fj [S(n}*  dr 
represent  continuous  functionals.*  The  continuous  mapping  theorem*  states  that  if 
S;(-)  > S(-)  and  g(-)  is a  continuous  functional,  then  g(S;(-))  > g(S(-)). 

~ 

‘Continuity  of a  functional  g(-)  in this  context  means  that  for any  ¢  >  0, there  exists  a  5 >  0 such 
that  if A(r)  and  k(r)  are  any  continuous  bounded  functions  on  [0, 1], A:  [0,  1] >  R'  and  k:  (0,  1] > 
R',  such  that  |A(r)  —  k(r)|  <  6 for  all  r €  [0, 1], then 

"See,  for  example,  Theorem  A.3  on  p.  276  in Hall  and  Heyde  (1980). 

IslA(-)]  —  sfk(-)H  <«. 

482  Chapter  17 

Univariate  Processes  with  Unit  Roots 

 
The  continuous  mapping  theorem  also  applies  to  a continuous  functional  g(:) 
that  maps a continuous  bounded  function  on  (0, 1] into  another  continuous  bounded 
function on  [0, 1]. For  example,  the  function  whose  value  at  r is a positive  constant 
a  times  h(r)  represents  the  result  of applying  the  continuous  functional  g[h(-)]  = 
a-h(-)  to  h(-).’  Thus,  it follows  from  [17.3.6]  that 

VT-X;7(-)  > o-  W(-). 
[17.3.8] 
Recalling  that  W(r)  ~  N(0,  r), result  [17.3.8]  implies  that  VT: X;,(r) =  N(0,  o?r). 
As  another  example,  consider  the  function  S;(-)  whose  value  at r is given 

by 

Sy(r)  =  [VT-X,()P. 

[17.3.9] 

Since  VT-X;(-)  > o-W(-),  it follows  that 

S7(-)  > o°[W(-)P. 
(17.3. 10] 
In other  words,  if the  value  W(r)  from a realization  of standard  Brownian  motion 
at every  date  r is squared  and  then  multiplied  by a, the  resulting  continuous-time 
process  would  follow  essentially  the  same  probability  law  as  does  the  continuous- 
time  process  defined  by S;(r)  in [17.3.9]  for  7 sufficiently  large. 

Applications  to  Unit  Root  Processes 

The  use  of the  functional  central  limit  theorem  to  calculate  the  asymptotic 
distribution  of  statistics  constructed  from  unit  root  processes  was  pioneered  by 
Phillips  (1986,  1987).*  The  simplest  illustration  of Phillips’s  approach  is provided 
by a  random  walk, 

Y=  Vr-1  +  Uy, 

{17.3.11] 

where  {u/} is an  i.i.d.  sequence  with  mean  zero  and  variance  o?.  If yy  =  0, then 
{17.3.11]  implies  that 

y =U,  +  up  toes  +  Uy, 

[17.3.12] 

Equation  [17.3.12]  can  be used  to  express the  stochastic  function  X;(r)  defined  in 
[17.3.3]  as 

| 

for0  <r  < 1/T 
 for/T<r<2T 
| 
X(N  =  {1 y/T   for’T<r<3/T 

0 
y/T 

(17.3.13] 

Figure  17.1  plots X,(r) as a function  of r.  Note  that the area  under  this step function 

yr/T  ~— for  rr  =  1. 

7Here  continuity  of the functional  g(-)  means  that  for  any  € > 0, there  exists  a  6 >  0 such that  if 

_  h(r) and  k(r) are  any continuous  bounded  functions  on  (0, 1], 4: (0, 1] +  R' and  k: (0, 1] >  R', such 

that  |h(r)  —  k(r)|  <  6 for all r €  [0, 1], then 

Isth(r)]  —  glk()}I  <  € 

for alr € [0,1]. 

| 
*Result  [17.4.7]  in the next  section  for the  case  with  i.i.d.  errors  was  first  derived  by White  (1958). 
Phillips  (1986,  1987)  developed  the  general  derivation  presented  here  based  on  the  functional  central 
limit  theorem  and the continuous  mapping theorem.  Other  important  contributions  include  Dickey and 
Fuller  (1979),  Chan  and  Wei  (1988), Park  and  Phillips  (1988,  1989),  Sims,  Stock,  and  Watson  (1990), 

and 

Phillips and Solo (1992). 

ae 

D> 

Poe ADS  Fab Functional  Cenrral  Limit  Theorem, 

483 

X-p(r) 

a  i - 

aE 

isl oe 

L  saa  4 

: 

rl wont sal of X7(r) asa a.  netion P ofr 15>  te fests:  9:15 

Ss  aap y 

aed 

or 

|  les. The ¢th ciel  cers ace  pe gene  2 
}  eye, Se 

yey T*. The integral of X;-(r) is thus  equivalent to 

' 

_ 

¥  a i Lei “Tine JIT? + ylT? 

ae  ce)  sae fa  = 

var  + ee  Be: capi 

g 

Recall  from  [16.1.24]  that 

TD  u, 
ra 

T-%2  > tu, 

| 

4 n({2], o°[] |). 

| 

(17.3.18] 

ai 

Thus,  [17.3. 17] implies  that  T~*?  S7_,  y,_,  is asymptotically  Gaussian  with  mean 
zero  and  variance  equal  to 

o%{1  —  2-(1/2)  +  1/3}  = 

Evidently,  o-f;W(r)dr  in  [17.3.16]  describes  a  random  variable  that  has  a 
N(0,  @7/3)  distribution. 

Thus,  if y, is a  driftless  random  walk,  the  sample  mean  T~'2/_, y, diverges 
but  7~**/_,  y, converges  to  a Gaussian  random  variable  whose  distribution  can 
be described as  the  integral  of the  realization  of Brownian  motion  with  variance 
? a. 

ees! [17. 3.17] also gives us a way to describe  the asymptotic distribution 

pf T-  2 ST,  mu, in terms of functionals  on  Brownian  motion: 

} 

TY 

mu,  = T° 3 uy —  TD  y,. 
P . 

3. ae 

2 5 

(50 
(17.3.19] 

so: Aci  -  oC: ah W(r)  dr, 

variable 

Se a line following  from [17.3.7]  and  [17.3.16]. Recalling 7. 3.18],  the 
on the right side of [17.3.19] evidently  has aN (0, 77/3) distribution. 
we 
nilat a to that in  [17.3.15]  can be used to describe the asymptotic 
: 
n of the sum  of mice? oo a |  random walk. The statistic Sr(0) defined  i in 
2  TARAS  « pres 

Reccat. eae 

sai 

ot  | ke 
1  at 

ci  (= =  FGF, 

i 
Lae  @ oS ne  “ Aten  Sait!  see Ad  ne a. iat} acne i a. 
‘ ; =! ie  sr<ur amet Ks ae Ase 
aoa  Aid hee ee  ee 

for  r  =  WT  and 

T 

T 

1 

T73  Sty?  =  T*DePy7. | - o-| r-(W(r)}?  dr. 

[17.3.24] 

1=1 

a | 

As  yet  another  useful  application,  consider  the  statistic  in  [17.1.11]: 

T-!  > y=:  =  (12): (UT )y}  —  (1/2)-(/T)  Du? 

Recalling  {17.3.21],  this  can  be  written 

T-  >> y,-14,  =  (1/2)S,(1)  —  (1/2)(1/T)  2 u?. 

. 

[17.3.25] 

T 

r=1 

T 

T 

r=! 

T 

But  (1/7)  =7_,u2  &  o?, by  the  law  of  large  numbers,  and  S;(1)  >  o?[W(1)f, 
by [17.3.10].  It thus  follows  from  [17.3.25]  that 

i; |  . yp  14) (1/2)o07{[W(1)]?  —  (1/2)o7. 

[17.3.26] 

1,  has  a 
Recall  that  W(1),  the  value  of  standard  Brownian  motion  at  date  r  = 
N(0,  1) distribution,  meaning  that  [W(1)]? has a y7(1) distribution.  Result  [17.3.26] 
is therefore  just another  way to express  the earlier  result  [17.1.15]  using a functional 
on  Brownian  motion  instead  of the  y? distribution. 

17.4.  7S  oor Properties of ¢ a iaaede 
Autoregression  when  the  True  Coefficient  Is  Unity 

We  are  now  in a position  to  calculate  the  asymptotic  distribution  of some  simple 
regressions  involving  unit  roots.  For convenience,  the  results  from  Section  17.3  are 
collected  in the  form  of a  proposition. 

Proposition  17.1:  Suppose  that  &, follows  a  random  walk  without  drift, 

é, —  €;- 1  +  U,, 
where  & =  0 and  {uj is an  i.i.d.  sequence  with  mean  zero  and  variance  a. Then 

(a)  T-'  > thy ats W(1)  [17.3.7]; 

(b)  T-!  > iin, wae =  tke. 

[07.3:26); 

(c) T-#  es Mu, 0  WE)  es fo Wr) dr 

[17.3.19}; 

(d)  ray =  a j. Wr) dr 

[17.3.16}; 

(e)  nS ga.  oS ga [, [Wp ar  [17.3.2]: 

Ch  aaa  2 1, mr o-| rW(r)  dr 

T 

(= 

Ki 

(=a 

; 

! 

1 

@).  T-?  x é2_,  > o-| r-[W(r)}?  dr 

—‘[17.3.23]; 

[17.3.24]; 

(h)  oerat  Da w—>Wv+  1) 

forv=0,1,...  — [16.1.15}. 

486  Chapter 17  | Univariate  Processes  with Unit Roots 

The  expressions  in  brackets  indicate  where  the  stated  result  was  earlier  de- 
rived.  Though  the  earlier  derivations  assumed  that  the  initial  value  € was  equal 
to  zero,  the  same  results  are  obtained  when  &, is any  fixed  value  or  drawn  from  a 
specified  distribution  as  in  Phillips  (1987). 

The  asymptotic  distributions  in  Proposition  17.1  are  all  written  in  terms  of 
functionals  on  standard  Brownian  motion,  denoted  W(r).  Note  that  this  is the  same 
Brownian  motion  W(r)  in  each  result  (a)  through  (g),  so  that  in  general  the  mag- 
nitudes  in Proposition  17.1  are  all  correlated.  If we  are  not  interested  in capturing 
these  correlations,  then  there  are  simpler  ways  to  describe  the  asymptotic  distri- 
butions.  For  example,  we  have  seen  that  (a)  is just  a N(0,  a7)  distribution,  (b)  is 
(1/2)o?-[y?(1)  —  1], and  (c) and  (d) are  N(0,  0/3).  Exercise  17.1  gives  an  example 
of one  approach  to  calculating  the  covariances  among  random  variables  described 
by these  functionals  on  Brownian  motion. 

Proposition  17.1  can  be  used  to  calculate  the  asymptotic  distributions  of 
Statistics  from  a  number  of  simple  regressions  involving  unit  roots.  This  section 
discusses  several  key cases. 

Case  1.  No  Constant  Term  or  Time  Trend  in  the  Regression; 
True  Process  Is  a Random  Walk 

Consider  first  OLS  estimation  of p based  on  an  AR(1)  regression, 

y  sais  PYr-1  +r U,, 
where  u, is i.i.d.  with  mean  zero  and  variance  a”.  We are  interested  in the properties 
of the  OLS  estimate 

[17.4.1] 

. 

pp =  = 

[17.4.2] 

when  the  true  value  of p is unity.  From  [17.1.6],  the  deviation  of the  OLS  estimate 
from  the  true  value  is characterized  by 

a 

‘g 

Hel  £2 Y,-  14, 
PGP  2  Bp os  ee 
ike  na vet 
t=1 

rn 

[17.4.3] 

If the  true  value  of p is unity,  then 

y,  =  Yo  +  u,  +  u>  ied 

haat. 

a  U,. 

[17.4.4] 

Apart  from  the  initial  term  y,  (which  does  not  affect  any  of the  asymptotic  distri- 
butions),  the  variable  y, is the  same  as  the  quantity  labeled  & in Proposition  17.1. 
From  result  (b) of that  proposition, 

ee  > y,-1M, > (1/2)04[W()P  —  1, 

[17.4.5] 

while  from  result  (e), 

? 

1 

T~?  2 Vee Et o| [W(r)}?  dr. 

[17.4.6] 

Since  [17.4.3]  is a  continuous  function  of  [17.4.5]  and  [17.4.6],  it follows  from 
Proposition  7.3(c)  that  under  the  null  hypothesis  that  p  =  1, the  OLS  estimate 

17.4.  Asymptotic  Properties  of a First-Order  Autoregression 

487 

6,  is characterized  by 

T(pr  he  1) 

gee 

:  [W(r)]}?  dr 

Recall  that  [W(1)]?  is a  x2(1)  variable.  The  probability  that  a  y7(1)  variable 
is  less  than  unity  is 0.68,  and  since  the  denominator  of  [17,4.7]  must  be  positive, 
1  is  negative  approaches  0.68  as  T becomes  large.  In 
the  probability  that  6,  — 
other  words,  in two-thirds  of the  samples  generated  by a random  walk,  the  estimate 
fr will  be  less  than  the  true  value  of unity.  Moreover,  in  those  samples  for  which 
[W(1)}?  is large,  the  denominator  of [17.4.7]  will  be  large  as  well.  The  result  is that 
the  limiting  distribution  of  7(6;  —  1) is skewed  to  the  left. 

Recall  that  in the  stationary  case  when  |p| <  1, the  estimate  6, is downward- 
biased  in  small  samples.  Even  so,  in  the  stationary  case  the  limiting  distribution 
of  VT(6;  —  p) is symmetric  around  zero.  By contrast,  when  the  true  value  of p 
is  unity,  even  the  limiting  distribution  of  7(6;  —  1) is asymmetric,  with  negative 
values  twice  as  likely  as  positive  values. 

| 

In  practice,  critical  values  for  the  random  variable  in  [17.4.7]  are  found  by 
calculating  the  exact  small-sample  distribution  of 7(67  —  1) for  given  T, assuming 
that  the  innovations  {u,} are  Gaussian.  This  can  be done  either  by Monte  Carlo,  as 
in  the  critical  values  reported  in  Fuller  (1976),  or  by using  exact  numerical  pro- 
cedures  described  in  Evans  and  Savin  (1981).  Sample  percentiles  for  T7(6,;  —  1) 
are  reported  in the  section  labeled  Case  1 in Table  B.5  of Appendix  B.  For  finite 
T,  these  are  exact  only  under  the  assumption  of Gaussian  innovations.  As  T be- 
comes  large, these  values  also describe  the asymptotic  distribution  for non-Gaussian 
innovations. 

It follows  from  [17.4.7]  that  6; is a superconsistent  estimate  of the  true  value 

(p =  1). This  is easily  seen  by dividing  [17.4.3]  by VT: 

: 

T 

ae  > Yr-1  Uy 
VT¢;  -  1) =  —— 

ia  2 Yin 

[17.4.8] 

From Proposition  17.1(b),  the numerator  in [17.4.8] converges  to T~ "7(1/2)o times 
(X  —  1), where  X is a  y*(1)  random  variable.  Since  a  y2(1)  variable  has  finite 
variance,  the  variance  of the  numerator  in  [17.4.8]  is of order  1/T,  meaning  that 
the  numerator  converges  in probability  to zero.  Hence, 

/T1hs — 1) 

0. 

~ 

Result  [17.4.7]  allows  the  point  estimate  67 to  be  used  by itself  to  test  the 
null  hypothesis  of  a  unit  root,  without  needing  to  calculate  its  standard  error. 
Another  popular  statistic  for testing  the  null  hypothesis  that  p =  1 is based  on  the 
usual  OLS ¢ test  of this  hypothesis, 

| 

ja  — Ps 

-  1 

br, 

—  1 

ce  }  a: 

—  ([17.4.9] 

{54 =  Dy vi} 

where  @, is the  usual  OLS standard  error  for the estimated  coefficient, 

of 

m=  {+ Dye} 

172 

t= 

488  Chapter 17  | Univariate  Processes  with  Unit Roots 

and  s} denotes  the  OLS  estimate  of  the  residual  variance: 

2 

‘ 

: 

' 

7 

Ss} 

=  py, (y,  —  bry,-:)?(T  — 

1). 

sv 

Although  the  ¢ statistic  [17.4.9]  is calculated  in  the  usual  way,  it does  not  have  a 
limiting  Gaussian  distribution  when  the  true  process  is characterized  by p  =  1. To 
find  the  appropriate  limiting  distribution,  note  that  [17.4.9]  can  equivalently  be 
expressed  as 

tr  =  T(pr  -  of T?>  yi}  +  {s3}!2, 

r 

1/2 

or,  substituting  from  [17.4.3], 

=] 

T 

i Ng  > Yi, -14, 
7 aieamer Weeriac  | meaene 

tr = 

{r-* > vt} {57}! 

[17.4.10] 

(17.4.11] 

As  in Section  8.2,  consistency  of 6, implies  s3-—>  a.  It follows  from  [17.4.5]  and 
[17.4.6]  that  as  T—  ~, 

5  WDeXMOP  = 1) 

CAMWOP =  ayy 

{0 [ [W(r)}?  ar} {a*}'2  {f [W(r)]}?  ir} 

Statistical  tables  for  the  distribution  of  [17.4.11]  for  various  sample  sizes  T are 
reported  in the  section  labeled  Case  1 in Table  B.6;  again,  the  small-sample  results 
assume  Gaussian  innovations. 

Example  17.3 
The  following  AR(1)  process  for  the  nominal  three-month  U.S.  Treasury  bill 
rate  was  fitted  by OLS  regression  to  quarterly  data,  ¢ =  1947:II  to  1989:I: 

with  the  standard  error  of 6 in parentheses.  Here  T  =  168  and 

i, =  0.99694  i,_,, 
(0.010592) 

~ 

[17.4.13] 

T(6  —  1) =  (168)(0.99694  —  1) =  —0.51. 
The distribution  of this statistic  was  calculated  in [17.4.7] under  the assumption 
that  the  true  value  of p is unity.  The  null  hypothesis  is therefore  that  p  =  1, 
and  the  alternative  is that  p <  1.  From  Table  B.5,  in  a  sample  of this  size, 
95%  of the  time  when  there  really  is a  unit  root,  the  statistic  7(6  —  1) will 
|  be above  —7.9.  The  observed  value  (—0.51)  is well  above  this,  and so the  null 
hypothesis  is accepted  at the  5%  level  and  we  should  conclude  that  these  data 
might well  be described  by a  random  walk. 

In order  to  have  rejected  the  null  hypothesis  for  a  sample  of this  size, 

the estimated  autoregressive  coefficient  6 would  have  to  be less  than  0.95: 
168(0.95  — 1) =  -8.4. 

The OLS t test of Ho: p  =  1is 

: 
1 =  (0.99694  —  1)/0.010592  =  -0.29. 
:
This  is well  above  the  5%  critical  value  from  Table  B.6 of  —  1.95,  so  the  null 

| 

! 

. 

17.4.  Asymptotic  Properties  of a  First-Order  Autoregression 

489 

 
hypothesis  that  the  Treasury  bill  rate  follows  a  random  walk  is also  accepted 
by this  test. 

The  test  statistics  [17.4.7]  and  [17.4.12]  are  examples  of the  Dickey-Fuller  test 
for  unit  roots,  named  for  the  general  battery  of tests  proposed  by Dickey  and  Fuller 
(1979). 

Case  2.  Constant  Term  but  No  Time  Trend  Included 
in  the  Regression;  True  Process  Is  a Random  Walk 

For  case  2, we  continue  to  assume,  as  in case  1, that  the  data  are  generated 

by a  random  walk: 

yr  ss  Je=1  mo U,, 

with  u, i.i.d.  with  mean  zero  and  variance  ao”.  Although  the  true  model  is the  same 
as  in case  1, suppose  now  that  a constant  term  is included  in the AR(1)  specification 
that  is to  be  estimated  by OLS: 

: 

y,  =  a@  +  py,_,  +  U,- 

[17.4.14] 

The  task  now  is to  describe  the  properties  of the  OLS  estimates, 

ba 

Pr 

ZY,  4.  2Yi-1 

2-11 

under  the  null  hypothesis  that  a  =  0 and  p  =  1 (here  > indicates  summation  over 
t=  1,2,...,  1).  Recall  the  familiar  characterization  in  [8.2.3]  of the  deviation 
of an  estimated  OLS  coefficient  vector  (b;)  from  the  true  value  (B), 

b;  —  B =  > xxi | > x 

(17.4.16] 

or,  in  this  case, 

oN 
az 

T 

= 
Yeu} 
: 

= 

Du 
t 

‘ 

: 
5 a3 | acs Mes  Fea 

= 

7 
As  in case  1, y, has  the  same  properties  as  the  variable  &, described  in Prop- 
osition  17.1  under  the  maintained  hypothesis.  Thus,  result  (d) of that  proposition 
establishes  that  the  sum  Ly,_,  must  be divided  by T??  before  obtaining  a  random 
variable  that  converges  in distribution: 

U 

7.4.1 

T-22Xy,_,>  0: | —W(r) dr. 

1 

[17.4.18] 

In other  words, 

2y,-,  =  O,(T**). 

Similarly,  results  [17.4.5]  and  [17.4.6]  establish  that 
' 

Ly,-u,  =  O,(T) 

and  from Proposition  17.1(a), 

Ly?)  =  O,(T?), 

zu,  =  O,(T'”). 

490  Chapter  17  | Univariate  Processes  with  Unit Roots 

Thus,  the  order  in probability  of the  individual  terms  in [17.4.17]  is as  follows: 

ay  |- | O,(1)  sak  ts  (17.4.19} 

O,(T)  . 

O(F  4  O,(T?) 

r= 

Pr  —  ] 

- 

It is clear  from  [17.4.19]  that  the  estimates  @, and  f,; have  different  rates 
of convergence,  and  as  in  the  previous  chapter,  a  scaling  matrix  Y is helpful  in 
describing  their  limiting  distributions.  Recall  from  [16.1.18]  that  this  rescaling  is 
achieved  by premultiplying  [17.4.16]  by  Y  and  writing  the  result  as  - 

Yr(b,  —  B)  =  vf 3 xxi Xp¥r  > su 

- {vif Saxifust} {ve {S «]} 

[17.4.20] 

From  [17.4.19],  for  this application  Y; should  be specified  to  be  the  following 
matrix 

eee 
aes  ce) 
¥-  | '  ‘ 

ae 

[17.4.21] 

for which  [17.4.20]  becomes 

: 

: 

Bs 

Petia 
0  cs | br -  ke oleh. | 

OTP 

<e  Say  [2-2 

o  53 

0  a  Zar  Weald  0  Tat  ‘ 

SG  ge 

one 

BY  34:  x{[70 ~ | 

Ley  al } : 

¢ 

x4 

sen?  | 
t 

‘ 
¢ 

the  second  term  in  [17.4.22]: 

T~'Su,  |4 

ao: W(1) 

Piast 

(1/2)o2{{W(1)}?  -— 
i ai  W(1)  | 

“10  o} 

La2xtwaeP  -  Bd 

Substituting  [17.4.23]  and  [17.4.24]  into  [17.4.22]  establishes 

Toa 
T(67  —  1) 

edly 
ee 
oe SEB  og 

| 

J me xf 
{ we)  dr  { wor  dr 

1  ol] 
X10  of 

{1  ai  W(1) 
[0 

of  }anr{wwP - 3 

oi 

pores 

[17.4.25] 

se lc )  l 

{ mo)  dr 

ey,  { W(r) dr  | [W(r)]?  dr 

ie (1/2){[W(L)P  -  Wy 

Notice  that 

W(r)  d. 
J _ 

| W(r)  dr  | [W(r))?  dr 

W(r)}?  dr 

W(r)  dr 

Alt "e 

“J 

-} W(r)  dr 

1 

. 

[17.4.26] 

where 

A=  | [W(r)]?  dr  -  I W(r) a " 

[17.4.27] 

Thus,  the  second  element  in the  vector  expression  in [17.4.25]  states  that 

4[w(1)P  —  1} —  W(1): | W(r) dr 

Ti+  =  1)> 

[17.4.28] 

[ wor  dr  —  i W(r) ir] 7 

Neither  estimate  a@, nor  6, has a limiting  Gaussian  distribution.  Moreover, 
the  asymptotic  distribution  of the  estimate  of p in [17.4.28]  is not  the’same  as  the 
asymptotic  distribution  in [17.4.7]—when  a  constant  term  is included  in the  dis- 
tribution,  a different  table  of critical  values  must  be used. 

The  second  section  of Table  B.5  records  percentiles  for  the  distribution  of 
T(r  —  1) for case  2. As in  case  1, the calculations  assume  Gaussian  innovations, 
though  as  T becomes  large,  these  are  valid  for non-Gaussian  innovations  as  well. 

492  Chapter  17  | Univariate  Processes  with  Unit Roots 

Notice  that  this  distribution  is even  more  strongly  skewed  than  that  for  case  1, so 
that  when  a  constant  term  is included  in  the  regression,  the  estimated  coefficient 
on  y,_,  must  be  farther  from  unity  in order  to  reject  the  null  hypothsis of a  unit  > 
root.  Indeed,  for  T >  25, 95%  of the  time  the  estimated  value  6, wil! 2  less  than 
unity.  For example,  if the estimated  value  f, is 0.999 in  a sample  oi ze  T =  100, 
the  null  hypothesis  of p  =  1 would  be  rejected  in  favor  of  the  alternative  that 
p >  1! If the  true  value  of p is unity,  we  would  not  expect  to  obtain  an  estimate 
as  large  as  0.999. 

Dickey  and  Fuller  also  proposed  an  alternative  test  based  on  the  OLS t test 

of the  null  hypothesis  that  p = 

1: 

pr-  1 
: 

Lr=- 
: 

9%, 

where 

42.  =  5210  ul,” 

-1 

2y,- 

é  | H 

0 

we 

rt  oe  Zy?_) 

1 

s} =  (T —  2)"! 

(y, —  &r —  pry,-1)’. 

17.4.29] 

[17.4.30} 

Notice that if both sides of [17.4.30] are  multiplied  by T, the result can  be written 

: 
_ 
x 

. 
T?-62, 
SAE  SG)  neizesrtaiots 

4  tiriae 
> 

e  “4  3 
=  s3[0 ©  Ty,  j  i : 

: 

By fois 4.  0 =<? 

: 

~  [17.4.31) 

“ee 

‘  By yh 

: 

iin (174.21) Recall from [17.4 23] that : 

~ 

i 
4 
} 
. 

.  1) ONIN AEY 01 lpia zieree banged 
co  yreoidsags edi ac} Ase NRE bstasuize: A 

3  Tepe sig a)  a3  ~  hee = 

: 

wisiT2 a  at 2 A AGT 

eae cake dae bil e : 

from  which  [17.4.33]  becomes 

| 

T?-63,  > [0  1] 

| W(r)  dr 

0 

he: 

if W(r)  dr  | [W(r)]*  dr 

1 

:  { wor  dz,  —  | W(r) ar] 

Thus,  the  asymptotic  distribution  of the  OLS ¢ test-in  (17.4.29]  is  | 
tr  =  ara T6,  =  1)  -*  {f tmone  ar  =  | W(r) ir 

yi  oe  wa)-| We)  dr 

2  1 [W(r)]}?  dr  —  i W(r) ar|} 

(17.4.35] 

(17.4.36] 

Sample  percentiles  for  the OLS t test  of p  =  1 are  reported  for  case  2 in  the 
second  section  of Table  B.6.  As  T grows  large,  these  approach  the  distribution in 
the  last  line  of [17.4.36]. 

Example  17.4 
When  a constant  term  is  included  in  the  estimated  autoregression  for  the 
interest  rate  data  from  Example  17.3, the  result is 

i, =  0.211  +  0.96691  7,_,, 

(0.112) 

(0.019133) 

[17.4.37] 

with  standard-errors  reported  in parentheses.  The  Dickey-Fuller  test  based  on 
the  estimated  value  of p for  this  specification  is 

T(6  —  1) =  (168)(0.96691  —  1)  =  —5.56. 

From  Table  B.5,  the  5%  critical  value  is found  by interpolation  to  be  —  13.8. 
Since  —5.56  >  —  13.8,  the  null  hypothesis  of a  unit  root  (p  =  1) is accepted 
at  the  5%  level  based  on  the  Dickey-Fuller  p test.  The  OLS tf statistic  is 

(0.96691  —  1)/0.019133  =  —1.73, 

which  from  Table  B.6  is to  be  compared  with  —  2.89.  Since  —1.73  >  —2.89, 
the  null  hypothesis  of a  unit  root  is again  accepted. 

These  statistics  test  the  null  hypothesis  that  p =  1.  However,  a  maintained 
assumption  on  which  the  derivation  of [17.4.25]  was  based  is that  the  true  value 
of  @  is  zero.  Thus,  it  might  seem  more  natural  to  test  for  a  unit  root  in  this 
specification  by testing  the  joint  hypothesis  that  a  =  0 and  p  = 
1.  Dickey  and 
Fuller  (1981)  used  Monte  Carlo  to  calculate  the  distribution  of the  Wald  form  of 
the  OLS F test  of  this  hypothesis  (expression  [8.1.32]  or  [8.1.37]).  Their  values 
are  reported  under  the  heading  Case  2 in  Table  B.7. 

Example  17.5 
The  OLS  Wald F statistic  for testing the  joint hypothesis  that a  =  0 and  p=1 
for  the  regression  in [17.4.37]  is 1.81.  Under  the  classical  regression  assump- 

494  Chapter  17  | Univariate  Processes  with  Unit  Roots 

tions,  this  would  have  an  F(2,  166)  distribution.  In  this  case,  however,  the 
usual  statistic  is to  be  compared  with  the  values  under  Case  2 in  Table  B.7, 
for which  the 5%  critical  value is found  by interpolation  to  be 4.67.  Since  1.81 
= is the  joint  null  hypothesis  that  a  =  0 and  p  =  1 is accepted  at  the  5% 
eve 

) 

Case i 8 Constant  Term  but  No  Time  Trend  Included 
in  the Regression;  True  Process  Is Random  Walk  with  Drift 
In case  3, the  same  regression  [17. 4.14] is estimated  as  in case  2, though  now 

it is supposed  that  the  true  process  is  a random  walk  with  drift: 

y=  arty;  +  U,, 

[17.4.38] 

where  the true  value  of a is not zero.  Although this might seem  like a minor change, 
it has  a radical  effect  on  the  asymptotic distribution  of @ and  #. To  see  why,  note 
that  [17.4.38]  implies  that 

| 

: 

¥, =  Yo  +  at  +  (u,  tu,  t-+>  4+ i =  yo  +  at  +  E,,  en 

Mor Mist uy  +  ee: uy 

for t  =  be Zap’ co Rae 

"Consider the belive of the sum 

if ioe  ae  6 ae ESA 

oti)  bE bin! ci i Pest mo 

Similarly,  we  have  that 

Sy  = Vo tae  yee 

1=1 

Se  > a(t  —  1)? +  ee 
a  eee  eee 
OMT) 
OAT) 

0,7) 

wf 

id 

a 

T 

+  2 2yyat  "~  1) rte x 2yogs-1  +  py 2a(t  =  1)é,_). 

OMT) 

| 

OAT) 

* 

O,(T*) 

When  divided by T*, the only  term that  ors not  vanish  Sores is that  pa 
to  the  time  trend  a2(t  —  1): 

T 

: 

T-3  > y?_,  > a. 

plivalivastecae  that. Gis 

. 

; 

‘ 

2 

Pp  a 

A 

< 

1=1 

: 

Bese 

be 
——  [17.4.42] 
TELS 

ae 2, by Yo  + alt = -1)- + fa 

. 
eee. 

Nee ae 

oes  SER) T we 
a 

T 
pede, 
2 ie Du i Py GM) 

= i 

BAT 

| 

4a 

he 
Spe 

Pe 
a 

ase 

Snag 

OA 

eee 

* 

or 

T'*(@;  —  @) 

42/6 

l 

™  |  p~2 

T  (pr  y  1) 

T  > | 

(ibe 8  le 
a 
T  “Ly*  i 

"T 

T-'23y 

r  ier >, AE 

' 

1. 

[17.4.44] 

From  [17.4.41]  and  [17.4.42],  the  first  term  in  [17.4.44]  converges  to 

Pinay) 
Tey,  T° *Zy?2.; 

of 

i 
al2 

a2] 
a?’/3 

ashy 6 

[17.4.45] 

From  [17.4.43]  and  [17.3.18],  the  second  term  in  [17.4.44]  satisfies 

| : Piao > ie  sa 
Trey.  it: 

— 

T-'23u, 
T ~*?Za(t  —  1)u, 

L 

0 

l 

a/2 

N 

eo 

33 (8 ae os]) 
=  N(0,  7Q). 

Combining  [17.4.44]  through  [17.4.46],  it follows  that 

T'*(a@, — a) ] ce 
%; 
5 a  ot ra —  N(O,  Q-'-0?Q-Q-')  =  N(O,  o7Q-'). 

2 

nen 

> 

A. 
ies 

[17.4.47] 

Thus,  for  case  3, both  estimated  coefficients  are  asymptotically  Gaussian.  In 
fact,  the  asymptotic  properties  of a; and   ; are  exactly  the  same  as  those  for  a7 
and  6; in  the  deterministic  time  trend  regression  analyzed  in  Chapter  16.  The 
reason  for  this  correspondence  is very  simple:  the  regressor  y,_,  iS asymptotically 
dominated  by the  time  trend  a-(¢  —  1). In large  samples,  it is as  if the explanatory 
variable  y,_,  were  replaced  by the time  trend  a-(t  —  1). Recalling  the  analysis  of 
Section  16.2,  it follows  that  for  case  3, the  standard  OLS  t and F statistics  can  be 
calculated  in the  usual  way  and  compared  with  the standard  tables  (Tables  B.3  and 
B.4,  respectively). 

Case  4.  Constant  Term  and  Time  Trend  Included 
in the  Regression;  True  Process  Is Random  Walk 
With  or  Without  Drift 

Suppose,  as  in the  previous  case,  that  the  true  model  is 

ya 

&  +  yr-1  of U,, 

where  u, is i.i.d.  with  mean  zero  and  variance  o?.  For  this  case,  the true  value  of 
a turns  out  not  to matter  for the asymptotic  distribution.  In contrast  to the previous 
case,  we  now  assume  that  a time  trend  is included  in the  regression  that  is actually 
estimated  by OLS: 

y,  =  a  +  py,-,  +  St  +  u,. 

[17.4.48] 

If a  #  0, y,_,  would  be  asymptotically  equivalent  to  a  time  trend.  Since  a  time 
trend  is already  included  as  a separate  variable  in the  regression,  this  would  make 
the  explanatory  variables  collinear  in  large  samples.  Describing  the  asymptotic 

17.4. Asymptotic Properties  of a First-Order  Autoregression 

497 

distribution  of the  estimates  therefore  requires  not  just  a  rescaling  of variables  but 
also  a  rotation  of  the  kind  introduced  in  Section  16.3. 

Note  that  the  regression  model  of  [17.4.48]  can  equivalently  be  written  as 

y,  =  (1  —  p)a  +  ply,-1  —  a(t  —  1)]  +  (6 +  pat  +  u, 
| 

sa"  +  p*é_»  +  Ot  +  u,, 

(17.4.49] 

where  a*  =  (1  —  p)a,  p*  =p,  &*  =  (6  + pa),  and  é, =  y,  —  at.  Moreover,  under 
the  null  hypothesis  that  p =  1 and  6  =  0, 

&,=  Yo  t+ Uy  tun  tees  +  Uy, 

that  is, €, is the  random  walk  described  in Proposition  17.1.  Consider,  as  in Section 
16.3,  a hypothetical  regression  of y, on  a constant,  €,_,,  and  a time  trend,  producing 
the  OLS  estimates 

ar 

pe) 
53 

TF 

Be, 

rt 

ig 

zy, 

=] 

2E-,  V7,  VE] 
Sr 
AVE 8 
St 

| ZE-wI- 
Lty, 

[17.4.50] 

ay,  p  =  1,  and  6  =  O,  which  in  the 
The  maintained  hypothesis  is  that 
transformed  system  would  mean  a*  =  0, p*  =  1, and  5*  =  ap.  The  deviations  of 
the  OLS  estimates  from  these  true  values  are  given  by 

a  = 

fe 

i  2. 

.4 

Fa  2 

pr  =! 
5% —  ay 

= 

2E,_|  Le?  2G, _  it 

pga 

> 5 me  aa <a 

LE, _  1,  s 
2tu, 

[17.4.51] 

Consulting  the rates  of convergence  in Proposition  17.1, in this case  the scaling 

matnx  should  be 

FSi;  Ou 

Y;  = 

0 
G 

i 
0 
py  Fe 

’ 

and  [17.4.20]  would  be 

T'2 

. 

90 

7; 

O 

 v 

ay 

pr  —  1 

patil 

cite 

by. —  & 

Tye 

¢% 

a, 

0 

gt 

8 “n  afin 

0 

0 

T 

Be,  5 

Zt 

>> aa  2e?_ | 

2E,_ it 

bie  bts 

Mssltbiws  4 2G 

T-'2 

rim 
0 

Q 
tT! 
0 

0 
4 
7-3 

FR, 
Oo) 

.G 
Tae 

0 
‘wipe 

Lu, 
XE,_ U4, 

- 

498  Chapter 17  | Univariate  Processes  with  Unit Roots 

1 

| i SOY ak” 

ee? Sa >  Be” 
Frege  Tye  Pe 

[17.4.52] 

| 

T”  2p, 

T "2, tt, 
TYE mu, 

The  asymptotic  distribution  can then  be found  from  Proposition  17.1: 

T'*a; 
T(t  —  1) 

LG —  a) | 

I 

o| wir) a po 

fest 

Wag 

o| Woe) dr oe = (WP dr  o hy rW(r) dr | 

, 

,~ 

"TF 

te 

o> 

é 

t 

é 

es 

P 

»  i 

> | 

af 

- 
ivvGe  iE 

rar 

» 

. 

ed 

- 

5 

a 

4 

é aif | abn 

y]  To. 

“gl 

i  4 2»  GS 

A 

| 

)  £4 

/siggieate  ses 

sate 

Note  that  67,  the  OLS  estimate  of p  based  on  [17.4.49],  is identical to Pr, 
the  OLS  estimate  of  p  based  on  [17.4.48].  Thus,  the  asymptotic distribution  of 
T(p;  —  1) is given  by the  middle  row  of [17.4.53].  Note  that  this  distribution  does 
not  depend  on  either  o  or  a;  in  particular,  it does  not  matter  whether  or  not  the 
true  value  of a  is zero. 

The  asymptotic  distribution  of  6,,,  the  OLS  standard  error  for  p7,  can  be 

found  using  similar  calculations  to  those  in  [17.4.31]  and  [17.4.32].  Notice  that 

T*65  =  WPesti0. 

1  OSE on 5 Deseo  t Sheeil 

7 

BE,  3 

Dt 

-1 

s2{0. 

1... 

C1. 

or 

St; 
T'2  0 

Bier 
07 
cMicieataeet) 
0 

0  T# 

0 

1 
0 

F  LE,_  | 

at 

X  {264  peep  (SE —sf 
Se  eh  (Oy 

FX4)04 
9 
0 

1.0 
0 
<£-40 
1 
0  T”)(0 

=  570  1:  0] 

1 

T-32y~  OT -2y 
Kyl  aay  ee  CO ee  H 

0 

TS 

T ~*2ye  _, 

fata 

: 
>a 0  1  OJ]/0  ao  O 
00  1 

tp  ue 

[17.4.54] 

: 

1 

[war 

: 

10  a'h 

‘  [ ww dr  | [W(r)]?  dr  | rW(r)  dr  f 4 j | 

4 

| rW(r)  dr 

4 

i 

[ wo  a 

3 

01. 

= 

=  Q. 

of the  hypothesis  that  p  =  1 is given  by 

1  Of weydr  [tworp dr  [wc a  H . 
bref  we  10 
eS 3 
From  this result  it follows  that  the asymptotic  distribution of the OLS t test 
. 
tr =  T(br  —  1) +  (T?+63,)'?-4  TG,  -  1) +  VO, 
[174.55] 
Again,  this distribution  does  not  depend  on  a or  o.  The small-sample  distribution 
of the  OLS ¢ statistic  under  the  assumption  of Gaussian  disturbances  is presented 
under case 4 in Table  B.6.  If this distribution  were  truly ¢, then  a value  below  —2.0 
would be sufficient  to reject the null  hypothesis.  However,  Table 
B.6 reveals  that, 
_  because  of the nonstandard  distribution,  the ¢ statistic  must be below  —3.4 before 
the null hypothesis  of a unit  root  could  be rejected. 
500  Chapter 17 | Univariate  Processes  with  Unit Roots 

hae 

| 

eS 

The assumption  that  the  true  value  of 8 is equal  to  zero  is again  an  auxiliary 
hypothesis  upon  which  the  asymptotic  properties  of  the  test  depend.  Thus,  as  in 
case  2,  it  is  natural  to  consider  the  OLS F test  of  the  joint  null  hypothesis  that 
5 = 0 and p =  1.  Though  this  F test  is calculated  in  the  usual  way,  its  asymptotic 
distribution  is  nonstandard,  and  the  calculated  F statistic  should  be  compared  with 
the  value  under  case  4 in Table  B.7. 

Summary  of Dickey-Fuller  Tests  in  the  Absence 
of Serial  Correlation 

We  have  seen  that  the  asymptotic  properties  of  the  OLS  estimate  6, when 
the  true  value  of p is unity  depend  on  whether  or  not  a  constant  term  or  a  time 
trend  is  included  in  the  regression  that  is estimated  and  on  whether  or  not  the 
random  walk  that  describes  the  true  process  for  y,  includes  a  drift  term.  These 
results  are  summarized  in Table  17.1. 

Which  is the  ‘“‘correct”’  case  to  use  to  test  the  null  hypothesis  of a  unit  root? 
The  answer  depends  on  why  we  are  interested  in  testing  for  a  unit  root.  If the 
analyst  has  a  specific  null  hypothesis  about  the  process  that  generated  the  data, 
then  obviously  this would  guide  the choice  of test.  In the  absence  of such  guidance, 
one  general  principle  would  be  to  fit  a specification  that  is a  plausible  description 
of the data  under  both  the  null  hypothesis  and the  alternative.  This  principle  would 
suggest  using  the  case  4 test  for  a  series  with  an  obvious  trend  and  the  case 2 test 
for series  without  a  significant  trend. 

For  example,  Figure  17.2  plots  the  nominal  interest  rate  series  used  in  the 
examples  in this  section.  Although  this  series  has  tended  upward  over  this  sample 
period,  there  is nothing  in economic  theory  to  suggest  that  nominal  interest  rates 
should  exhibit  a  deterministic  time  trend,  and  so  a  natural  null  hypothesis  is that 
the  true  process  is  a random  walk  without  trend.  In  terms  of framing  a  plausible 
alternative,  it is difficult  to  maintain  that  these  data  could  have  been  generated  by 
i, =  pi,_,  +  u, with  |p| significantly  less  than  1. If these  data  were  to  be described 
by a stationary  process,  surely  the  process  would  have a positive  mean.  This  argues 
for  including  a  constant  term  in  the  estimated  regression,  even  though  under  the 
null  hypothesis  the  true  process  does  not  contain  a  constant  term.  Thus,  case  2 is 
a sensible  approach  for  these  data,  as  analyzed  in  Examples  17.4  and  17.5. 

‘As  a  second  example,  Figure  17.3  plots  quarterly  real  GNP  for  the  United 
States  from  1947:1  to  1989:1.  Given  a  growing  population  and  technological  im- 
provements,  such a series  would  certainly  be expected  to exhibit  a persistent  upward 
trend,  and  this trend  is unmistakable  in  the  figure.  The  question  is whether  this 
trend arises  from the positive  drift  term  of a  random  walk: 

Hy:  y, =  at  y.-,+u, 

a>d, 

:  or  from  a deterministic  time  trend  added  to  a stationary  AR(1): 

H,:y,  =  a+  6t+py,,+u, 

|p| <1. 

Thus,  the  recommended  test  statistics  for this  case  are  those  described  in case  4. 

The  following  model  for  100  times  the  log of  real  GNP  (denoted  y,) was 

estimated  by OLS regression: 

y,  =  27.24  +  0.96252  y,_,  +  0.02753  t. 

(13.53) 

(0.419304) 

(0.01824) 

[17.4.56] 

(standard  errors  if parentheses).  The  sample  size  is T =  168.  The  Dickey-Fuller  . 

17.4.  Asymptotic Properties  of a First-Order  Autoregression 

501 

p test  is 

T(6  —  1)  =  168(0.96252  —  1.0)  =  —6.3. 

Since  —6.3  >  —21.0,  the  null  hypothesis  that  GNP  is characterized  by a  random 
walk  with  possible  drift  is accepted  at  the  5%  level.  The  Dickey-Fuller  f test, 

"-  O.96252  2" 
forme 

6 1051” pa cath 

—  1.94, 

exceeds  the  5%  critical  value  of  —3.44,  so  that  the  null  hypothesis  of a  unit  root 
is accepted  by this  test  as  well.  Finally,  the  F test  of the  joint  null  hypothesis  that 
5 =  0 and  p  =  1 is 2.44.  Since  this  is less  than  the  5%  critical  value  of 6.42  from 
Table  B.7,  this  null  hypothesis  is again  accepted. 

TABLE.17.1 
Summary  of Dickey-Fuller  Tests  for  Unit  Roots  in the  Absence 
of Serial  Correlation 

Case. i 

Estimated regression:  y,  =  py,_,  +  U, 
True  process:  y,  =  y,_, 
T(67  —  1) has  the  distribution  described  under  the  heading  Case  1 in Table 

 -u,  ~  i.i.d.  N(0,  a?) 

+  u, 

| 

B.5. 

(67  —  1)/6,,  has  the  distribution  described  under  Case  1 in Table  B.6. 

Case  2: 

Estimated  regression:  y,  =  a  +  py,_,  +  U, 
True  process:  y,  =  y,_;  tu, 
T(67  —  1) has  the  distribution  described  under  Case  2 in Table  B.5. 
(67  —  1)/6,, has  the  distribution  described  under  Case  2 in Table  B.6. 
OLS F test  of joint  hypothesis  that  a  =  0 and  p  =  1 has  the  distribution 
=" 

u,  ~  i.i.d.  N(0,  a?) 

described  under  Case  2 in Table  B.7. 

ay 

Case  3: 

Estimated  regression:  y,  =  a  +  py,_,  +  U, 
True  process:  y 
(67  —  1)/6,,  = N(O,  1). 

=a  +y,,+u, 

a  #0,u,~  iid.  (0, 2) 

Case  4: 

Estimated  regression:  y,  =  a +  py,_,  +  5t  +  u, 
True  process:  y,  =  a  +  y,,;  +  u, 
T(67  —  1) has  the  distribution  described  under  Case  4 in Table  B.S. 
(67 —  1)/6,, has  the  distribution  described  under  Case  4 in Table  B.6. 
OLS F test  of joint  hypothesis  that  p  =  1 and  5  =  0 has  the distribution 

@any,  u, ~  i.i.d.  N(0,  o) 

| 

| 

described  under  Case 4 in Table  B.7. 
Notes  to  Table  17.1  © 

___ 

Estimated  regression  indicates  the  form  in which  the  regression  is estimated,  using observations 

t=  1,2,...,  T and  conditioning  on  observation ¢ =  0. 

True process  describes  the null hypothesis  under  which  the distribution  is calculated. 
p, is the  OLS  estimate  of p from  the  indicated  regression  based  on  a sample  of size  T. 
(py  —  1)/6,, is the  OLS  test  of  p =  1. 
- 
OLS F test  of a hypothesis  involving  two  restrictions  is given  by expression  [17.7.39]. 
If u, ~  i.i.d.  N(O,  7),  then  Tables  B.5  through  B.7  give  Monte  Carlo  estimates  of the  exact 
small-sample  distribution.  The  tables  are  also  valid  for  large  T when  u, is non-Gaussian  i.i.d.  as well 
2 i eb eb pg  eeOey distributed  serially  uncorrelated  processes.  For serially correlated  u,, see 
a 
TG 

or 

17.3. 

.2 

} 

-_ 

502  Chapter 17  | Univariate  Processes  with  Unit Roots 

———— 

7  .5t 

*Ss 

59 

63 

67 

a  Se  a 

2  comme 

17.2 U. s. nominal interest aia en 3-month Treasury  bills, data sampled 

| at an 1  annu al  rate, oo sie 

| 

Of  the  tests  discussed  so  far,  those  developed  for  case  2 seem  appropriate 
for  the  interest  rate  data  and  the  tests  developed  for  case  4 seem  best  for  the  GNP 
data.  However,  more  general  tests  presented  in  Sections  17.6  and  17.7  are  to  be 
preferred  for  describing  either  of  these  series.  This  is  because  the  maintained 
assumption  throughout  this  section  has  been  that  the  disturbance  term  u,  in  the 
regression  is i.i.d.  There  is no  strong  reason  to  expect  this  for  either  of these  time 
series.  The  next  section  develops  results  that  can  be  used  to  test  for  unit  roots  in 
serially  correlated  processes. 

17.5.  Asymptotic  Results  for  Unit  Root  Processes 
with  General  Serial  Correlation 

This  section  generalizes  Proposition  17.1  to  allow  for  serial  correlation.  The  fol- 
lowing  preliminary  result  is quite  helpful. 

Proposition  17.2: 

Let 

u,  =  W(L)e,  =  D> WE,  js 

jaa 

[17.5.1] 

E(e,)  =  0 

fort=T 
o* 
a  ib  ‘8  otherwise 

y ily <~. 
= 

[17.5.2] 

where 

Then 

u;  =  uz  tree  +  “=  W(1)-(e,  «te  €2  $+  ss)  4-  E,)  +  h  —  Nhs 

[17.5.3] 

where  (1)  =  2Zi=0%,  bee  20 QE,  js  Qa;  =  —  (Wai  =  Wi 42  =  Wi+3  =  os  ), and 
Zj-0lq;|  es 

am  The condition  in [17.5.2] is slightly stronger than absolute  summability,  though 
it Is satisfied  by any stationary  ARMA  process. 

Notice  that  if y, is an  /(1)  process  y, whose  first  difference  is given  by u,,  or 

then 

Ay,  =  Uu,, 

+  uu, +  Yy  =  P(1)-(e,  T  &. + **>  +e  m%  — 

y= Uy tug  tees 
No  *  Yo- 
Proposition  17.2  thus  states  that  any  / (1) process  whose  first  difference  satisfies 
[17.5.1]  and  [17.5.2]  can  be  written  as  the  sum  of a  random  walk  (W(1)-(e,  + 
&  +  °°  tk €,)),  initial  conditions (y,  —  m),  and  a  stationary  process  (7,).  This 
observation  was  first  made  by Beveridge  and Nelson  (1981),  and  [17.5.3]  is some- 
times Baty? to  as  the  Beveridge-Nelson  decomposition. 
2  Otice  that  7, is a stationary  process.  An important  implication  of this i 
if [17.5.3]  is divided  by V1, only the  first  term  (UViyw(1)-(e,  +  &  +-::  <a 
should  matter  for the  distribution  of (W/V1) «(ua  +  Un  +  +++  +  u)ast—  &, 
As an  example  of how  this result  can  be used, suppose  that X;,(r) is,defined 

504°  Chapter 17 | Univariate Processes with Unit Roots 

as in [17.3.2]: 

X7(r)  =  (1/T)  2 U,, 

Tr|* 

[17.5.4] 

where  u, satisfies  the  conditions  of Proposition  17.2  with  ¢, i.i.d.  and  E(e4)  <  &. 
Then  the  continuous-time  process  VT: X,(r)  converges  to  a:  y(1)  times  standard 
Brownian  motion: 

VT-X,(-)  > o- W(1)-W(-). 

[17.5.5] 

To derive  [17.5.5],  note  from  Proposition  17.2  that 

VT-X_(r) 

wvt)- 3, Uu, 

. 
=  w(1)  (VT):  > E,  +  (W/VT) +  (17  —  mm) 

[Tr]* 

[17.5.6] 

=  W)-(IVT): 3, &, +  S40), 
where we have defined S;(r) =  (1/VT)-(njr--  —  1). Notice  as  in Example 17.2 

(7r|" 

as Sec x, Furthermore, from f17. 3.8], 

S-)S0 

| 

KM  ToS 

ete =e «3 ow). Sas 

*> 

2 |  [17.5.8] 

(a)  TD 

uy, 

{re 

A:  W(]1); 

|  S 

— S —  s 

Ma 'M- u,_j€,—  N(0,  oy) 

il  -_ 

_~ 

fory  MSZ,  PS 

=  ~~ 

1  Ms 

i 

Ul; 

Y; 

FOL 

UT sieati  eres 

S  S !  Ms 

é_\€,>  (12)o-a{{W(L)P  -  1s 

~ 

— 

s tl 

(e)  » Sabin 2 €,- 14a; 

L  Le eestWiige  —  yo} 

forj  =  0 

a 

(1/2){A?-[W(1)}?  —  Wt  +  Mot  M1  +  Y2  tO  F  YR 

for}  S420 =; 

ne. 

F 

i 

(6  oe  a —&_,->  af W(r)  dr; 

(g)  7-32  > tu,_;> {way -  I Wir) ar}  for fj. sMAMBved 
‘ee  ee: 
@  TY  e,5e-)  WOR ar: 

r=1 

0 

‘@  1-8  > ei  bt wf rW(r)  dr; 
cs  oe 
G)  73g Saf) WOOP  ar 

Cy  Fete  S t"—  1/(v  +  1) 

$08 VO  Ay 

ns 

r=] 

Again,  there  are  simpler  ways to describe  individual  results;  for example,  (a) 
is  a N(0,  A?) distribution,  (d)  is  (1/2)oA-[y?(1)  —  1], and  (f)  and  (g)  are  both 
N(0,  A?/3) distributions. 

These  results  can  be  used  to  construct  unit  root  tests  for  serially  correlated 
observations  in two  ways.  One  approach,  due  to  Phillips  (1987)  and  Phillips  and 
Perron  (1988),  is to  continue  to  estimate  the  regressions  in exactly  the  form  indi- 
cated  in Table  17.1,  but  to  adjust  the  test  statistics  to  take  account  of serial  cor- 
relation  and  potential  heteroskedasticity  in the  disturbances,  This  approach  is de- 
scribed  in Section  17.6.  The  second  approach,  due to Dickey  and  Fuller  (1979),  is 
to  add  lagged  changes  of y as  explanatory  variables  in  the  regressions  in  Table 
17.1.  This  is described  in Section  17.7. 

17.6.  Phillips-Perron  Tests for Unit  Roots 

Asymptotic  Distribution  for Case  2 Assumptions  with  Serially 
Correlated  Disturbances 

To illustrate  the basic  idea  behind  the Phillips (1987)  and  Phillips and Perron 
(1988)  tests  for unit  roots,  we  will  discuss  in detail  the treatment  they propose  for 
the  analog  of case  2 of Section  17.4.  After this  case  has  been  reviewed,  similar 

506  Chapter  17  | Univariate  Processes  with Unit Roots 

_Tesults  will  be stated  for  case  1 and  case  4, with  details  developed in  exercises  at 
the end  of the  chapter. 

— 2 of Section  17.4 considered  OLS estimation  of a and p in the  regression 

Yy=atpy,,;t+u, 
[17.6.1] 
under the assumption  that  the true  a  =  0, p =  1, and  u, isi.i.d.  Phillips  and  Perron 
(1988) generalized  these  results  to the case  when  u, is serially correlated  and possibly 
heteroskedastic  as  well.  For  now  we  will  assume  that  the  true  process  is 

. 

yr  N-1  =  &,  =  ¥(L)e,, 
where  ¥(L)  and  e, satisfy  the  conditions  of  Proposition  17.3.  More  general  con- 
ditions  under  which  the  same  techniques  are  valid  will  be  discussed  at  the  end  of 
this  section. 

If [17.6.1]  were  a  stationary  autoregression  with  |p| <  1, the  OLS  estimate 
fr in[17.4.15]  would not give a consistent  estimate  of p when  u, is serially correlated. 
However,  if p is equal  to  1, the  rate  T convergence  of #7 turns  out  to  ensure  that 
pr—  1 even  when  4, is serially  correlated.  Phillips  and  Perron  (1988)  therefore 
_  proposed  estimating  [17.6.1]  by OLS  even  when  u, is serially  correlated  and  then 
modifying the statistics  in Section  17.4  to take  account  of the  serial correlation. 

_  Let a; and 67 be the  OLS estimates  based  on  [17.6.1]  without  any correction 

for  serial  correlation;  that is,  @; and #7 are  the magnitudes defined in  (17.4. iS), 

If the true values  are a  =  0 and p = 1, then, as  in  [17.4.22], 

T'4,  |- |  1 

: sere L: : ae Su, a eat 
LTO 'Sy,-  ed  =  ap 
To Ey-1 TO *By7 
where  > denotes  cr over f from 1 to  T.  =) ste ber null hypothesis 
hata =  Oand p= oi lows in [17.48] at 

T(6r — 1) 

| 

i ne 
ep De a i Raa 

* 

Piste  %4 [7 4 apn: seh fe  ae 

t 

bi  7-325  7 

ae 

~  eye; 
sal 
com 

hasalt  G0) W: Jig fa  tH  ~  Om 

al wees -  La <—aspiaptot  sh stis Sis ious 

Substituting  [17.6.3]  and  [17.6.4]  into  [17.6.2]  produces 

Ta, 

:  E o.  hi5 

32 

| W(r)  dr 

T(6r  — 

1) 

OA 

[ mw  dr  | wor  dr 

FE  Mae  olla’  al 

3{(W(1))?  =  4} 

{42  —  Yo} 

:  * 4 f  0  A 

: 4 

J Wiha  | W(1) 

i 

[ ww  ar  | wor  ar 

H[W()P — 1 

; é 

i 

[ wo ana 

+ 

0  Aq! 

| W(r)  dr  | wor  dr  ge 7  Yoy/A 

il 

The  second  element  of this  vector  states  that 

T (6,  -  1) 

: 

batediais 

on 

J wo  ¥  | W(1) 

W(r)  dr  | [WinP  dr| 

+  2)  ®  =  wg  4] 
as 

LHTWODP  —  bb 

Wir)  dr] 

J 

"| 

[17.6.5] 

17.6.6 
[17.6.6] 

| W(r)  dr  | [W(r)}?  dr 

‘  H(W(D)P  cae  1} ~  wa)  | W(r) dr 
i 

(1/2)-(A?  ma  Yo) 

—_ 

| wor  dr  —-  I W(r) a]  Pe  if [W(r)?  dr  -  tl W(r) | 

The  first  term  of the  last  equality  in [17.6.6]  is the  same  as  [17.4.28],  which 
described  the  asymptotic  distribution  that  7(67;  —  1) would  have  if u, were  i.i.d. 
The  final  term  in  [17.6.6]  is a  correction  for  serial  correlation.  Notice  that  if wu, is 
serially  uncorrelated,  then  y%,  =  1 and  y =  0 forj  =  1,2,....  Thus,  if u,  is 
serially  uncorrelated,  then  A?  =  o?-[W(1)}?  =  o?  and  y,  =  E(u?)  =  o?.  Hence 
[17.6.6]  includes  the  earlier  result  [17.4.28]  as  a  special  case  when  u,  is serially 
uncorrelated. 

It is easy  to use  6, the  OLS standard  error  for 6, to construct  a sample statistic 
that  can  be used  to estimate  the correction  for serial  correlation.  Let Y; be the  matrix 
defined  in [17.4.21],  and  let s7 be  the  OLS  estimate  of the  variance  of u,: 

s} =  (T -  29°!  > (y,  —  Gr —  pry,-,)?. 

‘§ 

Then  the  asymptotic  distribution  of T?-é7, can be found  using the same  approach 
as  in (17.4.31]  through  [17.4.33}: 

508  Chapter  17  | Univariate  Processes  with  Unit Roots 

T?-63, 

ae 

s7[0  IM 

Ty 

0 
Sy?  ‘y,' 

~{ 
+  s3{0  uf, ‘| [wou  [wor  a  E ‘| 

[ ww  dr 

sa 

1 

=  (s3/a2f0  1] 

=(s ad A’) 

1 

| W(r)  dr  q 

| W(r) dr  | (w(npdr| 
eae 

LU} 

od (WoP dr  —  | W(r) hs 

ppeece [17.6.6] tat 

| 

:  . 

Sf  iV 

T(ér - 1) -  r-33 63, + SHO? — 
Te 
Oo ae oe 
op  ala ouss  =  $3  wet  i 
‘Of Examp ne  i?.4 
oy  ieee ay = U i en ar] [17.6.8] 

= 

ge 
pict 

A 

. 

Ey 

: 

| 

= 

) 

’ 

- 
by 

; 

° 

: 

r 

Laks th;  mae 

} 

aa 

; 

» 

a  m 
, 

ng 

: 

a 

=o  ;: 

» 

¥Sri  . 
om 2  al 
= 

J 
<=  ~  ———  ee  ‘ 

:  ‘o- 

‘a 

7. 

; 

d 

™.  ME 
5 

sens 

( 

.  oY 

uae 

‘oF  Be:  ir 
; 

tS 

= 

ls 

* 

A(WCDP  -  1) -  Wa) | WO) ar  (x) 
| wor  dp  =  | W(r) ar]  $3. 

x  {| [W(r)}?  dr  -  i W(r)  ar| 

+  {(1/2)(1/s)(A?  —  Yo)} x  {T?-65,  +  Sz” 

[17.6.9] 

with  the  last  convergence  following  from  [17.6.7].  Moreover, 

Ss; =  (7 iF?  > (y,  —  @r  —  bry,-1)? > E(u?)  =  %.  — [17.6.10] 

Hence,  [17.6.9]  implies  that 

s{{w(1)}*  —  1} —  W(1)  | W(r)  dr 

(¥/A?)'?  “ty a 

{{ [W(r)}?  dr  —  1 W(r) a} 

[17.6.11] 

+  {3(A?  —  yo)/A}  x  {T-65,  +  Sz}- 

Thus, 

. 

(Yo/A?)'? te —  {2(A?  —  Yo)/A}  X  {T- 6,  +  Sr} 

IWF  = 

L 

-  wa) | Wear  [17.6.12] 

%  {| [W(r)}?  dr  -  1 W(r) ar} |" 

which. is the  same  limiting  distribution  [17.4.36]  obtained  for  the  random  variable 
tabulated  for  case  2 in Table  B.6. 

The  statistics  in [17.6.8]  and  [17.6.12]  require  knowledge  of the  population 
parameters  y,  and  A”.  Although  these  moments  are  unknown,  they  are  easy  to 
estimate  consistently.  Since  y,  =  E(u?),  one  consistent  estimate  is given  by 

Jo =  T7"  J a3, 

T 

t=1 

7 

[17.6.13] 

where  ui,  =  y,  —  Gr  —  pry,-,  its the  OLS  sample  residual.  Phillips  and  Perron 
(1988)  instead  used  the standard  OLS estimate  7, =  (T —  2)~ "Xa?  =  s3. Similarly, 
from  result  (a) of  Proposition  17.3,  A?  is the  asymptotic  variance  of the  sample 
mean  of u: 

VTi  =  T-"?  > wu N(0, d2). 

ra 

. 

[17.6.14] 

t=1 

Recalling  the  discussion  of the  variance  of the  sample  mean  in Sections  7.2  and 
10.5,  this  magnitude  can  equivalently  be described  as 

. 

dW?  =  ao? -[W(1)}?  =  yo  +  25 yj; =  27,0), 

(17.6.15] 

where  y, is the jth autocovariance  of u, and  s,(0)  is the  population  spectrum  of u, 
at frequency  zero.  Thus,  any of the estimates  of this magnitude  proposed  in Section 

510  Chapter  17  | Univariate  Processes  with  Unit Roots 

10.5  might  be  used.  For  example,  if only  the  first  q autocovariances  are  deemed 
relevant,  the  Newey-West  estimator  could  be  used: 

a 
P=  H+  2>  [1 -  jg  +  15, 

4 

| 

[17.6.16] 

j=l 

a 

oe  F-'  > 40, 
t=] 

(17.6.17] 

and a, =  y,  —  @y  —  Pry,-1- 

To  summarize,  under  the  null  hypothesis  that  the  first  difference  of y,  is  a 
zero-mean  covariance-stationary  process,  the  Phillips  and  Perron’  approach  is 
to estimate  equation  [17.6.1]  by OLS  and  use  the  standard  OLS  formulas  to  cal- 
culate  6 and  its star dard  error  6; along with  the  standard  error  of the  regression 
's.  The jth autocovariance  of #, =  y,  —  @  —  py,_,  is then  calculated  from  [17.6.17]. 
The  resulting  estimates  7, and  A? are  then  used in  [17.6.8]  to  construct  a statistic 
that has the same  asymptotic distribution  as does the variable  tabulated in  the case 
2 section of Table  B.S.  The  analogous  adjustments  to  the  standard  OLS t test  of 
‘statistic: sat can  be ew with  the case 
p =  1 
2 section of  Table  B.6— 

described  in [17.6. - ag 
Ht 

Example 17.6 
7A: 
| 
| 

| Let ui a, denote the OLS sample ré residual for the interest rate regression (a7. 4. a7] 

oe:  eruit a  3  yy  &  f ih wet rs BY /  a a a @ BET  Jt 

4  =i  - 0.211 -— “0.96691 a “fort = 1 a‘ 2, a Oe 

ae poset ay  Shere nibs 

ie 

01913 

ee 

: 

. 

=  (UT) Ya  2° 

norchsieaay 

Thus.  if the  serial  correlation  of u, is to  be described  with  q =  4 autocovariances, 
f2  =  0.630  +  2(4)(0.114)  +  2(8)(-0.162)  +  2(3)(0.064)  +  2(3)(0.047) 

I} 

().688. 

The  usual  OLS  formula  for  the  variance  of  the  residuals  from  this  regression 

1S 

s?  =  (T  —  2)-'  > a?  =  0.63760. 

/ i 

r=1 

Hence,  the  Phillips-Perron  p statistic  is 
T(p  ~  1) =  (12)-(T?-63ls?)-(A?  —  4) 

168(0.96691  —  1)  —  4+{[(168)(0.019133)]?/(0.63760)}(0.688  —  0.630) 

=  —6.03. 

Comparing  this  with  the  5%  critical  value  for  case  2 of Table  B.5,  we  see  that 
—6.03  >  —  13.8.  We  thus  accept  the  null  hypothesis  that  the  interest  rate  data 
could  plausibly  have  been  generated  by a  simple  unit  root  process. 

Similarly,  the  adjustment  to  the f¢ statistic  from  Example  17.4  described 

in [17.6.12]  is 

(Fold?)  a  {3(A?  ve  Yo)(T-  G;/s)  ae A} 

=  {(0.630)/(0.688)}'7(0.96691  —  1)/0.019133 

—  {(1/2)(0.688  —  0.630)[[(168)(0.019133)/V/(0.63760)]  +  V(0.688)} 

=  —1.80. 

Since  —  1.80  >  —2.89,  the  null  hypothesis  of a  unit  root  is again  accepted  at 
the  5%  level. 

Phillips-Perron  Tests  for Cases  I and  4 

The  asymptotic  distributions  in [17.6.8]  and  [17.6.12]  were  derived  under  the 
assumption  that  the  true  process  for  the  first  difference  of y, is serially  correlated 
with  mean  zero.  Even  though  the  true  unit  root  process  exhibited  no  drift,  it was 
assumed  that  the  estimated  OLS  regression  included  a  constant  term  as  in  case  2 
of Section  17.4. 

The  same  ideas  can  be  used  to  generalize  case  1 or  case  4 of Section  17.4, 
and  the  statistics  [17.6.8]  and  [17.6.12]  can  be  compared  in  each  case  with  the 
corresponding  values  in Tables  B.5 and  B.6.  These  results  are  summarized  in Table 
17.2.  The  reader  is invited  to  confirm  these  claims  in exercises  at  the  end  of the 
chapter. 

Example  17.7 
The  residuals  from  the  GNP  regression  [17.4.56]  have  the  following  estimated 
autocovariances: 

%  =  1136 
4,  =  0.006 

4  =  0.424 
¥  = —0.110, 

% =  0.285 

from  which 

A? =  1.136  +  2{3(0.424)  +  3(0.285)  +  3(0.006)  —  $(0.110)}  =  2.117. 

512  Chapter  17  | Univariate  Processes  with  Unit Roots 

Also,  s*?  =  1.15627.  Thus,  for  these  data  the  Phillips-Perron p test  is 

VB  —  Ve  (T?-67/s?(A?  —  44) 
168(0.96252  —  1)  -  " 

[(168)(0.019304) |? 
1/15627 

je.u7 —  1.136) 

“Il 

—  10:76. 

Since  —  10.76  >  —21.0,  the  null  hypothesis  that  log  GNP  follows  a  unit  root 
process  with  or without  drift  is accepted  at  the  5%  level. 

The  Phillips-Perron  f test  is 

(F/A?)'7¢  Yi {3(A?  me  Yo)  T-  G,/s)  x  A} 

/ 
=  {(1.136)/(2.117)}'7(0.96252  —  1)/0.019304 

—  {3(2.117  -  1.136)[[(168)(0.019304)]/\/T.  15627) | +  V(2.117)} 

=  —2.44. 

Since  —2.44  >  “hag, the  null  hypothesis  of a  unit  root  is again  accepted. 

More  General  Processes  for u, 

The  Newey-West  estimator  X? in  [17.6.16]  can  provide  a  consistent  estimate 
of A’ for  an  MA(=)  process,  provided  that  q,  the  lag truncation  parameter;  goes 
to infinity  as  the  sample  size  T grows,  and  provided  that  g grows  sufficiently  slowly 
relative  to  T.  Phillips  (1987)  established  such  consistency  assuming  that  g;—  = 
and  q,/T'*  —  0; for  example,  gq;  =  A-T"*  satisfies  this  requirement.  Phillips's 
results  warrant  using  a  larger  value  of q with  a  larger  data  set,  though  they do not 
tell  us  exactly  how  large  to  choose q in practice.  Monte  Carlo  investigations  have 
been  provided  by Phillips  and  Perron  (1988),  Schwert  (1989),  and  Kim  and Schmidt 
(1990),  though  no  simple  rule  emerges  from  these  studies.  Andrews’s  (1991)  pro- 
cedures  might  be  used in  this  context. 

y 

Asymptotic  results  can  also  be obtained  under  weaker assumptions  about  u, 
than  those in  Proposition  17.3.  For  example,  the  reader  may  note  from  the  proof 
of result  17.3(e)  that  the  parameter  y,  appears  because  it is the  plim  of  T~'  x 
>7_,u?.  Under  the  conditions  of the  proposition,  the  law  of large numbers  ensures 
that  this  plim  is just the  expected  value  of u?, which  expected  value  was  denoted 
Y.  However,  even  if the  data  are  heterogeneously  distributed  with  E (u?)  =  yw, 
it  may  still  be  the  case  that  T~'/_,y),  converges  to  some  constant.  If 
T-'X7_,u?  also  converges  to  this  constant,  then  this  constant  plays  the  role  of y, 
in a generalization  of result  17.3(e). 

Similarly,  let  a, denote  the  sample  mean  from  some  heterogeneously  dis- 

tributed  process  with  population  mean  zero: 
ra 

inet  > i: 

1=1 

and  let  A}. denote  T times  the  variance  of u,: 

M3. = T-Var(ai7)  =  T~':  E(u,  +  up  +  +  **  +  Uy)’. 

The  sample  mean  a; may  still  satisfy  the  central  limit  theorem: 

T 

L 

; 

T-'2  > u,—  N(O,  a’) 

t=1 

17.6.  Phillips-Perron  Tests for Unit  Roots 

513 

TABLE  17.2 
Summary  of Phillips-Perron  Tests  for  Unit  Roots 

; 

Case  I: 

Estimated  regression:  y,  =  Py,-1  +  4, 
True  process:  y,  =  y,;-1  +  4, 
Z, has  the  same  asymptotic  distribution  as  the  variable  described  under  the 

ee 

. 

heading  Case  1 in  Table  B.5. 

. 

. 

Z, nas  the  same  asymptotic  distribution  as  the  variable  described  under  Case 

1 in Table  B.6. 

Case  2: 

Estimated  regression:  y,  =  a@  +  py,_,  +  U, 
True  process:  y,  =  y,-,  +  4, 
Z, has  the  same  asymptotic  distribution  as  the  variable  described  under  Case 

) 

2 in Table  B.S. 

Z, has  the  same  asymptotic  distribution  as  the  variable  described  under  Case 

2 in Table  B.6. 

Case  4: 

Estimated  regression:  y,  =  @  +  py,_,;  +  5t  +  4, 
True  process:  y,  =  a+  y,-,  +  uy, 
Z, has  the  same  asymptotic  distribution  as  the variable  described  under  Case 

@  any 

4 in Table  B.5. 

Z, has  the  same  asymptotic  distribution  as  the  variable  described  under  Case 

4 in Table  B.6. 

Notes  to  Table  17.2 

Estimated  regression  indicates  the  form  in which  the  regression  is estimated,  using  observations 

t=  1,2,...,  T and  conditioning  on  observation  ¢  =  0. 

True  process  describes  the  null  hypothesis  under  which  the  distribution  is calculated.  In  each 
case,  u, is assumed  to  have  mean  zero  but  can  be  heterogeneously  distributed  and  serially  correlated 
with 

lim  T-'  & E(u2) 
T= 
Dieta 
lim  T ‘E(u,  +  us.  +  °°:  +  uy)? 
T--= 
Z, is the  following  statistic: 

Yo 

*. 

T 

Z, =  T(py  —  1) —  (12){T?-63,  +  87} 7 —  For), 

where 

:; 

T 

Yr  mi 

2 a, _; 

s=jel 

ui,  =  OLS  sample  residual  from  the  estimated  regression 
AF =  nr 

+2°D (1 -  a  +  Dir 
fa 

. 

si =  (T -  k)"'  DQ} 
k =  number  of parameters  in estimated  regression 

t= 

6), =  OLS  standard  error  for p. 

Z, is the  following  statistic: 

Z,=  (Fo. Ki)" (Br  a  1)/65, 

—  (1/2)(A;  -  For  WA){T-  6,  +  Sy}. 

514  Chapter  17 | Univariate  Processes  with  Unit Roots 

or 

where 

T~  12  >> u,—  A-W(1), 

us 

L 

t=! 

d\?  =  lim  A, 
Tx 

[17.6.18] 

providing  a  basis  for  generalizing  result  17.3(a). 

If u,  were  a  covariance-stationary  process  with  absolutely  summable  auto- 
covariances,  then  Proposition  7.5(b)  would  imply  that  lim.  A}  =  27- ty 
Recalling  [7.2.8],  expression  [17.6.18]  would  in  this  case  just  be  another  way  to 
describe  the  parameter  A? in Proposition  17.3. 

Thus,  the  parameters  y,  and  A? in  [17.6.8]  and  [17.6.12]  can  more  generally 

be defined  as 

¥  =  lim  T-'  % E(u?) 

ys 

T= 

t=1 

[17.6.19] 

A2  aS  lim  T-'-  E(u,  +  ur  +  i 

Saat  +  uy). 

[17.6.20] 

T-=x 

Phillips  (1987)  and  Perron  and  Phillips  (1988)  derived  [17.6.8]  and  [17.6.12]  as- 
suming  that  u,  is a  zero-mean  but  otherwise  heterogeneously  distributed  process 
satisfying  certain  restrictions  on  the  serial  dependence  and  higher  moments.  From 
this  perspective,  expressions  [17.6.19]  and  [17.6.20]  can  be  used  as  the  definitions 
of the parameters  , and A. Clearly,  the estimators  [17.6.13]  and  [17.6.16]  continue 
to  be  appropriate  for  this  alternative  interpretation. 

On  the  Observational  Equivale: ge  of Unit  Roo 
| 
and  Covariance-Stationary  Processes 

* 

We saw  in Section  15.4 that  given any /(0) process  for y, and any finite  sample 
size  T, there  exists  an  /(1)  process  that  will  be  impossible  to  distinguish  from  the 
1(0) representation  on  the  basis  of the  first  and  second  sample  moments  of y.  Yet 
the  Phillips  and  Perron  procedures  seem  to offer  a  way  to  test  the  null  hypothesis 
that  the  sample  was  generated  from  an  arbitrary  /(1) process.  What  does  it mean 
if the  test  leads  us  to  reject  the  null  hypothesis  that  y, is /(1) when  we  know  that 
there‘exists  an  /(1) process  that  describes  the  sample  arbitrarily  well? 

| 

Some  insight  into  this  question  can  be gained  by considering  the  example  in 

equation  [15.4.8], 

| 

(1 -  Ly,  =  (1 +  OL)e,, 

(17.6.21] 

where @ is slightly  larger  than  —1 and  ¢, is i.i.d.  with  mean  zero  and  variance  0”. 
The  model  [17.6.21]  implies  that 

y 

=  (e, +  Oe,_,)  +  (€,-,  +  86,-2)  +  °°  +  + (€,  +  Oy)  +  Yo 

bt  Ah t+  Omar  OF  Be 2 +  °°:  +  Ul  t  O)e,  +  Oe)  +  Yo 
e,  +  (1 +  O)E,-1  + O€0  +  Yo, 

| 

where 

6-1  =  &  ae  er: 

17.6.- Phillips-Perron  Tests for Unit  Roots 

515 

For  large  ¢,  the  variable  y,  is dominated  by the  unit  root  component,  (1 » O)E,_ 15 
and  the  asymptotic  results  are  all  governed  by this  term.  However,  if 6 is close  to 
—},  then  in a finite  sample  y,  would  behave  essentially  like  the  white  noise  series 
e,  plus  a  constant  (0€)  +  yo).  In  such  a  case  the  Phillips-Perron  test  is likely  to 
reject  the  null  hypothesis  of  a  unit  root  in  finite  samples  even  though  it  is  true.'° 
For  example,  Schwert  (1989)  generated  Monte  Carlo  samples  of size  T  =  1,000 
according  to  the  unit  root  model  [17.6.21]  with  @ =  —0.8.  The  Phillips-Perron  test 
that  is supposed  to  reject  only  5%  of the  time  actually  rejected  the  null hypothesis 
in  virtually  every  sample,  even  though  the  null  hypothesis  is true!  Similar  results 
were  reported  by Phillips  and  Perron  (1988)  and  Kim  and  Schmidt  (1990). 

Campbell  and  Perron  (1991)  argued  that  such  false  rejections  are  not  nec- 
essarily  a  bad  thing.  If @ is near  —1,  then  for  many  purposes  an  /(0)  model  may 
provide  a  more  useful  description  of  the  process  in  [17.6.21]  than  does  the  true 
(1)  model.  In  support  of  this  claim,  they  generated  samples  from  the  process 
[17.6.21]  and  estimated  by OLS  both  an  autoregressive  process  in levels, 

y,  =  ¢  +  Gy,-)  +  Do Via  eee 

ak  $pYi-p  t+  €,, 

and  an  autoregressive  process  in  differences, 

Ay, 

=a  +t  Cy AYy  21+  fo Ay,-2  ee  ee  eS  am  +  €,. 

They  found  that  for  @ close  to  —1,  forecasts  based  on  the  levels  y,  tended  to 
perform  better  than  those  based  on  ‘the  differences  Ay,, even  though  the  true  data- 
generating  process  was  /(1). 

: 

A related  issue,  of course,  arises  with  false  acceptances.  Clearly,  if the  true 

model  is 

y,  =  py,-1  +  ©, 

[17.6.22] 

with  p slightly  below  1, then  the  null  hypothesis  that  p  =  1 is likely  to  be accepted 
in  small  samples,  even  though  it is false.  The  value  of accepting  a  false  null  hy- 
pothesis  in this  case  is that  imposing  the  condition  p  =  1 may  produce a better 
forecast  than  one  based  on  an  estimated  6,  particularly  given  the  small-sample 
downward  bias  of 6,.  Furthermore,  when p is close  to  1, the  values  in Table  B.6 
might  give a  better  small-sample  approximation  to  the  distribution  of (6;  —  1)  + 
G,, than  the  traditional  ¢ tables."! 

This  discussion  underscores.  that  the  goal  of unit  root  tests  is to  find  a  par- 
simonious  representation  that gives a reasonable  approximation  to the true  process, 
as  opposed  to  determining  whether  or  not  the  true  process  is literally  /(1). 

17.7.  Asymptotic  Properties  of a  pth-Order 
Autoregression  and  the Augmented 
Dickey-Fuller  Tests  for  Unit  Roots 
The  Phillips-Perron  tests  were  based  on  simple  OLS  regressions  of y, on  its  own 
lagged  value  and  possibly  a  constant  or  time  trend  as  well..Corrections  for  serial 
correlation  were  then  made  to the  standard  OLS  coefficient  and t statistics.  This 
section  discusses  an  alternative  approach,  due  to  Dickey  and  Fuller  (1979),  which 
controls  for serial  correlation  by including  higher-order  autoregressive  terms  in the 
regression. 

"For  more  detailed  discussion,  see  Phillips  and  Perron  (1988,  p. 344). 
''See  Evans  and  Savin  (1981,  1984)  for a description  of the small-sample  distributions. 

516  Chapter 17  | Univariate  Processes  with  Unit Roots : 

An Alternative  Representation  of an  AR(p)  Process 
Suppose  that  the  data  were  really  generated  from  an  AR(p)  process, 
(1 =  OL  -  gL?  —  +--+  -  $,1P)y,  =  &, 
(17.7.1) 
where  {e,} is an’ i.i.d.  sequence  with  mean  zero,  variance  o”,  and  finite  fourth  - 
moment.  It is helpful  to  write  the  autoregression  [17.7. 1] in  a  slightly  different 
form.  To do so,  define 

PHH  tht  +4, 
eet  tet  °° > +  6.) 

oforj=1,2,....p  — 

1. 

[17.7.2] 
[17.7.3] 

Notice  that  for  any  values  of ¢,, ¢,..  .,  ¢,, the  following  polynomials  in L are 
equivalent: 

— 

a =  pL)  —  (GL  +  GL? +=  +, L°-"1  -  L) 
SPS  ph - fb  + ger B®  pip pee.  tn 
=1- 

eagegn 
(p+ O)L-&%  - OL? - % - GL? - 

ey be  oh  fogs LP 
: 

evi tt Lisa, potter  espn apne 
+ da +--+ + by)  + bat 

al  ge +  ,) — (2 tot: oo Ole 
drt  ayeres 
[Hy 

Uo 

of  |  noite 

: 

=  [-(¢,)  +  ie: + $,))L?-' fel’ 

Spey  ah BS |, spiky  ai  peti  (es  gaisii 
_  bas & 

2928.  one iagtnace é  isothiw ff. 

$e 

16.53) 
5  oe 
ti wi i  ats as  bojemilizs  2 noeay 2g2 
ritten  — xd basileoag oie £ 

ere 

a  ee  | 
inn  cee 12  29erHOr> 

alt  Rig we  a  ree. es 

lie outside  the  unit  circle.  Under  the  null  hypothesis  that  p =  1, expression  [17.7.5] 
could  then  be  written  as 

(li  +  f.L.—  {,L?  TU 

ae  6, aL Ae)  Ay,  ™,  £, 

or 

where 

Ay,  =  U,, 

[17.7.10] 

u,=(1-¢,L  -  isl  i 

2 

em  box  phe)  ep 

Equation  [17.7.10]  indicates  that y, behaves  like  the  variable  £, described  in Prop- 
osition  17.3,  with 

PL)  =  (P-Gk  +84  sere  F 2, be~)s% 

One  of the  advantages  of writing  the  autoregression  of [17.7.1]  in the  equiv- 
alent  form  of [17.7.6]  is that  only  one  of the  regressors  in  [17.7.6],  namely,  y,_,, 
is  [(1),  whereas  all  of  the  other  regressors  (Ay,_,,  Ay,-2, 
-.--,  AY;-p+1)  are 
stationary.  Thus,  [17.7.6]  is the  Sims,  Stock,  and  Watson  (1990)  canonical  form, 
originally  proposed  for  this  problem  by  Fuller  (1976).  Since  no  knowledge  of 
any  population  parameters  is needed  to  write  the  model  in  this  canonical  form, 
in this case  it is convenient  to  estimate  the  parameters  by direct  OLS estimation  of 
[17.7.6]. 

Results  generalizing  those  for  case  1 in  Section  17.4  are  obtained  when  the 
regression  is estimated  as  written  in [17.7.6]  without  a  constant  term.  Cases  2 and 
3 are  generalized  by including  a constant  term  in [17.7.6],  while  case  4 is generalized 
by including  a constant  term  and a time  trend  in [17.7.6].  For  illustration,  the  case 
2 regression  is discussed  in detail.  Comparable  results  for case  1, case  3, and  case 
4 will  be summarized  in Table  17.3  later  in this  section,  with  details  developed  in 
exercises  at  the  end  of the  chapter. 

: 

Case  2.  The  Estimated  Autoregression  Includes  a  Constant 
Term,  but  the  Data  Were  Really  Generated  by a  Unit  Root 
Autoregression  with  No  Drift 

Following  the  usual  notational  convention  for OLS  estimation  of autoregres- 
sions, we assume  that the initial sample is of size  T + p, with observations  numbered 
{Y_p+1» ¥-p+2)-  ++»  Yr}, and  condition  on  the  first p observations.  We  are  inter- 
ested  in the  properties  of OLS  estimation  of 

b  en  CAIs  +  (hy ug  +++ 

aves" 

*  +  GAY, ~pa1  +0  +  py:  +  &, 

(17.7.11] 

where  B ”  (fi, $2  ,  Mt  bp=19  a,  p)’  and  x,  =  (Ay,-1,  Ay,_2,  [hak  ¢  AY,—p+is  1, 
y,~1)'.  The  deviation  of the  OLS  estimate  b; from  the  true  value  B is given  by 

b, -  B =  P «| [3 xa. , 

(7.7.12 

518  Chapter 17  | Univariate Processes with  Unit Roots 

Letting u, =  y,  —  y,_,,  the  individual terms  in [17.7.12]  are 

T 

> xx  = 

t=l 

[17.7.13] 

; 

2u?_, 

2u,_\U,-2 

**° 

2M;  iM, —  1 

2u,_  | 

Lu,  1  Ye-1 

Eu,  it; 

oat.» 

os  2,  Al, —  p41 

2u,_> 

Pu,  ois 

>> ara  aa  Lu, —  p+  1ly-2 

2u,_| 

(2u,-2 

‘Ss 

ae 

>» 7  ae 

2M, —p +1 

Zui  pai  ¥i-) 

2U,—p +1 

T 

>>) aa ae  yr 14-2 

is  ae >  = 14,_p4+i 

Zy,-1 

T 

= 
2 *  F) 

Lu, _1€, 

LU, _2€, 

ae 

: 
Du —p+i& 
Le, 

- 

2it-16 

ZY, —1 

Ly?_, 

17.7.14 

with © denoting summation over 1 = | Re a  ee 

Jnder  the null hypothesis 

that a =  O snd p = 1, we saw in [17.7.10]  that y, 
I f° = a u, + ---  +  u, in Proposition  17.3.  Consulting 
the rates  — 

ol co oa sTgence 

‘ion 17. 2. for this case  the scaling matrix should be 

fosevry 

ai  v VT  G4)  0 }0 

ye  ln?  ba; z tn a ah A  : | 

: /  [17.7.15]} 

ee es, 
wae 

and  the  integral  sign denotes  integration  over  r from  0 to  1. Thus, 

Y7'[2x,x;/]¥ 7 

Yo 

v1 

Vir-¥% 

Yp  2  sYVp- 

‘hae  Vp-2 

Yp-2, 
; 
Aa 
Wee  2 

0 

hwy 
‘tis? 
. 
0 

L 
= 

0 

0 

0 

Bg  ed 

2p 

fot 

sadlcamcs 

ein 

ee  Be  a-[ won  ar 
| W(r) dr  A2- | [Wn]? dr 

=  V0 

oR 

where 

est 

-* 

? 

Aca 
(17.7.18] 

7 

Ln 

| 
Mes aca fa  Fe sdf 

Yp-2  \ 

¢ a.  a;  Mies  i 

¢.tette  POL. nm ee Tadeo 

te = ebiw? i 

isa  r Beers fin SAS: was 

FH Bt gaitipann ters  | 

acbiporie-xiviam: ee Beloit: 

‘2 
bs 

; 

; 

Thus,  the  first  p —  1 terms  in [17.7.21]  satisfy  the  usual  central  limit  theorem, 

T-OSu  _.e 
eo  oe 

TPS  54  1, 

N(0,  o2V). 

(17.7.23] 

| 

| 

The distribution  of the  last  two  elements in  [17.7.21]  can be obtained  from  results 
(a) and  (d) of Proposition  17.3: 

tae  Ne 

is 

a:  W(1) 

| omy  oo "  Hidden ‘. x! 

Substituting  [17.7.18]  through  [17.7.24]  into  [17.7.16]  results  in 

anc  -B)> | ‘  ute hed =  bend} 

Vo] 

{nh} 

[v-'n, 

4.2241 

(17.7.25] 

| 

: 

conten on  Ay,_; 

__The first p —  1 elements  of B are en bar its:  iP  1, which are the coefficients 
zero-mean  stationary  regressors  (Ay,_,,  Ay,_2,..  - , Ay,-p+1)-  The block con- 

n 
sting  ¢  ‘the tte as if eregres  | in Ua. : - states that 

Lbs 

c  cee ¥  5S}  x ey  = 
CALs  Te. 

tl 
BxI2q- 
aw@ifol VY, QM ~ A sonie nd | 
o} AM 

op. ;  ine — 4  “VF 

ibd a  ie:  : 

T 

wea  |  lf  : 

r 
739 wl (fe oe eq t | 

P- 

I 

Pi  Eaggel eP0 ia 

where 

’ fi 

=  [T -  +  1)}~'  py (y,  —  $1.7Ay,-1  —  & rAy,-2 

? 
—  £-y.rby-por  —  Or  —  br¥vei) 

MY 

4 

; 
+ E(E?)  = 

| 

[17.7.29] 

If none  of the  restrictions  involves  a  or  p,  then  the  last  two  columns  of R contain 
all  zeros: 

R 

=  | R, 

0  | 

[mx(p+1)] 

jmx(p-1)} 

(mx  2) 

(17.7.30] 

In  this  case,  RVT  =  RY; for  Y; the  matrix  in [17.7.15],  so  that  [17.7.28]  can  be 
written  as 

7 =  [RY;(b;  —  pr  {s3Rv,| > xx: vn}  [RY;(b;  —  B)]. 

T 

From  [17.7.18],  [17.7.25],  [17.7.29],  and  [17.7.30],  this  converges  to 

at{m  fond} 

«fom of a) LT} fe fend} 

=  [R,V~'h,]'[o?R,V~'Ri)~  [RV ~‘hy]. 

But since  h, ~  N(0, 7V),  it follows  that the (m x  1) vector  [R,V~ ‘h,] is distributed 
N(0,  [c2R,V~'R;]).  Hence,  expression  [17.7.31] is  a quadratic  form in  a Gaussian 
vector  that satisfies  the  conditions  of iss  Wie  8.1: 

x3 > x°(m). 

This  verifies  that  the  usual  ¢ or  F tests  applied  to  any  subset  of the  coefficients 
é, %,...,¢,-,  have  the  standard  limiting  distributions. 

Note,  moreover,  that  [17.7.27]  is exactly  the  same  asymptotic  distribution 
that  would  be  obtained  if the  data  were  differenced  before  estimating  the  auto- 
regression: 

Ay,  =  ¢,Ay,-1  +  foAy,-2  +  +  °°  +  C,-1AYi-pai  +  @  +  €,. 

Thus,  if the  goal  is to  estimate  ¢,,  >,  ...,  ¢,-,  or  test  hypotheses  about  these 
coefficients,  there  is no  need  based  on  asymptotic  distribution  theory  for  differ- 
encing  the  data  before  estimating  the  autoregression.  Many  researchers  do  rec- 
ommend  differencing  the  data  first,  but  the  reason  is to  reduce  the  small-sample 
bias  and  small-sample  mean  squared errors  of  the  estimates,  not  to  change  the 
asymptotic  distribution. 

Coefficients  on  Constant  Term  and  y,_, 
The  last two  elements  of B aré a and p, which  are coefficients  on the constant 
term  and  the  /(1)  regressor,  y,_,.  From  [17.7.25],  [17.7.20],  and  [17.7.24],  their 

522  Chapter  17  | Univariate  Processes  with  Unit Roots 

limiting distribution  is given  by 

L 
— 

1 

af W(r) dr  |  g  W(1) 

A: | Wr) dr  a2-| [won dr] 

LeoaM(WDP  ~  1 

-o|  i: 

.  | Wn) ar  | (Wi) dr| 

Lo  4 

J min mn  3 <i , 

(17.7.32] 
< 

at 4 Fide! al 
[wow 
1 
a 
{ 
“|e zl hana J mop ar 

fe 

WL) 
eer =~ 

S 

The second element  of this vector implies that  (A/c) times T(6r -  J) has the same 
asymptotic distribution  as [17.4.28], which described  the estimate of pina regression  — 
without lageeck Ay and with  serially uncorrelated  disturbances: 

ee. i. 1) rie He = ye ——— — : 

_ MMR  ~  0 - wo), oy} min) dr 

. 

tif 

aed 

aX 

Bis) (yp ‘A  . ify 

) , 

[17.7.33]  | 

,  WG 

7.7344 
isc  arly  es i lated: evenmaenthy  UY  Les  ; he of 7 Hove : ik 

‘. 

gieaib omriBe wi a 2 

417a. 
TOT Ny Mm: fey Se 

;  vie 

Oo  tae 

~) 

° 

-  +9 

sults  in 

ty  = 

ere 

Bis, 

=) 

ia 

. 

(17.7.37]  . 

{5% e +  1Y 7(2x,x;) 7  'Y7eps } 

But 

~ 

©. 1Y7(2x,x;)'Yre,  41  a  csaif Ve"xays'f  C41 

Lie hee f. abe 

al 

i  ef i [W(DR  dr ™  | | W(r) ar) | 

by virtue  of (17. 7. ae ae a qi ae Hence, from lok 7.37]  and gone 7: +. 

a 

Foie  17-738 

a. 

cathe,  Satie  fies cine _ MWaOR = nwo | er T_T] mont igsest 
: : 

tt 

fi [(G- 

an ny ae 

* 

fone in a Gane 

Notice  that [17.7.39]  can be written 
: 

of 
Fr, =  (b;  -  ByR'Y{5}-¥,ROxx)  'R'Y,| 

x  Y;R(b; — B)/2. 

The matrix in Lah? 7.40]  has  the property  that 

Y;R = RY, 

(17.7.41] 

for R = ifs I,J and  Y; the  (p +  1) x  (p +  1) matrix  in [17.7.15].  From  [17.7.25], 

_RY;(b; - B)  > Q-'h,.  Thus,  [17.7.41]  implies  that 

Fr =  (br - By (RY) '{s}-RY2xx)"YR'| “RY (by ~  BY? 

5 (Q 'hy)'{07Q- (Qh, )/2 

| 

: 

| = 6Q~'hy/20%) 
7 

a = pectin W(1)  toa{W()P  -  y : 

macpe ee xf  moar  | |  | o W(1)  i sdivailine> 
St 
cs Lf W(r) dr  2d? “ih [WP dr}  ae a  didcngria 

ei. pur re 4 

Li  SS weir 0 

 [17.7.42] 

f*.0  01) mow aaor 

yi  a 

Se 
:  ‘4 hs iB. ith te ee 

Met 

Se 

e 

 “geegiye 

7 a} <— 

3 oe +  an at 2 

Or 

r’'B  =r. 

[17.7.43] 

The  distribution  of  the  ¢ test  of  this  hypothesis  will  be  dominated  asymptotically 
by the  parameters  with  the slowest  rate  of convergence,  namely,  ¢,, ¢2,-  -»  .  p-1- 
Since  these  are  asymptotically  Gaussian,  the  test  statistic  is asymptotically  Gaussian 
and  so  can  be  compared  with  the  usual  ¢ tables.  To  demonstrate  this  formally,  note 
that  the  usual  ¢ statistic  for  testing  this  hypothesis  is 

rb;  -—r 

T'2(r'b,  —  7) 

(17.7.44] 

e  lr 

re 
1/2 

1/2° 
{s3e/@a)- }  {S37 (x)  era| 

Define  fF; to  be  the  vector  that  results  when  the  last  element  of r  is replaced  by 
Fog  IMT, 

| 

bwin,  7. 

°°?  #7  O  eV a 

[17.7.45] 

and  notice  that 

for  Y; the  matrix  in  [17.7.15].  Using  [17.7.46]  and  the  null  hypothesis  that  r  = 
r’B,  expression  [17.7.44]  can  be  written 

T'’r  =  Y7f; 

[17.7.46] 

;,  ee  (17.7.47] 

rY,(b;  — 

: 

f t7Y  7(2x,x;)~'Y7# | 

Notice  from  [17.7.45]  that 

where 

Using  this  result  along  with  [17.7.18]  and  [17.7.25]  in [17.7.47]  produces 

Fr’ =  [r,  eo) 

ae 

Tp-1  0  0}. 

L 

— 

-;  a "hy 

rio-! 

Q-‘h, 
{o% 14  0  Je 

0 

“Qt 

[17.7.48] 

Ce 

{o7[r,  1  Ph Aake ARE 

 ae  lis 

Since  h, ~  N(0,  a?V),  it follows  that 

~ N(O, A), 
[7,2  ++  Py-a]V~'h, 

where 

lll ds  he |  A) 
Thus, the  limiting  distribution  in [17.7.48]  is that of a Gaussian  scalar  divided  by 
its standard  deviation  and  is therefore  N(0,  1). This confirms  the claim  that  the f 
test  of r’B  =  r can  be compared  with  the  usual  ¢ tables. 
526  Chapter 17  | Univariate Processes with Unit Roots 

One  interesting  implication  of  this  last  result  concerns  the  asymptotic  prop- 
erties  of  the  estimated  coefficients  if the  autoregression  is estimated  in  the  usual 
levels  form  rather  than  the  transformed  regression  (17.7.11].  Thus,  suppose  that 
the  following  specification  is estimated  by OLS: 

y=  a+  hy,  +  b2y,-2  +--+  +  dyYi-p  +  & 
[17.7.49] 
for some p =  2. Recalling  [17.7.2]  and  [17.7.3],  the  relation  between  the  estimates 
| 3  Sega  ¢p-1»  P)  investigated  previously  and  estimates  (d..  Gs,  ian  d,) 
based  on  OLS  estimation  of [17.7.49]  is 

d, 7»  * 
%  =G  -  §-, 

d,  =prt  &. 

forj  =  2,3,...,p-1 

Thus,  each of the coefficients d,, b>, -  se  db is a linear  combination  of the elements 
of (£1,  ¢2,.-  . 
.  ¢p-1,  6).  The  analysis  of [17.7.43]  establishes  that  any  individual 
estimate  ¢; converges  at  rate  VT  to  a  Gaussian  random  variable.  Recalling  the 
discussion  of  [16.3.20]  and  [16.3.21],  an  OLS t¢ or  F test  based  on  [17.7.49]  is 
numerically  identical  to  the  equivalent  ¢ or  F test  expressed  in  terms  of  the  rep- 
resentation  in  [17.7.11].  Thus,  the  usual  ¢ tests  associated  with  hypotheses  about 
any.  individual  coefficients  ¢,, ¢,,  ...,  d, in  [17.7.49]  can  be  compared  with 
standard  ¢ or  N(0,  1) tables.  Indeed,  any  hypothesis  about  linear  combinations  of 
the  ¢’s other than the  sum  ?, +  $2  +  ---  +  g, Satisfies  the  standard  conditions. 
The  sum  ¢,  +  ¢  +  ---  +  @,, of course,  has  the  nonstandard  distribution  of the 
estimate  p described  in [17.7.33]. 

Summary  of Asymptotic  Results for an  Estimated 
Autoregression  That  Includes  a  Constant  Term 

The  preceding  analysis  applies  to  OLS  estimation  of 

op 

aay, a; +  O0y,-2  +  °°  +  Oo -7Ayj-p+1  +  @  +  py)  + 

under  the  assumption  that  the  true  value  of a@  is zero  and  the  true  value  of p is 1. 
The  other  maintained  assumptions  were  that  e, is i.i.d.  with  mean  zero,  variance 
a’, and  finite  fourth  moment  and  that  roots  of 

Gesebitottiba 

esicom  Gua?’  =  2 

are  outside  the unit  circle.  It was  seen  that  the estimates  i ”  fs; WLS  é p-1  converge 
at  rate  VT to  Gaussian  variates,  and  standard  ¢ or  F tests  for  hypotheses  about 
these coefficients  have  the usual  limiting Gaussian  or x? distributions.  The estimates 
@ and p converge  at  rates  VT and  T, respectively,  to nonstandard  distributions. 
If the  difference  between  the  OLS  estimate  f and  the  hypothesized  true  value of 
unity is multiplied  by the  sample  size  and  divided  by (1 —  ¢,  —  g2  —  ++:  —  @,-1), 
the resulting statistic  has the same  asymptotic  distribution  as  the  variable  tabulated 
in the  case  2 section  of Table  B.5.  The  usual ¢ statistic  of the  hypothesis  p  =  1 
does not  need  to be adjusted  for sample  size or  serial  correlation  and  has the same 
asymptotic  distribution  as  the variable  tabulated  in the case  2 section  of Table  B.6. 
The  usual F statistic  of the  joint  hypothesis  a  =  0 and  p =  1 likewise  does  not 
have  to  be adjusted  for  sample  size  or  serial  correlation  and  has  the  same  distri- 
bution as  the  variable  tabulated  in the  case  2 section  of Table  B.7. 

When  the auforegression  includes  lagged changes  as here,  tests  for a unit  root 
based on  the value  of p, f tests,  or  F tests  are described  as augmented  Dickey-Fuller 
tests. 

, 

7.7.  Asymptotic Properties  of a pth-Order  Autoregression 

527 

Example  17.8 
The  following  model  was  estimated  by OLS  for  the  interest  rate  data  described 
in  Example  17.3  (standard  errors  in  parentheses): 

. 

| 

| 

i, =  0.335  Ai,_,  —  0.388  Ai,_,  +  0.276  Ai,, 
(0.0808) 

(0.0788) 
—  0.107  Ai,_4  +  0.195  +  0.96904  i,_,. 
(0.109) 

(0.018604) 

(0.0800) 

(0.0794) 

Dates  ¢  =  1948:1I  through  1989:I  were  used  for  estimation,  so  in  this  case  the 
sample  size  is  T =  164.  For  these  estimates,  the  augmented  Dickey-Fuller  p 
test  [17.7.35]  would  be 

164 
1  —  0.335  +  0.388  —  0.276  +  0.107 

(0.96904  —  1) =  —5.74. 

Since  —5.74  >  —  13.8,  the  null  hypothesis  that  the  Treasury  bill  rate  follows 
a  fifth-order  autoregression  with  no  constant  term,  and a single  unit  root,  is  - 
accepted  at  the  5%  level.  The  OLS t test  for  this  same  hypothesis  is 
(0.96904  —  1)/(0.018604)  = 

—  1.66. 

m 

Since  —  1.66  >  —2.89,  the  null  hypothesis  of  a  unit  root  is accepted  by the 
augmented  Dickey-Fuller  ¢ test  as  well.  Finally,  the  OLS F test  of  the  joint 
null  hypothesis  that  p  =  1 and  a  =  0 is 1.65.  Since  this  is less  than  4.68,  the 
null  hypothesis  is again  accepted. 

The  null  hypothesis  that  the  autoregression  in  levels  requires  only  four 

lags  is based  on  the  OLS t test  of £,  =  0: 

—0.107/0.0794  =  —1.35. 

From  Table  B.3,  the 5%  two-sided  critical  value  for a ¢ variable  with  158 degrees 
of freedom  is  —  1.98.  Since  —  1.35  >  —  1.98,  the  null  hypothesis  that  only four 
_lags  are  needed  for  the  autoregression  in levels  is accepted. 

Asymptotic  Results  for Other  Autoregressions 

Up to  this  point  in this  section,  we  have  considered  an  autoregression  that is 

a generalization  of case  2 of Section  17.4—a  constant  is included  in the  estimated 
regression,  though  the  population  process  is presumed  to  exhibit  no  drift.  Parallel 
generalizations  for  cases  1, 3, and  4 can  be  obtained  in  the  same  fashion.  The 
reader is invited  to derive  these generalizations  in exercises  at the end of the chapter. 
The  key results  are  summarized  in Table  17.3. 

TABLE  17.3 
Summary  of Asymptotic  Results  for  Autoregressions  Containing  a  Unit  Root 

Case  1; 

Estimated  regression: 

Y,  =  SAy 1 +  GAY H2 +0  +  SpA  par  +  PY-1  +  & 
True  process:  same  specification  as  estimated  regression  with  p  =  | 
Any t or F test  involving  ¢;, 2,  . 
. 
. 
or  F tables  for an  asymptotically  valid  test. 

,  g,-, cam  be compared  with  the  usual  ¢ 

528  Chapter  17  | Univariate  Processes  with  Unit Roots’ 

TABLE  17.3 

(continued) 

Zp-r has  the  same  asymptotic  distribution  as  the  variable  described  under  the 

heading Case  1 in  Table  B.S. 

OLS t test  of  p  =  1  has  the  Same  asymptotic  distribution  as  the- variable 

described  under  Case  | in Table  B.6. 
Case  2: 

Estimated  regression: 

ya  ™  CAy,_  +  oA y,_2  7  LOM  hit  Cp  1A¥  up  +  Trig  +  PY,  - 
True  process:  same  specification  as  estimated  regression  with  a  =  0  and 

1  +  E, 

p=} 

Any  tor  F test  involving  Z,,  4%, .. . , ¢,-1  can  be  compared  with  the  usual  f 

or  F tables  for  an  asymptotically  valid  test. 

Zor has  the  same  asymptotic  distribution  as  the  variable  described  under  Case 

2 in Table  B.S. 

OLS  t  test  of  p  =  1 has  the  same  asymptotic  distribution  as  the  variable 

described  under  Case  2 in  Table  B.6. 

OLS F test  of joint  hypothesis  that  a  =  0 and  p =  1 has  the  same  asymptotic 

distribution  as  the  variable  described  under  Case  2 in Table  B.7. 

Case  3: 

Estimated  regression: 

=  giAy,_,  +  gAy,_2  age  ts 2  SEY  Ee  a 

PY,-1  cs E, 

True  process:  same  specification  as  estimated  regression  with  a  #  (0  and 

p=  1 

| 

fr converges  at  rate  7*”  to  a  Gaussian  variable;  all  other  estimated  coeffi- 

cients  converge  at  rate  T'”  to  Gaussian  variables. 

Any tor F test  involving  any coefficients  from  the regression  can  be i  chao 

with  the  usual  ¢ or  F tables  for  an  asymptotically  valid  test. 

Case  4: 

Estimated  regression: 

| 

eee  ge  aoe, > + 
True  process:  same  specification  as  estimated  regression  with  @  any  value, 

opti  4  Hig  Pepys,  +  Oi  His, 

p=i,andé6.=0 

Any t or F test  involving  ¢,, %,  ... 
or  F tables  for  an  asymptotically  valid  test. 

. 

,  ,-,  can  be  compared  with  the  usual  ¢ 

4 in Table  B.5. 

Zpr has the same  asymptotic  distribution  as  the variable  described  under  Case 
| 
-  OLS  t test  of  p  =  1 has  the  same  asymptotic  distribution  as  the  variable 

described  under  Case  4 in Table  B.6. 

OLS F test  of joint hypothesis  that  p =  1 and  6 =  0 has the  same  asymptotic 

distribution  as  the  variable  described  under  Case  4 in  Table  B.7. 
Notes  to  Table  17.3 

Estimated  regression  indicates  the  form in  which  the  regression  is estimated,  using observations 

t=  1,2,:.., 

T and conditioning  on  observations  ¢ =  0,  —1, 

pt 

True process describes  the  null  hypothesis  under  which  the distribution  is calculated.  In  each 

case  it is assumed  that  roots  of 

| 

are  all outside the unit circle  and  that  e, is i.i.d.  with  mean  zero,  variance  a, and  finite  fourth moment. 

Chee  fr 

eG. ee y= 0 

—Zyy in each  case  is the following  statistic: 

; 

Zi  Toy  —  AW  -  bur  -  S20 

-  {,. ur). 

are  the  OLS  estimates  from  the  indicated  re  ression. . 
i be fur bu p=  iy b i, ee IVGp  ,  where 9p, is the  OLS  standard  ator of py. 

who,  «+». 

h,-17 

OLS F test  of a hypothesis  sisi two restrictions  is given  by’expression  [17.7.39]. 

V.7. "Asymptotic Properties of a pth- Order Autoregression 

529 

Example  17.9 
The  following  autoregression  was  estimated  by OLS  for  the  GNP  data  in  Figure 
17.3  (standard  errors  in  parentheses): 

i 

1a 

y,  =  0.329  Ay,_,  +  0.209  Ay,  —  0.084  Ay,_5 

(0.0777) 

(0.0813) 

(0.0818) 

—  0.075  Ay,_4  +  35.92  +  0.94969  y,_,  +  0.0378  t. 

(0.0788) 

(13.57) 

(0.019386) 

(0.0152) 

Here,  T  =  164 and  the  augmented  Dickey-Fuller p test  is 

————___—___—__—_————-  (0.94969  — 
1  —  0.329  —  0.209  +  0.084  +  0.075  (0 

164 

1)  =  —13.3. 
) 

Since  —  13.3  >  —21.0,  the  null  hypothesis  that  the  log of GNP  is ARIMA(4, 
1, 0) with  possible  drift  is accepted  at the  5%  level.  The  augmented  Dickey- 
Fuller  ¢ test  also  accepts  this  hypothesis: 

(0.94969  —  1)/(0.019386)  =  —2.60  >  —3.44. 

The  OLS F test  of the  joint  null  hypothesis  that  p =  1 and  6  =  O is 3.74  < 
6.42,  and  so  the  augmented  Dickey-Fuller  F test  is also  consistent  with  the  unit 
root  specification. 

Unit  Root  AR(p)  Processes  with  p Unknown 

Various  suggestions  have  been  proposed  for how  to proceed  when  the process 
is regarded  as  ARIMA(p,  1, 0) with  p unknown  but  finite.  One  simple  approach 
is to  estimate  [17.7.11]  with  p taken  to  be  some  prespecified  upper  bound  p. The 
OLS ¢ test  of {;_,  =  0 can  then  be  compared  with  the  usual  critical  value  for  a  ¢ 
statistic  from  Table  B.3.  If the  null  hypothesis  is accepted,  the  OLS F test  of the 
joint  null  hypothesis  that  both  ¢;_,  =  0 and  ¢;_,  =  0 can  be  compared  with  the 
usual  F(2,  T —  k) distribution  in Table  B.4.  The  procedure  continues  sequentially 
until  the  joint  null  hypothesis  that  ;_,  =  0,¢;-.  =  0,...  ,  {5-,  =  Ois  rejected 
for  some  €. The  recommended  regression  is then 

y,  =  (Ay,  +  GAy,-2  + 

°*  +  b5-cAY:-s+e  +  a  +  py,_,  +  St. 

If no  value  of @ leads  to  rejection,  the  simple  Dickey-Fuller  test  of Table  17.1  is 
used.  Hall  (1994)  discussed  a variety  of alternative  strategies  for  estimating  p. 

Just  as  in the Phillips-Perron  consideration  of the MA(©)  case,  the researcher 
might  want  to choose  bigger  values  for p, the  autoregressive  lag length,  the  larger 
is the  sample  size  T.  Said  and  Dickey  (1984)  showed  that  as  long  as  p goes  to 
infinity  sufficiently  slowly  relative  to  7, then  the  OLS  ¢ test  of  p =  1 can  continue 
to  be compared  with  the  Dickey-Fuller  values  in Table  B.6. 

Again,  it is worthwhile  to  keep  in  mind  that  there  always  exists  a  p such 
that  an  ARIMA(p,  1, 0) representation  can  describe  a  stationary  process  arbi- 
trarily  well  for  a  given  sample.  The  Said-Dickey  test  of  p =  1 might  therefore 
best  be  viewed  as  follows.  For  a  given  fixed  p,  we  can  certainly  ask  whether  an 
ARIMA(p  ~  1,  1, 0) describes  the  data  nearly  as  well  as  an  ARIMA(p,  0, 0). 
Imposing  p  =  1 when  the  true  value  of p is close  to  unity  may  improve  forecasts 
and small-sample estimates  of the other parameters.  The Said-Dickey  result permits 
the  researcher  to  use  a  larger  value  of p on  which  to  base  this  comparison  the 
larger  is the  sample  size  T. 

7 

530  Chapter  17  | Univariate  Processes  with  Unit Roots 

17.8.  Other  Approaches  to  Testing for Unit  Roots 
This section  briefly  describes  some  alternative  approaches  to testing  for unit  roots. 

Variance  Ratio  Tests 

\ 

Let 

where 

Ay,  =  a+  u,, 

-> We,  =  W(L)e, 

|  for €, a white  noise  sequence  with  variance  a.  Recall  from  expression  [15.3.10] 

that  the  permanent  effect  of e, on  the  level  of y,,,  is given  by 

1  Sa odess ee 4). 

| dl 
A cocked 
1 al? acti Ms Meee 

3 is sliina  or stationary around a deterministic time trend, an innovation  ¢, 

T  ae effect. on y, requiring y(1) = 

: 

Coc 

and Lo and MacKinlay  (1988 proposed a test fora unit roots 

Mat exploits  this property.  Consider the pean  in y over  s periods, 

sree  cSt 
ebuntcrintuitive 

‘Yey  Meas  Mes + West “+t  +  he 

ay.841) 

34 te t  ABESKE 

SA.  ; 

x  ’ 

* 

a 

a 

~~ 

ren  ch Sa ~P  COR  POR  BC  4  u xi  BAS  *  2 ‘ns a  Hz SYDa 
EEO  AAG: =  Wis = <a  ee  ot ed ytnsa 
a 

+  cee tO 

eal .e 

ne: 

Ce 

o 

this  statistic  should  go  to  zero  for  large  s.  If the  true  process  for  y,  is /(1),  this 
statistic  gives  a  measure  of  the  quantitative  importance  of permanent  effects  of e€ 
as  reflected  in  the  long-run  multiplier  (1).  However,  the  statistic  in  [17.8.4]  is 
not  reliable  unless  s  is much  smaller  than  T. 

If  the  data  truly  followed  a  random  walk  so  that  y(L)  = 

1,  then  J(s)  in 
[17.8.5]  would  equal s -  o for any s, where  a? is the variance  of u,.  Lo and MacKinlay 
(1988)  exploited  this  property  to suggest  tests  of the  random  walk  hypothesis  based 
on  alternative  values  of s.  See  Lo  and  MacKinlay  (1989)  and  Cecchetti  and  Lam 
(1991)  for  evidence  on  the  small-sample  properties  of these  tests. 

Other  Tests  for  Unit  Roots 

The  Phillips-Perron  approach  was  based  on  an  MA()  representation  for Ay,, 
while  the Said-Dickey  approach  was  based.on  an AR(~)  representation.  Tests  based 
on  a  finite  ARMA(p,  q) representation  for  Ay, have  been  explored  by Said  and 
Dickey  (1985),  Hall  (1989),  Said  (1991),  and  Pantula  and  Hall  (1991). 

' 

A  number  of other  approaches  to  testing  for  unit  roots  have  been  proposed, 
including  Sargan  and  Bhargava  (1983),  Solo  (1984),  Bhargava  (1986),  Dickey  and 
Pantula  (1987),  Park  and Choi  (1988),  Schmidt  and  Phillips  (1992),  Stock  (1991), 
and  Kwiatkowski,  Phillips,  Schmidt,  and  Shin  (1992).  See  Stock  (1994)  for  an 
excellent  survey.  Asymptotic  inference  for processes  with  near  unit  root  behavior 
has  been  discussed  by Chan  and  Wei  (1987),  Phillips  (1988),  and  Sowell  (1990). 

°17.9.  Bayesian Analysis  and  Unit  Roots 

Up to  this  point  in the  chapter  we  have  adopted a classical  statistical  perspective, 
calculating  the  distribution  of p conditional  on  a particular  value  of p such  as  p  =  1. 
This  section  considers  the  Bayesian  perspective,  in  which  the  true  value  of p is 
regarded  as  a  random  variable  and  the  goal  is to  describe  the  distribution  of this 
random  variable  conditional  on  the  data. 

Recall  from  Proposition  12.3 that if the prior density for the vector  of unknown 
coefficients  B and  innovation  precision  o  ~?  is of the Normal-gamma  form  of [12.1.19] 
_  and  [12.1.20],  then  the  posterior  distribution  of B conditional  on  the  data  is mul- 
tivariate  ¢.  This  result  holds  exactly  for  any  finite  sample  and  holds  regardless  of 
‘whether  the process  is stationary.  Thus,  in the case  of the  diffuse  prior distribution 
represented  by  N =  A  =  0 and  M~'  =  0, a  Bayesian  would  essentially  use  the 
usual  ¢ and F statistics  in the  standard  way. 

How  can  the classical  distribution  of 6 be strongly skewed  while  the  Bayesian 
distribution  of p is that  of a symmetric  ¢ variable?  Sims  (1988)  and  Sims  and  Uhlig 
(1991)  provided a detailed  discussion  of this question.  The  classical  test  of the  null 
hypothesis  p  =  1 is based  only  on  the  distribution  of 6 when  the  true  value  of p 
is unity.  By contrast,  the Bayesian  inference  is based  on  the  distribution  of |p for 
all the  possible  values  of p, with  the  distribution  of 6|p weighted  according  to  the 
prior probability  for p.  If the distribution  of 6|p had the same  skew  and  dispersion 
for every  p as  it does  at p =  1, then  we  would  conclude  that,  having  observed  any. 
particular  6, the true  value  of p is probably  somewhat  higher.  However,  the  dis- 
tribution  of |p changes  with  p—the  lower  the  true  value  of p,  the  smaller  the 
skew  and  the greater  the dispersion,  since  from  (17.13) the  variance  of VT(é  —  p) 
is approximately (1 —  p?). Because  lower  values  of p imply greater  dispersion  for 
p, in the absence  of skew  we  would  suspect  that a given observation  6 =  0.95  was 
more  likely  to  have  been  generated  by a  distribution  centered  at  p  =  0.90  with 
532  Chapter 17 | Univariate  Processes  with  Unit Roots 

large  dispersion  than  by  a  distribution  centered  at  p  = 
1 with  small  dispersion. 
The effects of skew  and  dispersion  turn  out  to  cancel,  so  that  with  a  uniform  prior 
distribution  for  the  value  of p,  having  observed  p =  0.95,  it is just  as  likely  that 
the  true  value  of p is greater  than  0.95  as  that  the  true  value  of p is less  than  0.95. 

Example  17.10 
For  the  GNP  regression  in Example  17.9, ‘the  probability  that  p =  1 conditional 
on  the data  is the  probability  that  a ¢ variable  with  T =  164  degrees  of freedom" 
exceeds  (1  —  0.94969)/0.019386  =  2.60.  From  Table  B.3,  this  probability  is 
around  0.005.  Hence,  although  the  value  of p must  be  large,  it  is  unlikely  to 
be  as  big as  unity. 

The  contrast  between  the  Bayesian  inference  in  Example  17.10  and  the  clas- 
sical  inference  in  Example  17.9  is  one  of  the  reasons  given  by  Sims  (1988)  and 
Sims  and  Uhlig  (1991)  for  preferring  Bayesian  methods.  Note  that  the  probability 
calculated  in  Example  17.10  will  be  less  than  0.025  if and  only  if a  classical  95% 
confidence  interval  around  the  point  estimate  p does  not  contain  unity.  Thus,  an 
alternative  way  of  describing  the  finding  of  Example  17.10  is  that  the  standard 
asymptotic  classical  confidence  region  around 
does  not  include  p  =  1.  Even  so, 
Example  17.9  showed  that  the  null  hypothesis  of  a unit  root  is  accepted  by the 
augmented  Dickey-Fuller  test.  The  classical  asymptotic  confidence  region  centered 
at  p  =  p seems  inconsistent  with  a  unit  root,  while  the  classical  asymptotic  con- 
fidence  region  centered  at p =  1 supports  a unit  root.  Such  disconnected  confidence 
regions  resulting  from  the  classical  approach  may  seem  somewhat  troublesome  and 
counterintuitive.'*  By contrast,  the  Bayesian  has  a  single,  consistent  summary  of 
the  plausibility  of  different  values  of  p,  which  is  that  implied  by  the posterior 
distribution  of p conditional  on  the data. 

One  could,  of course,  use  a prior distribution  that  reflected  more  confidence 
in the  prior information  about  the  value  of p.  As  long as  the  prior distribution  was 
in the  Normal-gamma  class,  this  would  cause  us  to shift  the  point  estimate  0.94969 
in the  direction  of the  prior  mean  and reduce  the  standard  error  and  increase  the 
degrees  of  freedom  as  warranted  by the  prior  information,  but  a  ¢ distribution 
would  still  be  used  to  interpret  the  resulting  statistic. 

_ 

Although  the  Normal-gamma  class  is convenient  to  work  with,  it might  not 
be sufficiently  flexible  to  reflect  the  researcher’s  true  prior  beliefs.  Sims  (1988,  p. 
470)  discussed  Bayesian  inference  in which  a  point  mass  with  positive  probability 
is placed on  the possibility  that  p =  1. DeJong and Whiteman  (1991) used  numerical  — 
methods  to  calculate  posterior  distributions  under  a  range  of prior  distributions 
defined  numerically  and  concluded  that  the  evidence  for  unit  roots  in  many  key 
) 
economic  time  series  is quite  weak. 
Phillips (1991a)  noted  that  there  is a prior distribution  for which  the  Bayesian 
inference  mimics  the  classical  approach.  He  argued  that  the  diffuse  prior  distri- 
bution  of Proposition  12.3  is actually  highly  informative  in a time  series  regression 
and  suggested  instead  a  prior  distribution  due  to  Jeffreys  (1946).  Although  this 
prior distribution  has  some  theoretical  arguments  on  its behalf,  it has  the  unusual 
property  in this  application  that  the  prior  distribution  is a  function  of the  sample 
size  T—Phillips  would  propose  using  a  different  prior  distribution  for f(p)  when 

be 

"Recall  from  Proposition  12.3(b)  that  the  degrees  of freedom  are  given  by N*  =  N  +  T. Thus, 
the  Bayesian  interpretation  is not  quite  identical  to  the  classical  / statistic,  whose  degrees  of freedom 
would  be  T —  k. 

“Stock  (1991)  has  recently  proposed  a  solution  to  this  problem  from  the  classical  perspective. 
Another  approach  is to rely on  the  exact  small-sample  distribution,  as  advocated by Andrews  (1993). 

f 

17.9.  Bayesian  Analysis  and Unit Roots  533 

the  analyst  is going  to  obtain  a  sample  of size  50 than  when  the  analyst  is going  to 
obtain  a  sample  of size  100.  This  would  not  be  appropriate  if the  prior  distribution 
is intended  to  represent  the  actual  information  available  to  the  analyst  before  seeing 
the  data.  Phillips  (1991b,  pp.  468-69)  argued  that,  in  order  to  be  truly  uninfor- 
mative,  a prior distribution  in this  context  would  have  this  property,  since  the  larger 
the  true  value  of p,  the  more  rapidly  information  about  p contained  in  the  sample 
{y,,  Yo  --  +»  Yr}  is going  to  accumulate  with  the  sample  size  7.  Certainly  the 
concept  of  what  it  means  for  a  prior  distribution  to  be  “‘uninformative’’  can  be 
difficult  and  controversial.'° 

The  potential  difficulty  in  persuading  others  of  the  validity  of  one’s  prior 
beliefs  has  always  been  the  key  weakness  of  Bayesian  statistics,  and  it  seems 
unavoidable  here.  The  best  a:Bayesian  can  do  may  be  to  take  an  explicit  stand  on 
the  nature  and  strength  of prior  information  and  defend  it as  best  as  possible.  If 
the  nature  of the  prior  information  is that  all  values  of p are  equally  likely,  then 
it is satisfactory  to  use  the  standard  OLS  t and F tests  in  the  usual  way.  If one  is 
unwilling  to  take  such  a stand,  then  Sims  and  Uhlig  urged  that  investigators  report 
both  the  classical  hypothesis  test  of  p  =  1  and  the  classical  confidence  region 
around  p and  let  the  reader  interpret  the  results  as  he  or  she  sees  fit. 

APPENDIX  17.A.  Proofs  of Chapter  17 Propositions 

®  Proof  of Proposition  17.2. 

Observe  that 

sare ee 1  +  Yré,2 

+  -°°  +  Hey  +  Ye.) 

>} 

+  {Ye,-1  +  Whe,-2  +  Woe,  +  -->  +  W,-1€  +  We_,  +  -*°} 
+  {Yoe,-2  +  Wie -3 +  Yre,-4  +  °°  +  U,-2€  +  We,  +  +--+} 
tore  +  {WHe,  +  Wey  +  We,  +  th 

=  We,  +  (to  +  We.  +  (Wo  +  wy  +  Wr)E,-2  +  °° 

+  (ty  +h  +  ty tore  + Wider  + (+  ty to  +  Hew 

a 

Ae  tO 

re 

Oe  eo 

=  (Yo  Ky  +  fr  +  -°+)Ee,  —  (Wy  +  Y.  +  YW,  +  dé, 

7 TS)  ES  (fv.  +  w,  +  Ry  | aa 

+  (Wh  +h  t+ 
+  (Wo  +  We  +  te  +  de  -  (Us  +  We  +  --+)e-2  +  °° 
+  (ty  +  Wy  +  Ya  Hoesen  (yy  +  Wert  code, 
+  (Ws, +  p,  +  y,  +  we  )ay  rT Ca  thet  ***  )Ey 
+  (Yo  +  ty  +  Wy  +  de,  —  (Wen  +  Bi  Bs  JEM  “ee 

2  =  WI): Xe,  +  1,  —  Mw 

(17.A.1] 

or 

“where 

m=  -(W  +h  +  Ww  +  *  S3G4  2  (W.  +  YW,  + a 

—  (Ws  +  Wet  Wy  +  eg  — 

Mm  =  —(y,  +  Y.  +  Py  +  *+)ey  =  (2.  +  Wy  +  Wy  Pitse}t., 

—  (Wr  +  Ve  + Uy  +  ey  = 

"See  the  many  comments  accompanying  Phillips  ( 1991a). 

534  Chapter  17  | Univariate  Processes  with  Unit Roots 

Notice  that  n,  =  2,a,¢,_,,  where  a,  =  —(Wsi  +  Wer  +  °°),  with  {a,}~,  absolutely 
summable: 

aa 

Dial =i  + w+  w+  Tt  Wt  t  he tol  tl  thet  be  tod  + 

=  {|¥,|  +  lw  a  lps!  +  so}  +  {|p|  +  lys|  +  | Wa! +  as  | 
=  |Wil + 2lvsl + 3lval + ==: 

+  {|vs] +  loa]  +  [Ws]  +  ---}  + 

.  ba ALAS 

,=0 

which  is bounded  by the  assumptions  in Proposition  17.2. 

# 

®  Proof of Proposition  17.3. 

CUS 
—a—— 

(a)  This  was  shown  in [17.5.9]. 
(b) This  follows  from  [7.2.17]  and  the fact hel E(u?)  =  y. 
(c) This is implied by [7.2.14]. 
_(d) Since  é, =  2{_,u,, Proposition  17.2  asserts  that 

Tha oe,  De 

tind  KUT 

=e  Sewer  d (v0 «+ e,  + 4. =  mje 2 

4 

ps 

=  eee <  etry (i=  Wee 

wtf fe  oi) OF  1  TREO 

= 40): ey seat + eer Ee siecle 

trom  result  (c). Thus,  [17.A.7]  converges  to 

Le 

T'S  &_.u, > (12)?  [WP  —  yobs 

< 

[17.4.9] 

; 

= 

which  establishes  result  (e) for 

For j >  0, observe  that 

=  0. 
| 

ue 

E,2=  €  38 +  U,_;  +  Uy  j;4)5  de  +t  HM 

the 

implying  that 

7. 

tr. 

shies  Dy $M)  -  7  DAC ie  ech  Stott 

1=jt 

; 

1=jt 

z 

m  tenes i i 

c 

+  U,—\)U,-; 

; 

. 

: 

[17.A.10] 

oT 

: 

+  TD  ogee Mare  4 oh  inact Marr ca 

: 

But 

AD 6, = UF -  reer =D E-m,S  (2)  (WOE  ~ 

et  the 

tigi 

“4 

ae 

bein  5 

Rei 
pepe - he,  ee 

HES 

i  hag 

3 

nb 

ees 

Chapter  17 Praperiines 

dee MHA.  Proofs  of 

sons 
2 
To  Bes tsa + oe Pay wt nt Be +: et He 

| 
from result (c). This, 17.4.10} eo  x. Se  eS  re 

$2 

* 

rt BA Bay ume  = 10h +. et n 7  % +  Pt  ole  o> 

= 

? 

a 

- 

and X;(r) defined  in [17.5.4],  we  have 

T~ E/T + §3/T + +++  + €3_,/T} = [ [VE-X,(07 dr 

[o- WP: [ [W(n)f ar, 

by virtue  of [17.5.5]. 

(i) As in [17.3.23], 

yf 
T°  >» 4,_, 

T 

r=  2 (/T)-(€,_,/T?) 

Te  f {([Tr]*  +  1)/T}-{(u,  +  wu,  +  +++  +  Ur,-)/T}  dr 

*  774  [ {({Tr]*  +  1)/T}-X;,(r)  dr 

—> o-w(1)-[ rW(r)  dr, 

from  [17.5.5]  and  the  continuous  mapping  theorem. 
(j) From  the  same argument  as  in (i), 

"ae Aa = -  3 wee ae  = 

ee 

. 

(a) al (Bh) 

=r] {({Tr]*  +  big {(u, + a ee  +  U7,-)/T¥ dr 

:  a sin  stab  ted =f ae ‘3 yr} ist mF dr 

bij  a3 Sue, 

+ 

.  -ouiny  yas  od | ff?  ® Se vr: [wor ar teeih  bas  , an isonsney  7133 oe eel thie 
“<  ¢  WG)  3  oohanitie 2. 16 rsbienoD ors  geibuion 

are 

so eb fen Pe 

7.1, repeated in this propos 

at  ‘tise soul beni Sd mitt owen JO CH ob 2a 
Bo  aggre ane  eeevee 

Nae! 
Sah 

Ce +  att +  ,. 375  +  “he  ae 
: 

bbe 

ei Riese ee 3. Sara eyo 

Dee  =. ss “cate! es 

aaa  he 

fs  MY 

A  =  o-W(1).  Use  Proposition  17.3  to  show  that 

(a)  T(py  -  1)> 

a  | [W(r))?  dr 

(b)  T?-63,> 

Yo 

a  | [W(r)}?  dr 

’ 

(c)  tp—> (AW)? 

4{WQ)P  =  1 
| [W(NP  ar}  | | [W(r)P  ar} 

$0?  —  wv) 

, 

(d)  T(6r  —  1) -  $(T?-65,  +  87)’  —  ) 

4 MW(DE  =  1 
| [W(r)}?  dr 

(6) (IA2)!2-te  ~  WOE  —  WA)  x  {Td y, +  57)-5  SUPOE 

c 

4{{wi)P  -1 

= 
{| [W(r)}  ar} 

Suggest  estimates  of y,  and  A? that  could  be  used  to  construct  the  statistics  in  (d) and  (e), 
and  indicate  where  one  could  find  critical  values  for  these  statistics. 

17.3. 
Phillips  and  Perron  (1988)  generalization  of case  4.  Suppose  that  data  are  generated 
from  the  process  y,  =  a  +  y,_,  +  u,,  where  u,  =  #(L)e,,  Zo j:|h|  <  ~,  and  «,  is i.i.d. 
with  mean  zero,  variance  o°,  and  finite  fourth  moment,  and  where  a  can  be  any  value, 
including  zero.  Consider  OLS  estimation  of 

As in [17.4.49],  note  that  the  fitted  values  and  estimate  of p from  this  regression  are  identical 
to those  from  an  OLS  regression  of y, on  a constant,  time  trend,  and  é,_,  =  y,_,  —  a(t  —  1): 

y,  =  a  +  py,_,  +  Ot  +  u,. 

y,  =  a®*  +  p*é,_,  +  674  +  u,, 

where,  under  the assumed  data-generating  process,  é, satisfies  the assumptions  of Proposition 
17.3.  Let (&$, 6%, 5¢)' be the  OLS estimates  given  by equation  [17.4.50],  s2 =  (T —  3)-'  x 
Zu;  the  OLS  estimate  of the  variance  of  the  regression  error,  63; the  OLS  estimate  of 
the  variance  of p7 given  in  [17.4.54],  and  t}  =  (67  —  1)/é,;  the  OLS  + test  of p  = 
1. 
Recall  further  that  p;, 63;, and  f+ are  numerically  identical  to  the  analogous  magnitudes 
for the original  regression,  6;, 63,, and  t;.  Finally,  define  A =  o- p(1).  Use  Proposition  17.3 
to show  that 

1  >  ae 
(a).  | TG.  TDR,  THEE 
C2? 

(ie SM  Magia”  AEN 

1 7°2t 

ae 

1 

| W(r)  dr 

1/2 

% 

A  0  | W(r)  dr  | [W(r)}?  dr  | rW(r)dr|}|}0  A 
0  1 

0 

1/2 

| rW(r)  dr 

1/3 

' 

Ty, 
(b)  | T~'Z,u,] 
TS, 

10  0 

W(1) 

+A}  O  A  O}  | HEW)P  =  [v/a]  ; 

0  0  1)] 

way - | ww  ar 

538  Chapter 17 | Univariate Processes with Unit Roots 

T "a2 

A  00 

1 

| W(r)  dr 

1/2 

()  | T@-1)  | >] 
T*(5%  —  a) 

0  1  0  | W(r) dr  [ wor  ar  | rW(r) dr 
Yo  2 

1/2 

| rW(r)  dr 

1/3 

W(1) 

x  | H[W(I)P  —  [y0/A7]} 

|; 
W(1)  -  | W(r) ar} 

(d)  T?- 6j,> (s7/A*)[(0 

1  O]  | W(r)  dr  : [W(r)}?  dr  | rW(r)  dr 

1 

| W(r)  dr 

1/2 

0 
l 
0 

1/2 

7  | rW(r) dr 

1/3 

=  (s7/A*)-Q; 

| 

afed em Aly)" Tbr —  Ivo; 
() T(r ~  1) - 4(7?-63, + S30? — yw) 

> 

oi 

ee 

ee 

Sta  pil 12) 
worid  moizeszg31 inant 

asd:  +h  =  [wo dr  : 

4 

1/2 

| 

40  1  0]  te - dr  fi (WOOP  ar [ wo dr 

| 

ileal  8 
oo 
ee  fi 

i  nee  9) 

aif agen cca al 
inte.  lw 

ie  ar 

UY 

’ 

= ba  es % 3  ¥ 3  . 

(1)  i 

how 

id 
yd 
riers 

“ 

(©)  Gr  -  14, &  AOE  = 
{[ wear ar| 

(wi)?  -  1 

c 

Where  could  you  find  critical  values  for  the  statistics  in  (b)  and  (c)? 

17.5. 

Generalization  of case  3 for autoregression.  Consider  OLS  estimation  of 

y,  =  g,Ay,-1  +  G2dyiis  +  oes  +  Gy -AY,-poi  +  a  +  OM-1  +  En 
where  e,  is i.i.d.  with  mean  zero,  variance  a”,  and  finite  fourth  moment  and  the  roots  of 
—  g,-,z"~')  =  0 are  outside  the  unit  circle. 
(1  —  2,z  —  2.27  — 

+++ 

(a)  Show  that  the  fitted  values  for this regression  are  identical  to those  for the  following 

transformed  specification: 

. 

Yo=  Os  th  lotsa 
where  u, = Ay,  —  wand wp =  a/(l  —  ¢,  —  g,  —  ++: 

iat  +  £5 aise  es  +  p+  py,-,  +  &, 
—  &,_,). 

(b)  Suppose  that  the  true  value  of p is 1 and  the  true  value  of  a  is nonzero.  Show 

that  under  these  assumptions, 

Se  eve!  re  ¢,L  a  g,L*  i?  12  ee  (7  Er  Ye, 

ee  ey  +  f,- 4. 

where 

2G  pe 

a 

ht  se  tM 

Conclude  that  for fixed  y,,  the  variables  u, and  é, satisfy  the assumptions  of Proposition  17.3 
and  that  y, is dominated  asymptotically  by a  time  trend. 

(c)  Let  y,  =  E(uu,_;),  and  let  C; =  (Z,. T  | eS s+  aki 1.7)’  be  the  (p  —  1)  x  1 
vector  of estimated  OLS  coefficients  on  (u,_,,  U2,  ...,  U,- p+1);  these,  of  course,  are 
identical  to the  coefficients  on  (Ay,_,,  Ay,->,..  - 
that if p =  1 anda  #  0, 

,  Ay,_,,,) in  the  original  regression.  Show  — 

T'é;  -  0] 
T'Gi7  =  e212 
Tp,  —,  1) 

0 
[Vo 
gal2ide 
1 
0  p/2  p/3 

L 

> 

’ 

a 

7°'fh, 
ihsh. 
h, 

where 

: 

h, 
h,|~  N{ 

oO) 
Pv 
{0},  07/0’ 

gaty 
p/2 
31 

h, 

0 

0’  pl2  7/3 

and  V  is  the  matrix  in  [17.7.19].  Conclude  as  in the analysis  of Section  16.3  that  any  OLS 
t or  F test  on  the original  regression  can  be compared  with  the standard  ¢ and F tables  to 
give an  asymptotically  valid  inference. 

17.6. 

Generalization  of case  4 for autoregression.  Consider  OLS  estimation  of 

Pi  his  g,Ay,-  +  ¢,Ay,-2  isn  a  bo- Ay, - -p+l  +a+t  PY,-1  a  +  é,, 
where  €, is i.i.d.  is  hy mean  zero,  variance  o*,  and  finite  fourth  moment  and the roots  of 
(1  -  gz  —  g,z7  —  +: 
—  {,.,z”~')  =  0 are  outside  the  unit  circle. 

(a)  Show  that  the  fitted  values  of this  regression  are  numerically  identical  to those  of 

the  following heh." 

=  fi.)  +  flt,-2  ts  +  $y  1M, - =-pot  *  p*  +  pé,_;  +  dt  +  €,, 

vere  «;'6-dy, Chin Wiaee ae 
oie 
w(t —  1), and 6* =  6 +  pp.  Note  that  the estimated 1 coefficients ; and Figs their ten 
errors will  be identical  for  the  two  regressions. 

f,-1), B” =  (1 — pm, &-, 

eh  WE 

these  assumptions, 

(b)  Suppose  that  the  true  value  of pis  1 and  the  true  value  of dis 0. Show  that — 
w=  [IM  ~  {Lb  -  GL?  = +  -  % Ye, 

Gini 

Yo  +  Uy,  +  Uy  Hes 

Hy, 

540  Chapter 17  | Univariate  Processes  with  Unit Roots 

we that  for  fixed  y,,  the  variables  u,  and  é, satisfy  the  assumptions  of  Proposition 

(c)  Again  let p  =  1 and  &  =  0, and  define  y,  =  E(uyu,_,)  and 

Aol  -  Q-QGQree-  a 

Show  that 

r'x(¢,  - 
T(6r  ~  1) 
T($%  —  8*) 

pags » 

v 

7 

0’ 

0 

1 

0 

0 

1/2 

af W(r)  dr 

) 
0  a-| Wir)  dr  A  [wo  dr  i  [ wo  dr 

aha 

0’ 

1/2 

a-| rW(r)  dr 

1/3 

h, 
a:  W(1) 
toA{(W(1)]}?  —  1} 

{| Wit)  -  [ wo  ar} 

where  h, ~  N(0,  o?V)  and V is as  defined  in [17.7.19]. 

(d)  Deduce  from  answer  (c) that 
T(E,  -  ) > NO,  o?V-'); 
T (6; —  1)/(1  ia fi, =  Gz re 

ee  | SA) 

1 

| W(r) dr 

1/2 

W(1) 

+[0  1  0]  | W(r)  dr  { [W(r)}*  dr  { rW(r)  dr 

4{(W(1)}?  —  1} 

7 

| rW(r)  dr 

1/3 

W(1)  -  | W(r) dr 

=  V; 

(6;  -  1/6, > V  +  VO, 

where 

1 

| W(r)  dr 

1/2 

-1 

0 

Q=(0  1  Q 

W(r)  dr  | [W(r)}?  dr  | rW(r)  dr  q 

1/2 

| rW(r)  dr 

1/3 

0 

Notice  that  the  distribution  of V is the  same  as  the  asymptotic  distribution  of the  variable 
tabulated for case 4 in Table B.5, while the distribution  of V/VQ is the same  as the asymptotic 
distribution  of the  variable  tabulated  for case  4 in Table  B.6. 

Chapter 17  References 

Andrews,  Donald  W.  K.  1991.  ‘‘Heteroskedasticity  and  Autocorrelation  Consistent  Co- 
variance  Matrix  Estimation.”  Econometrica  59:817-58. 
——.  1993.  “Exactly  Median-Unbiased  Estimation  of First  Order  Autoregressive/Unit 

_ Root  Models.””  Econometrica  61:139-65. 
Beveridge,  Stephen,  and  Charles  R. Nelson.  1981.  “A  New  Approach  to  Decomposition 
of Economic  Time  Series  into  Permanent  and  Transitory  Components  with Particular  At- 
-  tention to Measurement  of the  ‘Business  Cycle.’  Journal  of Monetary  Economics  7:151- 

74. 

Chapter 17 References  541 

Bhargava,  Alok.  1986.  “On  the  Theory  of Testing  for Unit  Roots  in Observed  Time  Series.”’ 
Review  of Economic  Studies  53:369-84. 
Billingsley,  Patrick.  1968.  Convergence  of Probability  Measures.  New  York:  Wiley. 
Campbell,  John  Y.,  and  Pierre  Perron.  1991.  ‘Pitfalls  and  Opportunities:  What  Macroe- 
conomists  Should  Know  about  Unit  Roots.”  NBER  Macroeconomics  Annual.  Cambridge, 
Mass.:  MIT  Press. 
Cecchetti,  Stephen  G.,  and  Pok-sang  Lam.  1991.  **What  Do  We  Learn  from  Variance  Ratio 
Statistics?  A  Study  of  Stationary  and  Nonstationary  Models  with  Breaking  Trends."’  De- 
partment  of  Economics,  Ohio  State  University.  Mimeo. 
Chan.  N.  H..  and  C.  Z.  Wei.  1987.  “Asymptotic  Inference  for  Nearly  Nonstationary  AR(1) 
Processes."  Annals  of Statistics  15:10S50-63. 

_  1988.  “Limiting  Distributions  of  Least  Squares  Estimates  of  Unstable 

Autoregressive  Processes.”  Animals  of Statistics  16:367-401. 
Cochrane.  John  H.  1988.  “How  Big  Is  the  Random  Walk  in  GNP?”  Journal  of Political 
Economy  96:893-920. 
DeJong,  David  N.,  and  Charles  H.  Whiteman.  1991.  Reconsidering  ‘Trends  and  Random 
Walks  in  Macroeconomic  Time  Series.’"’  Journal  of Monetary  Economics  28:221-54. 
Dickey,  David  A.,  and  Wayne  A.  Fuller.  1979.  “Distribution  of  the  Estimators  for  Auto- 
regressive  Time  Series  with  a  Unit  Root.”  Journal  of the  American  Statistical  Association 
74:427-31. 
and 

.  1981.  “Likelihood  Ratio  Statistics  for  Autoregressive  Time  Series  with 

and 

a  Unit  Root.”  Econometrica  49:1057-72. 

and  S. G.  Pantula.  1987.  Determining  the  Order  of Differencing  in Autoregressive 

Processes.”’  Journal  of Business  and  Economic  Statistics  5:455-61. 
Evans,  G.  B.  A., andN.  E. Savin.  1981.  “Testing  for  Unit  Roots:  1."°  Econometrica  49:753- 
79. 

and 

.  1984.  “Testing  for  Unit  Roots:  2.  Econometrica  52:1241-69. 
Fuller,  Wayne  A.  1976.  Introduction  to  Statistical  Time  Series.  New  York:  Wiley. 
Hall,  Alastair.  1989.  “‘Testing  for  a  Unit  Root  in  the  Presence  of Moving  Average  Errors.” 
Biometrika  76:49-56. 

.  1994.  “Testing  for  a  Unit  Root  in  Time  Series  with  Pretest  Data  Based  Model 

Selection.”’  Journal  of Business  and Economic  Statistics  12:461—70. 
Hall,  P.,  and  C.  C.  Heyde.  1980.  Martingale  Limit  Theory  and  Its  Application.  New  York: 
Academic  Press. 
Hansen,  Bruce  E.  1992.  ‘‘Consistent  Covariance  Matrix  Estimation  for  Dependent  Heter- 
ogeneous  Processes."’  Econometrica  60:967-72. 
Jeffreys,  H.  1946.  “‘An  Invariant  Form  for  the  Prior  Probability  in  Estimation  Problems.™ 
Proceedings  of the  Royal  Society  of London  Series  A,  186:453-61. 
Kim,  Kiwhan,  and  Peter  Schmidt.  1990.  ‘Some  Evidence  on  the  Accuracy  of Phillips-Perron 
Tests  Using Alternative  Estimates  of Nuisance  Parameters.”  Econoynics  Letters  34:345-S0. 
Kwiatkowski,  Denis,  Peter  C.  B.  Phillips,  Peter  Schmidt,  and  Yongcheol  Shin.  1992.  **Test- 
ing the  Null  Hypothesis  of Stationarity  against  the  Alternative  of  a Unit  Root:  How  Sure 
ae We  That  Economic  Time  Series  Have  a  Unit  Root?”  Journal  of Econometrics  54:159- 

Lo,  Andrew  W.,  and  A.  Craig  MacKinlay.  1988.  “Stock  Prices  Do  Not  Follow  Random 
Walks:  Evidence  from  a  Simple  Specification  Test."’  Review  of Financial  Studies  1:41-66. 
.  1989.  ‘The  Size and  Power of the Variance  Ratio Test  in Finite Samples: 

and 

A Monte  Carlo  Investigation."’  Journal  of Econometrics  40:203-38.  . 
Malliaris,  A. G.,  and  W.  A.  Brock.  1982.  Stochastic  Methods  in  Economics  and  Finance.  — 
Amsterdam:  North-Holland. 
Pantula,  Sastry  G.,  and  Alastair  Hall.  1991.  “Testing  for  Unit  Roots  in  Autoregressive 
Reet: ita Models:  An  Instrumental  Variable  Approach.”  Journal  of Econometrics  — 

Park,  Joon  Y., and  B, Choi.  1988.  ‘A  New  Approach  to Testing  for a Unit  Root."’  Cornell 
University.  Mimeo. 
Park,  Joon  Y.,  and  Peter C. B.  Phillips.  1988.  “Statistical  Inference  in  Regressions  with 
Integrated  Processes:  Part  1."  Econometric  Theory  4:468-97. 

alias 

nes 

542  Chapter 17  | Univariate  Processes  with  Unit Roots 

and 

1989.  “Statistical  Inference  in  Regressions  with  Integrated  Processes: 

Part  2.”  Econometric  Theory  5:95-131. 
eee  Pe fe — 3 psenges standing  Spurious  Regressions  in Econometrics.”  Journal  of 
.  1987.  “Time  Series  Regression  with  a  Unit  Root.”  Econometrica  55:277-301. 
.  1988.  “Regression  Theory  for Near-Integrated  Time  Series.”  Econometrica  56:1021- 

43. 

.  199la.  “To  Criticize  the  Critics:  An  Objective  B 

A 

Trends."’ Journal  of Applied  Econometrics  6:333- 64.  2  oN 
.  1991b.  “Bayesian  Routes  and  Unit  Roots:  De  Rebus  Prioribus  Semper  Est  Dis- 

putandum. "  Journal  of Applied  Econometrics  6:435-73. 

and  Pierre  Perron.  1988.  ‘Testing  for  a  Unit  Root  in  Time  Series  Regression.” 

Biometrika  75:335—46. 

and  Victor  Solo.  1992.  ““Asymptotics  for  Linear  Processes.”  Annals  of Statistics 

20:971-1001. 
Said,  Said  E.  1991.  “Unit-Root  Tests  for  Time-Series  Data  with  a  Linear  Time  Trend.”’ 
Journal  of Econometrics  47:285-303. 

and  David  A.  Dickey.  1984.  “Testing  for  Unit  Roots  in  Autoregressive — —-Moving 

Average  Models of Unknown  Order.”  Biometrika  71:599-607. 

and 

.  1985.  “‘Hypothesis  Temine | in  SIMA?  1, q) Models. ”  Journal  of the 

American  Statistical Association  80:369-74 
Sargan,  J. D., and Alok  Bhargava.’  1983.  “Testing Residuals  from  Least  Squares  Regression 
for Being  Generated by the  Gaussian  Random  Walk.”  Econometrica  51:153-74. 
Schmidt,  Peter,  and  Peter  C.  B.  Phillips.  1992.  ‘LM  Tests  for  a Unit  Root in  the  Presence 
of Deterministic  Trends.”  Oxford  Bulletin  of Economics  and  Statistics  54:257-87. 
Schwert,  G.  William.  1989.  “Tests  for  Unit  Roots:  A Monte  Carlo  Investigation.”*  Journal 
of Business  and  Economic  Statistics  7:147-59. 
Sims,  Christopher  A.  1988.  ‘“‘Bayesian  Skepticism  on  Unit  Root  Econometrics.”  Journal  of 
Economic  Dynamics  and  Control  12:463-74. 

. 

,  James  H.  Stock,  and  Mark  W.  Watson.  1990.  “Inference  in  Linear  Time  Series 

Models  with  Some  Unit  Roots."’  Econometrica  58:113-44. 

and  Harald  Uhlig.  1991.  ‘“‘Understanding  Unit  Rooters:  A Helicopter  Tour.”  Econ- 

ometrica  59:1591-99. 
Solo,  V.  1984.  ‘“‘The  Order  of Differencing  i in  ARIMA  Models.”  Journal  of the American 
Statistical  Association  79:916—21. 
Sowell,  Fallaw.  1990.  ‘“‘The  Fractional  Unit  Root  Distribution.”  Econometrica  58:495-—505. 
Stinchcombe,  Maxwell,  and Halbert  White.  1993.  “‘An Approach  to Consistent  Specification 
Testing  Using  Duality  and  Banach  Limit  Theory.”  University  of California,  San  Diego. 

Stock,  James  H.  1991.  ‘Confidence  Intervals  for  the  Largest  Autoregressive  Root  in U.S. 
Macroeconomic  Time  Series.”’  Journal  of Monetary  Economics  28:435-59. 
—.  1994,  “Unit  Roots  and Trend  Breaks,”  in  Robert  Engle  and  Daniel  McFadden, 
eds.,  Handbook  of Econometrics,  Vol.  4.  Amsterdam:  North-Holland. 
White,  J. S.  1958.  ‘‘The  Limiting  Distribution  of the  Serial  Correlation  Coefficient in  the 
Explosive  Case.”  Annals  of Mathematical  Statistics  29:1188-97. 

Chapter  17  References 

543 

18 

Unit  Roots 
in  Multivariate 
Time  Series 

The  previous  chapter  investigated  statistical  inference  for univariate  processes  con- 
taining  unit  roots.  This  chapter  develops  comparable  results  for  vector  processes. 
The  first  section  develops  a vector  version  of the  functional  central  limit  theorem. 
Section  18.2  uses  these  results  to  generalize  the  analysis  of Section  17.7  to  vector 
autoregressions.  Section  18.3  discusses  an important  problem,  known  as  spurious 
regression,  that  can  arise  if the  error  term  in a  regression  is /(1).  One  should  be 
concerned  about  the  possibility  of a  spurious  regression  whenever  all  the  variables 
in a  regression  are  /(1)  and  no  lags  of the  dependent  variable  are  included  in the 
regression. 

18.1.  Asymptotic  Results 
for Nonstationary  Vector  Processes 

Section  17.2  described  univariate  standard  Brownian  motion  W(r)  as  a scalar  con- 
tinuous-time  process  (W:  r  €  [0, 1] >  R’).  The  variable  W(r)  has  a  N(0,  r) dis- 
tribution  across  realizations,  and  for  any  given  realization,  W(r)  is a  continuous 
function  of the  date  r with  independent  increments.  If a set  of n such  independent 
processes,  denoted  W,(r),  W2(r),..  . 
,  W,(r),  are  collected  in an  (n  x  1) vector 
W(r),  the  result  is n-dimensional  standard  Brownian  motion. 

. 

Definition: 
n-dimensional  standard  Brownian  motion  W(-)  is a  continuous-time 
process  associating  each  date  r  &  [0, 1] with  the  (n x  1) vector  W(r) satisfying  the 
following: 
(a)  W(0)  =  0; 
(6)  For  any  datesO 

Sr,<1r,<+++<r,  <1,  the  changes  [W(r2)  —  W(r,)], 
[W(r3)  —  W(r2)],.  - 
- 
.[W(re)  —  W(r,~,)] are independent multivariate  Gaus- 
sian  with  [W(s)  —  W(r)]  ~  N(0, (s —  r)-I,); 
For  any  given  realization,  W(r)  is continuous  in r with  probability  1. 

(c) 

Suppose  that  {v,}* , is a univariate  i.i.d.  discrete-time  process  with  mean  zero 

and  unit  variance,  and  let 

7(r)  =  T~'(v,  +  i  RL 

Virne)s 

where  [7r]*  denotes  the  largest  integer  that  is less  than  or equal to  Tr.  The  func- 
544 

tional  central  limit  theorem  states  that  as  T—>  2, 
VT-X4(-)  > W(). 
This  readily generalizes.  Suppose  that  {v,}*, is an  n-dimensional i.i.d.  vector proc- 
ess  with  E(v,)  =  0 and  E(v,v/)  =  I,, and  let- 

af 

XH(r)  =  T-"(v,  +  V2  t+  +++  +  7). 

Then 

[18.1.1] 
Next,  consider  an  i.i.d.  n-dimensional  process  {e,}*_,  with  mean  zero  and 

VI-X4(-) > W(). 

variance-covariance  matrix  given  by 2.  Let  P be  any  matrix  such  that 

Q  =  PP’; 
[18.1.2] 
for endaipt: P might  be the Cholesky  factor  of 2.  We  could think  of €, as  having 
been smectite from 
: 

e,  = Pv,, 
{18.1.3] 
for v, i.i.d.  with mean  zero and variance  I,,. To  see why, noses that  [18.- 1.3] implies 
Abat £, is iid.  with  mean zero  and variance given by 

‘ 

| 

Lpaoitesit «: 

220  earths RAY) a =P. ari  iz +" 

tet 

bani  je) 

ie 2  Mo rX oh Gk 

: 

eats  mA,  |  fit  a 

a) 

ic 
An *  8) ie ina 

ssion#\n  = PTHMy,  + vy  +: et  vera) 

: 

os  \s coe ata is fs teat  BORG ISS 38 ici 

L 

~  ¥ 

. 

: 

Ne  ade 

+), and  {a,,}5  0 is absolutely  summable.  Expression 
—(Wen1  +  Vosr  t+  Weea  +  °° 
[18.1.6]  provides  a  multivariate  generalization  of  the  Beveridge-Nelson  decom- 
position. 

If  u,  satisfies  [18.1.5]  where  e,  is  i.i.d.  with  mean  zero,  variance  given  by 
Q  =  PP’,  and  finite  fourth  moments,  then  it is  straightforward  to  generalize  to 
vector  process  the  statements  in  Proposition  17.3  about  univariate  processes.  For 
example,  if we  define 

[Tr]* 

X7(r)  =  (1/T)  5 u,, 

s=l 

[18.1.7] 

then  it follows  from  [18.1.6]  that 

VT-X7(r)  =  r“( wa) 

4  €&,  +  Wry  —  ne). 

[Tr]* 

As  in Example  17.2,  one  can  show  that 

sup  T~**  |nirae  ‘i nol  > 0. 
ré(0.1] 
i=1,2,....m 

It then  follows  from  [18.1.4]  that 

[18.1.8] 
VEX 7(:) & V(1)-P-VT-XE(-)  > Y(1)-P-W(-), 
where  W(1):P-W(r)  is distributed  (0, r[W(1)]--[W(1)]’)  across  realizations.  Fur- 
thermore,  for  &, =  u,  +  uw.  +  -:~*  +  u,,  we  have  as  in [17.3.15]  that 

T-32  > E,=  i VT-X(r)  dr  > ¥(1)-P- i W(r)  dr, 

[18.1.9] 

which  generalizes  result  (f) of Proposition  17.3. 

Generalizing  result  (e) of Proposition  17.3 requires a little more  care.  Consider 
for  illustration  the  simplest  case,  where  y, is an  i.i.d.  (m  x  1) vector  with  mean 
zero  and  E(v,v;)  =  L,,.  Define 

Viuttovas  tn  ete 

Et = 
i] 

fonder  deiden  ib) 

for ¢t = 0; 

we  use  the  symbols  v, and  €* here  in place of u, and  €, to emphasize  that  vy, is i.i.d. 
with  variance  matrix  given  by I,,. For  the  scalar  i.i.d.  unit  variance  case  (m  =  1, 
A  =  ¥  =  1), result  (e) of Proposition  17.3  stated  that 

ire  > é* .v, > H(W(1)F  —  1}. 

(18.1.10] 

The  corresponding  generalization  for  the  i.i.d.  unit  variance  vector  case  (nm >  1) 
turns  out  to  be 

i 

T-'  > (6tavi  +  vt  [WOW]  -  1,3 

[18.1.1] 

see  result  (d) of Proposition  18.1,  to  follow.  Expression  (18.1.11]  generalizes  the 
scalar  result  [18.1.10]  to an  (m X  n) matrix.  The  row  i, column i diagonal  element 
of this  matrix  expression  states  that 

T 

T>!  Y (EfraYu  +  Yabtes} > (WP  =  1, 

[18.1.12] 

where  7, v,,,  and  W,(r)  denote  the  ith elements  of the vectors  &*, v,, and  W(r), 

546  Chapter  18  | Unit Roots  in Multivariate  Time Series 

respectively.  The  row  i, column  j off-diagonal  element  of [18.1.11]  asserts  that 

T 

re!  2 {E2,-1¥je  +  Vugte—1} > (WAC)}{W,(1)] 

fori  # j. 

[18.1.13) 

Thus,  the sum of the  random  variables  Tn  EFM  je  and  T7127  Vué}.-1 
converges in distribution  to the product of two  independent  standard  Normal  variables. 
It  is  sometimes  convenient  to  describe  the  asymptotic  distribution  of 

T~'  7, &,_1v;,  alone.  It turns  out  that 

r 

1 

Sy  >> €%,1%;.>  [ Wi(r)  dW,(r). 

[18.1.14] 

This  expression  makes  use  of the  differential  of Brownian  motion,  denoted  dW,(r). 
A formal  definition  of the  differential  dW,(r)  and  derivation  of [18.1.14]  are  some- 
what  involved—see  Phillips  (1988)  for  details.  For  our  purposes,  we  will  simply 
regard  the  right  side  of [18.1.14]  as  a compact  notation  for  indicating  the  limiting 
distribution  of the sequence  represented  by the left side.  In practice,  this distribution 
is constructed  by Monte  Carlo  generation  of the statistic  on  the left side  of [18.1.14] 
for  suitably  large  T. 

It is evident  from  [18.1.13]  and  [18.1.14]  that 

I, Wi(r)  dW,(r)  +  I W,(r) dW,(r)  =  W,(1)-W(1) 

fori  # j, 

whereas  comparing  [18.1.14]  with  [18.1.12]  reveals  that 

[ Wi(r)  dW,(r)  =  H[Wi(1)?  —  1. 

(18.1.15] 

The  expressions  in  {18.1.14]  can  be  collected  for  i,j  =  1,2,...,mninan 

(nm X  n) matrix: 

| 

: 

T 

TY  ew  5  | won  (awoor’ 

L 

1 

(18.1.16] 

The  following  proposition  summarizes  the  multivariate  convergence  results 

that  will  be  used  in this  chapter.” 

Proposition  18.1: 

Letu,  be an(n  X  1) vector  with 

u,  =  W(L)e,  =  > We,_., 

s=0 

where  {s-W,}*_.  is absolutely  summable,  that  is, X39 s:\¥{)|  <  © for each  i, j =  1, 
2,...,n  for > the  row  i, column  j element  of V,.  Suppose  that {e,} is an  i.i.d. 
sequence  with mean  zero,  finite fourth moments,  and E(e£,)  =  Qa  positive definite 
matrix.  Let  2 =  PP’  denote  the  Cholesky factorization  of M, and  define 

oj, =  E( ene.)  =  row  i, column  j element  of 2 

r,  = E(uu/_,)  =  2, v2  fors  =0,1,2,... 

(nxn) 

=| 

(nv x 1) 

U,-| 

U,—2 
. 
; 

U,_, 

for arbitrary  v  =  1 

x 

| 

{18.1.17] 

ibe acadtianrieap- were derived 

Sims,  Stock,  and  Watson  (1990),  and  Phillips  and Solo  (1992). 

by Phillips  and Durlauf (1986), Park and Phillips (1988, 1989), 

-  18.1.  Asymptotic  Results for Nonstationary  Vector  Processes 

547 

Vi 

(nv xX nv) 

=  E(z,2;) 

A  = ¥(1)-P 

(nxn) 

Mo 
Pes 

r, 
i 

wee 
“oft  hil See 

Vevey  DP _y+2 
(%o  +  V+  Wt) 

rere) 

To 

r. 

| 

[18.1.18] 

fort=  i  eee 

[18.1.19] 

: 

&  su,  tu,  t+  cyt 
(21) 

with & =  0.  Then 
( a )   T-  : 4 ,  £ AW i t e  

3 

(b)  nS nema ovV)  "fori = Bt  ea ge  rere  Site 

pads  [L482  } is 
eG 
tc)  F  :  Sy se tia  © 1 2,. 

ee 

. 
pore 

ort 

Lose 

ELA  yan: Hy pervarn a"  hab  is} 

% 

Iasbiave  oF 
tides 
1-18 

pees 

7 

i 

ij 

os i “2 E05 tu) bet  gan  y gi  rege  oe  AotW  ; 

= 

- 

(se [W)J-A'- To  fors = 0 

: 

|  A wi 1 

[Aor ments cae A  for  a eae te a 

wags  odfT 

ia  | 

Ext 

H  Berge  sol 
ISOS 

2  BP 

- 

18.2.  Vector  Autoregressions  Containing  Unit  Roots 
Suppose  that  a  vector  y,  could  be  described  by  a  vector  autoregression  in  the 
differences  Ay,.  This  section  presents  results  developed  by Park  and  Phillips  (1988, 
1989) and Sims,  Stock,  and  Watson  (1990)  for  the  consequences  of estimating  the 
VAR  in  levels.  We  begin  by generalizing  the  Dickey-Fuller  variable  transformation  . 
that  was  used  in analyzing  a  univariate  autoregression. 

An  Alternative  Representation  of  a VAR(p). Process 
Let  y,  be  an  (m  x  1) vector  satisfying 

(I,  -  ®,L  -  ©,L?  -----  ® L’)y,=a+  e,, 
where  ®,  denotes  an  (nm  X  n)  matrix  fors  =  1,2,...,  p  and  q@  and  €,  are 
(m  x  1) vectors.  The  scalar  algebra  in  [17.7.4]  works  perfectly  well  for  matrices, 
establishing  that  for  any  values  of  ®,,  ®,,...  ,  ®,,  the  following  polynomials 
are  equivalent: 
(I,  —  ®,L  —  ®,L?  —  --+  -  ®, 1?) 

{18.2.1] 

[18.2.2] 

=  {E,   pLy—  (jb  +  or  t+ 

4  bed  —£), 

where 

pO,  +O,  +. --.+.®, 

[18.2.3] 

C=  —(@..,  +  @  4+  «--  +O). 

fors.=.1,2,....,p  —.1, 

_[18.24) 

It follows  that  any  VAR(p)  process  [18.2.1]  can  always  be written  in the  form 

(I,  —  eL)y,  —  GL  +  OL?  +  ---  +  £,_,L?-)(1  —  Ly,  =  a  +  €, 

or 

y, =  CiAy,-1  + S2Ay,-2  ++  °°  +  bp-1Ay-pei  ta  + pyrite, 

[18.2.5] 

The  null  hypothesis  considered  throughout  this  section  is that  the  first  dif- 

ference  of y follows  a VAR(p  —  1) process; 

: 

Ay,  =  C,Ay,-1  +  GAy,-2 

++  **  +  €-1Ay-por  + 

m+  €,, 

[18.2.6] 

requiring  from  [18.2.5]  that 

or,  from  [18.2.3], 

p=1,  - 
| 

®,  +  ®+°:-+®,  =I,. 

| 

[18.2.7] 

[18.2.8] 

Recalling  Proposition  10.1,  the  vector  autoregression  [18.2.1]  will  be said  to 

contain  at  least  one  unit  root  if the  following  determinant  is zero: 

I, -  9,  -  ®,--::-@®,|  =  0. 

{18.2.9} 

Note  that [18.2.8]  implies  [18.2.9]  but  [18.2.9]  does  not  imply  [18.2.8].  Thus,  this 
section  is considering  only a subset  of the class of vector  autoregressions  containing  - 
a unit  root,  namely,  the  class  déscribed  by [18.2.8].  Vector  autoregressions  for 
which  [18.2.9]  holds  but  [18.2.8]  does  not will  be considered  in Chapter  19. 

This  section  begins  with  a vector  generalization  of case  2 from  Chapter  17.  — 

18.2.  Vector Autoregressions  Containing  Unit Roots 

549 

A  Vector  Autoregression  with  No  Drift  in Any 
of the  Variables 
Here  we  assume  that  the  VAR  [18.2.1]  satisfies  [18.2.8]  along  with  a  =  0 
and  consider  the  consequences  of estimating  each  equation  in levels  by OLS  using 
»Y-p+1-  A constant 
observations ¢ =  1,2,...  ,  7 and conditioning  on  yo,  Y-1,-  - 
term  is assumed  to be included  in each  regression.  Under  the maintained  hypothesis 
[18.2.8],  the  data-generating  process  can  be  described  as 

- 

(I,  —  ib  —  02h?  — 

+  °° 

—  $p-1L?~*)Ay,  =  &:- 

[18.2.10] 

Assuming  that  all  values  of z satisfying 

[£12  0c  baz?  —  2°.  —  bpaaz?™"|  =  0 

lie outside  the  unit  circle,  [18.2.10]  implies that 
Ay,  =  u,, 
| 
where 

(18.2.11] 

u  =  (I, -  GL  -  GL? -  +++ -  Poe, 
If €,  is i.i.d.  with  mean  zero,  positive  definite  variance-covariance  matrix  2  = 
PP’,  and  finite  fourth  moments,  then  u, satisfies  the  conditions  of Proposition  18.1 
with 

| 

W(L)  =  (1,—  GL  —  10°  -*+"-  Gy  -")-*. 

[18.2.12] 

Also  from  [18.2.11],  we  have 

Y=Ytu  tuts:  +  U, 

so  that y, will  have  the  same  asymptotic  behavior  as  €, in Proposition  18.1. 

Recall  that  the fitted  values  of  a VAR  estimated  in levels  [18.2.1]  are  identical 
to  the  fitted  values  for  a VAR  estimated  in the  form  of [18.2.5].  Consider  the  ith 
equation  in [18.2.5],  which  we  write  as 

Yi  =  CaW—1  +  Sote-2  +  °° 

+  Ci pi  p4r  + Oi  +  OF ¥-1  +  Ex, 

[18.2.13] 

where  u,  =  Ay, and (;, denotes  the ith row  of, fors  =  1,2,.  .~.,p  —  1. Similarly, 
9; denotes  the  ith  row  of p.  Under  the  null  hypothesis  [18.2.7],  p;  =  e;, where 
e; is the ith  row  of the  (n x  n) identity  matrix.  Recall  the usual  expression  [8.2.3] 
for the  deviation  of the  OLS  estimate  b; from  its hypothesized  true  value: 

br —  B =  (2x,x;)~'(2x,¢,), 

[18.2.14] 

where  2 denotes  summation  over  ¢ =  1 through  T. In the case  of OLS  estimation 
of [18.2.13}, 

: 

br-Bp=|. 

a 

Ci “5 Ca 

ba —  ba. 
° 

| 

[18.2.15] 

S50  Chapter  18  | Unit Roots  in Multivariate  Time Series 

=Xx,x; 

2u,_,U;-; 

Zu, 2U;-1 

2u,_,Wj2 

+  °°  2-8  ps1 

2u,-2Wy-2 

°° 

2U,_2U)_ p41 

2u,_1 

2u,_2 

2u,-1Yr-1 

Lu,  2Y;-1 

2u,-p+1Ur-1  2u,_p+1U;-2  +A  2M  ps1Bi—p +1  aM» <1  2U,-p+1¥r-1 

=u;_) 

fas 

* 

Up +1 

T 

Zyi-1 

ee  eee  08  Oye,  Sy,  «= Sy, -,y'_-, 

[18.2.16] 

2U,_ 184 

2u,_ 2; 

-  (18.2.17] 

s 

. 

> 

2U,_  +1: 

" 

rej,  P 

Our earlier convention would  append a subscript TW tie etiniunied i 
cients {,, in [18.2.15]. For this discussion, the subscript  7 will be suppressed to 
avoid  excessively  cum ibersome  F notation. 
ps Define ¥r to bee following matrix: 
pst  j 

;  $34  ¥ Tey  . 

a Se 

where 

Po 

r, 

ee  ae 

: 

[n(p —  1)  x  a(p—1)) 

Lad 
anes 
mt 

eae, 

a 

V_p+2 

A  ae 

ete 

lo 

r, =  E(Ay,)(Ay,-5)’ 

1 

1 Wr)  | “A 

Q 
(n+1)x(n +1) 

= 

A: | W(r)  dr  a | [W(r)]-[W(r)]’  ar} a’ 

[18.2.21] 

(18.2.22] 

Also,  the  integral  sign  denotes  integration  over  r from  0 to  1, and 

| 

~  [18.2.23] 
Am(i,-%  -&-r+:*>t-) ? 
with  E(e,e’)  =  PP’.  Similarly, applying  results  (a), (b), and  (f) from  Proposition 
18.1  to  the  second  term  in [18.2.19]  reveals 

T~ Zu, _1€i1 
T-*22u,_2€; 

Spe  ae 

(Y7'2x,e,) 

TS  ge8e 

L  ~ 
=  ’ ‘ 

2. 
[18.2.24] 

where 

Ty 722, 

T~*Zy,-1€% 

ET 
~  N(0,  0;;V) 

h, 
[n(p-1) x1) 

oj,  =  E(e) 

e; PW(1) 

h, 

= 

[(a+1)  x1]  af fiwon) tawonr} Pr 

for e, the ith column  of I,,. Results  [18.2.19],  [18.2.20],  and  [18.2.24]  establish  that 

& 
Y7(b;  -  B)—>  remy 

V-'h, 

[18.2.25] 

The  first  n(p  —  1) elements  of [18.2.25]  imply that the coefficients  on  Ay,_,, 

Ay,-2,--.,  Ay,-p+1  Converge  at  rate  VT to  Gaussian  variables: 

Gh: =  Ci 

Ci2  -  Ci2 

VT 

bok  ad Cip-1 

5S V—h,  ~  NO, o°V~!). 

[18.2.26] 

This  means  that  the  Wald  form  of the  OLS x? test  of any  linear  hypothesis  that 
involves  only the coefficients  on  Ay,_,  has the usual  asymptotic  x? distribution,  as 
the reader  is invited  to confirm  in Exercise  18.1. 

552  Chapter  18  | Unit Roots  in Multivariate  Time  Series 

Notice  that  [18.2.26]  is  identical  to  the  asymptotic  distribution  that  would 

characterize  the  estimates  if the  VAR  were  estimated  in  differences: 

AY,  =  a;  +  i, Ay,  +  Sr Ay,-2  +  +++  +0),-1Ay,-por  +  a:  _ [18.2.27] 
Thus,  as  in  the  case  of  a  univariate  autoregression,  if the  goal  is to  estimate  the- 
parameters  (,,,  Ci2,  - 
,  ¢ip-1  Or  test  hypotheses  about  these  coefficients,  there 
is no  need  based  on  the  asymptotic  distributions  for  estimating  the  VAR  in  the 
difference  form  [18.2.27]  rather  than  in  the  levels  form, 

. 

- 

Yi  »  C;, Ay,-1  7  C;.Ay,_2  v5:  Sack  Cip-1AY,-p+1 

[18.2.28] 

+  a  +  PiY-1+  Eq. 

Nevertheless,  the  small-sample  distributions  may  well  be  improved  by estimating 
the  VAR  in differences,  assuming  that  the  restriction  [18.2.8]  is valid. 

Although  the  asymptotic  distribution  of the  coefficient  on  y,_,  is non-Gaussian, 
the fact  that  this estimate  converges  at rate  T means  that  a hypothesis  test  involving 
a single  linear  combination  of p; and  (;;, C2, . 
,  $i,p-1  will  be dominated  asymp- 
totically  by the  coefficients  with  the  slower  rate  of convergence,  namely,  ¢,,,  Cio, 
,  Gp-1,  and  indeed  will  have  the  same  asymptotic  distribution  as  if the  true 
value  of p  =  I,, were  used.  For  example,  if the  VAR  is estimated  in levels  form 
[18.2.1],  the  individual  coefficient  matrices  ®, are  related  to  the  coefficients  for 
the  transformed  VAR  [18.2.5]  by 

. 

. 

. 

. 

© =  -f, 
Ook.  £29: 
©, =p+&.. 

fot  Ss =  2, 3,-4.0)  5-1 

-  [18.2.29] 
[18.2.30] 
(18.2.31] 

Since  VT (t,-  {,) is asymptotically  Gaussian  and  since  # is  O,(T~’),  it follows 
that  \/7(,  —  ®,) is asymptotically  Gaussian  for  s =  1, 2,...  , p assuming  that 
p =  2. This  means  that  if the  VAR  is estimated  in levels  in the  standard  way, any 
individual  autoregressive  coefficient  converges  at  rate  \/T to  a Gaussian  variable 
and  the usual  ¢ test  of a hypothesis  involving  that  coefficient  is asymptotically  valid. 
Moreover,  an F test  involving  a linear  combination  other  than  ®,  +  ®,  +--+  + 
®,, has  the  usual  asymptotic  distribution. 

Another  important  example  is testing  the  null  hypothesis  that  the  data  follow 
a  VAR(po)  with po =  1 against  the  alternative  of  a VAR(p)  with p > po.  Consider 
OLS  estimation  of the  ith equation  of the  VAR  as  represented  in levels, 

Yu  =  a  +  Pyy,-1  + Opy-2  t+  +  Bpyi-p  +  eu»  — [182.32] 

where  ®;, denotes  the  ith  row  of ®,.  Consider  the  null  hypothesis 

[18.2.33] 
Hy:  ®; 2,41  =  ®, ,,+2  =  sy  3%  ®,, =  0. 
The  Wald  form  of the  OLS 7 test  of this  hypothesis  will  be numerically  identical 
to  the  test  of 

Ho: on  =  Ci po+t  a  *  & 

=  0 

[18.2.34] 

for  OLS  estimation  of 

Yu  =  GrAyrn-i  +  Sedy-2  + 
+  a  +  Pi¥r-1  +  En- 

°°  +  Sip-1A¥-p+1 

[18.2.35] 
tronic, 

Since  we  have  seen  that  the  usual  F test  of [18.2.34]  is asymptotically  valid  and 
since  a  test  of [18.2.33]  is based  on  the  identical  test  statistic,  it follows  that  the 
usual  Wald  test  for  assessing  the  number  of lags  to  include  in the  regression  is 
perfectly appropriate  when  the regression  is estimated  in levels form as in [18.2.32]. 

18.2.  Vector  Autoregressions  Containing  Unit Roots  553 

Of course,  some  hypothesis  tests  based  on  a  VAR  estimated  in levels  will  not 
have  the  usual  asymptotic  distribution.  An  important  example  is  a Granger-causality 
test  of  the  null  hypothesis  that  some  of  the  variables  in  y,  do  not  appear  in  the 
regression  explaining  y,,.  Partition  y,  =  (Y},,  Y2,)',  where  y2, denotes the subset of 
variables  that  do  not  affect  y,  under  the  null  hypothesis.  Write  the  regression  in 
levels  as 

.  =! 

Vit 

tae  eras 

es  1 

vies  1 
+  Yip  +  ApYar-p  F  Qi  +  Cx 

2Y1  9-2 

_,  +  @ 

32 +  A2Yay-2  +  °° 
2924-2 

* 

(18.2.36] 

and  the  transformed  regression  as 

Yu  =  Bi Ayye-1  +  Yi AYou-1  +  B2oAYiy-2  +  Y2AYor-2  +  °  °° 

+  Bi :Ayis—pai  +  Yp-14Yae-poi  +  Oj  +  WY15-1 

[18.2.37] 

- 

+  B'Y2 5-1  mF Ein. 
-  =  A, =  0 based  on  OLS estimation 
The F test  of the null hypothesis  A,  =  Az =  - 
of [18.2.36]  is numerically  identical  to  the  F test  of the  null  hypothesis  y,  =  y2  = 
++  =  y,_,  =  8  =  0 based  on  OLS  estimation  of [18.2.37].  Since  6 has  a non- 
standard  limiting  distribution,  a  test  for  Granger-causality  based  on  a  VAR  esti- 
mated  in levels  typically  does  not  have  the  usual  limiting  x distribution  (see  Ex- 
ercise  18.2  and  Toda  and  Phillips,  1993b,  for  further  discussion).  Monte  Carlo 
simulations  by Ohanian  (1988),  for example,  found  that  if an  independent  random 
walk  is added  to a vector  autoregression,  the random  walk  might spuriously  appear 
to  Granger-cause  the other  variables  in 20%  of the samples  if the 5%  critical  value 
for a x? variable  is mistakenly  used  to  interpret  the  test  statistic.  Toda  and  Phillips 
(1993a)  have  an  analytical  treatment  of this  issue. 

A Vector Autoregression  with  Drift  in Some  of the  Variables 

Here  we  again  consider  estimation  of  a VAR  written  in the  form 
y, =  C,Ay,_,  + GAy,_.  +--+  ++  ,-iAy,_p+,  +  a+  py,_;  + ©,. 

 [18.2.38] 

As  before,  it is assumed  that  roots  of 

Ee 

CFS  RRS  gees  ef iggeoy  sand 

are  outside  the unit circle,  that  e, is i.i.d.  with  mean  zero,  positive  definite  variance 
©), and  finite  fourth  moments,  and  that  the  true  value  of p is the  (n x  n) identity 
matrix.  These  assumptions  imply  that 

where 

Ay,  =6  +4, 

Se,  —  Ge 
u, =  W(L)e, 

teeters  Gly)  Te 

WL)  @  LOB  —  GLP  eH 

es  =  Gc  bem ')-'. 

[18.2.39] 

[18.2.40] 
[18.2.41] 

In  contrast  to  the  previous  case,  in which  it was  assumed  that  8  =  0, here  we 
suppose  that  at least  one  and  possibly  all of the  elements  of  are  nonzero. 

Since  this is a vector  generalization  of case  3 for the univariate  autoregression 
considered  in Chapter  17, one’s first thought  might be that, because  of the nonzero 
drift  in the  /(1)  regressors,  if all  of the  elements  of & are  nonzero,  then  all the 
coefficients  will  have  the usual  Gaussian  limiting distribution.  However,  this turns 
out  not  to be the case.  Any individual  element  Yj, Of the vector  y, is dominated  by 
554  Chapter 18  | Unit Roots in Multivariate  Time Series 

a deterministic  time  trend,  and  if Yj appeared  alone  in the regression,  the  asymptotic 
results  would  be  the  same  as  if y, were  replaced  by the  time  trend  f.  Indeed,  as 
noted  by West  (1988),  in a  regression  in which  there  is a single  /(1) regressor  with 
nonzero  drift  and  in which  all other  regressors  are  /(0), all of the coefficients  would 
be asymptotically  Gaussian  and F tests  would  have  their  usual  limiting  distribution. 
This  can  be  shown  using  essentially  the  same  algebra  as  in  the  univariate  auto- 
regression  analyzed  in  case  3 in  Chapter  17.  However,  as  noted  by Sims,  Stock, 
and  Watson  (1990),  in [18.2.38]  there  are  n different  /(1) regressors  (the n elements 
of y,_,),  and  if each  of these  were  replaced  by 5,(¢  —  1), the  resulting  regressors 
would  be  perfectly  collinear.  OLS  will  fit n  separate  linear  combinations  of y, so 
as  to  try  to  minimize  the  sum  of squared  residuals,  and  while  one  of  these  will 
indeed  pick  up  the  deterministic  time  trend  ¢, the  other  linear  combinations  cor- 
respond  to  /(1)  driftless  variables. 

To develop  the  correct  asymptotic  distribution,  it is convenient  to  work  with 
a transformation  of {18.2.38]  that  isolates  these  different  linear  combinations.  Note 
that  the  difference  equation  [18.2.39]  implies  that 

“Y¥,  =  Yo  +  St  +  u,  +  Sat  2 

+  B., 

[18.2.42] 

Suppose  for  illustration  that  the  mth  variable  in the  system  exhibits  nonzero  drift 
(5,  #  0); whether  in addition  56,  # 0 fori  =  1,2,...,m  —  1 then  turns  out  to 
be irrelevant,  assuming  that  [18.2.8]  holds.  Define 

Vie aay | eee  (5,/5,)  Yat 
Yor =  Yar  —  (8/8) Ine 

Yr-1s  =  y,-ty  =  (6, -1/8n)Ynt 

Yar =  Yne- 

weds Ste. 4  082  5. 

1, 

Yi  =  [Yo  +  St  +  Ua  +  U2  +  °°  +  Ud 

=  (8/8)  Yno +  Spt  +  Un  +  U2  +  00+  +  Und 

=  yn  +  bis 

where  we  have  defined 

Yn =  [Yo —  (8/5,)Ynol 
ePseunptupgt::++ uy 
Uj =  Uy  —  (5/8, )Une- 

Collecting  u%,, uz,  ...,  us_1,  in  an  [(n  —  1)  x  1] vector  uj,  it follows from 
(18.2.41]  that 

where  W*(L)  denotes  the  following  [(n —  1) x  n] matrix  polynomial: 

u? =  W*(L)e,, 

for 

w*(L)  = H-W(L) 

oe. 
((n- 1) xn} 

1  0  0 

se 
; 

-(6,/5,) 
—-(6/6,) 
; 

0 
1010:::  0 
. 
0.0  +++  1  ~(6,-1/6,) 
"18.2. Vector Autoregressions Containing  Unit Roots 

Oo 

$55 

1) 
Since  {s‘W,}*_)  is  absolutely  summable,  so  is  {s-‘W?}*_ 5. Hence,  the  [(n  — 
+--+»  Ya-14)’  has  the  same  asymptotic  properties  as 
x  1] vector  y*  =  (yf,  yz,  - 
the  vector  &, in  Proposition  18.1  with  the  matrix  W(1)  in Proposition  18.1  replaced 
by W*(1). 

If we  had  direct  observations  on  y?  and  u,,  the  fitted  values  of the  VAR  as 

estimated  from  [18.2.38]  would  clearly  be  identical  to  those  from  estimation  of 

ye  1m~1 ? C2U,_2  rh 

ome 

th  Co -1U,-p41  +  a* 

[18.2.43] 

+  P°Y-1  +  VYna-1  +  Es, 

where  p* denotes  an  [nm  x  (mn  —  1)] matrix  of coefficients  while  y  is an  (n  x  1) 
vector  of coefficients.  This  representation  separates  the  zero-mean  stationary  re- 
gressors  (u,_,  =  Ay,_,  —  5),  the  constant  term  (a*),  the  driftless  /(1) regressors 
(y*_,),  and  a term  dominated  asymptotically  by a time  trend  (y,,,_,).  As in Section 
16.3,  once  the  hypothetical  VAR  [18.2.43]  is analyzed,  we  can  infer  the  properties 
of the  VAR  as  actually  estimated  ((18.2.38]  or  [18.2.1]}  from  the  relation  between 
the  fitted  values  for  the  different  representations. 
Consider  the  ith  equation  in [18.2.43], 

cir tat, 

Coulee  2 

Yi 

Ci  i ;  C2  1-2 

2  toe ee 

his  1“r-—p+1 

ponenge 

[18.2.44] 

+  Pi  Yr-1  F  VIns-1  +  Ens 

where  (;, denotes  the  ith  row  of ¢, and  p?’  is the  ith  row  of p*.  Define 

a  RH Fs (u;_1;  Uy-25--  +»  U-pet,  1, Yi-1»  Yna-1) 

aa  Pil 
0’ 

0 
0 
7S  0 

0: 
0 

JT Ezare 
0’ 

& 
Ea 

Ub 

Y 
bei)  <p  19) 

= 

@ 
0’ 
=  wW*(1)-P, 

A* 
((n—1)  xa} 

where  E(e,e;)  =  PP’.  Then,  from  Proposition  18.1, 

T 

(ve 2 (x? Next  v7") 

[18.2.46] 

Vv 

0’ 

0 

1 

0 

| | W(r) | “A*’ 

0 

6,,/2 

0  as. W(r)  dr  anf (W(r)}-[W(r)]’  ar} aw  8,-A*-| rW(r)  dr 

. 

0’ 

6,,/2 

3. | rW(r) a “A” 

62/3 

5 

where 

[n(p  -1)  xn(p=1)) 

To 

a 

| 

r, 

Mo 
te 

r,-2 

Te-3 
tome 

A 
ls 
clustytet 

P5e3  Pipes  vee  ry 

epee 

556  Chapter 18  | Unit Roots  in Multivariate  Time Series 

and W(r) denotes n-dimensional  standard  Brownian  motion  while  the  integral  sign 
indicates  integration  over  r  from 0 to  1.  Similarly, 

=)  . 

7)  > x76, 

l 

* 

t=] 

h, 

h, 

h, 

h, 

(18.2.48] 

where  h,  ¥ N(0,  o;;V).  The  variables  h, and  h, are  also  Gaussian,  though  h, is 
non-Gaussian.  If we  define  w  to  be  the  vector  of coefficients  on  lagged  Ay, 

then  the  preceding  results  imply  that 

=  (Ci.  Ci,  am 

&  Cie—1)  : 

T'*(@,  -  @) 

Y;(b%  —  B*)  = 

te 

oy 

T!? (a7,  —  a*)  | 2  Yea 
Q-'y 
T(6*7  —  p*) 
™  (4.7  -  i) 

“a: 

Ae, 

18.2.49 

where  » =  (h2, hj, h4)'  and  Q is the  [(n  +  1) x  (n +  1)] lower  right  block  of the 
matrix  in [18.2.46].  Thus,  as  usual,  the  coefficients  on  u,_,  in [18.2.43]  are  asymp- 
totically  Gaussian: 

VT(@;.7  —  0;) EA N(0,  o;;V  ~'). 

These  coefficients  are,  of course,  numerically  identical  to  the  coefficients  on  Ay,_, 
in [18.2.38].  Any  F tests  involving  just  these  coefficients  are  also  identical  for  the 
two  parameterizations.  Hence,  an  F test  about  ¢,, ¢,  ..  . 
,  ¢,-,  in  [18.2.38]  has 
the  usual  limiting  x? distribution.  This  is  the  same  asymptotic  distribution  as  if 
[18.2.38]  were  estimated  with  p  =  I, imposed;  that  is, it is the  same  asymptotic 
distribution  whether  the  regression  is estimated  in levels  or  in differences. 

Since  p7 and  ¥; converge  at a faster  rate  than  @,, the asymptotic  distribution 
of a linear  combination  of @,,  p7,  and  ¥; that  puts  nonzero  weight  on  @, has the 
same  asymptotic  distribution  as  a  linear  combination  that  uses  the true values  for 
p and  y.  This  means,  for  example,  that  the  original  coefficients  ®, of the  VAR 
estimated  in levels  as in [18.2.1]  are  all individually  Gaussian  and can  be interpreted 
using  the  usual  ¢ tests.  A Wald  test  of the  null  hypothesis  of po =  1 lag against  the 
alternative  of p > po lags again  has  the  usual  x? distribution.  However,  Granger- 
causality  tests  typically  have  nonstandard  distributions. 

18.3.  Spurious  Regressions 
Consider  a  regression  of the  form 

y,  = xB  +  u,, 
for which  elements  of y, and x, might  be nonstationary.  If there  does  not  exist some 
population  value  for  B for  which  the  residual  u,  =  y,  —  x,;B  is 1(0),  then OLS is 
quite  likely  to  produce  spurious  results.  This  phenomenon  was  first discovered  in 
Monte  Carlo  experimentation  by Granger  and Newbold  (1974)  and later explained 
theoretically  by Phillips  (1986). 

A  general  statement  of  the  spurious  regression  problem  can  be  made  as 
follows.  Let y, be  an (n  x  1) vector  of /(1) variables.  Define  g =  (n —  1), and 

18.3.  Spurious  Regressions 

557 

partition  y, as 

y  =  “ed 

: 

Ya 

where  y,, denotes  a(g  x  1) vector.  Consider  the consequences  of an  OLS regression 
of the  first  variable  on  the  others  and  a  constant, 

The  OLS  coefficient  estimates  for  a  sample  of size  T are  given  by 

Yu  =  @  SS Y Yar  +  U,. 

s 

ea f  | r 
Vr 

-1 

Ly; 
yy 
ae  y1  | 

LY  La 

LY is 

[18.3.1] 

[18.3.2] 

where  > indicates  summation  over  ¢ from  1 to  7.  It turns  out  that  even  if y,,  is 
completely  unrelated  to  yz,,  the  estimated  value  of  y  is  likely  to  appear  to  be 
statistically  significantly  different  from  zero.  Indeed,  consider  any  null  hypothesis 
of  the  form  H,: Ry  =  r  where  R  is  a  known  (m  X  g) matrix  representing  m 
_separate  hypotheses  involving  y and  r is a  known  (m  x  1) vector.  The  OLS F test 
of this  null  hypothesis  is 

T 
pean coe  arse nis, i  e]}  [18.3.3] 

sy;  |  foll. 

x  {RY;,  —  r} +  m, 

where 

s? =(T  —  nn)"  > a2. 

T 

~ 

*[18.3.4] 

Unless  there  is some  value  for y such  that y,,  —  y'y2, is stationary,  the OLS estimate 
47 will appear  to be spuriously  precise  in the sense  that the F test  is virtually certain 
to  reject  any  null  hypothesis  if the  sample  size  is sufficiently  large,  even though 
41 does  not  provide  a consistent  estimate  of any well-defined  population  constant! 
The  following  proposition,  adapted  from  Phillips  (1986),  provides  the formal 

basis  for  these  statements. 

Proposition  18.2: 
by 

Consider  an  (n  x  1) vector  y, whose first difference  is described 

Ay,  =  W(L)e,  =  “3 Y,8,., 

s=0 

for €, an  i.i.d.  (n  x  1) vector  with  mean  zero,  variance  E(e,e/)  =  PP’,  and finite 
fourth  moments  and where {s-W,}¥_o  is absolutely  summable.  Let g =  (n —  1) and 
A =  W(1)-P.  Partition  y, as  y,  =  (yy, Y3,)',  and partition  AA'  as 

211  2 
AA!  =  | @*)  (xg) 
=n 
pat  hd 
(x8) 

21 
(ex1) 

|. 

[18.3.5] 

Suppose  that AA‘  is nonsingular,  and define 

[18.3.6] 
(of)?  =  (2,  —  2y2H"Za). 
Let L,, denote  the Cholesky factor of 23'; that is, L.. is the lower triangular matrix 
558  Chapter 18  | Unit Roots  in Multivariate  Time  Series 

satisfying 

Then  the following  hold. 

Zn)  =  LyL},. 

[18.3.7] 

(a)  The  OLS estimates  &7 and  4  in (18.3.2)  are  characterized  by 

T-V6 

i 2,1 [orto 

4. 

arth, 

where 

q = 
el 

all 

[ wer  a 

[fwrerdr  J wrens  & 

me wir) dr 
“Lf wreo-wre ae} 

rer 

canes 

and the integral  sign indicates integration  over r from  0 to 1, Wi(r) denotes 
standard 

scalar  standard  Brownian motion, aap sabi  est  ;-dimensional 
with W3(r) inde, endent of W;(r). 
Brownian  motion 
ey  sum clnameael residuals RSS, 7 from OLS estimation  a us. 3.1] satisfies 
T-?RSS;3 (03H, 
8.3.10] 

=  Then where ; re Oifterence  2  . Hey’ Le ¥ pa 

3 

ae  ca 

: 

Ted 

dy  : og <0\.0):  4  = 

ie 2 

The  simplest  illustration  of Proposition  18.2  is provided  when  y,,  and  y>,  are 

scalars  following  totally  unrelated  random  walks: 

Yi 

Viet  %  Sx 

[18.3.13]  | 

Yo.  F  Y2u-1  +  £2» 
where  €,,  is i.i.d.  with  mean  zero  and  variance  07,  €2,  is i.i.d.  with  mean  zero  and 
variance  a3,  and  ¢,,  is independent  of €,,  for  all  ¢ and  t.  For  y,  =  (y,,,  Y>,)',  this 
specification  implies 

[18.3.14] 

Ft 

0 

es 

0  oa 

W(1)  =  I, 

St x] vorewor-[9  a 

oi  =a, 
"Le =  Maen, 

Result (a) then  claims  that  an  OLS regression  of y,,  on  yz,  and  a  constant, 

| 

| 

Yu =at  yyz,  +  u,, 

[18.3.15] 

produces  estimates  a, and  ¥; characterized  by 

pee  5  | oh, 

; 

a 

TT 

= 

>. 

(o,/02)-h, 

Note  the contrast  between  this result and any previous asymptotic  distribution 
analyzed.  Usually,  the  OLS  estimates  are  consistent  with  b; -> 0 and  must  be 
multiplied  by some  increasing  function  of  T in order  to  obtain  a  nondegenerate 
asymptotic  distribution.  Here,  however,  neither  estimate  is consistent  - 
-  different 
arbitrarily  large  samples  will  have  randomly  differing  estimates  7,.  Indeed,  the 
estimate  of the  constant  term  @, actually  diverges,  and  must  be  divided  by T'?  to 
obtain  a random  variable  with  a well-specified  distribution—the  estimate  a; itsélf 
is likely  to  get  farther  and  farther  from  the  true  value  of zero  as  the  sample  size 
T increases. 

8 

Result  (b) implies  that  the  usual  OLS  estimate  of the  variance  of u,, 

s% =  (T —  n)—'-RSS;, 

again diverges  as  T—>  ~.  To obtain  an  estimate  that does  not  grow  withthe sample 
size,  the  residual  sum  of squares  has  to  be  divided  by T? rather  than  T.  In  this  ~ 
respect,  the  residuals  @, from  a spurious  regression  behave  like a unit  root  process; 
if £, is a  scalar  /(1)  series,  then  T~'Lé?  diverges  and  T-*2£?  converges.  To  see 
why a, behaves  like  an  /(1) series,  notice  that  the  OLS  residual  is given  by 

from  which 

a, =  Yu  —  &r —  PrY2, 

, 

- 

A 

Ad,  =  Ay,  —  Yr Ayr, =  [1  -¥4]| i 
Aya 

|4u  —h3'JAy,, 

 [18.3.16] . 

where  h} =  23,22,  +  ofL,h,.  This  is  a random  vector  [1  —h}']  times  the  1(0) 
vector  Ay,. 

| 

560  Chapter  18  | Unit Roots  in Multivariate  Time  Series 

Result  (c) means  that  any  OLS ¢ or  F test  based  on  the  spurious  regression 
[18.3.1]  also  diverges;  the  OLS F statistic  [18.3.3]  must  be  divided  by  T to  obtain 
a  variable  that does  not  grow  with  the  sample  size.  Since  an  F test  of  a  single 
restriction  is  the  square  of  the  corresponding  ¢ test,  any  ¢ statistic  would  have  to 
be  divided  by  - to  obtain  a  convergent  variable.  Thus,  as  the  sample  size  T 
becomes  larger,  it  becomes  increasingly  likely  that  the  absolute  value  of  an  OLS 
f  test  will  exceed  any  arbitrary  finite  value  (such  as  the  usual  critical  value  of 
t=  2). For example,  in  the  regression  of  [18.3.15],  it  will  appear  that  y,,  and  y,, 
are  significantly  related  whereas  in  reality  they  are  completely  independent. 

In more  general  regressions  of  the  form  of  [18.3.1],  Ay,,  and  Ay,,  may  be 
dynamically  related  through  nonzero  off-diagonal  elements  of P and  W(L).  While 
such  correlations  will  influence  the  values  of the  nuisance  parameters  of , 2,,,  and 
22, provided  that  the conditions  of Proposition  18.2  are  satisfied,  these  correlations 
do  not  affect  the  overall  nature  of the  results  or  rates  of  convergence  for  any  of 
the  statistics.  Note  that  since  W*(r)  and  W%(r)  are  standard  Brownian  motion,  the 
distributions  of h,,  h,,  and  H  in  Proposition  18.2  depend  only  on  the  number  of 
variables  in the  regression  and  not  on  their  dynamic  relations.  | 
; 
The  condition  in  Proposition  18.2  that  A-A’  is  nonsingular  might  appear 
innocuous  but  is actually  quite  important.  In the  case  of a  single  variable  (y, =  y,, 
with  Ay,,  =  y(L)e,,),  the  matrix  A-A’  would  just be  the  scalar  [y(1)-o,]*  and  the 
condition  that  A-A’  is nonsingular  would  come  down  to the  requirement  that  (1) 
be  nonzero.-  To  understand  what  this  means,  suppose  that  y,,  were  actually  sta- 
tionary  with  Wold  representation: 

Yu  =  &y  +  Cy, 4-1  me  Gees  tec 

c=  C(L)e,,- 

Then  the  first  difference  Ay,,  would  be  described  by 

Ay,  =  (1  —  L)C(L)ey,  =  WL)er,, 
where  y(L)  =  (1 —L)C(L),  meaning  (1)  =  (1 —  1)-C(1)  =  0. Thus,  if y,,  were 
actually  /(0) rather  than  /(1),  the  condition  that  A-A’  is nonsingular  would  not  be 
satisfied. 

| 
For  the  more  general  case  in which  y, is an  (n X  1) vector,  the condition  that 
A-A’  is nonsingular  will  not  be  satisfied  if some  explanatory  variable  y,, is /(0) or 
if some  linear  combination  of the  elements  of y, is /(0).  If y, is an  /(1) vector  but 
some  linear  combination  of y, is /(0),  then  the elements  of y, are  said  to  be coin- 
tegrated.  Thus,  Proposition  18.2  describes  the  consequences  of OLS  estimation  of 
[18.3.1]  only  when  all  of the  elements  of y, are  /(1)  with  zero  drift  and  when  the 
vector  y, is not  cointegrated.  A regression  is spurious  only when  the  residual  u, is 
nonstationary  for  all possible  values  of the  coefficient  vector. 

Cures  for Spurious  Regressions 
There  are  three  ways  in which  the  problems  associated  with  spurious  regres- 
sions  can  be  avoided.  The  first  approach  is to  include  lagged  values  of both  the 
dependent  and  independent  variable  in the  regression.  For  example,  consider  the 
following  model  as  an  alternative  to  [18.3.15): 

Yu  = 

&  +  PYir-1  +  Wa  +  by2,-1  +  Uy. 

[18.3.17] 
This  regression  does  not  satisfy  the  conditions  of Proposition  18.1,  because  there 
exist  values  for  the  coefficients,  specifically  @ =  1 and  y  =  6  =  0, for  which 
the  error  term  u,  is  /(0).  It  can  be  shown  that  OLS  estimation  of  [18.3.17] 
yields consistent  estimates  of all of the parameters.  The  coefficients  77 and 8; each 

18.3.  Spurious  Regressions 

561 

aia 

individually  converge  at  rate  \/T to a  Gaussian  distribution,  and  the  ¢ test  of  the 
hypothesis  that  y  =  0 is  asymptotically  (0,  1), as  is the  ¢ test  of the  hypothesis 
that  5 =  0.  However,  an  F test  of the  joint  null  hypothesis  that  y and  6 are  both 
zero  has  a  nonstandard  limiting  distribution;  see  Exercise  18.3.  Hence,  including 
lagged  values  in the  regression  is sufficient  to solve  many  of the problems  associated 
with  spurious  regressions,  although  tests-of  some  hypotheses  will  still  involve  non- 
standard  distributions. 

A  second  approach  is to  difference  the  data  before  estimating  the  relation, 

as  in 

Ay,,  =  a+  yAy,,  +  u,. 

[18.3.18] 

Clearly,  since  the  regressors  and  error  term  u, are  all J(0) for  this  regression  under 
the  null  hypothesis,  @; and  7; both  converge  at  rate  \/T to  Gaussian  variables. 
Any tor  F test  based  on  [18.3.18]  has the  usual  limiting  Gaussian  or x? distribution. 
A  third  approach,  analyzed  by Blough  (1992),  is to  estimate  [18.3.15]  with 
Cochrane-Orcutt  adjustment  for  first-order  serial  correlation  of the  residuals.  We 
will  see  in  Proposition  19.4  in  the  following  chapter  that  if Z, denotes  the  sample 
residual  from  OLS  estimation  of [18.3.15],  then  the  estimated  autoregressive  coef- 
ficient  p; from  an  OLS  regression  of 4, on  %,_,  converges  in probability  to  unity. 
Blough  showed  that  the  Cochrane-Orcutt  GLS  regression  is then  asymptotically 
equivalent  to  the  differenced  regression  [18.3.18]. 

Because  the  specification  [18.3.18]  avoids  the  spurious  regression  problem  as 
well  as  the  nonstandard  distributions  for  certain  hypotheses  associated  with  the 
levels  regression  [18.3.15],  many  researchers  recommend  routinely  differencing 
apparently  nonstationary  variables  before  estimating  regressions.  While  this  is the 
ideal  cure  for the problem  discussed  in this section,  there  are  two  different  situations 
in  which  it  might  be  inappropriate.  First,  if the  data  are  really  stationary  (for 
example,  if the  true  value  of  @ in  [18.3.17]  is 0.9  rather  than  than  unity),  then 
differencing the data  can  result  in a  misspecified  regression.  Second,  even  if both 
yi, and y,, are  truly /(1) processes,  there  is an  interesting  class  of models  for which 
the  bivariate  dynamic  relation  between  y,  and  y,  will  be  misspecified  if the  re- 
searcher  simply  differences  both  y,  and  y,.  This  class  of models,  known  as  coin- 
tegrated processes,  is discussed in  the  following  chapter. 

APPENDIX  18.A.  Proofs  of Chapter  18 Propositions 
@  Proof  of Proposition  18.1. 

(a)  This  follows  from  [18.1.7]  and  [18.1.8]  with  r  =  1. 
(b)  The  derivation  is identical  to  that  in [11.A.3]. 
(c)  This  follows  from  Proposition  10.2(d). 
(d)  Note  first  in a  generalization  of [17.1.10]  and  [17.1.11]  that 

T 

re 

T 

2 £6  =  p> (G1  +  u)(E_.  +  uw)’ =  > (€-18'-1  +  & 10;  +  u€/_,  +  uu), 

so  that 

2, G-aw)  +  WE)  =  2 68;  -  2 (16-1)  -  > (un) 

=  €rE;  —  Eo€s  -  > (u,u;) 

T 

=  &r&  —  2 (uu). 

7 

[18.A.1] 

562  Chapter 18  | Unit  Roots  in Multivariate  Time  Series 

Dividing  by T, 

: 

| 
[18.A.2] 
T' > (€-.u;  +  u€/_,)  =  T-',6,  -  To  > uw. 
But from [18.1.7], §;  =  T-X,(1).  Hence, from [18.1.8] and the cositiiiees mapping theorem, 
T~'€r€r  =  [VT-X,(1)]  (VT-X7())'  > A-[W)}-{W))-A’.[18.A.3] 

Substituting  this along with result  (c) into  [18.A.2]  produces 

TY (1)  +  w£/,)  > A[W)}{WD]'-A’  -  Py, 

oe 

[18.4.4] 

result (d) for s =  0. 

which establishes 
For s > 0, we have 
T- > Gm,  +  wi) 

Az 

: 

dete 

| 

Sa 

he “e Kens ~ Uns mii,  a H  ovina  ra, 
* ah  -  .  Uys < Wess  +:  +  w_,)] 

ca 

a  =  rs 2, (6-1 +  a6.) 

bg  ot  Uti] 

ees 

Sas 

+r “4 (2. + (rest) a od 

j 

; 

-  a Berens  sation 

*  ;  AILA81} 
=, 
| 
3 ’  teeeit pgretert  |  pol  als Peace Tea  riper 
4 
(c).  2  * 7 

ot +  gas rs dient  ay) 5 ae  a  ae 

A-[W(1)]}-[W(1)]’-A’ — 

|  i a  Vy irt = of | 1 .A.4] 

and result 

beetle lie): 

rey; 

7 

or 

1} 

ys 

f 

e 

1 

T-2>  wu,  =  T-?  Du,  —  TY  €.,  > Awl)  =  a-| W(r)  dr, 

[18.4.8] 

r=1 

j= 

1 

t=1 

from  results  (a)  and  (g).  This  establishes  result  (h) for  s  =  0.  The  asymptotic  distribution 
is the  same  for  any  s,  from  simple  adaptation  of the  proof of Proposition  17.3(g). 

(i)  As  in  [17.3.22], 

TY g46)  =  | [VEX  AOHVEX (ON  dr 

+4 a [ 7  [W(r)]-[W()]’  arh.n’ 

(j),  (k),  and  (I) parallel  Proposition  17.3(i),  (j), and  (k).  @ 

@  Proof of Proposition  18.2. 
with  the  following  transformed  variables: 

The  asymptotic  distributions  are  easier  to calculate  if we  work 

Yu =JVu  —  ByBq'Ya 
yz, =  Lyy2,. 

[18.A.9] 
[18.A.10] 

Note  that  the  inverses  23', (o7)" '_ and  L;'  all  exist,  since  AA’  is  symmetric  positive 
definite.  An  OLS  regression  of yf. on a  constant  and  y3,, 

Yin  =  a*  +  y*'y2  +  Ur, 

[18.A.11] 

would  yield  estimates  7 

fe =  | ss 
17 

Lyx  Lyzyz') 

\ | i 
L2Zyzyt 

[18.A.12} 
Ty 

Clearly,  the  residuals  from  OLS  estimation  of [18.A.11]  are  identical  to  those  from  OLS 
estimation  of [18.3.1]: 

: 

Vi 

ar  —s  VrYx  =  Yn  rs  ar  7  Vr yi 

=  (Yu  —  22,25'y2,)  —  af  -  Yr (Liy2,) 

=  Yu =  OF  =  (97  be  +  2nd  Tx. 

The  OLS  estimates  for  the  transformed  regression  [18.A.11]  are thus  related  to  those  of 
the  original  regression  [18.3.1]  by 

Q> T 

2 

implying  that 

* 

fi é 

=Le??  + Is'Za, 

Yr  =  Le'4r  -—  Lg'23'2,, 

=  Lg'¥r  —  Ly! (LyL3,)>,, 

=  Lz'¥r  —  Lz2,,. 

The  usefulness  of this  transformation  is as  follows.  Notice  that 

[18.A.13] 

[18.A.14] 

ne ‘  we ae  ik  4 =  ty 

Y3, 

0 

Li, 

Yor 

for 

; 

1 

ia  eee (= WVof)-23,25' 

0 

Lh 

564  Chapter  18 | Unit Roots  in Multivariate Time Series 

Moreover, 

| 

L'AA'L  =  wt er  “eg  (of)  a 

0 

Li, 

22 

(-Wot)-23'2,,  Ly 

"  pig. —  Zi 52'5.,) 

 @  ||  (1/o*) 

"| 

L222, 

L322 || (-Wot)-2y'2.,  Ly 

-  s —  2,,25'2,,)Mot)? 

0’ 

0 

L;,2.L., 

But [18.3.7]  implies that  | 

(18.A.15] 

2n  =  (Lyb»)~'  =  (Lz)  'La' 

Li,2yL2  =  Li{(L2)~'L2'}Ly  =  1,. 

Substituting  this and [18.3.6]  into  [18.A.15] results in 

‘L’AA'L = I, 

| 

. 

a 

b 

Ong  8 A. 16] 

One of the ‘agian that if W(r) is n-dimensional  Par Bhotesan: matens . 

ae the n-dimensional process W*(r)  defined  by 

|  Hy 

WA) =  LAW) 

te xan 

: 

. 

aon 

L = I,. In other words,  W*(r) could also 

standard B 

otion. gh result (g) of Proposition 18.1 impies that 7 
ah 

| ad 

“Be 

i  Ese ae" 

c’ re dy4 a wor) dr, 

{ i 
ar: bas zoramit> bomnoteasti ad noswiad orator 

=1 

ted) 

eodaiiate 

the  result  is 
T-¥  A  lbase 
0 

LJILyfor 

T2  olf 

T 
I,JL2yx 

Syz'  |  [7-4 
Zy2y2' 

-  | 0 

re  ee 
[fT 
of  lf T 
-  ¥  TU  Mi dys.  Sytye  iL 9 

tT 

 Syz' 

<0  eae  0’  || eke 

0 

T-7I,\L2Zy2yt/or 

0’  Mies  0’  ll Syt/o?  }) 
ly 

T-7I,jL2Zy2yi/ot 

0 

or 

a  ad  ey 

Yr/ot 

T-3*Zy2, 

' 

T  PEs 
T-*2y3,93 

rd 

a 

pe ee  [18.A.20] 
T~*2y2yt/e% 

Partition  W*(r)  as 

W*(r)  = 
(nx  1) 

W7(r) 

(11) 

W?(r) 
(sx 1) 

: 

Applying  [18.A.18]  and  [18.A.19]  to  [18.A.20]  results  in 

1  “ar /laf 

Yr/ot 

£ 

[owsey ar 

| 
-l{weoar [owsortwrerar| 
“fn 

—1 

| Wi(r) dr 

[[wie-wr@d| 

08421) 

Recalling  the  relation  between  the  transformed  estimates  and  the  original  estimates  given 
in [18.A.14],  this  establishes  that 

. 

|  l-“26-/a; 

ja(t] 

(ot)  (Lz'¥7r  —  L222) 

h,  | 

Premultiplying  by 

oT 

0’ 

0 

«ofl, 

and  recalling  [18.3.7]  produces  [18.3.8]. 

Proof  of (b).  Again  we  exploit  the  fact  that  OLS  estimation  of [18.A.11]  would  produce 
the identical  residuals  that  would  result  from  OLS  estimation  of [18.3.1].  Recall  the expres- 
sion  for the residual  sum  of squares  in [4.A.6]: 

, 

Rss,= Bvt)  ~  {Bvt ayiat'|  sre eat  |} 

T. 

Sys! 

Lt 

Sys 

2077  -  {19% aia)  | 
(Gadel 

earns part  attire b Pom: 

[18.A.22] 

566  Chapter 18 | Unit Roots in Multivariate  Time Series 

If both sides  of [18.A.22]  are  divided  by (7-7),  the result  is 

T-*RSS (a?) 

=  T-*Xyi/ot}  -  (r *Z(yi/ot)  T-*X(ytlot)yz'] 
«|  Set  ‘d T-"3yt/ot 
T~*2yiy,/o7 

T° *Sy32,  T-*2ysy3' 

4  fowsep 

:  | Wi(r)  dr  iene i 

l 

| [W2(r)]'  dr 

[ws (r) dr 

“T[ws0 dr  fowsnriws@y ar  | [ws 0Ws(0) ar| 

om 

eas of 

ret  Nn a Ft he pate  esa 

F  of the h 

regression 

N  th: 

=” 

BE 

Fi 

. 

of [18.4.1], where, from [18.A.13], 

¥,, at,  y  © 

. 

Ry —Po  rie +  2a'Za}  -r=  R*y*  naiti  oo 

ening  al gi: 

on  omtete  taai 

{ 

q 
| 

7. 

i 

et  ene osu mais: 

ae, 33. 
=H 

¢})  Shew  that the  rates: 

= Be aloo asymptoticaity rt ar - REz'E,,. 

Re =  R: Li 
of 

es The OLS  F test ot R*y*  =  r* is, jiven by 
i. 

- -  L  dings be 
: 

ay 

onan  7 

jst  etho3 +  eho a 

. Se  gh  ook  Os 

cape 

Fa ge  Ae ie  eee  cee 

ig aoitarides  235) mikec> 

epeGeadtrt  —  1) 

into 

{R*9* 
r 
. 

P 

. 

. 

. 

ae 

and  so,  from  result  (b), 

T--(st?  =  (TT  -  n)):T-?-RSS; > (o%)-H. 

[18.A.26] 

Moreover,  [18.A.18]  and  [18.A.19]  imply  that 

-3725 

ye! 
:  el  + 

17! 

: 

POMDYa  TRY EYe 

1 

[ weer  a 

[ wso  ar  [ pwe@rtwser  ar 

, 

 [18.A.27] 

while  from  [18.A.21], 

Substituting  [18.A.26]  through  [18.A.28]  into  [18.A.25],  we  conclude  that 

47>  of-h,. 

(18.A.28] 

T-'-F,—  {o%-R*h,  —  r*}’  x  {ott  R*] 

x 

1 

: [W3(r)]'  dr 

[wze ar  fowsrtwscr ar] 

= 
I} {ov-R*h,  —  r*}  +m.  @ 
UR 

Chapter  18 Exercises 

18.1. 

Consider  OLS  estimation  of 

=  Coy,.i  +  GoAy,25-  + 

+  Gin  18-55 1  +'  a;  +p  9...)  2b gs 

where  y,,  is  the  ith  element  of  the  (m  xX  1) vector  y,  and  e,  is  the  ith  element  of  the 
(nm X  1) vector  €,.  Assume  that  €, is i.i.d.  with  mean  zero,  positive  definite  variance  N, and 
finite  fourth  moments  and  that  Ay,  =  W(L)e,,  where  the  sequence  of  (m  X  nm) matrices 
{s‘W,}*_o is absolutely  summable  and  W(1) is nonsingular.  Let  k =  np + 1 denote  the number 
of regressors,  and  define 

=,  (yh)  Plas  «AI  ess  1, 2-1)’. 

Let  b; denote  the  (kK  x  1) vector  of estimated  coefficients: 

by =  (2x,x;)~ 

(2x, yi), 

where  2 denotes  summation  over  ¢ from  1 to  T. Consider  any  null  hypothesis  H,:  RB  =  r 
that  involves  only  the  coefficients  on  Ay,_,—that  is, R is of the  form 

cits 

“rend 

Let  y+ be  the  Wald  form  of the  OLS  y? test  of H,: 

(mx  k) 

[mxn(p-1)) 

 [mx(1+n)] 

x7 =  (Rb;  —  r)'[s}R(2x,x;)~'R']- 

(Rb;  —  r), 

where 

5} =  (T —  k)-'X(y,  —  b7x,)?. 
Under  the  maintained  hypothesis  that  a,  =  0 and  p;  =  e; (where  e; denotes  the  ith  row 
of I,), show  that  x3  > y?(m). 

18.2. 

Suppose  that  the  regression  model 

Ye  *  GnAy,us  +  Cady,-s  +  °°:  +  Cip-14Y,- p41  +  a,  +  PiY-1+  &% 

568  Chapter 18 | Unit Roots in Multivariate  Time Series 

satisfies  the  conditions  of Exercise  18.1.  Partition  this  regression  as  in  [18.2.37]: 

Yn  =  Bidyis-1  +  Yidyo.-1  +  BrAyi,-2  +  y2dyo,-2  +  °° 

+  a  ae  *  a  ORE  +  a,  v  WY ic-1 

7  d’y.,-.+  ei, 

where  y,,  is  an  (”, x  1) vector  and  y,,  is an  (nm,  x  1) vector  with  n,  +  n,  =  n.  Consider 
the  null  hypothesis  y,  =  y,  = 
++  =  y,_,  =  8  =  0.  Describe  the  asymptotic  distribution 
of the  Wald  form  of the  OLS  ,? test  of this  null  hypothesis. 

18.3. 

Consider  OLS  estimation  of 

Yu  =  yAy,  +  a+  y,,-1  +  NY2u-1  +  UL, 

where  y,, and y,,  are  independent  random  walks  as  specified  in [18.3.13]  and  [18.3.14].  Note 
that the  fitted  values  of this  regression are  identical  to  those  for  [18.3.17]  with  a@;,  7;,  and 
@, the  same  for  both  regressions  and  6;  =  fr  —  ¥r. 

(a)  Show  that 

T'?47 

T"°4y 

» 

ys 

= 

T(¢r  —  1) 

Thr 

s 

Vy 

V2 

V3 

V4 

where  v,  ~ N(0,  o7/03)  and  (v2, v3,  v4)’ has  a nonstandard  limiting  distribution.  Conclude  that 
¥r, &;,  by,  and  7, are  consistent  estimates  of 0, 0, 1, and  0, respectively,  meaning  that  all 
of the  estimated  coefficients  in  [18.3.17]  are  consistent. 
0 

Show  that  the  ¢  test  of  the  null  hypothesis  that  y  =  0  is  asymptotically 

5  iB). 

(c)  Show  that  the  ¢ test  of the  null  hypothesis  that  5 =  0 in the  regression  model  of 

[18.3.17]  is also  asymptotically  N(0,  1). 

Chapter  18 References 

Blough,  Stephen  R.  1992.  ‘‘Spurious  Regressions,  with  AR(1)  Correction  and  Unit  Root 
Pretest.’’  Johns  Hopkins  University.  Mimeo. 
Chan,  N.  H.,  and  C.  Z.  Wei.  1988.  ‘Limiting  Distributions  of Least  Squares  Estimates  of 
Unstable  Autoregressive  Processes.”  Annals  of Statistics  16:367—401. 
Granger,  C.  W.  J.,  and  Paul  Newbold.  1974.  “Spurious  Regressions  in  Econometrics.” 
Journal  of Econometrics  2:111-20. 
Ohanian,  Lee  E.  1988.  “‘The  Spurious  Effects  of Unit  Roots  on  Vector  Autoregressions:  A 
Monte  Carlo  Study.”  Journal  of Econometrics  39:251-66. 
Park,  Joon  Y.,  and  Peter  C.  B.  Phillips.  1988.  ‘Statistical  Inference  in  Regressions  with 
Integrated  Processes:  Part  1."  Econometric  Theory  4:468-97. 

and 

.  1989.  ‘Statistical  Inference  in Regressions  with  Integrated  Processes: 

Part  2.”  Econometric  Theory  5:95-131. 
Phillips,  Peter  C. B.  1986.  “Understanding  Spurious  Regressions in Econometrics.”  Journal 
of Econometrics  33:311-—40. 

.  1988.  ““Weak  Convergence  of Sample  Covariance  Matrices  to  Stochastic  Integrals 

via  Martingale  Approximations.”  Econometric  Theory  4:528-33. 

and  S.  N.  Durlauf.  1986.  ‘Multiple  Time  Series  Regression  with  Integrated  Proc- 

esses.”  Review  of Economic  Studies  53:473-9S. 
————_  and  Victor  Solo.  1992.  ‘“‘Asymptotics  for  Linear  Processes.”  Annals  of Statistics 
20:971-1001. 
Sims,  Christopher  A.,  James  H.  Stock,  and  Mark  W.  Watson.  1990.  “Inference  in Linear 
Time  Series  Models  with  Some  Unit  Roots.”  Econometrica  58:113-44. 
Toda,  H. Y., and P. C. B. Phillips.  1993a.  “The Spurious Effect of Unit Roots on  Exogeneity 

a 

: 

Chapter  18  ae  569 

Tests  in  Vector  Autoregressions:  An  Analytical  Study.” Journal  of Econometrics  59:229- 
55. 
.  1993b.  “Vector  Autoregressions  and  CACeAY  Econometrica  61: 

| 

and 

1367-93. 
West,  Kenneth  D.  1988.  ‘“‘Asymptotic  Normality,  When  Rearesers Have  a  Unit  Root." 
Econometrica  56:1397-1417. 

at 

ites 
Eat  te  ;  a 
ee | 

fs 
Phe ANS  =  OEY  Aig 
ars 
ae 
SY 

vas  ir?  2 

42 
aes 

Y 

aye 

: 
f 

| 
: 

a 

; 

: 

. 

; 

Pe 

= 

E 

< 

Ls 
” 
3 

4 

; 
“4 
a 
; 

; 

i 
sd 
sath »  suka noduditelb gnisimil trsbastenon  & 2nd ‘(.¥ 
ifm  taexk  adineom oClorisagest  Gbne  i. 4 Oto is) i  S16  +f  ane 

ost 

{roltp 

me ‘A= 

me  {7X tv 

bas 

oe: 

ep 

vits hort vit  Bi  tt =  Sa iat  = seohones. fie. ait  ro 
et 
er  i 
Bf  ie 
ORY Motes: ast oh  G  =  & tert cheattoggya  tos  ods Yo  tens 4 ery aii. A 

i 

instante  sxe  (TT-£.8F  ai ain  sew 200 basemiizs. 

cas 

‘ 

se 

ii bide  aerate. 

2  SE  Fe we: the a. = 
e =. be Bins  ange  ess 4354 if  i Mi.  mete  3 Ae 

—— snk 

bas 

19 

Cointegration 

This  chapter  discusses  a  particular  class  of vector  unit  root  processes  known  as 
cointegrated  processes. Such  specifications  were  implicit  in the  “‘error-correction™ 
models advocated  by Davidson,  Hendry, Srba, and Yeo (1978).  However,  a formal 
development of the key. concepts i not poner’ wee se stork: of Granger (1983) 
and Engle and Granger  (1987). 

‘Section 19.1  introduces  the  Bikes er caeterpratiagh AB Be several 
mG ae  representations  of a cointegrated  system.  Section  19.2 discusses tests of 

qamaliok 

4 

oie 
‘pied 

|  whether a vector process  is cointegrated. These tests  are  summarized in Table 19.1. 
. 
“equation  methods  for  estimating  a cointegrating  vector  and  testing a hy- 
pothesis about its value are  presented in  Section  19.3, Full-information  maximum 
|  likelihood estimation is discussed in Chapter 20. 

geiwesom al eer  soinserq ql i Bot 
- 

i  RIS 

—> 

1  bes, ‘2.  9 

; 
25; 
Sow  - AA sab, Toyp $8  Ylisexs  enihiad  mon  < irene 
v 10 
beni age a  o@t  tuts 

DGS  tsa  ROUSPRMENEY? 

—  YY2,)  is stationary.  Hence,  we  would  say  that  y,  =  (y4,, y2,)’  is cointegrated 

(y1, 
with  a’  = 

(1,  —¥). 

.  tha 19.1  plots  a  sample  realization  of [19.1.1]  and  [19.1.2]  for  y  =  1 and 
u,,  and  u>,  independent  N(0,  1) variables.  Note  that  either  series  (y,, or Yor) will 
wander  arbitrarily  far  from  the  starting  value,  though  y,,  should  remain  within  a 
fixed  distance  of  yy>,,  with  this  distance-determined  by the  standard  deviation  of 

ies 
Cointegration  means  that  although  many  developments  can  cause permanent 
changes  in the  individual  elements  of y,,  there  is some  long-run  equilibrium  relation 
tying  the  individual  components  together,  represented  by the linear  combination 
a'y,.  An  example  of such  a system  is the  model  of consumption  spending  proposed 
by Davidson,  Hendry,  Srba,  and  Yeo  (1978).  Their  results  suggest  that  although 
both  consumption  and  income  exhibit  a  unit  root,  over  the  long  run consumption 
tends  to  be a roughly  constant  proportion  of income,  so  that  the difference between 
the  log of consumption  and  the  log of income  appears  to  be a stationary  process. 
Another  example  of an  economic  hypothesis  that  lends  itself  naturally  toa 
cointegration  interpretation  is the  theory  of purchasing  power  parity.  This  theory 
holds  that,  apart  from  transportation  costs,  goods  should  sell  for the  same  effective 
price in two  countries.  Let P, denote  an  index  of the  price level  in the  United  States 
(in dollars  per  good),  P; a  price  index  for  Italy  (in lire  per  good),  and  S, the  rate 
of exchange  between  the  currencies  (in dollars  per  lira).  Then  purchasing  power 
parity  holds  that 

or,  taking  logarithms, 

P, =  S,Pr, 

PP,  =  OF  t  Pr, 

where  p,  =  log P,, s, =  log 5,, and  p;*  =  log P*.  In  practice,  errors  in measuring 
prices,  transportation  costs,  and  differences  in  quality  prevent  purchasing  power 
parity  from  holding  exactly  at  every  date  t.  A weaker  version  of the  hypothesis  is 
that  the  variable  z, defined  by 

2  =  py  —  B=  Py 

[19.1.6] 

at VJ 

-4 

FIGURE  19.1  Sample  realization  of cointegrated  series. 

572 

Chapter  19  | Cointegration 

is  stationary,  even  though  the  individual  elements  (p,,  s,,  or  p*)  are  all  /(1). 
Empirical  tests  of this  version  of the  puchasing  power  parity  hypothesis  have  been 
explored  by Baillie and  Selover  (1987)  and  Corbae  and  Ouliaris  (1988). 

Many  other  interesting  applications  of  the  idea  of  cointegration  have  been 
investigated.  Kremers  (1989)  suggested  that  governments  are  forced  politically  to 
maintain  their debt at  a  roughly  constant  multiple  of  GNP,  so  that  log(debt)  — 
log(GNP)  is  stationary  even  though  each  component  individually  is not.  Campbell 
and  Shiller  (1988a,  b) noted  that  if y,,  is (1)  and  y,,  is a  rational  forecast  of future 
values  of Ya»  then  y,  and  y,  will  be  cointegrated.  Other  interesting  applications 
include  King,  Plosser,  Stock,  and  Watson  (1991),  Ogaki  (1992),  Ogaki  and  Park 
(1992),  and  Clarida  (1994). 

It was  asserted  in the  previous  chapter  that  if y, is cointegrated,  then  it is not 
correct  to  fit  a  vector  autoregression  to  the  differenced  data.  We  now  verify  this 
claim  for  the particular  example  of [19.1.1]  and  [19.1.2].  The  issues  will  then  be 
discussed  in  terms  of a general  cointegrated  system  involving  n  different  variables. 

Discussion  of the  Example  of [19.1.1]  and  [19.1.2] 

Returning  to the  example  in [19.1.1]  and  [19.1.2],  notice  that  €,,  =  uz,  is the 
error  in  forecasting  Y2,  on  the  basis  of  lagged  values  of  y,  and  y,  while  e,,  = 
yuz,  +  u,,  is  the  error  in forecasting  y,,.  The  right  side  of [19.1.4]  can  be  written 

(Ya  +  Uy)  —  yyy  =  Exe  —  (Erna  —  YEre-1)  =  (1  —  Ley  +  yLex. 
Substituting  this  into  [19.1.4]  and  stacking  it in a vector  system  along  with  [19.1.3] 
produces  the  vector  moving  average  representation  for  (Ay,,,  Ay2,)’, 

Ayy  a 

Ei 

bet ‘a wn) ‘ 

where 

W(L)  =  | 4 ‘  ye 

eae 

[19.1.8] 

| 

A  VAR  for  the  differenced  data,  if it existed,  would  take  the  form 
| 

@(L)Ay,  =  €,, 
where  ®(L)  =  [W(L)]-!.  But  the  matrix  polynomial  associated  with  the  moving 
average  operator  for  this  process,  W(z),  has  a  root  at  unity, 
W()|  = 

0 
Hence  the  matrix  moving  average  operator  is noninvertible,  and  no  finite-order 
vector  autoregression  could  describe  Ay,. 
The  reason a finite-order  VAR  in differences  affords  a  poor  approximation 
to  the  cointegrated  system  of [19.1.1]  and  [19.1.2]  is that  the  level  of y2 contains 
information  that is useful  for forecasting y, beyond  that contained  in a finite  number 
of lagged  changes  in y2  alone. 
If we  are  willing  to  modify  the  VAR  by including  lagged  levels  along  with 
lagged changes,  a stationary  representation  similar  to a  VAR  for Ay, is easy  to find. 
Recalling  that  u,,-1  =  Yis-1  —  YWas-1  notice  that  [19.1.4]  and  [19.1.3]  can  be 

(1 coh.  % 
1 

=  0. 

aah  adit  La 

09 

19.1.  Introduction 

573 

The  general  principle  of which  [19.1.9]  provides  an  illustration  is that with  a 
cointegrated  system,  one  should  include  lagged  levels  along  with lagged  differences 
in a  vector  autoregression  explaining  Ay,- The  lagged  levels  will  appear  in  the  form 
of those  linear  combinations  of y that  are  stationary. 

General  Characterization  of the  Cointegrating  Vector 

Recall  that  an  (n  x  1) vector  y, is said  to be cointegrated  if each  of its elements 
individually  is /(1)  and  if there  exists  a  nonzero  (n  X  1) vector  a  such  that ay, IS 
stationary.  When  this  is the  case,  a  is called  a  cointegrating  vector. 

Clearly,  the  cointegrating  vector a is not  unique,  for  if a’y, is stationary,  then 
so  is ba’y,  for  any  nonzero  scalar  5; if a  is a  cointegrating  vector,  then  so 1s ba.  In 
speaking  of the  value  of the  cointegrating  vector,  an  arbitrary  normalization  must 
be  made,  such  as  that  the  first  element  of a is unity. 

If there  are  more  than  two  variables  contained  in y,,  then  there  may  be  two 
nonzero  (n X  1) vectors  a, and  a, such  that  ayy, and  ay, are  both  stationary,  where 
a, and  a, are  linearly  independent  (that  is, there  does  not  exist  a scalar  b such  that 
a,  =  ba,).  Indeed,  there  may  be  h < n linearly  independent  (n  x  1) vectors  (a, 
@,...  ,a,) such  that  A’y, is a stationary  (h x  1) vector,  where  A’  is the  following 
(h  X  n) matrix:! 

Wao 

a; 
a) 
eae 

a, 

[19.1.10] 

Again,  the  vectors  (a,,  a,... 
,  a,) are  not  unique;  if A’y,  is stationary,  then  for 
any  nonzero  (1  x  h)  vector  b’,  the  scalar  b’A’y,  is  also  stationary.  Then  the 
(n  x  1) vector  m  given  by ’  =  b‘A’  could  also  be  described  as  a  cointegrating 
vector. 

| 

Suppose  that  there  exists  an  (h  Xx  n) matrix  A’  whose  rows  are  linearly 
independent  such  that  A’y, is a  stationary  (h  x  1) vector.  Suppose  further  that  if 
c’ is any  (1  x  7) vector  that  is linearly  independent  of the  rows  of A’,  then  c’y, is 
a nonstationary  scalar.  Then  we  say that  there  are  exactly  h cointegrating  relations 
among  the  elements  of y, and that  (a,,  a,,  . 
,  a,) form  a  basis  for  the  space  of 
cointegrating  vectors. 

. 

. 

Implications  of Cointegration 
for the  Vector  Moving  Average  Representation 
We  now  discuss  the  general  implications  of  cointegration  for  the  moving 
average  and  vector  autoregressive  representations  of a  vector  system.”  Since  it is 
assumed  that  Ay, is stationary,  let  8 =  E(Ay,)  and  define 

| 

u, =  Ay,  —  8. 

{19.1.11] 

Suppose  that  u, has  the  Wold  representation 

u,  =  E,  +  W,e,_,  >  W,€,_,  5  MS 

W(L)e,, 

‘If  h =  n  such  linearly  independent  vectors  existed,  then  y, would  itself  be  /(0).-This  claim  will 
become  apparent  in the  triangular  representation  of a cointegrated  system  developed  in [19.1.20]  and 
[191.21]. 

These  results  were  first  derived  by Engle  and  Granger  (1987). 

574  Chapter 19  | Cointegration 

where  E(e,)  =  0 and 

= 

ie,  fe  “  fort  =T 
otherwise. 
0 
Let  W(1)  denote  the  (n  x  n) matrix  polynomial  W(z)  evaluated  at  z  =  1; that  is, 
WI)  =1,+  WW, ++  W,+--- 

“ We first  claim  that  if A’y,  is Stationary,  then 

[19.1.12] 
To verify  this  claim,  note  that  as  long  as  {s-W,}*_,  is  absolutely  summable,  the 
difference  equation  [19.1.11]  implies  that 

A'W(1)  =  0. 

y=  ¥o  FS 

ty  +  ore  ty, 

Yo  +  5:  +  W(1)-(e, 

+e, 

+--+  +e)  +H,  -  H, 

[19.1.13] 

where  the  last  line  follows  from  [18.1.6]  for n, a stationary  process.  Premultiplying 
{19.1.13]  by A’  results  in 

A’y,  =  A'(¥o  —  No)  +  A’S-t  +  A'W(1)-(€, 

+e,  +  +++  +e)  +  Ay,  [19.1.14] 

_If E(e,£;)  is nonsingular,  then  c’(e,  +  ©,  +--+  +  e€,) is I(1)  for  every  nonzero 
: (n  X  1) vector  c.  However,  in order  for  y, to  be  cointegrated  with  cointegrating 
vectors  given  by the  rows  of A’,  expression  [19.1.14]  is required  to  be  stationary. 
This  could  occur  only  if A’¥(1)  =  0.  Thus,  [19.1.12]  is a  necessary  condition  for 
cointegration,  as  claimed. 

As  emphasized  by Engle  and  Yoo  (1987)  and  Ogaki  and  Park  (1992),  con- 
dition  [19.1.12]  is not  by itself  sufficient  to  ensure  that  A’y,  is stationary.  From 
[19.1.14],  stationarity  further  requires  that 

A'S  =  0. 

(19.1.15] 

If some  of  the  series  exhibit  nonzero  drift  (6  #  0),  then  unless  the  drift  across 
series  satisfies  the  restriction  of  [19.1.15],  the  linear  combination  A'y,  will  grow 
deterministically  at  rate  A’5.  Thus,  if the  underlying  hypothesis  suggesting  the 
'  possibility  of cointegration  is that  certain  linear  combinations  of y, are  stable,  this 

requires  that  both  [19.1.12]  and  [19.1.15]  hold. 

Note  that  [19.1.12]  implies  that  certain  linear  combinations  of the  rows  of 
W(1),  such  as  ajW(1),  are  zero,  meaning  that  the  determinant  |W(z)|  =  0 at 
z  =  1. This  in turn  means  that  the  matrix  operator  W(L)  is noninvertible.  Thus, 
a  cointegrated  system  can  never  be  represented  by a finite-order  vector  auto- 
regression  in the  differenced  data  Ay,. 

For  the example  of [19.1.1]  and  [19.1.2],  we  saw  in [19.1.7]  and  [19.1.8]  that 

and 

W(z)  =  | a ~ 

0 
W(1)  =  x 7 

This  is  a singular  matrix  with  A’W(1)  =  0 for A’  =  [1  —y]. 

| 

19.1.  Introduction  575 

Phillips’s  Triangular  Representation 

Another  convenient  representation  for  a  cointegrated  system  was  introduced 
by  Phillips  (1991).  Suppose  that  the  rows  of  the  (h  x  n) matrix  A’  form a basis 
for  the  space  of  cointegrating  vectors.  If the  (1,  1) element  of  A’  is nonzero,  we 
can  conveniently  normalize  it to  unity.  If, instead,  the  (1,  1) element  of A’  is zero, 
we  can  reorder  the  elements  of  y,  so  that  y,,  is included  in  the  first  cointegrating 
relation.  Hence,  without  loss  of generality,  we  take 

a; 

a; 

A  => 

—_— 

bi 

@jz: 

443. 

°° 

5  Ban 

Gay 

RGa> 

" 

A234" 

8 

ey, 

aj, 

Gn, 

Anz 

4px 

°°  *° 

Ann 

If a,,  times  the  first  row  of A’  is subtracted  from  the  second  row,  the  resulting  row 
is a  new  cointegrating  vector  that  is still  linearly  independent  of a,,  a3,  ...  ,  a,.° 
Similarly  we  can  subtract  a3,  times  the  first  row  of A’  from  the  third  row,  and  a,, 
times  the  first  row  from  the  hth  row,  to  deduce  that  the  rows  of  the  following 
matrix  also  constitute  a  basis  for  the  space  of cointegrating  vectors: 

Pipe:  Bigot 
0  ay  a  *** 
pee 

; 

By, 
Q3, 

|  cats 

O  Gir  ans 

°°  * 

Gin 

- 

- 

Next,  suppose  that  a3,  is nonzero;  if a3,  =  0,  we  can  again  switch  y,,  with  some 
variable  y3,,  Ya,  - 
»  Yn  that  does  appear  in  the  second  cointegrating  relation. 
Divide  the  second  row  of A;  by a3,.  The  resulting  row  can  then  be  multiplied  by 
a,,  and  subtracted  from  the  first  row.  Similarly,  a3}, times  the  second  row  of Aj can 
be  subtracted  from  the  third  row,  and  aj, times  the  second  row  can  be subtracted 
from  the  Ath.  Thus,  the  space  of cointegrating  vectors  can  also  be  represented  by 
ee 
1  ...0.-=  a¥s 

eee 

* * 
ase 
oe  ** 
a3, 

Owl  a35 

A;  = 

- 

wh 
0  0  an3 

CNG 

** 
ann 

*Since  the  first  and  second  moments  of the  (A  x  1) vector 

a 
a; 
hy  y, 

a, 

do  not  depend  on  time,  neither  will  the  first  and  second  moments  of 

a; 
a  —  a2,a, 
ms 

‘ 

' 

y,- 

a, 
Furthermore,  the  assumption  that  a,,  @,,...  ,  a, are  linearly  independent  means  that  no  linear  com- 
bination of a,, @,...  ,  a, is zero,  and  so  no  linear  combination  of @,, @)  —  @),8,,...  ,  a, can  be zero 
cither.  Hence  a,,  a,  ~  @,,a,,.  . 

,  a, also constitute  a basis  for the space  of cointegrating  vectors. 

. 

__ 

576  Chapter 19  | Cointegration 

Proceeding  through  each  of the  A rows of A’  in this  fashion,  it follows  that 
given any (n X  1) vector y, that is characterized  by exactly A cointegrating  relations, 
it  is  possible  to  order  the  variables  (y,,,  Ya  -  +++  Ye)  in  such  a  way  that  the 
cointegrating  relations  can  be  represented  by an  (h  Xx  n) matrix  A’  of the  form 

Ae  Be  b-4.,,0  Tvesemiemiase. 

°° 
0  ees  | Maes  ot" 

lime  °  a< 

Nin 
om Vee 

[19.1.16] 

oe 

het  “these 

°° 

~~ Van 

=.  7 ‘| 

where r’ is an  (h  x  g) matrix  of coefficients  for  g 

=n  —  h. 

otet z, denote  the residuals  associated  with  the  set  of cointegrating  relations: 

Since  z, is stationary,  the  mean  p¥* = E(z,) exists,  and we  can  define 

z,  =A’'y,._ 

(Ax 1) 

, 

(19.1.17] 

: 

3 

xé 

+ =z, -  pt. 

; 

Partition y, as 
oa  eaitqchs  dev 

—t 

} 

Se 
) 

Veh 

ir 

PG 

, 

[19.1.18] 
a 
iio,  ae  ort 

ee 

y=  fre) g 
(nx  1) 

I 
(g x  1) 

else  ie 

[19.11.19] 

i 

|  Gita: sia: 1. 1.16], ps. FE ves =e his. bs 19) i ae U9. ; mi results in 

“Stir G8i He64  5  ik 

ae 

) 

a 
hi 

-  Pegi.  bas ;._.a2Po2, theds =F ] .  1 ki 
soheev (F % gy ond  s2ion ssisive 33 Pacaalte hi  7,  Soni?  { 

x 
ioe 
hat pat sou, FOF  = ae NSB ft: vite. 9 ae 

eae fe 

| 

[19.1.6],  the  triangular  representation  would  be 

Pr  = VS  +  Pr  t+  wit  2 

As,  =  5,+  u,, 

Ap?  =  5,  +  up»); 

where  the  hypothesized  values  are  y,  =  y2  =  1. 

The  Stock-Watson  Common  Trends  Representation 

Another  useful  representation  for  any  cointegrated  system  was  proposed  by 
Stock  and  Watson  (1988).  Suppose  that  an  (m  x  1) vector  y, is characterized  by 
exactly  A cointegrating  relations  with  g =  n  —  h.  We  have  seen  that  it is possible 
to  order  the  elements  of y,  in  such  a  way  that  a  triangular  representation  of  the 
form  of  [19.1.20]  and  [19.1.21]  exists  with  (z*’,  u3,)’  a  stationary  (m  x  1) vector 
with  zero  mean.  Suppose  that 

: 

ea] 

s=0  | Js€;_; 

U2, 

for  €,  an  (n  x  1) white  noise  process,  with  {s-H,}7_)  and  {s-J,}7_.  absolutely 
summable  sequences  of (h  x  n) and  (g x  n) matrices,  respectively.  Adapting  the 
result  in [18.1.6],  equation  [19.1.21]  implies  that 

- 

+  3,°t  + 
Ya  = Yoo  + Bt  + 2 Uy 
aps  8,-#.+.J0)-(e, 

u 

ie 

Fe, 

++  <~  +  &)  +  %,  —  toe, 

[19.1.2] 

where  J(1)  =  (Jo  +  Ji  +  Jn  +  +--+),  Me;  =  Vom, €,_,,  and  a=  —(J,,,  + 
Js42  +  J,43  +  -  +  -). Since  the  (n x  1) vector  €, is white  noise,  the  (g x  1) vector 
J(1)-€,  is also  white  noise,  implying  that  each  element  of the  (g  X  1) vector  &,, 
defined  by 

€, =  S(l)-(e1  + et  --  >  +e) 

(19.1.23] 

‘is described  by a  random  walk. 

Substituting  [19.1.23]  into  [19.1.22]  results  in 

Yor  =  fy  +  Bt  +  Et  ny 

[19.1.24] 

for 2 =  (Y2o  —  M20).  Substituting  [19.1.24]  into  [19.1.20]  produces 

Vie  =  Py  +  I'(8,°¢  +  E,,)  +  Ty 

{19.1.25] 

for  p, =  pf  +  P'p,  and  H,, =  z7  +  I''y,. 

Equations  [19.1.24]  and  [19.1.25]  give  Stock  and  Watson’s  (1988)  common 
trends  representation.  These  equations  show  that  the vector  y, can  be described  as 
a stationary  component, 

BP 

Nha: 

plus linear  combinations  of up to  g common  deterministic  trends,  as  described  by 
the (g x  1) vector  6, :t, and linear combinations of gcommon  random walk variables 
as  described  by the  (g x  1) vector  ,,.  - 

| 

578  Chapter 19 | Cointegration 

Implications  of Cointegration 
for the  Vector  Autoregressive  Representation 
Although  a  VAR in  differences is  not  consistent  with  a cointegrated  system, 
a  VAR in  levels  could  be.  Suppose  that  the  level  of y,  can  be  represented  as  a 
nonstationary  pth-order  vector  autoregression: 

y=  a+  My,_,  +  Dy,»  +--+:  +  ®.4. , +  &,, 

[19.1.26] 

or 

where 

D(L)y,  =  a  +  €,, 

[19.1.27] 

WL)  at,  -  OL  -  @L*  -"-*-+  OV. 

[19.1.28] 

Suppose that Ay, has  the  Wold  representation 

(1 -  Ly,  = 68 +  W(L)e,, 

[19.1.29] 

Premultiplying [19.1.29] by ®(L) results in 

(1 —  L)@(L)y, = ©(1)8  +  Age  o  Piha 

at 
(1 -  De, = O18  + OWL), 

being | fi. 1 27 into fis. 1.30],wehave 
mews: 
since (=  = a  0. vie 2 ea LY, 1. 2a has to hold for all 
age (i>. setkiedhs MES [Fx  A} (18  = 0 tS Seat  yaiaon 
Os pres the  * 

I 

“po, 31 
we oe, 
realizations of 

¥ 
Zo  a 

Fs 

[19.1.32] 
es 

aie tha hae ws “Hie ee  eee  oe, 

LYW(L) t  ee a  polynomial in Tis 
ee 

correc 
:  LM, 

i {SPs : — con 1 33] 

SASS  ae 

Indeed,  in the  light  of the  Stock-Watson  common  trends  representation  in [19.1.24] 
and  [19.1.25],  we  could  say  that  ®(z)  contains  g =  n  —  h unit  roots. 

Error-Correction  Representation 

A final  representation  for  a cointegrated  system  is obtained  by recalling  from 
equation  [18.2.5]  that  any  VAR  in the  form  of [19.1.26]  can  equivalently  be written 
as 

y,  =  ¢, Ay,_,  +  G, Ay,-2 

++  °°  +  0,-,Ay,-p+1  + @+  Py,-1  +  &, 

[19.1.36] 

where 

p=@,+@,+---+9@, 

[19.1.37] 

C, = —(®,.,  +  B42  + 

ooh  Dp) 

fone 

12  4 

ee  1  ee  ee 

Subtracting  y,_,  from  both  sides  of [19.1.36]  produces 

Ay, =f)  Ay,-1  +  G2 AY2  +77 

+ bp  AY  per 

$+  LoY-1  + €,, 

 [19-1.39] 

where 

(=p  -I,  =  -(l,  —  ®,  —  ®.  -—-:::  —  ®,)  =  —®(1)._ 

[19.1.40] 

Note  that  if y, has  A cointegrating  relations,  then  substitution  of [19.1.35]  and 

[19.1.40]  into  [19.1.39]  results  in 

Ay,  =  ©, Ay,-1  +  GAR 2 +  +>  *  + G1 AY, _p+i  +  @  —  BAY,,3,3  ©,- 

[19.1.41] 

Define  z, =  A’y,,  noticing  that  z, is a stationary  (h x  1) vector.  Then  [19.1.41]  can 
be  written 

7 

Ay, =, Ay,_;  +  & Ay,-.  +--+  +  -,AY,-p+1  + @  —  Bu,_,  ¥  ©.  ae 

Expression  [19.1.42]  is known  as  the  error-correction  representation  of the 

cointegrated  system.  For  example,  the  first  equation  takes  the  form 

Ayr =  CYPAY  a1  + Sp Ayan  to  + SPAY 

+  OP AY 1 y-2  +  (DAy,,_2  ere  +f  a).  +** 

+  Ge  bi ssonih  a  eg  ARS  lee 

Cerny,  £  Jt 

He  ea  hy(2  lee  azened  BE 

Op Zae ae.  tof 

| 

where  £‘) indicates  the  row  i, column  j element  of the  matrix  {,, b;, indicates  the 
row  i, column  j element  of the  matrix  B, and  z,, represents  the  ith  element  of z,. 
Thus,  in  the error-correction  form,  changes  in  each  variable  are  regressed  on  a 
constant,  (p  —  1) lags of the  variable’s  own  changes,  (p —  1) lags of changes  in 
each  of the  other  variables,  and  the  levels  of each  of the h elements  of z,_,. 

For example,  recall  from  [19.1.9]  that  the system  of [19.1.1]  and  [19.1.2]  can 

be written  in the  form 

) 

pa .  é 7 +  * 7 “] 

Ay», 

0 

OfLY2e-1 

ur, 

Note  that  this  is a special  case  of [19.1.39]  with  p  =  1, 

| 

bb  G 

ole  oh 

E1, =  YUuy  +  Uy,  Ey  =  U>,,  and  all other  parameters  in [19.1.39]  equal  to  zero. 
580  Chapter 19  | Cointegration 

The  error-correction  form  is 

~~ Ss a |: 9 

Ay, 

0 

t-1 

Ex, 
E>,  ’ 

where  z,  =  y,,  —  YY. 

An  economic  interpretation  of  an  error-correction  representation  was  pro- 
posed  by  Davidson,  Hendry,  Srba,  and  Yeo  (1978),  who  examined  a relation 
between  the  log of consumption  spending  (denoted  c,) and  the  log of  income  (y,) 
of th€ form 

i -  L*)c,  =  B,(1  -  L*)y,  +  B(1  —  L*)y-  +  B3(C,-4  —  Yi-4)  tu, 

using  only  its  own  lags  or  those  of  seasonally  differenced 

[19.1.43] 
This  equation  was  fitted  to quarterly  data,  so  that  (1 —  L*)c, denotes  the  percentage 
change  in consumption  over  its  value  in  the  comparable  quarter  of the  preceding 
year.  The  authors  argued  that  seasonal  differences  (1  —  L*)  provided  a better 
description  of  the  data  than  would  simple  quarterly  differences  (1  —  L).  Their 
claim  was  that  seasonally  differenced  consumption  (1  —  L*)c,  could  not  be  de- 
in- 
scribed 
come.  In  addition  to  these  factors,  [19.1.43]  includes  the  ‘‘error-correction”  term 
B3(c,_4  —  y,—4).  One  could  argue  that  there  is a long  run,  historical  average  ratio 
of consumption  to  income,  in which  case  the  difference  between  the  logs  of con- 
sumption  and  income,  c,  —  y,, would  be a stationary  random  variable,  even  though 
log consumption  or  log income  viewed  by itself  exhibits  a  unit  root.  For  B; <  0, 
equation  [19.1.43]  asserts  that  if consumption  had  previously  been  a  larger-than- 
normal  share  of income  (so that c,_4  —  y,_4  is larger than  normal),  then  that  causes 
c, to  be  lower  for  any  given  values  of the  other  explanatory  variables.  The  term 
(c,_4  —  Y,;-4)  is viewed  as  the  “‘error’’  from  the  long-run  equilibrium  relation,  and 
. 
B; gives  the  “‘correction”  to  c, caused  by this  error. 

Restrictions  on  the  Constant  Term 
in the  VAR  Representation 

Notice  that  all  the  variables  appearing  in the  error-correction  representation 
[19.1.42]  are  stationary.  Taking  expectations  of both sides  of that  equation  results 
in 

(I, ve S z  §) Siene  Sh  Lui  ¢,-1)8  =  >  Bhi, 
where  6  =  E(Ay,)  and  pt =  E(z,).  Assuming  that  the  roots  of 

[19.1.44] 

pg 

at 

ket,  ~ 

ee  tai 

are all outside  the unit circle, the matrix  (I, —€;  —-G.-—-"  °°  -  C,,—1) is nonsingular. 
Thus,  in order to represent  a system  in which  there  is no  drift  in any of the variables 
(8 =  0), we  would  have  to impose  the  restriction 

[19.1.45] 
a  =  BpY. 
In the  absence  of any  restriction  on  a,  the  system of [19.1.42]  implies  that  there 
are  g separate  time trends  that  account  for  the  trend  in y,. 

Granger Representation  Theorem 

| 

For  convenience,  some  of the preceding  results  are  now  summarized  in the 
form of a proposition. 

ars 

| 

e

m

c

19.1.  Introduction 

581 

 
(Granger  representation  theorem).  Consider  an  (n x 1) vector 
Proposition  19.1; 
y,  where  Ay,  satisfies  (19.1.29]  for  €,  white  noise  with  positive  definite  variance- 
covariance  matrix  and {s:W ,}*9  absolutely  summable.  Suppose  that  there  are  exactly 
h  cointegrating  relations  among  the  elements  of y,.  Then  there  exists  an  (h  X  n) 
matrix  A'  whose  rows  are  linearly  independent  such  that  the  (h X  1) vector  z, defined 
by 

z,=A’y, 

is stationary.  The  matrix  A'  has  the property  that 
A’W(1)  =  0. 

If,  moreover,  the  process  can  be  represented  as  the  pth-order  VAR  in  levels  as  in 
equation  [19.1.26],  then  there  exists  an  (n  x  h) matrix  B such  that 

@(1)  =  BA’, 

and  there further  exist  (n  X  n) matrices  €,, €:,...  ,  €)-1 Such  that 

Ay,  =  C,Ay,_;  +  GAy,-2  +  °° 

+  Cp -1AYr-p4+1  +  a  —  Bz,_,  +  &. 

19.2.  Testing  the  Null  Hypothesis 
of No  Cointegration 
This  section  discusses  tests  for cointegration.  The  approach  will  be  to  test  the  null 
‘’  hypothesis  that  there  is  no  cointegration  among  the  elements  of  an  (n  x  1) 

vector  y,; rejection  of the  null  is then  taken  as  evidence  of cointegration. 

Testing for Cointegration  When 
the  Cointegrating  Vector  Is Known 

Often  when  theoretical  considerations  suggest  that  certain  variables  will  be 
_  cointegrated,  or  that  a’y, is stationary  for some  (n x  1) cointegrating  vector  a, the 
theory  is based  on  a particular  known  value  for a.  In the  purchasing  power  parity 
example  [19.1.6], 
a  =  (1,  —1,  —1)’.  The  Davidson,  Hendry,  Srba,  and  Yeo 
hypothesis  (1978)  that  consumption  is  a  stable  fraction  of  income  implies  a  co- 
integrating  vector  of  a  =  (1,  —1)',  as  did  Kremers’s  assertion  (1989)  that  govern- 
ment  debt  is a stable  multiple  of GNP. 

If the  interest  in cointegration  is motivated  by the  possibility  of a particular 
known  cointegrating  vector  a,  then  by far  the  best: method  is to  use  this  value 
directly  to construct  a test  for cointegration.  To implement  this  approach,  we  first 
test  whether  each of the  elements  of y, is individually  /(1). This  can  be done  using 
any  of the  tests  discussed  in  Chapter  17.  Assuming  that  the  null  hypothesis  of a 
unit  root  in each  series  individually  is accepted,  we  next  construct  the  scalar z,  = 
a’y,.  Notice  that  if a is truly  a  cointegrating  vector,  then  a’y, will  be  J(0).  If a is 
not  a cointegrating  vector,  then  a’y, will  be /(1). Thus,  a test of the null  hypothesis 
that z, is 1(1) is equivalent  to a test  of the null hypothesis  that y, is not cointegrated. 
If the  null  hypothesis  that  z, is [(1)  is rejected,  we  would  conclude  that  z,  =  a'y, 
is stationary,  or  that  y, is cointegrated  with  cointegrating  vector  a. The  null  hy- 
pothesis  that  z, is /(1)  can  also  be  tested  using  any  of the  approaches  in Chap- 
+, 
ter  17. 

| 

| 

For  example,  Figure  19.2  plots monthly data  from  1973:1  to  1989:10  for the - 

consumer  price  indexes  for  the  United  States  (p,) and  Italy (p*),  along with  the 

582  Chapter 19  | Cointegration 

ore 

er 

73 

7S 

77 

79 

81 

83 

8S 

87 

89 

FIGURE  19.2  One  hundred  times  the  log of the  price  level  in the  United  States 
(p,),  the  dollar-lira  exchange  rate  (s,),  and  the  price  level  in Italy  (p*),  monthly, 
1973-89.  Key:  ----  p,;  —-—  §,; 

P;.- 

exchange  rate (s,),  where  s, is in  terms  of the  number  of US.  dollars needed  to 
purchase  an  Italian  lira.  Natural  logs  of the  raw  data  were  taken  and  multiplied 
by 100,  and  the  initial  value  for  1973:1  was  then  subtracted,  as  in 

PP.  =  100-[log(P,)  ss log(P1973:1))- 

The  purpose  of subtracting  the  constant  log(Pi973.;)  from  each  observation  is to 
normalize  each  series  to  be  zero  for 1973:1  so  that  the  graph  is easier  to  read. 
Multiplying  the log by 100 means  that p, is approximately  the percentage  difference 
between  P, and its  starting  value  P,973.,.  The  graph  shows  that  Italy  experienced 
about  twice  the  average  inflation  rate  of the  United  States  over  this  period  and 
that  the  lira dropped  in value  relative  to  the  dollar  (that  is, s, fell)  by roughly  this 
same  proportion. 

| 

Figure  19.3  plots  the real  exchange  rate, 

: 

oo a 
‘It appears  that  the trends  are  eliminated  by this transformation,  though  deviations 
of the  real  exchange  rate  from  its historical  mean  can  persist  for several  years. 
To test for cointegration,  we first verify that p,, p?, and s, are  each  individually 
(1). Certainly,  we  anticipate  the  average  inflation  rate  to  be  positive  (E(Ap,)  > 
0), so that the  natural  null hypothesis  is that p, is  a unit  root  process  with  positive 
drift,  while the alternative  is that p, is stationary  around  a deterministic  time trend. 
nonthly  data it is a good idea to include  at least twelve  lags in the regression. 
following  model was estimated  by OLS for the U.S.  data  for  ¢ =  1974:2 
583 
19.2.  Testing the Null Hypothesis of No Cointegration 

73 

7S 

77 

79 

8! 

83 

6S 

87 

8s 

FIGURE  19.3  The  real  dollar-lira  exchange  rate,  monthly,  1973-89. 

through  1989:10  (standard  errors  in parentheses): 

=  0.55  Ap,_,; 

(0.08)  Pr-1 

—  0.06  Ap,_, 

(0.09)  P+  -2 

+  0.07  Ap,_, 

(0.08)  Pr-3 

+  0.06  Ap,_ 

(0.08)  Pr-4 

P; 

—  0.08  Ap,_;  —  0.05  Ap,_,  +  0.17  Ap,_,  —  0.07  Ap,_. 

(0.08) 

(0.07) 

(0.07) 

(0.07) 

{19.2.1] 

+  0.24  Ap,_g  —  0.11  Ap,_19  +  0.12  Ap,_3,  +  0.05  Ap,_ 12 

(0.07) 

(0.07) 
+  0.14  +  0.99400  p,_,  +  0.0029  ¢. 

(0.07) 

(0.07) 

(0.09) 

(0.00307) 

(0.0018) 

The ¢ statistic  for testing  the null  hypothesis  that p (the coefficient  on p,_,)  is unity 
is 

t =  (0.99400  —  1.0)/(0.00307)  =  —1.95. 
Comparing  this  with  the 5%  critical  value  from  the case  4 section of Table  B.6 for 
a sample  of size  T =  189,  we  see  that  —  1.95  >  —3.44.  Thus,  the  null  hypothesis 
of a  unit  root  is accepted.  The F test  of the  joint  null  hypothesis  that  p  =  1 and 
5 =  0 (for p the coefficient  on p,_,  and 6 the coefficient  on  the time  trend)  is 2.41. 
Comparing  this  with  the critical  value  of 6.40 from  the case  4 section  of Table  B.7, 
the  null  hypothesis  is again  accepted,  further  confirming  the  impression  that  U.S. 
prices  follow  a unit  root  process  with  drift. 

If p, in [19.2.1]  is replaced  by p?, the augmented  Dickey-Fuller  ¢ and F tests 
are  calculated  to be  —0.13  and  4.25,  respectively,  so  that  the null  hypothesis  that 
the Italian  price level follows  an /(1) process  is again accepted,  When p, in [19.2.1] 
is replaced  by s,, the  t and  F tests  are  —  1.58 and  1.49,  so  that  the exchange  rate 
likewise admits  an ARJMA(12,  1, 0) representation.  Thus, each of the three  series 
individually  could  reasonably  be described  as a unit  root  process  with drift. 
584  Chapter 19  | Cointegration 

The  next  step  is to  test  whether  z, = Pp,  =  8,  ~  pps  stationary.  According 
to  the  theory,  there  should  not  be  any  trend in  z,,  and  none appears  evident  in 
Figure  19.3.  Thus,  the  augmented  Dickey-Fuller  test  without  trend  might  be used. 
The following  estimates  were  obtained  by OLS: 

z,  =  0.32  Az,_,  =  0.01 Az,»  +  0.01 Az,-y  +  0.02 Az,-4 

(0.07) 
+  0.08 Az,_,;  -  NE, 6+  a Az,.7  +  0108 As, 8 

(0.08) 

(0.08) 

(0.08) 

[19.2.2] 

-  O05 A, 9  +  ore 62. 10  +  0.05  Az,_1,  — 0.01 Az,-12 

(0.08) 

>  ae +  poms Pee 

(0.18) 

(0.01410) 

Here the augmented  Dickey-Fuller r test  is 

t =  (0.97124  —  1.0)/(0.01410)  =  ~2.04. 

Comparing  this  with  the  5%  critical  value  for  case  2 of Table  B.6,  we  see  that 
—2.04  > —2.88, and so the null hypothesis of a unit root is accepted.  The  F test 
_  of the joint  null hypothesis that p  =  1 and  that the constant term  is zero is 2.19 
<  4.66,  which is  again  accepted.  Thus,  we  could  accept  the null hypothesis that 
the series  are  not  cointegrated. 

Alternatively,  the null hypothesis that Zr is nonstationary could be tested using 
| 

the Fiiepe-Perron t tests. OLS estimation  gives 
_  BBHIDITISG & 12933 
107 

mes = 0.030 de 0.98654  ee th ay 

soiis1gsIaI09 
sig miI22  faisins « id Gsnaoe 

;  oi  » SZ)  5,  ; 

2s} 

—  a 

| 

x 

2 

= 
in fi i  case, te pane =f  quit. wh 
4 =T1>  ai,  | 

= (T- 2)? 

es ¥ ie'y?  a  =  Si ag Ut 

ete  it 

Re 

Clearly,  the  comments  about  the  observational  equivalence  of J(0)  and  /(1) 
processes  are  also  applicable  to  testing  for cointegration.  There  exist  both  /(0) and 
1(1)  representations  that  are  perfectly  capable  of describing  the  observed  data  for 
z, plotted  in  Figure  19.3.  Another  way  of describing  the  results  is to  calculate  jhow 
long  a  deviation  from  purchasing  power  parity  is likely  to  persist.  The  regression 
of [19.2.2]  implies  an  autoregression  in levels  of the  form 

Z,  =  a  +  GyZ,_1  +  Goz%_2  +  °°  *  +  Oi32%-13  +  E, 

for  which  the  impulse-response  function, 

_  O24 4; 

=  de,’ 

can  be calculated  using  the  methods  described  in Chapter  1. Figure  19.4  plots  the 
estimated  impulse-response  coefficients  as a function  of j. An unanticipated  increase 
in z, would  cause  us  to  revise  upward  our  forecast  of z,,; by 25%  even  3 years  into 
the  future  (W,,  =  0.27).  Hence,  any  forces  that  restore  z, to  its  historical  value 
‘must  operate  relatively  slowly.  The  same  conclusion  might have  been  gleaned  from 
Figure  19.3  directly, in  that  it is clear that  deviations  of z, from  its historical  norm 
can  persist  for  a number  of years. 

Estimating  the  Cointegrating  Vector 

If the  theoretical  model  of the  system  dynamics  does  not  suggest  a particular 
value  for the  cointegrating  vector  a, then  one  approach  to testing  for cointegration 
is first  to estimate  a by OLS.  To see  why this produces  a reasonable  initial  estimate, 

0.8 

06 

0.4 

0.2 

0.0 

OS  FE  2 18  (84)  GOT IVEES)  WET  aE  Cente 

Ge 

FIGURE  19.4  Impulse-response  function  for the  real  dollar-lira  exchange  rate. 
Graph  shows  ; =  A(pi4; —  5,4; - Pis;)/€,  as  a function  of j.. 
586  Chapter 19  | Cointegration 

note  that  if z,  =  a’y,  is stationary  and  ergodic  for  second  moments,  then 

T 

T 

TS eh =  TD  (ay)?  > Elz?) 
By contrast,  if a  is not  a  cointegrating  vector,  then  z,  =  a’y,  is /(1),  and  so,  from 
result  (h) of Proposition  17.3, 

[19.2.3] 

t=1 

i= 

| 

Tr 

1 

i  x (a’y,)?  > wf (W(r)}?  dr, 

[19.2.4] 

where  W(r)  is standard  Brownian  motion  and  A is a  parameter  determined  by the 
autocovariances  of (1 ~  L)z,.  Hence,  if a is not  a cointegrating  vector,  the  statistic 
in [19.2.3]  would  diverge  to  +. 

This  suggests  that  we  can  obtain  a consistent  estimate  of a cointegrating  vector 
by choosing  a  so  as  to  minimize  [19.2.3]  subject  to  some  normalization  condition 
on  a.  Indeed,  such  an  estimator  turns  out  to  be superconsistent,  converging  at  rate 
T rather  than  7?”. 

If it is known  for certain  that the cointegrating  vector  has a nonzero  coefficient 
for  the  first  element  of y,  (a,  #0),  then  a  particularly  convenient  normalization 
is to  set  a,  =  1 and  represent  subsequent  entries  of a  (a),  a3,  ...  ,  a,)  as  the 
negatives  of a  set  of unknown  parameters  (72,  Y3,  ---  5  Yn)! 

a, 

1 

a2 
=  99 
a4,  |} =  | —¥3 

|}. 

a, 

—  Yn 

[19.2.5] 

In this  case,  the  objective  is to  choose  (72,  73,  . 

- 

- 

»  Y,) SO  as  to  minimize 

a 

p 

i  >> (a’y,)?  =  T~’  »> (Var  —  YoYor  —  “Vase!  Ti?  tet)  Yn Yee)? 

[19.2.6] 

This  minimization  is, of course,  achieved  by an  OLS  regression  of the first  element 
of y, on  all  of the  others: 

| 

Yur  =  YoYo  +  V3¥ar  +  °°  +  Vane  F  Ute 

[19.2.7] 

Consistent  estimates  of y,  Y3,...  - 
included  in [19.2.7],  as  in 

»  Yn  are  also  obtained  when  a constant  term  is 

| 

Yin  =  @ +  Vayu  +  Ys¥un  b+  **  +  Yum  + 

[19.2.8] 

or 

Yu  =  at  Yn  +  Uy, 

where ¥ ”  (Y25 Yar+++  5  Yn) and  a  (Yo 30+  ++»  Ynt)' « 

These  points  were  first  analyzed  by Phillips  and  Durlauf  (1986)  and  Stock 

(1987)  and  are  formally  summarized  in the  following  proposition. 

Let y,, be a scalar  and y>, be a (g X  1) vector.  Letn  =g  +  1, 
Proposition  19.2: 
and suppose  that  the  (n  x  1) vector  (Y1  Y2)'  is characterized  by exactly  one 
cointegrating relation  (h =  1) that has a nonzero  coefficient on y,,.  Let the triangular 
587 

19.2.  Testing the Null Hypothesis of No Cointegration 

representation  for the  system  be 

Suppose  that 

Yr  =  at  Y'Yy  +  2 

Ay,  =  U,,. 

ae =| =  W*(L)e,, 

U2, 

[19.2.9] 

[19.2.10] 

(19.2.11] 

where  ©,  is  an  (n  X  1) i.i.d.  vector  with  mean  zero,  finite  fourth  moments,  and 
positive  definite  variance-covariance  matrix  E(e,e;)  =  PP’.  Suppose further  that  the 
sequence  of (n  X  n) matrices  {s:-W*}*_,  is absolutely  summable  and  that  the  rows 
of W*(1)  are  linearly  independent.  Let  &; and  7 be  estimates  based  on  OLS  esti- 
mation  of [19.2.9], 

, 
om e  |  _  2Yur  |  | 
Yr 

Zo,  2Y2Vn} 

L2YwVi 

4-1 

where  &  indicates  summation  over  t from 1 to  T.  Partition  ¥*(1)-P  as 

w*(1)-P  = 
oe 

Az 

Then 

P(4,—  J plans  # 
T(¥r  —  ¥) 

Az-| Wo) dr  ar-{ [iwortwont ar}-ay 

[wor ar}-ar 

[19.2.12] 

9 

| n| 

2 

[19.2.13] 

where  W(r)  is n-dimensional  standard  Brownian  motion,  the  integral  sign  denotes 
integration  over  r from  0 to  1, and 

hy =  At’-W(1) 
me  A{ (0) wor} ar +  5 ewer.) 

Note  that  the  OLS  estimate  of  the  cointegrating  vector  is consistent  even 
though  the  error  term  u, in [19.2.8]  may  be serially  correlated  and  correlated  with 
Ayz,,  Ays,,.  . 
, AY.  The  latter  correlation  would  contribute  a bias  in the  limiting 
distribution  of T(47  —  ), for then  the  random  variable  h, would  not  have  mean 
zero.  However,  the  bias  in 7; is O,(T-'). 

. 

Since  the  OLS estimates  are  consistent,  the  average  squared  sample  residual 

- 

converges  to 

| 

| 

T 

TS  a> E(u?), 

t=1 

whereas  the  sample  variance  of y,,, 

T 

‘er 2X Ou a  yi)’, 

588  Chapter  19  | Cointegration 

diverges  to  +.  Hence,  the  R? for  the  regression  of [19.2.8]  will  converge  to  unity 
as  the  sample  size  grows. 

| 

Cointegration  can be  viewed  as a structural  assumption  under  which  certain 
behavioral  relations  of interest  can  be  estimated  from  the  data  by OLS.  Consider 
the  supply-and-demand  example  in  equations  [9.1.2]  and  [9.1.1]: 

(19.2.14] 
qi  =  yw,  +  é 
(19.2.15] 
qi =  Bp,  +  ef. 
We  noted  in  equation  [9.1.6].  that  if e¢ and  ef are  i.i.d.  with  Var(e%)  finite,  then 
as  the  variance  of  e7 goes  to  infinity,  OLS  estimation  of  [19.2.14]  produces  a 
consistent  estimate  of  the  supply  elasticity  y despite  the  potential  simultaneous 
equations  bias.  This  is because  the large shifts  in the demand  curve  effectively  trace 
out  the  supply  curve  in  the  sample;  see  Figure  9.3.  More  generally,  if e? is 1(0) 
and ef is 1(1),  then  [19.2.14]  and  [19.2.15]  imply  that  (q,, p,)'  is cointegrated  with 
cointegrating  vector  (1,  —  y)’.  In  this  case  the  cointegrating  vector  can  be  consis- 
tently  estimated  by  OLS  for  essentially  the  same  reason  as  in  Figure  9.3.  The 
hypothesis  that  a certain  structural  relation  involving  /(1) variables  is characterized 
by an  /(0)  disturbance  amounts  to a structural  assumption  that  can  help  identify 
the  parameters  of the  structural  relation. 

Although  the  estimates  based  on  [19.2.8]  are  consistent,  there  often  exist 
alternative  estimates  that  are  superior.  These  will  be  discussed  in  Section  19.3. 
OLS  estimation  of  [19.2.8]  is proposed  only  as  a  quick  way  to  obtain  an  initial 
estimate  of the  cointggrating  vector. 

It  was  assumed  in  Proposition  19.2  that  Ay,,  had  mean  zero.  If,  instead, 
E(Ay>2,)  =  5, it is straightforward  to  generalize  Proposition  19.2  using  a  rotation 
of variables  as  in [18.2.43];  for  details,  see  Hansen  (1992).  As  long as  there  is no 
time  trend  in the true  cointegrating  relation  [19.2.9],  the estimate  77 based  on  OLS 
estimation  of [19.2.8]  will  be superconsistent  regardless  of whether  the  /(1) vector 
Y2, includes  a deterministic  time  trend  or  not. 

The  Role  of Normalization 

The  OLS  estimate  of the  cointegrating  vector  was  obtained  by normalizing 
the first element  of the  cointegrating  vector  a to  be  unity.  The  proposal  was  then 
to regress  the first  element  of y, on  the others.  For example,  with  n =  2, we would 
regress  y;, ON  y2,: 

| 

| 

, 

Yu  =  a+  Ya  +  Uy 
Obviously,  we  might  equally  well  have  normalized  a,  =  1 and  used  the  same 
argument  to suggest  a regression  of Yo, ON Yr, 

Ya  =  9+  Nyy  + Ve 
The  OLS estimate X is not  simply  the  inverse  of 7, meaning that these two  regres- 
sions  will  give different  estimates  of the  cointegrating  vector:  - 

| 

ee  Se 

Only in the  limiting  case  where  the  R? is  1 would  the  two  estimates  coincide. 

Thus,  choosing  which  variable to  call  y,  and  which  to  call  y, might  end  up 
_  making  a material  difference  for the estimate  of a as  well  as  for  the evidence  one 
finds  for cointegration  among  the  series.  One  approach  that  avoids  this  normali- 
19.2.  Testing the Null Hypothesis of No Cointegration  589 

zation  problem  is the  full-information  maximum  likelihood  estimate  proposed  by 
Johansen  (1988,  1991).  This  will  be  discussedin  detail  in  Chapter  20. 

What  Is the  Regression  Estimating  When  There  Is  More 
Than  One  Cointegrating  Relation? 

The  limiting  distribution  of the  OLS  estimate  in Proposition  19.2  was  derived 
under  the  assumption  that  there  is just one  cointegrating  relation  (4  =  1).  In  the 
more  general  case  with  h  >  1, OLS  estimation  of  [19.2.8]  should  still  provide  a 
consistent  estimate  of  a° cointegrating  vector  by virtue  of  the  argument  given  in 
[19.2.3]  and  [19.2.4].  But  which  cointegrating  vector  is it? 

Consider  the  general  triangular  representation  for  a  vector  with  A  cointe- 

grating  relations  given  in  [19.1.20]  and  [19.1.21]: 

yu.  =  wt  +  I’y2,  +  2? 
Ayo,  =  52  +  Uy, 

(19.2.16} 
[19.2.17] 

) 

where  the  (A x  1) vector  y,, contains  the  first  h elements  of y, and  y2, contains  the 
remaining  g elements.  Since  z* =  (z{,, Z3,,  .  ..  ,  Za,)'  iS Covariance-stationary  with 
,  B, to  be  the  population  coefficients  asso- 
mean  zero,  we  can  define  B62, B3,  . 
ciated  with  a  linear  projection  of zj, on  z3,,  Z3,,...-,  Zay: 

. 

. 

z*  = Boze,  +  Bsze  +  >>  *  +  B,z%  +  us, 

[19.2.18] 

where  u,  by construction  has  mean  zero  and  is uncorrelated  with  z3,,  z3,,..., 
Faye 

The  following  proposition,  adapted  from  Wooldridge  (1991),  shows  that  the 
sample  residual  u, resulting  from  OLS  estimation  of [19.2.8]  converges  in proba- 
bility to the population  residual  u, associated  with  the linear  projection  in [19.2.18]. 
In other  words,  among  the  set  of possible  cointegrating  relations,  OLS  estimation 
of [19.2.8]  selects  the  relation  whose  residuals  are  uncorrelated  with  any other  /(1) 
»  Ynt)- 
linear  combinations  of (y2,, y3,,  -~  - 

Proposition  19.3: 
Let  y,  =  (Yi, ¥x,)'  Satisfy  [19.2.16]  and  [19.2.17]  with  y,,  an 
(h  x  1) vector  with  h >  1, and let Bz,  Bs,  ...,  B, denote  the  linear  projection 
coefficients  in  [19.2.18].  Suppose  that 

i f 

Ur, 

=  > Wre,_,, 

s=0 

where {s-W *}*_, is absolutely  summable  and €, is ani.i.d.  (n x  1) vector  with  mean 
zero,  variance  PP’,  and finite fourth  moments.  Suppose  further  that  the  rows  of 
W*(1)-P  are  linearly  independent.  Then  the  coefficient  estimates  associated  with 
OLS  estimation  of 

Yi 

Vay  TSI  OOO  saa 

‘[19.2.19] 

converge  in probability  to 

where 

dé; > [1  -B'Jp}, 

[19.2.20] 

=  (B2, Bs, we 

he  B,)’ 

(A-1)x1 

590  Chapter  19  | Cointegration 

and 

where 

Y2,7r 

Var |  P| 
; 

Y2 

Vn. 

yy 
(x1) 

1 
=f. d | 

B 

(19.2.21] 

. 

Proposition  19.3  establishes  that  the  sample  residuals  associated  with  OLS 

estimation  of [19.2.19]  converge  in probability  to 

Yu  —  Gr  —  Y2,7¥2e  —  Vs.7Vu  —  °°  *  —  Vn 

- 
| 
Vie UPI  HP 

; 

. 

Ya 

¥3e 

Yat 

Yn+is 

met  BIT 

|  Ya+2, 

Yn 

=()  —B  biyac  ws.  —  F's} 
=(1  —Bp')}-z;, 

with  the  last  equality  following  from  [19.2.16].  But  from  [19.2.18]  these  are  the 
same  as  the  is  rec  residuals  associated  with  the  linear  projection  of  zf, on 
zd  ix  ao  Zc. 

This is  an  illustration  of a  general  probes observed  by Wooldridge  (1991). 

Consider  a regression  model  of the  form 

y=  a+  xB  +  uy. | 

; 

[19.2.22] 

If y, and  x, are  J(0),  then  a  +  x;B was  said  to  be  the  linear  projection  of y, on  x, 
and  a  constant  if the  population  residual  u,  =  y,  —  a  —  x,  has  mean  zero  and 
is uncorrelated  with  x,.  We.saw  that  in  such  a  case  OLS  estimation  of [19.2.22] 
would  typically  yield consistent  estimates  of these  linear  projection  coefficients.  In 
the  more  general  case  where  y, can  be /(0) or  J(1) and  elements of x, can  be J(0) 
or  J(1),  the  analogous  condition is  that  the  residual  u, = y,  —  a  —  x;B 
is  a zero- 
mean  stationary  process  that  is uncorrelated  with  all  /(0)  linear  combinations  of 
x,.  Then  a  +  x;B can  be viewed  as  the  /(1) generalization  of a population  linear 
projection  of y, on  a  constant  and  x,.  As  long  as  there  is some  value  for  B such 
is [(0),  such a linear projection  a  +  x;B exists,  and  OLS  estimation 
that y,  —  x/B 
of [19.2.22]  should  give a consistent  estimate  of this projection. 

Relation? 

What  Is the Regression Estimating When  There  Is No 
Cointegrating 
We have seen  that  if there is at least  one  cointegrating  relation  involving Yur 
then  OLS  estimation  of [19.2.19]  gives  a  consistent  estimate  of a cointegrating 
vector.  Let  us  now  consider  the  properties  of  OLS  estimation  when  there  is no 
cointegrating  relation.  Then  [19.2.19] is  a regression  of an  /(1) variable  on a set 
of (n —  1) I(1) variables  for which  no  coefficients  produce an  /(0) error  term.  The 

19.2.  Testing the Null Hypothesis  of No Cointegration  591 

regression  is  therefore  subject  to  the  spurious  regression  problem  described  in 
Section  18.3.  The  coefficients  &; and  7, do  not  provide  consistent  estimates  of any 
population  parameters,  and  the  OLS  sample  residuals  4, will  be  nonstationary. 
However,  this  last  property  can  be  exploited  to  test  for  cointegration.  If there  is 
no  cointegration,  then  a  regression  of @, on  @,_,  should  yield  a  unit  coefficient.  If 
there  is cointegration,  then  a regression  of 4, on  &,_,  should  yield  a coefficient  that 
is less  than  1. 

The  proposal  is thus  to  estimate  [19.2.19]  by OLS  and  then  construct  one  of 
the  standard  unit  root  tests  on  the  estimated  residuals,  such  as  the  augmented 
Dickey-Fuller  ¢ test’or  the  Phillips  Z, or  Z, test.  Although  these  test  statistics  are 
constructed  in  the  same  way  as  when  they  are  applied  to  an  individual  series  y,, 
when  the  tests  are  applied  to  the  residuals  4, from  a spurious  regression,  the  critical 
values  that  are  used  to  interpret  the test  statistics  are  different  from  those  employed 
in  Chapter  17. 

| 

Specifically,  let  y, be  an  (n  X  1) vector  partitioned  as 

ee 

(nx  1) 

Vis 
(1x1) 

Yu 
(x1) 

[19.2.23] 

; 

for  g =  (n  —  1). Consider  the  regression 

Vy  =  at  y'yr,  +  U,. 

[19.2.24] 

Let  u, be  the  sample  residual  associated  with  OLS  estimation  of  i. 2.24]  in  a 
sample  of size  T: 

bb, =  Dip  ae  Oy 

Vee 

dott  =  1, 2, <a 

[19.2.25] 

where 

a 

bal 3 
Yr 

| 

T 

’ 
LY>, 
2x  LYxYn} 

oa 

2Yir 
Lau 

and  where 
regressed  on  its own  lagged  value  #,_,  without  a constant  term: 

indicates  summation  over  ¢ from  1 to  T. The  residual  @, can  then  be 

B=  phy  Fre  °° foreMea,  BpriengT 

[19.2.26] 

yielding  the  estimate 

>a. ih, 
ppatta  eis 
Se, 

t=2 

[19.2.27] 

Let s} be the  OLS  estimate  of the  variance  of e, for the  regression  of [19.2.26]: 

=4i  +4)"  2 (a,  =  Prit,_,)’, 

[19.2.28] 

and  let 6, be the  standard  error  of , as  calculated  by the  usual  OLS  formula: 

GF .  = Bini: {3 a? } 

[19.2.29] 

592  Chapter 19  | Cointegration 

Finally,  let C;.r be  the  jth  Sample  autocovariance  of  the  estimated  residuals  asso- 
ciated  with  [19.2.26): 

7  *  (7 -;1)T'  2 24,-; 

T 

t=j+ 

for j =  0,1,2,...,  7  — 

2° 

[19.2.30] 

for  é, =  a,  —  pri,_,;  and  let  the  square  of X; be  given  by 

A 

AZ =  Cy7  +  2-2 (1 —  j(q  +  1)]é,r, 

q 

a 

a 

[19.2.31] 

where  q is the  number  of autocovariances  to  be  used.  Phillips’s  Z, statistic  (1987) 
can  be  calculated  just  as  in  [17.6.8]: 

Z_.r=(T  -  Upp ~  1) -  (1/2)-{(T - 1)?-63, +  5$}-{43-—  Gor}. 

[19.2.32] 
However,  the asymptotic  distribution  of this statistic  is not  the expression  in 17.6.8] 
but  instead  is a  distribution  that  will  be  described  in  Proposition  19.4. 

If the  vector  y, is not  cointegrated,  then  [19.2.24]  will  be a spurious  regression 
and  #; should  be  near  1.  On  the  other  hand,  if we  find  that  6; is well  below  1— 
that  is, if calculation  of [19.2.32]  yields  a negative  number  that  is sufficiently  large 
in absolute  value—then  the  null  hypothesis  that  [19.2.24]  is a  spurious  regression 
should  be  rejected,  and  we  would  conclude  that  the  variables  are  cointegrated. 

Similarly,  Phillips’s  Z, statistic  associated  with  the  residual  autoregression 

[19.2.26]  would  be 

Z,.7  =  (Co,r/AZ) ty —  (1/2)-{(T  —  1)-65,  +  sr}{AF  -  ConA  [192.33] 

for  t; the  usual  OLS ¢ statistic  for  testing  the  hypothesis  p  =  1: 

tr  =  (67  -  1)/6;,. 

Alternatively,  lagged  changes  in the  residuals  could  be  added  to  the  regression  of 
[19.2.26]  as  in the  augmented  Dickey-Fuller  test  with  no  constant  term: 

ft,  =  GA,  +  GAd,»  +  +++  +  O_,Ad,_ps,  +  pli,  +e, 

[19.2.34] 

Again,  this  is estimated  by OLS  for  t  =  p +  1,p  +  2,...,  7, and  the  OLSt 
test  of p =  1 is calculated  using the standard  OLS  formula  [8.1.26].  If this  ¢ statistic 
or  the  Z, statistic  in  [19.2.33]  is negative  and  sufficiently  large  in  absolute  value, 
this  again  casts  doubt  on  the  null  hypothesis  of no  cointegration. 

The following proposition,  adapted from Phillips and Ouliaris  (1990), provides 

a formal  statement  of the  asymptotic  distributions  of these  three  test  statistics. 

Proposition  19.4: 

Consider  an  (n X  1) vector  y, such  that 

dy,  =  2 W.e,-s 

for €, an i.i.d.  sequence  with  mean  zero,  variance  E(e,€;)  =  PP’,  and finite fourth 
moments,  and  where  {s‘W,,}7~0  is absolutely  summable.  Let  g =n  —  1 and  A= 
W(1)-P.  Suppose  that the  (n x  n) matrix  AA'  is nonsingular,  and let L denote  the 
Cholesky factor of (AA’)~*: 
— 
eded. 

(AA’)7)  =  LL’. 

[19.2.35] 

cigar 

19.2.  Testing  the Null Hypothesis  of No  Cointegration 

593 

Then  the following  hold: 

(a) 

The  statistic  py defined  in  (19.2.27]|  satisfies 

(T -  1)(6r  -  1) 4 i {1 -wwanwror|  |, || 

E  miwrat| pa 

[19.2.36] 

1 
2 

= 

en  (1  —h3|L  {E(Ay,)(Ay;)}L 

1 
—h, 

*  dij. 

Here,  W*(r)  denotes  n-dimensional  standard  Brownian  motion  partitioned 
as 

Wi(r) 
W*(r)  =| 
Ox)  | 
mn  | W3(r) 

(g x  1) 

h, is a scalar  and  h, a  (g X  1) vector  given  by 

—A 

i [W3(r)]'  dr 
[*]-  a 
|  hs  | W2(r) dr  { [W3(r)]-[Wi(@)]'  ar  | W3(r)-Wi(r)  dr| 

| W3(r)  dr 

where  the integral  sign  indicates integration  over  r from  0 to  1; and 

H, =  | (WHR dr  -  | [wroar  | wrortwror  ar|| | 

h 

2 

-(b)  Ifq—~as  T— & but q/T — 0, then  the statistic  Z, 7 in  [19.2.32]  satisfies 
Z,.1r-> Zn; 

[19.2.37] 
| 

where 

1 

Z,=4=4(1  —hj}-(W*(1)]-(W*(1)]’ 

Efe  saerenronf 
-  aaweoor|  | ~  5 (1 +  hgh)} 

AN 
=  H,. 

1 

aaa 

(c)  Ifq—  as  T->  ~  but q/T — 0, then  the statistic  Z,,7 in [19.2.33]  satisfies 

Z,7-> Z,'VH,  +  (1 +  hjh,) 2. 
(19.2.39] 
(d)  If, in addition  to the preceding assumptions,  Ay, follows a zero-mean  stationary 
vectar  ARMA  process and if p—> © as  T—> ~  but p/T“? — 0, then the augmented 
Dickey-Fuller  t test associated  with  [19.2.34]  has the same  limiting distribution 
Z,, as  the test statistic  Z, , described  in [19.2.37]. 

Result (a) implies  that 6r  > 1. Hence,  when  the estimated  “cointegrating” 
regression  (19.2.24] is spurious,  the estimated  residuals  from this regression  behave 
594  Chapter 19  | Cointegration 

like  4 unit root  process  in  the  sense  that  if 4, is regressed  on  @,_,,  the  estimated 
coefficient  should  tend  to  unity  as  the  sample  size  grows.  No  linear  combination 
of  y, is  Stationary,  and  so  the  residuals  from  the  spurious  regression  cannot  be 
stationary. 

.  Note  that  since  W}(r)  and  W(r)  are  standard  Brownian  motion,  the  distri- 
butions  of  the  terms  h,,  h,,  H,,  and  Z,  in  Proposition  19.4  depend  only  on  the 
number  of stochastic  explanatory  variables  included  in the  cointegrating  regression 
(m  —  1) and  on  whether  a constant  term  appears  in  that  regression  but  are  not 
affected  by the  variances,  correlations,  and  dynamics  of  Ay,. 

In  the  special  case  when  Ay, is i.i.d.,  then  W(L)  =  I, and  the  matrix  AA’  = 
E{(Ay,)(Ay/)].  Since  LL’  =  (AA‘)~!,  it follows  that  (AA’)  =  (L’)~  '(L)~!.  Hence, 
for  this  special  case, 

L'{E[(Ay,)(Ay;)DL  =  L'(AA)L  = L'{(L’)-(L)-)L  =  1. 

[19.2.40] 

If [19.2.40]  is substituted  into  [19.2.36],  the  result  is that  when  Ay, is i.i.d., 

(T -  Ir  -  1) > Z, 

for  Z,, defined  in [19.2.38]. 

In  the  more  general  case  when  Ay, is serially  correlated,  the  limiting  distri- 
bution  of  7(6;  —  1) depends  on  the-nature  of this  correlation  as  captured  by the 
elements  of  L.  However,  the  corrections  for autocorrelation  implicit  in  Phillips’s 
Z, and  Z, statistics  or  the  augmented  Dickey-Fuller  ¢ test  turn  out  to  generate 
variables  whose  distributions  do  not  depend  on  any  nuisance  parameters. 

Although  the  distributions  of Z,, Z,, and  the  augmented  Dickey-Fuller ¢ test 
do  not  depend  on  nuisance  parameters,  the  distributions  when  these  statistics  are 
calculated  from  the  residuals  @, are  not  the same  as  the  distributions  these  statistics 
would  have  if calculated  from  the  raw  data  y,.  Moreover,  different  values  for  n  —  1 
(the  number  of stachastic  explanatory  variables  in the  cointegrating  regression  of 
{19.2.24])  imply  different  characterizations  of the  limiting  statistics  h,, h., H,,, and 
Z,,  Meaning  that  a  different  critical  value  must  be  used  to  interpret  Z, for  each 
value  of n  —  1. Similarly,  the asymptotic  distributions  of h,, H,,, and Z,, are  different 
depending  on  whether a constant  term  is included  in the  cointegrating  regression 
{19.2.24]. 

The  section  labeled  Case  1 in Table  B.8  refers  to  the  case  when  the  cointe- 

grating  regressian  is estimated  without  a  constant  term: 

. 

Vie  =  Yo¥u  TF  Ya¥ur  *  °°  °F  Vem  +  Uy,. 
[19.2.41] 
The  table  reports  Monte  Carlo  estimates  of the  critical  values  for the  test  statistic 
Z, described  in [19.2.32],  for a, the date ¢ residual  from  OLS estimation  of [19.2.41]. 
The  values  were  calculated  by generating  a  sample  of size  T  =  500  for  y,,,  yx, 
,  Yn,  independent  Gaussian  random  walks,  estimating  [19.2.41]  and  [19.2.26] 
by OLS,  and  tabulating  the  distribution  of (T —  1)(67  —  1). For  example,  the 
table  indicates  that  if we  were  to regress  a random  walk y,, on  three  other  random 
walks  (y,, y3,,  and  y4,),  then  in 95%  of the  samples,  (T —  1)(67  —  1) would  be 
greater  than  —27.9,  that  is,  6 should  exceed  0.94  in a sample  of size  T =  500. If 
might be taken  as  evidence  that  the  series 
the estimate  f; is below  0.94,  then  this 
are cointegrated. 

The section  labeled Case 2 in Table  B.8  gives critical  values  for  Z,; when  a 

constant  term  is included  in the  cointegrating  regression: 

(19.2.42] 
Vu =  &  +  Yar  +  VY  to  +  WnYne  +  Ue 
For  this  case, [19.2.26]  is  estimated  with  2, now  interpreted  as  the  residual  from 
595_ 

19.2.  Testing the Null Hypothesis  of No Cointegration 

OLS  estimation  of [19.2.42].  Note  that  the  different  cases  (1 and  2) refer  to  whether 
a  constant  term  is  included  in  the  cointegrating  regression  [19.2.42]  and  not  to 
whether  a  constant  term  is included  in  the  residual  regression  [19.2.26].  In  each 
case,  the  autoregression  for  the  residuals  is estimated  in the  form  of [19.2.26]  with 
no  constant  term. 

Critical  values  for  the  Z, statistic  or  the  augmented  Dickey-Fuller  ¢ statistic 
are  reported  in Table  B.9.  Again,  if no  constant  term  is included  in the cointegrating 
regression  as  in [19.2.41],  the  case  1 entries  are  appropriate,  whereas  if a  constant 
term  is included  in  the  cointegrating  regression  as  in  [19.2.42],  the  case  2 entries 
should  be  used.  If  the  value  for  the  Z, or  augmented  Dickey-Fuller  ¢ statistic  is 
negative  and  large  in  absolute  value,  this  is evidence  against  the  null  hypothesis 
that  y, is not  cointegrated. 

When  the corrections  for serial  correlation  implicit  in the Z,, Z,, or  augmented 
.  Dickey-Fuller  test  are  used,  the  justification  for  using  the  critical  values  in Table 
B.8  or  B.9  is  asymptotic,  and  accordingly  these  tables  describe  only  the  large- 
sample  distribution.  Small-sample  critical  values  tabulated  by Engle and  Yoo  (1987) 
and  Haug  (1992)  can  differ  somewhat  from  the  large-sample  critical  values. 

Testing for Cointegration  Among  Trending  Series 

It was  assumed  in  Proposition  19.4  that  E(Ay,)  =  0, in  which  case  none  of 
the  series  would  exhibit  nonzero  drift.  Bruce  Hansen  (1992)  described  how  the 
results  change  if instead  E(Ay,)  contains  one  or  more  nonzero  elements. 

Consider  first  the  case  n  =  2, a  regression  of one  scalar  on  another: 

Yu  =  at  yr,  +  U,. 

[19.2.43] 

Suppose  that 

with  6,  #  0.  Then 

Ay2,  =  5, +  Uy, 

Yu  =  Y29  +  6,°t  +  D> Urs, 

s= 

t 

which  is  asymptotically  dominated  by  the  deterministic  time  trend  6,-t.  Thus, 
estimates  a; and 7; based  on  OLS estimation  of [19.2.43] have  the same  asymptotic 
distribution  as  the  coefficients  in a  regression  of an  J(1)  series  on  a  constant  and 
a  time  trend.  If 

Ay;,  =  6,+  uy,  | 
(where  6, may  be  zero),  then  the  OLS  estimate  7, based  on  [19.2.43]  gives  a 
consistent  estimate  of  (6,/5,),  and  the  first  difference  of the  residuals  from  that 
regression  converges  to  u,,  —  (6,/5,)u>,;  see  Exercise  19.1. 

If, in fact,  [19.2.43]  were  a simple  time  trend  regression  of the  form 

Vir  =art 

+  Uu,, 

then  an  augmented  Dickey-Fuller  test  o  the residuals, 

@, =  (Ad)  +  gAd_.  +  +++  +  6, ,Ad pit  pth,  +e,  [19.2.4] 
would be  asymptotically  equivalent  to  an  augmented  Dickey-Fuller  test  on  the 
original  series  y,,  that  included  a constant  term  and a time  trend: 

a  CAYiy-1  +  GAY 14-2  wher  $p-14)i  2-41 

+  a+  py,,-,  +  t+  u,. 

begs 

596  Chapter 19  | Cointegration 

Since  the residuals  from  OLS  estimation  of [19.2.43]  behave  like  the  residuals  from 
a  regression  of [y,,  —  (5,/5,)y>,]  on  a  time  trend,  Hansen  (1992)  showed  that  when 
Ya  has  a nonzero  trend,  the  ¢ test  of p  =  1 in  [19.2.44]  for  a, the  residual  from 
OLS  estimation  of  [19.2.43]  has  the  same  asymptotic  distribution  as  the  usual 
augmented  Dickey-Fuller  ¢ test  for  a  regression  of  the  form  of  [19.2.45]  with  y,, 
replaced  by [yu  —  (8,/8,)y2,}.  Thus,  if the  cointegrating  regression  involves  a single 
variable  y2,  with  nonzero  drift,  we  estimate  the  regression  [19.2.43]  and  calculate 
the  Z, or  augmented  Dickey-Fuller ¢ statistic  in exactly  the  same  manner  that  was 
specified  in equation  [19.2.33]  or  [19.2.34].  However,  rather  than  compare  these 
Statistics  with  the  (n —  1) =  1 entry  for case  2 from  Table  B.9,  we  instead  compare 
these  statistics  with  the  case  4 section  of Table  B.6. 

For  convenience,  the  values  for  a  sample  of size  T  =  500  for  the  univariate 
case  4 section  of Table  B.6  are  reproduced  in the  (n  —  1)  =  1 row  of the  section 
labeled  Case  3 in Table  B.9.  This  is described  as  case  3 in  the  multivariate  tabu- 
lations  for  the  following  reason.  In  the  univariate  analysis,  ‘‘case  3”  referred  to  a 
regression  in  which  the  single  variable  y,  had  a  nonzero  trend  but  no  trend  term 
was  included  in  the  regression.  The  multivariate  generalization  obtains  when  the 
explanatory  variable  y,, has  a nonzero  trend  but  no  trend  is included  in the  regres- 
sion  [19.2.43].  The  asymptotic  distribution  that  describes  the  residuals  from  that 
regression  is the same  as  that for a univariate  regression  in which a trend  is included. 
Similarly,  if yz,  has  a  nonzero  trend,  we  can  estimate  [19.2.43]  by OLS  and 
construct  Phillips’s  Z, statistic  exactly  as  in  equation  [19.2.32]  and  compare  this 
with  the  values  tabulated  in  the  case  4 portion  of Table  B.5.  These  numbers  are 
reproduced  in row  (nm —  1) =  1 of the  case  3 section  of Table  B.8. 

More  generally,  consider  a regression  involving  n  —  1 stochastic  explanatory 

variables  of the  form  of [19.2.42].  Let  5; denote  the  trend  in the ith  variable: 

E(Ayin)  =  §;. 

Suppose  that  at  least  one  of the  explanatory  variables  has  a  nonzero  trend  com- 
ponent;  for  illustration,  call  this  the  nth  variable: 

: 

5,  #0. 
Whether  or  not  other  explanatory  variables  or  the  dependent  variable  also  have 
nonzero  trends  turns  out  not  to  matter  for  the  asymptotic  distribution;  that  is, the 
values  of 5,, 5,,...  ,  6,_,  are  irrelevant  given  that  5,  #  0.- 

Note  that  the  fitted  values  of [19.2.42]  are  identical  to  the  fitted  values  from 

OLS  estimation  of 

Yh =  a"  + YEyn  +  VEE  Ho  + nana  +  VAY ee F  Mey 

[19.2.46] 

where 

| 

Yi  = Yu  —  (8/8,)¥u 

| 
for’  =  1,2,...,0  -  1. 

As in the  analysis  of [18.2.44],  moments  involving  y,,  are  dominated  by the  time 
trend  5,t, while  the y# are  driftless  /(1) variables  for  i =  1,2,...,”  —  1. Thus, 
the residuals  from  [19.2.46]  have  the  same  asymptotic  properties  as  the  residuals 
from  OLS  estimation  of 

sie 
yt =  at  +  ytyh  +  vive  to  +  Marea  +  We Oat +  uy. 

[19.2.47] 
The appropriate critical values for statistics constructed  when @, denotes the residual 
from  OLS  estimation  of [19.2.42]  can  therefore  be calculated  from  those  for an 
OLS regression  of an  (1) variable  on a constant,  (n —  2) other  J(1) variables,  and 
a time trend.  The appropriate  critical  values are tabulated  under  the heading Case 
3 in Tables  B.8 and B.9. 

| 

— 

f

19.2.  Testing the Null  Hypothesis  of No  Cointegration  597 

—- 

ob 

 
Of  course,  we  could  instead  imagine  including  a  time  trend  directly  in  the 

regression,  as  in 

Yu  =  AF  Wat  Va¥ar  t+  aN  nat  Ob 

eis 

[19.2.48] 

Since  [19.2.48]  is in  the  same  form  as  the  regression  of [19.2.47],  critical  values  for 
such  a  regression  could  be  found  by treating  this  as  if it were  a regression  involving 
(n  +  1) variables  and  looking  in  the  case  3 section  of Table  B.8  or  B.9  for  the 
critical  values  that  would  be  appropriate  if we  actually  had  (n +  1) rather  than  n 
total  variables.  Clearly,  the  specification  in [19.2.42]  has  more  power to  reject  a 
false  null  hypothesis  than  [19.2.48],  since  we  would  use  the  same  table  of critical 
values  for  [19.2.42]  or  [19.2.48]  with  one  more  degree  of  freedom  used  up  by 
[19.2.48].  Conceivably,  we  might  still  want  to  estimate  the  regression  in  the  form 
of [19.2.48]  to  cover  the  case  when  we  are  not  sure  whether  any  of the  elements 
of y, have  a  nonzero  trend  or  not. 

Summary  of Residual-Based  Tests for Cointegration 

The  Phillips-Ouliaris-Hansen  BEORSNEE for  testing  for  cointegration  is sum- 

marized in  Table  19.1. 

To  illustrate  this  approach,  consider. again  the  purchasing  power  parity  ex- 

ample  where  p,  is the  log of  the  U.S.  price  level,  s, is the  log of  the  dollar-lira  - 
exchange  rate,  and  p> is the  log of the  Italian  price level.  We have  already  seen 
ies the  vector  a  =  (1,  —1,  —1)'  does not  appear  to  be  a cointegrating  vector  for 
=  (p,, 5,, p;)'. Let  us  now  ask whether  there is any cointegrating  relation  among. 

se variables, 

The  following  regression  was  estimated  by OLS  for  t  =  1973:1  to  1989:10 

(standard  errors  in parentheses): 

Pp,  =  2.71  +  0.051  s,  +  0.5300  p?  +  i,. 

(0.37) 

(0.012) 

(0.0067) 

[19.2.49] 

The  number  of  observations  used  to  estimate  [19.2.49]  is  T =  202.  When  the 
sample  residuals  #2, are  regressed  on  their  own  lagged  values,  the  result  is 

i,  =  0.98331 u,_,  +  é, 

(0.01172) 

3  °=  (T=  2)7  ya 6?  =  (0.40374)? 

=  0.1622 

<r II 

tS  Ty  steed 

if 

t=j+2 

12 

A?  = 6 +  2d [1 —  (j/13)]é,;  =  0.4082. 
Fash 

| 

The 2  seapenia  Zz, test  is 

; 
| 
=  (T -  1)(6  - 1) -  (12(T -  1)-6,  +  sh? -  6G) 
=  (201)(0.98331  —  1) 

~  i{(201)(0. 01172)  +  (0. 40374)}(0. 4082 — got 1622) 

=  -—7,54. 

Given  the  evidence  of nonzero  drift  in the  explanatory  variables,  this  is to  be / 
compared  with  the  case  3 section  of Table  B.8.  For  (n —  1) =  2, the 5% critical’ 
598  Chapter  19  | Cointegration 

TABLE  19.1 
Summary  of Phillips-Ouliaris-Hansen  Tests  for  Cointegration 
Case  1: 

Estimated  cointegrating  regression: 

True  process  for  y,  =  (y,,,-¥2,, 

carn  Wvee  Mave  +  6°?  +  FLV  +  es, 
- 

,  Yu)": 

.- 

Ay,  ”  2 We 

4 has  the  same  asymptotic  distribution  as  the  variable  described  under  the 

heading  Case 1 in Table  B.8. 

Z, and  the  augmented  Dickey-Fuller  ¢ test  have  the  same  asymptotic  distri- 

bution  as  the  variable  described  under  Case  1 in  Table  B.9. 

Case  2: 

Estimated  cointegrating  regression: 

Vie  1.  T  YoVan  Taha  Fo  FT  Veen  FU, 

True  process  for  y,  =  (yi5 Yas»  - 

+ 

>  Yat)? 

Ay,  =  > W,e,_, 

Z, has  the  same  asymptotic  distribution  as  the  variable  described  under  Case 

2 in Table  B.8. 

Z, and  the  augmented  Dickey-Fuller  ¢ test  have  the  same  asymptotic  distri- 

bution  as  the  variable  described  under  Case  2 in Table  B.9. 

Case  3: 

Estimated  cointegrating  regression: 

Vir  =  @  +  Yaa  +  V2¥a  F  °° 

+  Wane  TF  Ue 

True  process  108  ¥,  —  “Oia Yan:  rs a 

Ay,  =  65+  2, ¥,£,_, 

with  at  least  one  element  of 5,, 5;,.  . 

. 

,  5, nonzero. 

Z, has the  same  asymptotic  distribution  as  the variable  described  under  Case 

3 in Table  B.8. 

; 

| 

Z, and  the  augmented  Dickey-Fuller  ¢ test  have  the  same  asymptotic  distri- 

. bution  as  the  variable  described  under  Case  3 in Table  B.9. 

Notes  ta  Table  19.1 

Estimated  cointegrating  regression  indicates  the  form  in which  the  regression  that  could  describe 

the  cointegrating  relation  is estimated,  using observations  ¢  =  1, ee  Rites  Ee 

True  process  describes  the  null  hypothesis  under  which  the  distribution  is calculated.  In each 
case,  €, is assumed  to  be i.i.d.  with  mean  zero,  positive  definite  variance-covariance  matrix,  and  finite 
fourth  moments,  and  the  sequence  {s-W,};~o  is absolutely  summable.  The  matrix  W(1)  is assumed  to 
be  nonsingular,  meaning  that  the  vector  y,  is not  cointegrated  under  the  null  hypothesis.  If the  test 
statistic  is below  the  indicated critical  value  (that  is, if Z,, Z,, or  ¢ is negative  and  sufficiently  large  in 
absolute  value),  then  the  null  hypothesis  of no  cointegration  is rejected. 

Z, is the following  statistic, 

a 

eae 
" 

Z, =  (T -  1)(6r  -  1) -  (1/2){(T  —  1)?-63,  +  stHA}  —  €0.7), 
where f  is the estimate of p based on  OLS estimation  of @, =  pti,  + &, for A, the OLS sample  residual 

19.2.  Testing the Null Hypothesis of No  Cointegration 

599 

alias 

value  for Z, is  —  27.1.  Since  —7.54  >  —27.1,  the null hypothesis  of no  cointegration 
is accepted.  Similarly,  the  Phillips-Ouliaris  Z, statistic  is 

Z,  =  (€p/A?)!7(6  —  1/6,  —  (1/2){(T  -  1)°6%  +  s}A?  -  Cy)/A 

{(0.1622)/(0.4082)}?2(0.98331  —  1)/(0.01172) 
—  4{(201)(0.01172)  +  (0.40374)}(0.4082  —  0.1622)/(0.4082)"2 
=  2.02. 

Comparing  this  with  the  case  3 section  of Table  B.9,  we  see  that  —  2.02  >  —3.80, 
so  that  the  null  hypothesis  of  no  cointegration  is  also  accepted  by  this  test.  An 
OLS  regression  of @, on  4,_,  and  twelve  lags  of Ad,_;  produces  an  OLS ¢ test  of 
p  =  1 of  —2.73,  which  is again  above  —3.80.  We  thus  find  little  evidence  that p,, 
S,,  and  p*  are  cointegrated.  Indeed,  the  regression  [19.2.49]  displays  the  classic 
symptoms  of a spurious  regression—the  estimated  standard  errors  are  small  relative 
to  the  coefficient  estimates,  and  the  estimated  first-order  autocorrelation  of  the 
residuals is  near  unity. 

As  a  second  example,  Figure  19.5  plots  100  times  the  logs  of real  quarterly 
aggregate  personal  disposable  income  (y,) and  personal  consumption  expenditures 
(c,) for the  United  States  over  1947:I  to  1989:  III.  In a regression  of y, on  a constant, 
a time  trend,  y,_,,  and  Ay,_,;forj  =  1,2,...  ,  6, the  OLS ¢ test  that  the coefficient 
on  y,-;  is  unity  is  —  1.28.  Similarly,  in  a  regression  of  c,  on  a  constant,  a  time 
trend,  c,_,,  and  Ac,_; forj  =  1,2,...,6,  the  OLS ¢ test  that  the  coefficient  on 
C,-;  is unity  is  —  1.88.  Thus,  both  processes  might  well  be  described  as  J(1)  with 
positive  drift. 

The  OLS  estimate  of the  cointegrating  relation  is 
c,  =  0.67  +  0.9865  y,  +  u,. 

(2.35) 

(0.0032) 

A first-order  autoregression  fitted  to  the  residuals  produces 

(19.2.50] 

i, =  0.782  a,_,  +  é, 

(0.048) 

Notes  to  Table  19.1  (continued). 

from  the  estimated  regression.  Here, 

sea (T-  2)"  @, 
where  é, =  @, —  f;i,_,  is  the  sample  residual  from  the  autoregression  describing  @, and  G,_ is the 
standard  error  for 6, as  calculated  by the  usual  OLS  os 

T 

Also, 

5. =  si +  Fa. 07. ;. 

2. 

) 

ér7=(T-  I)!  2 4-1 

he 

eed 

At =  lyr  +  Dy [1 -  fq  +  1)}é,r- 

Z, is the ei statistic: 

1  ™  (€o,r/AZ)"*(br  —  1)/y, - “aayitt -  to. r(WArM(T  —  1): 64, *  $7}. 
aioe Dickey-Fuller  t statistic  is the  OLS ¢ test  of ee. null  hypothesis  that  p =  1 in the 

regr 

A, =  {,A2,.,  +  6240,  +++ 

+  er  ee +  pa,_,+  e,. 

600  Chapter  19  | Cointegration 

47 

5! 

SS 

=] 

63 

67 

71 

75 

79 

83 

386  87 

FIGURE  19.5  One  hundred  times  the  log of personal  consumption  expenditures 
(c,) and  personal  disposable  income  (y,) for  the  United  States in  billions  of 1982 
dollars,  quarterly,  1947-89.  Key: 

for which  the  corresponding  Z, and  Z, statistics  for  g =  6 are  —32.0  and  —  4.28. 
Since  there  is  again  ample  evidence  that  y,  has  positive  drift,  these  are  to  be 
compared  with  the  case  3  sections  of  Tables  B.8  and  B.9,  respectively.  Since 
—32.0  <  —21.5  and  —4.28  <  -—3.42,  in  each  case  the  null  hypothesis  of  no 
cointegration  is rejected  at the  5%  level.  Thus  consumption  and  income  appear  to 
be cointegrated. 

Other  Tests for Cointegration 

» 

+ 

The  tests  that  have  been  discussed  in this-section  are  based  on  the  residuals 
» Yne)  Since  these  are  not  the same 
from an OLS  regression  of y,, on  (yz, 3:5  - 
»  Yar)»  the  tests  can  give 
_as  the  residuals  from a regression  of y2, on  (y1;, Y3.  - 
‘different  answers  depending  on  which  variable is  labeled  y,.  Important  tests  for 
cointegration that are  invariant to the ordering  of variables  are  the full-information 
maximum  likelihood  test  of Johansen  (1988,  1991)  and  the  related  tests  of Stock 
and Watson  (1988) and Ahn  and Reinsel  (1990).  These  will be discussed in Chapter 
20. Other useful  tests  for cointegration  have  been  proposed  by Phillips and Ouliaris 
G79), Pask, Ouliaris,  and  ere (1988),  Stock  (1990),  and  Hansen  (1990). 

- 

» 

19. Sys Testing Hypotheses  About  the  Cointegrating  Vector 
The previous section described some ways to test whether a vector y, is cointegrated. 
It was noted that if y, is cointegrated, then a consistent  estimate  of the eyes 

' 

- 

19,3.  Testing Hypotheses  About the Cointegrating  Vector 

601 

vector  can  be obtained  by OLS.  This  section  explores  further  the  distribution  theory 
of this  estimate  and  proposes  several  alternative  estimates  that  simplify  hypothesis 
testing. 

Distribution  of the  OLS  Estimate  for a  Special  Case 

Let  y,,  be  a  scalar  and  y>,  be  a  (g  x  1) vector  satisfying 

Yu  =  at  y'yn  +  27 

V2  ="ys,0)  FP  y,. 

[19.3.1] 

[19.3.2] 

If y,,  and  y,,  are  both  (1)  but  z* and  u,,  are  /(0),  then,  for  n  =  (g  +  1), the  n- 
dimensional  vector  (y,,,  y3,)’  is cointegrated  with  cointegrating  relation  [19.3.1]. 

Consider  the  special  case  of  a Gaussian  system  for which  y2, follows  a random 

walk  aid  for  which  z? is white  noise  and  uncorrelated  with  u,,  for  all  ¢ and  7: 

a 
0] 
|| ~  isa  ([o)- | Ma,  | 

[ot 

oO 

oe 

[19.3.3] 

Then  [19.3.1]  describes  a  regression  m  which  the  explanatory  variables  (y>,)  are 
independent  of  the  error  term  (z})  for  all  ¢ and  7.  The  regression  thus  satisfies 
Assumption  8.2  in Chapter  8. There  it was  seen  that  conditional  on  (y>,, yo,  - 
, 
Y27),  the  OLS  estimates  have  a  Gaussian  distribution: 

. 

- 

faipssie 

ss 
(Yr  —  ¥) 

(Yai, Y22,-  ++  >  Y2r)  4 

ti 
LY2,   LY2Yr, 

* 
LY2Z; 

; 

[19.3.4] 

oS 

=i 

Dz? 

0 

Te  — Bei  | ) 

~wN 

j  ail 

(Hl  ‘LE  yo  LyzYs, 

: 

where  > indicates  summation  over  ¢ from 1 to  T. 

Recall  further  from  Chapter  8 that  this  conditional  Gaussian  distribution  is 
all  that  is needed  to justify  small-sample  application  of the  usual  OLS t or F tests. 
Consider  a  hypothesis  test  involving  m  restrictions  on  a  and  y of the  form 

R,a  +  Ry  =r, 

where  R, and  r  are  known  (m  x  1) vectors  and  R, is  a known  (m  x  g) matrix 
describing the  restrictions.  The  Wald  form  of the  OLS F test of the  null  hypothesis 
is 

wpe 

oe ed 
(R,4,7  +  R,¥,;  —  r)  {sR Rall Sol  Fal 

eee 

i 

pt 

[19.3.5] 

where 

x  (Rd,  +  Ry¥r  —  Fr) +  m, 

s% =(T-n)7!  > (Yu  —  Gr  —  PrY2,)?. 

T 

Result  [19.3.4]  implies  that  conditional  on  (y>,,  y25,  .  og »  Yor),  under  the  null 
hypothesis  the  vector  (R,@,  +  R,¥7  —  r) has a Gaussian  distribution  with  mean 
0 and  variance 

~ 

he  te  E~ 

o7(R,  R  |  a  ‘| 

t 

7 LYor  LY2Ya 

R, 

602  Chapter 19  | Cointegration 

RE  EET  OIE  REI  ES  IE  Era  ee PAT  Re, 

It follows  that  conditional  on  (y>),  Yoo,  . 

. 

. 

,  yor),  the  term 

ns 

3 

(Rady  +  Ry¥r  —  1)’ {orlR, RI] 

Tr 

i 

, 

I 

, 

1 

ZY2,  ZY2,Y>, 

‘ni | bl 

R : 
7 

[19.3.6] 

x  (R,47  +  R,¥r  -  r) 

- 

. 

- 

is  a quadratic  form  in a Gaussian  vector.  Proposition  8.1  establishes  that  conditional 
on  (Y21,  Y22,  - 
»  Yor),  the  magnitude  in [19.3.6]  has  a y2(m)  distribution.  Thus, 
conditional  on  (y2;, y22,.  . 
,  yo),  the  OLS F test  [19.3.5]  could  be  viewed  as  the 
ratio  of  a  x*(m)  variable  to  the  independent  y?(T  —  n) variable  (T  —  n)s3/o?, 
with  numerator  and  denominator  each  divided  by its  degree  of freedom.  The  OLS 
F test  thus  has  an  exact  F(m,  T  —  n) conditional  distribution.  Since  this  is  the 
same  distribution  for  all  realizations  of (y2;,  Y22,  . 
,  Y27),  it follows  that  [19.3.5] 
has  an  unconditional  F(m,  T  —  n) distribution  as  well.  Hence,  despite  the  /(1) 
regressors  and complications  of cointegration,  the correct  approach  for this example 
would  be  to  estimate  [19.3.1]  by OLS  and  use  standard f¢ or F statistics  to  test  any 
hypotheses  about  the  cointegrating  vector.  No  special  procedures  are  needed  to 
estimate  the  cointegrating  vector,  and  no  unusual  critical  values  need  be  consulted 
to  test  a  hypothesis  about  its value. 

. 

. 

We  now  seek  to  make  an  analogous  statement  in terms  of the  corresponding 
asymptotic  distributions.  To do so  it will  be helpful  to rescale  the results  in [19.3.4] 
and  [19.3.5]  so  that  they define  sequences  of statistics  with  nondegenerate  asymp- 
totic  distributions.  If [19.3.4]  is premultiplied  by the  matrix 

‘id 
0 

T4,]’ 

the implication  is that  the  distribution  of the  OLS  estimates  conditional  on  (y>, 

Y22>--->  Yor)  is given  by 

T’*(a7  —  a) 
T4r  -  ¥)  (Yas  Y22,  - 

- 

- 

»  Yr) 

- 

Of 

[fre 

0 
a ([¢ a(| 0  Ky a, kl  aid 
yo  LTA  mf 

VT). 

| 

“ (BF oily,  T~*2yn¥x)   ] 

2y  AT  0  }}) 

19.3.7 
ae 

| 

To  analyze  the asymptotic  distribution,  notice  that [19.3.1] through  [19.3.3] 
are  a special  case  of the  system  analyzed  in Proposition  19.2  with  w*(L)  =I,  and 

where  P,, is the  Cholesky  factor  of 22,,: 

' 

For this special  case, 

| 

om  =  P,P. 

* 

wor  | 4 

ies 
19.3.  Testing Hypotheses  About the Cointegrating  Vector  603 

19.3.8 

= 

. 

‘ 

The  terms  A*’  and  A}  referred  to  in  Proposition  19.2  would  then  be  given  by 

Ar’  = 
(1  xn) 

GO 
(1x1) 

0’ 
(1g) 

Aes  are  | 

(g  xn) 

(gx1) 

(8) 

Thus,  result  [19.2.13]  of  Proposition  19.2  establishes  that 
gt  i dust] 

i 

LTT?3y  To*2 yay) 

LT 'Zyxe? 

T'?(ay  —  a) 
TM¥r-y) 

J 

L 

4 

1 

| [W(r)]’  ir}| 4 

P2 

0  Pal {Woar  [0  Pall [ worwo)  ar}| &| 

r)|’ 

0’ 

dr 

. 

[o,  0’]W(1) 

fs 

Vol) 
(0  Pa f OWOIAWO)  \| 1 

[19.3.9] 

where  the integral  sign indicates  integration  over  r from  0 to  1. If the n-dimensional 
standard  Brownian  motion  W(r)  is partitioned  as 

W(r)  a 

W,(r) 
(xd) 

|. 

(n  x1) 

W,(r) 

(g x  1) 

then  [19.3.9]  can  be  written 

l(a  -  = 

T(¥r  —  Y) 

rat 

“4 

[ wwaor  ar}, 

P2. | W,(r)  dr  Pa  | [W2(r)]}-[W2(7)]'  ar} 

[19:3.10] 

x 

o,W,(1) 

P,  [ wae)  moni 

where 

hah 

o

o

”

einer re 

-1 

P,, | W.(r)  dr  P nf | (W2(r)}:[W2(7)]’ 

m1) 
:  na J wr amin} 

ares 

[19.3.11] 
% 

604  Chapter 19  | Cointegration 

 
Since  W,(-)  is independent  of W,(-),  the  distribution  of  (v,,  v3)’  conditional 
on  W,(-)  is found  by treating  W,(r)  as  a deterministic  function  of r  and  leaving  the 
process  W,(-)  unaffected.  Then  {[W,(r)]  dW,(r)  has  a simple  Gaussian  distribution, 
and  [19.3.11]  describes  a  Gaussian  vector.  In  particular,  the  exact  finite-sample 
result  for  Gaussian  disturbances  [19.3.7]  implied  that 
T'*(a7  —  a) 
T(4.. 
— 

(Yo,  Yo.  --->Yor)| 

ee 
<4 

pe 
32 

= 

ag 
_2 
' 
Ly,  T  LY a¥ 21 

T  Ly22; 

(7 T 

Y) 

T 

x  N 

0 
0 

oa? 

1 
Tr-**E95, 

T-223y!, 
. 
Ya 
-T-*29293 

|" 

Comparing  this  with  the  limiting  distribution  [19.3.10],  it appears  that  the  vector 
(%,,  v3)’  has  distribution  conditional  on  W,(-)  that  could  be  described  as 

V; 

v2 

. 

N  Bi 

[ pany  ar}, 

—=1 

[19.3.12] 

P2, | W2(r)  dr  P. a | [W.(r)]:[W2(7)]'  dr les 

Expression  [19.3.12]  allows  the  argument  that  was  used  to  motivate  the  usual 
OLS  t and F tests  on  the system  of [19.3.1]  and  [19.3.2]  with  Gaussian  disturbances 
satisfying  [19.3.3]  to give an  asymptotic  justification  for these  same  tests  in a system 
with  non-Gaussian  disturbances  whose  means  and  autocovariances  are  as  assumed 
in [19.3.3].  Consider  for illustration  a hypothesis that involves  only the cointegrating 
vector,  so  that  R,  =  0.  Then,  under  the  null  hypothesis,  m  times  the F test  in 
[19.3.5]  becomes 

m:Fr  =  (R,(¢r  -  os Rr 7  e']| Rr -  WI 

cane, -onfaw wf, 28] [eel 

x  [R,-7T(¥r  —  ¥)] 

oat 
=  (RM  zi=  mre  R.i(| 0  ie 

Ti2 

Q’ 

ioe 
x  Ae Rejo | 0  a  Bi [R, T(¥r 

35 \"'[o']) 

sy, 

fm 

tT 

y)) 

5, (R,o,v,}'(3)-4  [0  R,] 

{fwscor ares 

f 

x 

| [R,o,¥2] 

P2, | W,(r) dr  P a | [W.(r)}-[W2)]  dr }rs  ™ 

19.3.  Testing  Hypotheses  About  the  Cointegrating  Vector  605 

=  (o7/s7)[R,v2]')  [0  R,] 

1 

{f [W.(r)]’  ar}, 

0’ 

x 

| [R,v2]. 

P2, | W,(r)  dr  Pa  | [W2(r)]-[W2(7)]'  ar}, 

[19.3.13] 

Result  [19.3.12]  implies  that  conditional  on  W,{-),  the  vector  R,v,  has  a  Gaussian 
distribution  with  mean  0 and  variance 

[0  R,] 

{f owsor  ales  a 

—s 

Pa | Wa) dr  Pat | WOW  dr} Ps 

Since  s?- provides  a  consistent  estimate  of  oj,  the  limiting  distribution  of  m-F; 
conditional  on  W,(:)  is thus  y?(m),  and  so  the  unconditional  distribution  is y7(m) 
as  well.  This  means  that  OLS  ¢ or  F tests  involving  the  cointegrating  vector  have 
their  standard  asymptotic  Gaussian  or  x? distributions. 

It is also  straightforward  to  adapt  the  methods  in Section  16.3  to  show  that 
the  OLS  x? test  of  a  hypothesis  involving  just  a,  or  that  for  a  joint  hypothesis 
involving  both  a  and  y,  also  has a limiting  x? distribution. 

The  analysis  to  this  point  applies  in the  special  case  when  y,,  and  y>,  follow 
random  walks.  The  analysis  is easily  extended  to  allow  for  serial  correlation  in 
Zf OF  U,,,  as  long  as  the  critical  condition  that  z* is uncorrelated-with  u,,  for  all 
t  and  7  is  maintained.  In  particular,  suppose  that  the  dynamic  process  for 
(z*,  u;,)’  is given  by 

vag 

‘| 

=  W*(Le, 
i  De 

with  {s-W%}*_)  absolutely  summable,  E(e,)  =  0,  E(e,e!)  =  PP’  if t=  7  and  0 
otherwise,  and  fourth  moments  of €, finite.  In order  for z* to  be  uncorrelated  with 
u,,  for  all  ¢ and  7,  both  W*(L)  and  P must  be  block-diagonal: 
a) 

Ae 
vu  -| 0 

ow 
oulil 

implying  that  the  matrix  ¥*(1)-P  is also  block-diagonal: 

P=  & id 
0,  Pz 

’ 

*71).Pp 

elt 

—  a7  (1) 

0 
5 0' 
0  At, 

0" 

Habte'f 

. 

[19.3.14] 

= 

606  Chapter  19  | Cointegration 

Noting the parallel  between  [19.3.14]  and  [19.3.8],  it is easy  to  confirm  that  if 
A}  .* 0 and the rows of A, are  linearly  independent,  then  the analysis  of [19.3.10] 
continues  to hold,  with  o,  replaced  by A¥ and  P,, replaced  by Ad): 

Mir  - r~¥) 
T'*(G@y —  @)  z 

(/ ead  Z - 

At, { W,(r) ar  Ax | (W.2(r)}-[W2(7)]' ar} ay 
ATW, (1) 
x  As | wo) any  }at | 

(19.3.15] 

Conditional  on  W,(-),  this  again  describes  a  Gaussian  vector  with  mean  zero  and 
variance 

| 

{J ewacoy ar}ass 

-1 

fas  Swede  ast { waontwcr alas 

re 

wei 
m  condition 

Oeitis 

Raat)  -  Sash  EC  PER ACES 

; 

4 

a 

.  WIC  ~$—  ? 

> 

vides a COL S1 stent 

t  Stee;  1St -, eh 

of the v  _ 

- 

ee  Sa 

The  difficulties  with  nonstandard  distributions  for  hypothesis  tests  about  the 
cointegrating  vector  are  thus  due  to  the  possibility  of nonzero  correlations  between 
z*  and  u,,.  The  basic  approach  to  constructing  hypothesis  tests  will  therefore  be 
to  transform  the  regression  or  the  estimates  so  as  to  eliminate  the  effects  of  this 
correlation. 

Correcting  for Correlation  by Adding  Leads 
and  Lags  of Ay, 

One  correction  for the correlation  between  z? and u,,,  suggested  by Saikkonen 
(1991),  Phillips  and  Loretan  (1991),  Stock  and  Watson  (1993),  and  Wooldridge 
(1991),  is to  augment  [19.3.1] with  leads  and  lags of Ay,,.  Specifically,  since  z* and 
uy,  are  stationary,  we  can  define  Z, to  be the  residual  from a linear  projection  of 
Zz,  on  (Wages  Uo r—peir  >>  > 

»  Ure—1  Ure,  Une gi,  --- 

U>  +h: 

* 

. 

P 

zy <—  > B;u,,-;  +  21 

s=  —p:  4 

a. 

hes 

‘ 

where z, by construction  is uncorrelated  with  u,,_,  for  s  of  ie  preg 
p.  Recalling  from  [19.3.2]  that  u,,  =  Ay,,,  equation  [19.3.1]  then  can  be written 

4,:.<, 

Yu  =  a  +  Y'Y2,  +  = B: Ay,,_,  +  Z,. 

s=  =p 

[19.3.19] 

P 

If we  are  willing  to  assume  that  the  correlation  between  z* and  u,,_,  is zero  for 
|s| > p, then  an F test  about  the true  value of y that has an  asymptotic x? distribution 
is easy  to  construct  using  the  same  approach  adopted  in [19.3.18]. 

For  a more  formal  statement,  let y,, and  y>, satisfy  [19.3.19]  and  [19.3.2]  with 

Zz 

© [a= 3 te. 

= 

Up, 

s=0 

where  {s-W,}*_,  is  an  absolutely  summable  sequence  of  (m  x  n) matrices  and 
{e,}7_ _.  is an  i.i.d.  sequence  of (n X  1) vectors  with  mean  zero,  variance  PP’,  and 
finite  fourth  moments  and  with  W(1)-P  nonsingular.  Suppose  that  Z, is uncorre- 
lated  with  u,,  for  all  ¢ and  7,  so  that 

. 

a, 
he | ‘  aa 

0' 

[19.3.20] 

ry  (Mull) 

=O 

| 

where  P,, and  W,(L)  are  (g X  g) matrices  for  g 

=n  —  1. Define 

aun 

we 

, 

, 
(U3 ,_>,  U2 ¢-p+i>  Mg  em 

’ 

, 

’ 

7 

U2 7-15  U>,,  U2  +15  49  Wisp)’ 

B _  (B;, B,-1,  Pe  Mn  6  p}rs 

so  that  the  regression  model  [19.3.19]  can  be written 

Yuo=  Bw,  +  a  +  y'yn  +  Z,. 
[19.3.22] 
The reader  is invited  to confirm  in Exercise  19.2 that the OLS estimates  of [19.3.22] 

608  Chapter  19  | Cointegration  — 

satisfy 

- 

Q-'h, 

T'(B;  —  B)] 
T'(ar  —  a)|  >|  X,,y,  |, 
T(¥r  —  ¥) 

AY. 

(19.3.23] 

where  Q =  E(w,w,),  T~!?2w,z,  + h,, Ai. =  o,-h,,(1),  and 

[r . 

1 

| [W.(r)] ar}as 

An | Wan  dr  Aa f (W2(r)]‘[W2(r)]'  ar} 
Wilhue, 
in 
sa{fimseoramco} 

Here  A,, = W,,(1)- P,,,  W,(r) is univariate standard Brownian  motion,  W,(r) is 
g-dimensional  standard  Brownian  motion  that  is independent of W,(-), and the 
eee sign prentes integration over  r from 0 to I Hence, as in [19. 3.12], 

[19.324] 

iA 

Siar tw bate 

i 36s  FES HELA 

ep 30 elesd 

see  Exercise  19.3.  But  result  [19.3.24]  implies  that  conditional  on W2("), the expres- 
sion  in  [19.3.25]  is  (Aj,/s7)  times  a  y?(m)  variable.  Since  this distribution  is  the 
same  for  all  W,(-),  it follows  that  the  unconditional  distribution  also  satisfies 

x}  (A2,/s3,)-x7(m). 

(19.3.26] 
Result  [19.3.26]  establishes  that  in order  to  test  a  hypothesis  about  the  value 
of  the  cointegrating  vector  y,  we  can  estimate  [19.3.19]  by  OLS  and  calculate  a 
standard  F test  of the  hypothesis  that  R,y  =  r  using  the  usual  formula.  We  need 
only  to  multiply  the  OLS F statistic  by a  consistent  estimate  of  (s3/A7,),  and  the 
F statistic  can  be  compared  with  the  usual  F(m,  T  —  k) tables  for  k the number 
of parameters  estimated  in [19.3.19]  for  an asymptotically  valid  test.  Similarly,  the 
OLS t statistic  could  be  multiplied  by (s3/A?,)'*  and  compared  with  the  standard 
t tables. 

‘ 

‘ 

A  consistent  estimate  of  A?,  is  easy  to  obtain.  Recall  that  Ay;  =  91°  %,(1), 
where  Z,  =  ,(L)e,,  and  E(e?,)  =  07.  Suppose  we  approximate  y,,(L)  by an 
AR(p)  process,  and  let @, denote  the sample  residual  resulting  from  OLS  estimation 
of [19.3.19].  If 2, is regressed  on  p of its  own  lags: 

th, =  Pyt,_y  +  ots-2  +  °°  *  +  Gpl-p  +  4, 

then  a  natural  estimate  of A,, is 

| 

A=  6/0  -¢-—-¢&—°°°  —  %), 

[19.3.27] 

where 

a teil  © Sead 

T 
| ae  ES 

t=p+l1 

and  where T indicates  the  number  of observations  actually  used  to estimate  [19.3.19]. 
Alternatively,  if the  dynamics  implied  by #,,(L)  were  to  be  approximated  on  the 
basis  of g autocovariances,  the  Newey-West  estimator  could  be  used: 

A 

Ah =  > +  2) [1 -  iq  +  é,, 

q 

j=l 

aes 

where 

T 

hes  T-"  >. Gh... 
t=j+1 

_  These  results  were  derived  under  the  assumption  that  there  were  no  drift 
terms  in any  of the  elements  of y,,.  However,  it is not  hard  to  show  that  the  same 
procedure  works  in exactly  the  same  way  when  some  or  all of the  elements  of y,, 
involve  deterministic  time  trends.  In  addition,  there  is no  problem  with  adding  a 
time  trend  to  the  regression  of [19.3.19]  and  testing  a  hypothesis  about  its  value 
using  this  same  factor  applied  to  the  usual  F test.  This  allows  testing  separately 
the  hypotheses  that  (1) y,,  —  ‘y’y2,  has  no  time  trend  and (2) y,,  —  y'y>,  is J(0), 
that  is,  testing  separately  the  restrictions  [19.1.15]  and  [19.1.12].  The  reader  is 
invited  to  verify  these  claims  in Exercises  19.4  and  19.5. 

Illustration—  Testing  Hypotheses  About  the  Cointegrating 
Relation  Between 

Consumption  and Income 

As  an  illustration  of this  approach,  consider  again the  relation  between  con- 
sumption  c, and  income  y,, for which  evidence  of cointegration  was  found  earlier. 

610  Chapter 19  | Cointegration 

The  following  regression  was  estimated  for ¢ =  1948:II  to  1988: III  by OLS,  with 
the  usual  OLS formulas  for  standard  deviations  given  in parentheses: 

c,  =  —4.52  +  0.99216  ¢?  OES 
py  a.  y 

a  AY. +4  a AY,43  +  ee AY.+2 

+  0. 

©  Se  ty  ~  0.24 a  bate  _, 
ems 

(0.12 

= 

m 

(0. 

+  0.04 Ay, ae  Me  0.02 Ay, + a 

s =  (T-  11) > a? =  (1.516). 

+  0.07 Ay 
(0.11) 

Mena 

[19.3.29] 

Here  T, the number  of observations  actually  used  to estimate  [19.3.29],  is 162.  To 
test the null hypothesis  that the cointegrating  vector  is  a  =  (1,  —1)’,  we  start  with 
the usual  OLS ¢ test  of this  hypothesis, 

=  (0.99216  —  1)/0.00306  =  —2.562. 

A second-order  autoregression  fitted  to the residuals of [19.3. 29) by OLS produced 
a 
2  127-3:90) 

a, = 0.7180 ay-1+ 0.2057 f+ 4,  bes 

wher: 2 

OT, 

a) 

$5  oi: 

1  tog BRT  YS 

tert 

4: 

(sey bower  2 as? 

Bonen dives)  2)  03 = -(r- yy; 3 @ = 0.38082.  tpn  eal 
badigal  a ROiTE xi 
tnhes 
Thus, the estimate of _ aah in [19.3.27} is Sunicioveicosis 
made adjustm dy osnyy aly ee 

ee 
saamiaecal 
2 
wie 

attncsmes 

089. 

or 

| 

wi. 

,  ae mee, thet hae the null hypo pothes esis tl F  ae a= +  : ?  :  1)’ ce 
ae 
ee amet A eta) = (- a $62)(1.516)/(8.089) 
= 

:  *:  :  oi 2d 

A test  of the  hypothesis  that  the  time  trend  does  not  contribute  to  [19.3.31]  is thus 
given  by 

, 

[(0.2690)/(0.0197)]-[(1.017)/(3.194)]  =  4.35. 

Since  4.35  >  1.96,  we  reject  the  null  hypothesis  that  the  coefficient  on  the  time 
trend  is zero. 

The  OLS  results  in  [19.3.29]  are  certainly  consistent  with  the  hypothesis  that 
consumption  and  income  are  cointegrated  with  cointegrating  vector  a  =  (1,  —1)’. 
However,  [19.3.31]  indicates  that  this  result  is dominated  by the  deterministic  time 
trend  common  to  c,  and  y,.  It  appears  that  while  a  =  (1,  —1)’  is  sufficient  to 
eliminate  the trend  components  of c, and y,,  the residual  c, —  y, contains  a stochastic 
component  that  could  be  viewed  as  J(1).  Figure  19.6  provides  a  plot  of c,—  y,.  It 
is  indeed  the  case  that  this  transformation  seems  to  have  eliminated  the  trend, 
though  stochastic  shocks  to  c,  —  y,  do  not  appear  to  die  out  within  a  period  as 
short  as  2 years. 

Further  Remarks  and  Extensions 

It was  assumed  throughout  the  derivations  in  this  section  that  Z, is 1(0),  so 
that  y,  is cointegrated  with  the  cointegrating  vector  having  a  nonzero  coefficient 
on  y,,..  If y, were  not  cointegrated,  then  [19.3.19]  would  be  a  spurious  regression 
and  the  tests  that  were  described  would  not  be  valid.  For  this  reason  estimation 
of  [19.3.19]  would  usually  be  undertaken  after  an  initial  investigation  suggested 
the  presence  of a  cointegrating  relation. 

-1.6 

“HZ 

-12.8 

“14.4 

47 

S| 

SS 

$9 

63 

67 

71 

7S 

73 

83 

87 

FIGURE  19.6  One  hundred  times  the  difference  between  the log of personal  — 
consumption  expenditures  (c,) and  the  log of personal  disposable  income  (y,) for 
the  United  States,  quarterly,  1947-89. 

612  Chapter 19  | Cointegration 

It was also assumed  that  A,,  is nonsingular,  meaning  that  there  are  no  coin- 
tegrating relations among the variables  in y,,.  Suppose  instead  that  we  are  interested 
Dede  t h >  | different  cointegrating  vectors,  as  represented  by a  system  of 
e  form 

=  /’-y,  + 

YY, 
(Axi) 
Ay,  =  &  +.  0, 
(g <1) 
(g x  1) 
(g x  1) 

(h xg)  dy  iw) 

we 

+ 

2? 
(we 1) 

19.3.32 
| 
{19.3.33] 

with 

] 
=  w*(Lie, 

c& nN - “» Set 

| 

| 
‘arr 
eie 

‘  and  W*(1)  nonsingular.  Here  the  generalization  of  the  previous  approach  would 
7 

be  to  augment  [19.3.32]  with  leads  and  lags of Ay,,: 

vy 

SBS  +  T's,  + 

eee!  +s, 

s=—p 

[19.3.34} 

where  B;  denotes  an  (A  X  g) matrix  of coefficients  and  it is  assumed  that  2, is 
uncorrelated  with  u,,  for  all  ¢ and  7.  Expression  [19.3.34]  describes  a  set  of  h 
equations.  The  ith  equation  regresses  y,, on  a constant,  on  the  current  value  of all 
the  elements  of y2,,  and  on  past,  present,  and  future  changes  of all  the  elements 
of y2,.  This  equation  could  be  estimated  by OLS,  with  the  usual  F statistics  mul- 
tiplied  by [s$2/A(2]*,  where  s{ is the  standard  error  of the  regression  and  A{’ could 
be  estimated  from  the  autocovariances  of the  residuals  Z, for  the  regression. 

The  approach  just described  estimated  the  relation  in [19.3.19]  by OLS  and 
made  adjustments  to the  usual ¢ or  F statistics  so  that  they could  be compared  with 
the  standard  ¢ and  F tables.  Stock  and  Watson  (1993)  also  suggested  the  more 
efficient  approach  of first estimating  [19.3.19]  by OLS,  then  using the  residuals  to 
construct  a consistent  estimate  of the  autocorrelation  of u, as  in [19.3.27]  or  [19.3.28]}, 
and  finally  reestimating  the  equation  by generalized  least  squares.  The  resulting 
GLS  standard  errors  could  be used  to construct  asymptotically  x” hypothesis  tests. 
Phillips  and  Loretan  (1991,  p. 424)  suggested  that  instead  autocorrelation  of 
the residuals  of [19.3.19] could be handled  by including  lagged values  of the residual 
of the  cointegrating  relation  in the  form  of 

Yu  =  at  Y’yx,  + 

Bi Ay,,-,+  > Dyes  7  ¥'Yae~2)  +  Bu. 

[19.3.35] 

P 2 

s=-p 

s= 

Their proposal  was  to  estimate  the  parameters  in  [19.3.35]  by numerical  minimi- 
zation  of the sum  of squared  residuals. 

Phillips  and  Hansen’s  Fully  Modified  OLS  Estimates 
A  related  approach  was  suggested  by Phillips  and  Hansen  (1990).  Consider 

again  a system  with  a single  cointegrating  relation  written  in the  form 

Yn  =  at  y'yn,  +  2; 

Ay2,  =  Uy, 

2h. [2] =  W*(L)e, 
u,, 
E(e,e;)  =  PP’, 

[19.3.36] 

[19.3.37] 

19.3.  Testing  Hypotheses  About  the Cointegrating  Vector  613 

where  y>,  is  a  (g  X  1)  vector  and  €,  is  an  (n  xX  1) i.i.d.  zero-mean  vector  for 
n  =  (g  +  1).  Define 

; 

"=  wr(1)-P 

=*  =  A*-[A*]’  = 
ee 

A 

yh 

Patt  ei. 

eee 

Rak 

e 

[19.3.38] 

with  A*  as  always  assumed  to  be  a  nonsingular  matrix. 

Recall  from  equation  [10.3.4]  that  the  autocovariance-generating  function  for 

(z7,  us,)'  is given  by 

(gx1) 

(gg) 

Ga)= 

>  i  raat  one  ae 
"(2 *)I’. 
[W*(z)]-PP'Tw 
Thus,  2* could  alternatively  be described  as  the autocovariance-generating  function 
| 
G(z)  evaluated  at  z  =  1: 

E(u,,z/- |  E(u,  ,-,) 

v=  —s 

>  ee  4 

ces 

of © pe pe  Sot  (19.3.39] 

23,  2 

v=  -= 

E(u,,z7-,) 

E(uza2,-y) 

The  difference  between  the  general  distribution  for  the  estimated  cointe- 
grating  vector  described  in  Proposition  19.2  and  the  convenient  specfal  case  in- 
vestigated  in [19.3.15]  is due  to two  factors.  The  first  is the possibility  of a nonzero 
value  for 23,,  and  the second  is the  constant  term  that  might  appear  in the  variable 
h, described  in Proposition  19.2  arising  from  a  nonzero  value  for 

R=  >) E(u,,z¥,,). 

v=0 

[19.3.40] 

7 

The  first issue  can  be addressed  by subtracting  23; (23,) ~Ay,, from  both  sides 

of [19.3.36],  arriving  at 

where 

Vise  teW' 

Yah  zh, 

Vie =  Yue  ~  DH (2E)~  "Ayo, 
Zz} =  zh  —  Z3/(2h)7  Aya. 

[19.3.41] 

Notice  that since  Ay>,  =  U,,,  the  vector  (z/,  u,)’  can  be written  as 

|| =  U2 | 

[19.3.42] 

for 

{” 

‘Weyer  a 

L’  -|  Nas 
I, 

0 

Te  | OT 

€; 

L; 
(gn) 

[19.3.43] 

Suppose  we  were  to  estimate  a and  y with  an  OLS  regression  of yj, on  a constant 
and  y,,: 

*] a  | ie 
hs  2Yor  Lan) 

)] Sy, 

L2ynyid 

:  ieee: 

614  Chapter 19  | Cointegration 

The distribution  of the resulting  estimates  is readily  found  from  Proposition  19.2. 
Note  that the vector Aj’ used  in Proposition  19.2 can  be written  as e,A* for ej the 
first row  of I,,, while  the matrix  A} in Proposition  19.2  can  be written  as L;A*  for 
L, the  last g rows of L’.  The asymptotic  distribution  of the  estimates  in [19.3.44] 
is found  by writing A} in [19.2.13]  as LiA*,  replacing  At’  =  e;A®*  in [19.2.13]  with 
€,A*,  and  replacing  E(u,,z*,,)  with  E(u,,z!,,): 

Loa -  2] 3 
L  T47-  y) 

1 

Tt  fyi:  2 i ite  > 3 | 

T-*2yn,  T*2yn¥n} 

LTO 2ynzi 

1 

| [W(r)]’ ar} an, 

=i 

L 

L,A* | W(r) dr  Lis} | [W(r)]-[W(r)]’  ar} an, 

e/A*W(1) 

| 

Pi  ua fowontawonr  fare + RTP  | 
|  oe  : 

j 

: 

: 

[19.3.45] 

where  W(r) denotes  n-dimensional standard  Brownian’ motion and 

= 

, 

Oz e¢i| gb  s1011  pt => E(u,,z},,) 
iw fanouéibace  1633  Woh?  bisc  7 
StH  360?  eiseiseie  tes) 

SON op  AB  art  Bice“  vias eee 

ot)  in  of  idgow 
Sigs 
“y  (NSO  OVER 

gaieqigD  od  Siypwe  iO!  ©: 

aod a  ar  ites  GU  DAVOS 

Hag 
yee) 

4 
Bia  To77  PF}.  Rod] 

ay  ¥d *Lavidiniies  ot  se0qerg  gira  aap  jC2.c.1}  erilicsss 

sidsnay 

~~ 

- 

73 

ree 

7" 

| 

> 

2 

:  a3  3 

208  saser-is  Fi 

= 

=  & Eltaleiey  wre dl  ge 

sider  the  (n  x  1) 

ae 

eo 

vector  process  defined by 

pty 
“-_ 

ae 

( 

PRS a  phe. 

We Ae I 

ETA x 

7 

ae 

4 

Then  [19.3.48]  implies  that  B,(r)  is  scalar  Brownian  motion  with  variance  (ot)? 
while  B,(r)  is g-dimensional  Brownian  motion  with  variance  matrix  £3,  with  B,(-) 
independent  of  B,(-).  The  process  B(r)  in  turn  can  be  viewed  as  generated  by a 
different  standard  Brownian  motion  W'(r),  where 

_ 

B(r)| 
B,(r) 

0’  ane 

[of 
0  PZILW3(7) 

for  P3,P%;  =  =%,  the  Cholesky  factorization  of 23,.  The  result  [19.3.45]  can  then 
equivalently  be  expressed  as 

Cg -  . 
(47  —  ¥) 

{fworr arly 

= 

pif wie) ar  Pa{ | wsintwsioy  ar bes 
«| raf wie ams + x. 

Wi (1) 

(19.3.50] 

If it were  not  for the  presence  of the constant  X*, the  distribution  in [19.3.50] 
would  be  of the  form  of [19.3.11],  from  which  it would  follow  that  conditional  on 
W3(-),  the  variable  in  [19.3.50]  would  be  Gaussian  and  test  statistics  that  are 
asymptotically  x? could  be  generated  as  before. 

Recalling  [19.3.39],  one  might  propose  to  estimate  2*  by 

E | =I,o+  > (1 -  [Wq+i}T,  +),  — [19.3.51] 

231  >> 

v=1 

where 

los  a  dese 

| 

=j> < 

t=v+1 

(a,,2",)  (a5,  ,_,) 

bo  22 
LE  FY 

[19.3.52] 

for  2?  the  sample  residual  resulting  from  estimation  of  [19.3.36]  by  OLS  and 
i,,  =  Ay,,.  To  arrive  at  a similar  estimate  of X*, note  that  [19.3.46]  can  be written 

Rt  =  py E{u,,_,[z?  s

v=0 

e

1 

fztey-+] 

Sf 
», rite \| sohatak  | 
*, Ir  -@2)  "a 

ETL  tna 
oi}{ fier ther]  ges age |. 093500 

21 

This  suggests  the  estimator 

w=  S$ {1- tones 

v=0 

616  Chapter  19  | Cointegration 

 
The fully modified  OLS  estimator  proposed  by Phillips  and Hansen  (1990)  is then 

HEA  veh |  >y1, 
43 

Lyn  Lynyx) 

L{ZynVt,  —  TRH 

for 9},  = Yu  -  E3/ (D3) -Ay,,. This  analysis  implies  that 
[ 

rer  -a)] 
TO?  =  y)  J  [T-*8y.,-T-*2y2y2) 

1 

T~>y;,  \ | T-  Eat 

LT 2Zyn2f —  Ry 

‘3 
—  a} 

Vy; 

V2 

= 

where 

Ree  er  ala 

|  psf wurde  raf f omsonwsor ares 

{J owseor abe 

*) 

-1 

a  eleodiogyies  {FS 

x 

57! RKD=  “ia-3d 

ea  fwmamo} | 

oH) 

SSIaTNNES 

sz: 

: 

y 

26  & 

stories 

nivice 

Be ais five | ,  ' es 

“  =  (9 (To)  a  ~  Salk 

“woos 

a  cyte  i 

tena  ete pees) tnainvens> z A 

This  description  has  assumed  that  there  was  no  drift  in  any  elements  of the 
system.  Hansen  (1992)  showed  that  the  procedure  is easily  modified  if E(Ay2,)  = 
5,  +  0, simply  by replacing  ,,  =  Ay,,  in  [19.3.52]  with 

where 

G,,  =  Ay2,  —  b., 

ri 

B oT-+  >, Aven 

t=1 

Hansen  also  showed  that  a time  trend  could  be added  to the  cointegrating  relation, 
as  in 

for  which  the  fully  modified  estimator  is 

Yu  =  at  V'Yu  +  bt +  z7, 

T. 

ay 
Sy,  4 2  eee 
4it]  =|  Sy,  Lyoys  Dyot|  | Lyavt,  —  TRE. 
st! 

Tee, Mita > 

ee 

rryt, 

5 > 

Collecting  these  estimates  in  a  vector  bit  =  (441,  [4tt]’,  6%)’,  a  hypothesis  in- 
volving  m  restrictions  on  B of the-form  RB  =  r can  be  tested  by 

eee  Se 
{Rb}!  —  r}')(6{)?R]  Lyx  Lyx  Lyot| 
Der 

Styyecd 

Zt 

a 

R’? 

{Rb7  —  r}>  x7(m). 

L 

Park’s  Canonical  Cointegrating  Regressions 

A closely  related  idea has been  suggested  by Park  (1992).  In Park’s  procedure, 
both  the  dependent  and  explanatory  variables  in  [19.3.36]  are  transformed,  and 
the resulting  transformed  regression  can  then  be estimated  by OLS  and tested  using 
standard  procedures.  Park  and  Ogaki  (1991)  explored  the  use  of  the  VAR  pre- 
whitening  technique  of Andrews  and  Monahan  (1992)  to  replace  the  Bartlett  es- 
timate  in expressions  such  as  [19.3.51]. 

APPENDIX  19.A.  Proofs  of Chapter  19 Propositions 

@  Proof 
¥io  =  0. Then 

of Proposition  19.2.  Define  y,,= zi  +  z}  +--+:  +  z* fort  =1,2,...,T  and 

Vu 

+  &F, 
|  a  : 

- 

0 

* 

where 

=> bal 

s=1  | Uo, 

Hence,  result  (e) of Proposition  18.1  establishes  that 

T'>  fel  wil  af [two tawoor'} av  :  > ry’ 

rt 

LYor-1 

[19.A.1] 

618  Chapter  19  | Cointegration 

A*  =  W*(1)-P 

Tag  =  e| Ace Oe 

It follows  from [19.A.1] that 

T-! 5) Ac  uw).  =  T-  s pie ‘le -  ero >) iG  u;,] 
LYoy- 

1 

=) 

. 

(U2, 

=1 

; 

:  arf [W(r)] jawin'} an  +  >» ee 

Similarly,  results  (a), (g), and  (i) of Proposition  18.1  imply 

-~12 

TEL  hem 

zP  Sales 

T-¥ > (% "  sal Win dr 

[19.4.2] 

= 

19.A.3 

[19.4.4] 

qa ay yf ‘Jou al off (woo pom A a} aa 

HLA 

__ Observe  that  the deviations  of the  OLS  estimates in mits 2.12] from the Poulton 

values a and  y that describe  the cointegrating relation [19.2.9]  are given  by — 

LS 

4 
, 

BEE)  nc 
~ 

ia 

Cee  ee | Z  cl | se 
2Y2,2;  ; 

Zn  LY  V2 

Yr 

. 

~ 

. 

“T  bee & Gs  wept’ -T  tatlPeorle SES  sao  8  =  fuzssa  hs dha ieoedl 
Oe area oe tet MES  aR,  219d  opral ss eet om 

a 

y  7  e-4 

a 4 
. 
z 

—1/2 

< 

g 
4  oy  =  ‘ 

ts 

7 
2p ae 

aa 

a 

y  ile 
4 

i 

i. 

- 

S 
>“)  DTA  ye  ae 

hice 

ee 

- 

" 

. 

é 

. 

a 
yee 

2 

4? 

| 

. 

: 

ee  to Ve  WY ay  oe  ee 
P 
if  tt  v-1/2. 
Betas 
: 
fh 

1 

} 

.  me 

Consider  first  what  the  results  would  be  from  an  OLS  regression  of  z*  on  z% = 

¢  ae  a  oo  z,,)',  a constant,  and  y,,: 

zy,  =  B’zy,  +  a?  +  R*’yz,  +  U,. 

[19.A.8] 

If this  regression  is evaluated  at  the  true  values  a*  =  0, X*  =  0, and  B = C..  2:  “5 
B,)'  the  vector  of population  projection  coefficients  in [19.2.18],  then  the  disturbance  u, will 
be  the  residual  defined  in [19.2.18].  This  residual  had  mean  zero  and  was  uncorrelated  with 
z3,.  The  OLS  estimates  based  on  [19.A.8]  would  be 

B, 
as|=} 
Rt 

Lee  Uzpyy| 
Lesen 
|  2eezn 
zz  |. 
Ly, 
TT 
Sze’ 
LY2Zy  = ZLYxn 
 LY2Yand  = LZYuZt 

[19.A.9] 

The  deviations  of these  estimates  from  the  corresponding  population  values  satisfy 

ie  A?  OO  Bekes’ 
Weh 
-1 
0  T'7I,j|Zy223, 

Grote  | O 
0 

T!R2 - 

ON) 

Eat  Sety’,  =f 
ee  i Se 
Lyx  Lyn¥x 
0 
0 
RP  ee Ee 

J[T-1,_, 
]°'[Sz%u, 
"tle: 
ci 
0 
OO  TES 
[Zy,u, 
T-27Ezty),)  fT Szku, 
Ts a>  a 
¥en >>  s 

«| 

T1,-; 

0 
0 
ae  ee 
0 
0  TEI 
To Dzizs' 
7 iSz%! 

T= "Sak 
1 

LT-2y,a3,  T-3?2y, 

T*2ynyz,} 

T~>*2y2u, 

[19.A.10] 

Recalling  that  E(zu,)  =  0,’one  can  show  that  T-'Xz3u,  >  0 and  T-'Xu, > 0 by 
the  law  of  large  numbers.  Also,  T->7Sy.,,u,  >  0,  from  the  argument  given  in  [19.A.7]. 
Furthermore, 

T-"Qzkz' 
-T-"Sz8 
T-32Ezty!, 
T>Ez¢' 
1 
T-*23ys, 
T~>?2y,23,,  T-Xy,  T-*2yryz, 

*  E(z322') 

a) 

8 

l 

0 

{f [Wel  artar 

,  [19.A.11] 

0 

asf waar  asff owner ar}ar’ 

where  W(r)  is n-dimensional  standard  Brownian  motion  and  Aj is a(g  X  n) matrix  con- 
structed  from  the last g rows  of W*(1)-P.  Notice  that the matrix  in [19.A.11] is almost  surely 
Nonsingular.  Substituting  these  results  into.{19.A.10]  establishes  that 

0 

6, 1.  B 

a;  |  /0], 
0 
T!?R 

so  that  OLS  estimation  of [19.A.8]  would  Pee consistent  estimates  of the  parameters 
of the  population  linear  projection  [19.2.18 
An  OLS  regression  of y,,  on  a  constant  and  the  other  elements  of y, is a  simple 
transformation  of the regression  in [19.A.8].  To see  this, notice  that [19.A.8] can  be written 

| 

as 

(1  -p')e? nd 

py. N*’y,,  + up 

i 

i 

[19.A.12} 

620  Chapter 19 | Cointegration 

Solving  [19.2.16]  for z* and  substituting  the  result  into  [19.A.12]  gives 
(1  -B'\(y,,  -  wt  -  Py.)  =  a*  +  X*y,,  +  u,, 

Or,  since  ¥,  =  (Yu Yas.  . 

,  Ya)’,  we  have 

; 

°°  +  Bry,  +  a  + R'y, +  U,, 

Yu  =  Bry,  +  Bsyy  + 
[19.A.13] 
where  a =  a* + [1  —B’)py  and  &’  =  X*’  +  {1  -p'jI’. 
OLS estimation  of [19.4.8] will produce  identical  fitted  values  to those  resulting  from 
OLS estimation  of (19.A.13],  with  the  relations  between  the  estimated  coefficients  as  just 
given.  Since  OLS  estimation  of [19.A.8]  yields  consistent  estimates  of [19:2:18], OLS  es- 
resiais of [19.A.13]  yields consistent  estimates  of the  corresponding  transformed  param- 
eters,  as  claimed  by the proposition.  m 

@ Proof 

of Proposition  19.4.  As in Proposition  18.2, partition  AA’  as 

| 

Shi) 

- 

eect 

21 

221 

Abo)  . 984, 
Pan 
(x1) 

eae 
(xa) 

var 

ae  Mae it A  Braye | 

= 

\. 

SS 

a1 

t 

(of?  = (2,  —  Bn 22'Z,) 

| 

[19.A.14] 

[19.A.15] 

[19.A.16] 

> 

S-1. 

’ 

Mee  lIgqus 

SoeP  sence 

But 

for 

a =  of {(yi/ot)  —  (Wot)  ¥r'yz  —  (@r/o7)} 
sor[l  —¥F'lot}er  -  (az/ot)} 

Differencing [19.A.22] results  in 

a  pee H  =  Ey, 

Va 

| 

| 
> 

£0 

ice 

fe 
-A7 

[19.A.23] 

(a? —  a7.)  = of-[1 

—97'lof]Ag?. 

[19.A.24] 

Using  [19.A.22]  and  [19.A.24],  the  numerator  of [19.A.21]  can  be written 

Cole 2 oP hae — a2) 

GAR 

=  (of )?-(T*)-! >» fe —4F ot ]Et ae c@xe7)}{ ase] - co 

200 
fte  ert 

Bugis  Tier} {ery pe eS; case} ae  etic by 
- cra nadetaths {oo 32 zee ear 

ai ok Hou  <3 

— 

: 
\ 

OL A2h 
Notice that the c  expression 

ey  ate <2, 2 (a = 

[19.A.25] 

[te  A. et 

Lon walieig 

- cag  the tee 

¢  red > jewel a 

of  sit  of 3 tab 
al 

EST. 

=  >  a  of rs 9% es  0  256  35  i  oe 

gre Nai jae. Ya aan 

“OF EAD Ve Pee 

for W*(r)  =  L'A- W(r)  the n-dimensional  standard  Brownian  motion  discussed in  equation 
(18.A.17].  Substituting  (19.4.27]  and  [19.A.20]  into  [19.A.26]  produces 

ft  -4$} Hot  {(re)-* Se (Ag? o}| 7 
—Fr/oF 

S (121  —b3k{W*())-[We()]’  -  El(Oet)(OEF oy  aa 

(19.A.28] 

— analysis  of the  second  term  in {19.A.25]  using result  (a) of Proposition  18.1  reveals 

(Ty  (a303)-| (Tr) = casz}|  3 | > hy twra)'|  ‘  | [19.A.29] 

T 

1=2 

~—Fr lo 

~—h, 

Substituting  [19.A.28] and  [19.A.29]  into  [19.A.25],  we  conclude  that 

(ey = a* (a*  —  a*.,) 

4 (of): {} {u —h3]-[W*(1)]- (wea) 1+.) -  h-wrny| = 

:

r  Oua)-{t  -nseienaervagr  | | = ee, 

re: 

- f 

: 

“ 

segnigeps  Loe  EE)  M9-A30] 

The limiting distribution  for the  denominator  of i A. = was  obtained thn 
Of Proposition 18.2: 
| 
Bae  EE. A.€1} cnn: RSA. es} oo Sate rs 3m  Ri  | 

SECT  ai gees: 

 
T 

(7)  > (67  - Wat  Agr,  >  0. 

tej+2 

Similarly, 

_ 

thee > Or =  1)?a”  07; 

=  (ot Fur. $3 Gr -  ft  ~4y'lot let, -  (a¢/ot 

x  {1 ~  HF oF ES  1  -  (as/07)} 

ry 

.  ee  24 2  1 a ae azlot ss 

x LEM 1  (PY  -94lot  -(T)-azoty 
=  (of) (Tr - Ce  Rage  Bee  2)  © 
oN wali 2, (Per Ae Py  ]  }
Be 
AO 
Be  x [1  —¥2'lot  <erpeanial tt de yga % eee  his- . ins)  : 

ar:  rg  ei (Ty  s.  } ye} he  ged  Ei 

2 - 

(19.4.35] 
|

Ria 

 
 
But  from  [19.2.29]  and  [19.A.31], 

(T* 63, +  sh= 

—, 
rt  >. ie 
(™)  p>  ‘ 

L 
1 
=>  —___ 
(of)?-H,, 

It then follows  from [19.A.36]  and [19.A.37]  that 

{(T*)?-63,  =  53}+{AZ  =  foo} 

eh  -hb4,  -  @retay)ariu-| | | +  H,. 

Subtracting } times  [19.A.39]  from  [19.2.36]  yields  [19.2.37]. 

[19.A.38] 

(19.A.39] 

Proof of (c).  Notice  from  [19.2.33]  that 

Zur =  (Way): sali sa {T*-6, ee: ae 

satisfy 

T-""6, 
T'(4,—  y)} 

|], 

[1 
[6,2 

68/2]7'[ 
53/3 

T-*42(€i  —  robs) 
T~°?Z6M(E,  —  Yok2) 

Conclude  that  @, and  7; have  the  same  asymptotic distribution as the wi apa 
from  a  regression  of (£,,—  y€,,)  on  a  constant  and  6, times  a  time  trend.: 

(b)  Show  that  first  differences  of the  OLS  residuals  converge  to 

(€.-  Yo$2,)  =art  y' dot  +  U,. 

Au, & Ui,  _—  YoUr- 

19.2. 

Verify  [19.3.23]. 

19.3. 

Verify  [19.3.25]. 

19.4. 

Consider  the  regression  model 

where 

yy  =  B'w,  +  a  +  Y'y2,+  St  +  U,, 

Wy  =  (Oe  OY  ites  -.« 

»  AYA 1p Aan  Rereag 

>  3  Ase) 

Let  Ay,  =  u,,,  where 

| PCUDE  | 0 
and  where  e, is i.i.d.  with  mean  zero,  finite  fourth  moments,  and  variance 

esses  2 

A 

id 

W,(L) 

0’ 

ey 

awl 

ioe 

Ola) 

¢y-ncf! 

| 

ECE  a mie  P?,  |’ 
Suppose  that  {s-W  =, is absolutely  summable,  An =  0,'W,,(1)  #  0, and  A, =  W2(1)-P2 
is nonsingular.  Show  that  the  OLS  estimates  satisfy 

cab  igel 

Q-'h, 
T™7(B;  —  B) 
T'*(a;  —  a)  -  Anh, 

T(¥r  -  Y) 

Plo;  —  6) 

Ay", 

Ais 

: 

i 

where  Q =  plim  T-'Iw,w! ,  T-*Iwu,  = h,, and 

W,(1) 

>| =H!  hn  f [W.(r)]  ano} 
pee 
W,(1)  -  | W,(r) ar} 

1 

| [W,(r)]’  arbi 

1/2 

H= | A, | W,(r)  dr  An | [W.(r)]}-[W2(r)]  ars A  | rW,(r)  dr  |. 

1/2 

) | rw, (ny ar}as, 

1/3 

Reason  as  in [19.3.12]  that  conditional  on  W,(-),  the  vector  (v,, v3,  v;)'  is Gaussian  with 
mean  zero  and  variance  H~'.  Use  this  to show  that  the Wald  form  of the  OLS  x? test  of 
any  m  restrictions  involving  a,  y, or  5 converges  to  (Aj,/s})  times  a y7(m)  variable. 
19.5.  Consider  the  regression  model 

Yu  =  Bw,  +  a  +  y'yz,  +  U,, 

626  Chapter 19  | Cointegration 

where 

Me  RBA  ine  sive  A  dav Ae,  Ae  a 

Suppose that 

Ay,,  =  5,  +  u,, 

— — — the  elements  of 8, is nonzero.  Let  u, and  u,,  satisfy  the  same  conditions 

Let  yn  =  (Yo  Yass. 

,  Yu)  and  8,  =  (6,,  5,, 
...,  5,)’,  and  suppose  that  the 
elements  of Y2, are  ordered  so  that  E(Ay,,)  =  5,  #  0.  Notice  that  the  fitted  values  for  the 
regression  are  identical  to  those  of 

3° 

where 

Vu  a  B'w?  +  a*  + 

7,  23 

san  os +  8*y,,,  +  U,, 

w,  =  [(Ay2,-,  -  8)’,  (Ay. ,-p+1  DDS  datwiaie  (Ay2,+,  =  6,)']' 

Ya  —  (8,/8,)y¥ x 

Ya  —  (85/6, )¥ 1 

Yn-1s  oe  (5, -  ,/5,)¥ nr 

ies 

Yi 

Y2 

Ys 
. 

Yn-1 

6°  =  y,  +  ¥2(5,/5,)  +  ¥3(65/5,)  +--+  +  Yn -1(5, —1/6,) 

“=a  +  B'(1@  6), 
with  1 a  [(2p  +  1)  x  1] column  of  1s. 

5 

Il 

Show  that  the  asymptotic  properties  of  the  transformed  regression  are  identical  to 
in 
those  of the  time  trend  regression  in Exercise  19.4.  Conclude  that  any  F test  involving 
the  original  regression  can  be  multiplied  by (s?/2?,)  and  compared  with  the  usual  F tables 
for  an  asymptotically  valid  test. 

Chapter  19 References 

Ahn,  S.  K.,  and  G.  C.  Reinsel.  1990.  ‘Estimation  for Partially  Nonstationary  Multivariate 
Autoregressive  Models.”  Journal  of the American  Statistical  Association  85:813-23. 
Anderson,  T. W.  1958.  An Introduction  to Multivariate Statistical  Analysis. New York:  Wiley. 
Andrews,  Donald  W.  K.,.and  J.  Christopher  Monahan.  1992.  ‘An  Improved  Heteroske- 
dasticity and Autocorrelation  Consistent  Covariance  Matrix  Estimator.”  Econometrica  60:953- 
66. 
Baillie,  Richard  T., and  David  D.  Selover.  1987.  ‘‘Cointegration  and  Models  of Exchange 
Rate  Determination.”  /nternational  Journal  of Forecasting  3:43-51. 
Campbell,  John  Y.,  and  Robert  J.  Shiller.  1988a.  ‘Interpreting  Cointegrated  Models.” 
Journal  of Economic  Dynamics  and  Control  12:505-22. 
———  and 
and  Discount  Factors.””  Review  of Financial  Studies  1:195-228. 
Clarida, Richard.  1994. “Cointegration, Aggregate Consumption,  and the Demand for Imports: 

-. 1988b.  “The Dividend-Price  Ratio and Expectations of Future  Dividends 

' 

, 

A Structural  Econometric  Investigation.”  American  Economic  Review  84:298- 308. 
Corbae,  Dean,  and  Sam  Ouliaris.  1988.  “Cointegration  and  Tests  of  Purchasing  Power 
Parity.”  Review  of Economics  and Statistics  70:508-11. 

Chapter  19  References 

627  | 

Davidson,  James  E.  H.,  David  F.  Hendry,  Frank  Srba,  and  Stephen  Yeo.  1978.  “‘Econo- 
metric  Modelling  of the  Aggregate  Time-Series  Relationship  between  Consumers’  Expend- 
iture  and  Income  in  the  United  Kingdom.”  Economic  Journal  88:661-92. 
Engle,  Robert  F.,  and  C.  W.  J.  Granger.  1987.  “Co-Integration  and  Error  Correction: 
Representation,  Estimation,  and  Testing.”  Econometrica  55:251-76. 

and  Byung  Sam  Yoo.  1987.  ‘Forecasting  and  Testing  in  Co-Integrated  Systems.” 

Journal  of Econometrics  35:143-59. 
Granger,  C.  W.  J.  1983.  “‘Co-Integrated  Variables  and  Error-Correcting  Models.””  Unpub- 
lished  University  of California,  San  Diego,  Discussion  Paper  83-13. 

and  Paul  Newbold.  1974.  ‘“‘Spurious  Regressions  in Econometrics.”  Journal  of Econ- 

ometrics  2:111-20. 
Hansen,  Bruce  E.  1990.  ‘“‘A  Powerful,  Simple  Test  for  Cointegration  Using  Cochrane- 
Orcutt.’”’  University  of Rochester.  Mimeo. 

.  1992.  “Efficient  Estimation  and  Testing  of Cointegrating  Vectors  in the  Presence 

of Deterministic  Trends.”  Journal  of Econometrics  53:87-121. 
Haug,  Alfred  A.  1992.  “Critical  Values  for the  Z,,-Phillips-Ouliaris  Test  for Cointegration.” 
Oxford  Bulletin  of Economics  and  Statistics  54:473-80. 
Johansen,  S¢ren.  1988.  “Statistical  Analysis  of Cointegration  Vectors.”  Journal  of Economic 
Dynamics  and  Control  12:231-54. 

.  1991.  “Estimation  and  Hypothesis  Testing  of  Cointegration  Vectors  in  Gaussian 

Vector  Autoregressive  Models.”  Econometrica  59:1551-80. 
King,  Robert  G.,  Charles  I.  Plosser,  James  H.  Stock,  and  Mark  W.  Watson.  1991.  “‘Sto- 
chastic  Trends  and  Economic  Fluctuations.’’  American  Economic  Review  81:819—40. 
Kremers,  Jeroen  J. M.  1989.  “‘U.S.  Federal  Indebtedness  and the Conduct  of Fiscal  Policy.” 
Journal  of Monetary  Economics  23:219=38. 
Mosconi,  Rocco,  and  Carlo  Giannini.  1992.  ‘‘Non-Causality  in Cointegrated  Systems:  Rep- 
resentation,  Estimation  and  Testing.””  Oxford  Bulletin  of Economics  and  Statistics  54:399- 
417. 
Ogaki,  Masao.  1992.  “‘Engel’s  Law and Cointegration.”  Journal of Political  Economy  100:1027- 
46. 

and  Joon  Y.  Park.  1992.  ‘‘A  Cointegration  Approach  to  Estimating  Preference 

Parameters.”  Department  of Economics,  University  of Rochester.  Mimeo. 
Park,  Joon  Y.  1992.  ‘Canonical  Cointegrating  Regressions.”  Econometrica  60:119-43. 

and  Masao  Ogaki.  1991.  “Inference  in  Cointegrated  Models  Using  VAR  Prewhi- 

tening  to  Estimate  Shortrun  Dynamics.”  University  of Rochester.  Mimeo. 

,  8. Ouliaris,  and  B. Choi.  1988.  “Spurious  Regressions  and Tests  for Cointegration.” 

Cornell  University.  Mimeo. 
a Peter C. B.  1987.  “‘Time  Series  Regression  with  a Unit  Root.”  Econometrica  55:277- 

1. 

.  1991.  “Optimal  Inference  in Cointegrated  Systems.”’  Econometrica  59:283-—306. 
and  S.  N.  Durlauf.  1986.  ‘‘Multiple  Time  Series  Regression  with  Integrated  Proc- 

esses.”’  Review  of Economic  Studies  $3:473-95. 

and  Bruce  E. Hansen.  1990.  “Statistical  Inference  in Instrumental  Variables  Regres- 

sion  with  I(1)  Processes.”  Review  of Economic  Studies  57:99-125. 

and  Mico  Loretan.  1991.  “Estimating  Long-Run  Economic  Equilibria.”  Review  of 

Economic  Studies  58:407-36. 

and  S.  Ouliaris.  1990.  ““Asymptotic  Properties  of  Residual  Based  Tests  for  Coin- 

tegration.”  Econometrica  58:165—-93. 
Saikkonen,  Pentti.  1991.  “Asymptotically  Efficient  Estimation  of  Cointegration  Regres- 
sions.””  Econometric  Theory  7:1-21. 
Sims,  Christopher  A.,  James  H.  Stock,  and  Mark  W.  Watson.  1990.  “Inference  in Linear 
Time  Series  Models  with  Some  Unit  Roots.’’  Econometrica  58:113-44. 
Stock, James  H.  1987.  ‘Asymptotic  Properties  of Least  Squares  Estimators  of Cointegrating 
Vectors.”  Econometrica  55:1035-56. 
stile  1990.  “A  Class  of Tests  for  Integration  and  Cointegration.”’  Harvard  University. 

, 

imeo. 

628  Chapter  19  | Cointegration 

\ 

Stock, James H.,  and  Mark  W.  Watson.  1988. “Testing  for Common Trends.”  Journal  of 
the American  Statistical Association  83:1097-1107. 
———  and ———.  1993.  “A  Simple  Estimator  of Cointegrating  Vectors  in Higher  Order 
Integrated  Systems.”  Econometrica  61:783-820. 
Wooldridge,  Jeffrey  M.  1991.  ‘Notes  on  Regression  with  Difference-Stationary  Data.” 
Michigan  State  University.  Mimeo. 

109  1SSMl  yas  sortie  Depts  wis: 8 ee 

; 

Saino wygs  ait MiBisg?  gntergsti63  25  bodirsesb  ad  ozs  bluow 
frelon, 24: Se  eoatss: eigeris  caivey 

sea are Omen 

Bixs 

sa. 

@ 

8} 

oP Bodies" 

ig Dea So  ore  aus 407  1  =  .,0  a0 
i  .. : sat  no  .v  Io atgsursis, Jottio  adit  bas  « vera  td 

& 

ni ec bivow st cen0  to Aw 1 besten t = =  ub.box tilsamion SYER 
Si  Self ke 

basese 

3 

a3 

20  Full-Information 

Maximum  Likelihood 
Analysis 
of Cointegrated  Systems 

. 

. 

An  (n  x  1) vector  y, was  said  to  exhibit  / cointegrating  relations  if there  exist  h 
,  a,  such  that  a;y,  is  stationary.  If such 
. 
linearly  independent  vectors  a,,  a>,  . 
vectors  exist,  their  values  are  not  uniquely  defined,  since  any  linear  combinations 
,  a, would  also  be described  as  cointegrating  vectors.  The  approaches 
of a,,  a>,.  . 
described  in  the  previous  chapter  sidestepped  this  problem  by imposing  normali- 
zation  conditions  such  as  a,,  =  1.  For  this  normalization  we  would  put y,,  on  the 
left  side  of a regression  and  the  other  elements  of y, on  the  right  side.  We  might 
equally well  have  normalized  a,,  =  1 instead,  in which  case y,, would  be the variable 
that  belongs  on  the  left  side  of the  regression.  The  results  obtained  in practice  can 
thus  depend  on  an  essentially  arbitrary  assumption.  Furthermore,  if the  first  var- 
iable  does  not  appear  in  the  cointegrating  relation  at  all  (a,,  =  0),  then  setting 
a;,  =  1 is  not  a  harmless  normalization  but  instead  results  in  a  fundamentally 
misspecified  model. 

For  these  reasons  there  is  some  value  in  using  full-information  maximum 
likelihood  (FIML)  to estimate  the linear  space  spanned  by the cointegrating  vectors 
@,,  a,  ...,  a,.  This  chapter  describes  the  solution  to  this  problem  developed  by 
Johansen  (1988,  1991),  whose  work  is closely  related  to  that  of Ahn  and  Reinsel 
(1990),  and  more  distantly  to that  of Stock  and  Watson  (1988).  Another  advantage 
of FIML  is that  it allows  us  to  test  for the  number  of cointegrating  relations.  The 
approach  of  Phillips  and  Ouliaris  (1990)  described  in  Chapter  19  tested  the  null 
hypothesis  that  there  are  no  cointegrating  relations.  This  chapter  presents  more 
general  tests  of the  null  hypothesis  that  there  are  hy cointegrating  relations,  where 
h, could  be  0,1,...,o0rn  —  1. 

; 

To  develop  these  ideas,  Section  20.1  begins  with  a  discussion  of canonical 
correlation  analysis.  Section  20.2  then  develops  the FIML  estimates,  while  Section 
20.3  describes  hypothesis  testing in cointegrated  systems.  Section  20.4 offers  a brief 
overview  of unit  roots  in time  series  analysis. 

20.1.  Canonical  Correlation 

Population  Canonical  Correlations 
Let the (n,  x  1) vector  y, and the (n, x  1) vector  x, denote  stationary  random 
variables.  Typically,  y, and  x, are  measured  as  deviations  from  their  population 
means,  so  that  E(y,y,;)  represents  the  variance-covariance  matrix  of y,. In general, 
there  might  be  complicated  correlations  among  the  elements  of y, and  x,,  sum- 

630 

marized  by the joint  variance-covariance  matrix 

E(y.yi)  E(y,x;) 

(m,  Xn) 

(7m, Xn) 

5 

Zyy 
(ny ny) 

2 yx 
(n,  X nz) 

E(x,y;)  E(x,x;) 
(m2 X  nj) 

(m2 X  nz) 

2xy 

2 xx 
(m2)  (m2 np) 

‘We  can often  gain  some  insight  into  the  nature  of these  correlations  by defining 
two  new  (n  xX  1) random  vectors,  y, and  &,, where  n is the  smaller  of n,  and  np. 
These  vectors  are  linear  combinations  of y, and  x,,  respectively: 

[20.1.1] 
n,  =  X’'y, 
[20.1.2] 
§,  =  A'x,. 
Here  X’and  st’  are  (n  x  n,) and  (n  X  n,)  matrices,  respectively.  The  matrices 
X’ and  st’  are  chosen  so  that  the  following  conditions  hold. 

One  another: 

(1)  The  individual  elements  of y, have  unit  variance  and  are  uncorrelated  with 
[20.1.3] 
(2) The individual pe of é, fae unit variance ‘im are  uncorrelated  with 

E(nini) = HE yy =  Uy. 

: 

| 

| 
) 

one  another: » 
. 

: 

| 

| EGE) = AZ  axl =  I 

meee 

8 4] 

- @) The ith element  of nh is uncorrelated with the jth element  of &, for i i  + j; for 

:  i =  j, the correlation is positive and i is given by 7;: 
| 

E(Gmni) = A'ExyK  = ninye  sik 

sitttatite Stisd  (A,  . 
x where  a ce  Ae  Ys 
Teter  .. te.)  [11.3 
Ee smtenpret  this  expres 

“po. 1. 5] 
rk SK  bewhre  a8. (.OSi-8 ib  eat  aft es 
(  Shs (2 x  Absinindize < oth 
*  walnantigie  sis adabwsy 
[20.1.6] 

4 

a:  DSO 

a 
« 

F 

; 

s 
—  ey  ee 
hi, 

, 

> 
cars 

. 

a) 

kde. 

te 

ak 
ro 

. 

Shdeas 

3 

)  is ¥ 

= 

“ert  SP  Cee  2 

4 

1 
¢  ot  2 

. 

. 
|) 
ae  ae  Le 
¥ 
oles 
‘ 
a  ee  ee ee 

y  -~ 

i 

eee  | pee  ie  a  .  Sao 

7 

4 

ve  - 
om 

, 

be 

. 

i 

rr 
Te,  fe 

a 

as 

x 

¥ 
; 
we  -i™ 

al 

ee  x"  = 

a. 

nome, 
' 

, 

A 

eee 

; 

normalization  convention  for choosing c and  thus  for determining “‘the’’  eigenvector 
k, to  associate  with  A, is to set  k/k,  =  1. For canonical  correlation  analysis,  however, 
it is more  convenient  to  choose  c  so  as  to  ensure  that 

k}Zyyk;  =  ] 

fori  =  i 2, 

»  ny. 

{20. i 11] 

If  a computer  program  has  calculated  eigenvectors  (k,, C. . de 
giee ) of the  matrix 
in  (20.1. aM normalized  by (k;k;)  =  1, it is trivial  to  change  these to eigenvectors 
k,,) normalized  by the  condition (20.1.11]  by setting 
(k,,  k,,..., 

We  further  may  multiply  k; by  —1  so  as  to  satisfy  a  certain  sign  convention  to  be 
detailed  in  the  paragraphs  following  the  next  proposition. 

k, =  k, +  VK iD yyk;- 

. 

The  canonical  correlations  (7,, 72,  . 

,  7,) turn  out  to  be given  by the  square 
. 
roots  of  the  corresponding  first  n  eigenvalues  (A,  A2,...,  A,) of [20.1.8].  The 
associated  (m,  X  1) eigenvectors  k,,  k,,  ...  ,  k,,  when  normalized  by [20.1.11] 
and  a  sign  convention,  turn  out  to  make  up  the  rows  of the  (nm  X  n,)  matrix  X’' 
appearing  in [20.1.1].  The  matrix  #’ in [20.1.2] can  be obtained  from  the normalized 
eigenvectors  of a  matrix  closely  related  to  [20.1.8].  These  results  are  developed  in 
the  following  proposition,  proved  in  Appendix  20.A  at  the  end  of this  chapter. 

Proposition  20.1: 

Let 

© 

(ny  +n)  x(n,  +72) 

2yx 
=  | (1x)  (Xn) 

2Zyy 

Zyq 

xXx 
(m2)  (m2 Xn) 

be a positive  definite  symmetric  matrix  and  let (A,, A>,...  ,  A,,,) be the  eigenvalues 
of the  matrix  in  [20.1.8],  ordered  4. =>  4,  =--:  =A, n,-  Let  (K,,  k,,...,k,,)  be 
the associated  (n,  X  1) eigenvectors  as  normalized  by (20.1.11].  Let  bios Hass  1% 
M,,,) be the  eigenvalues  of the  (n.  X  nz) matrix 

. 

Lis Dey Dey 

Dvx- 

[20.1.12] 

ordered  p,  =  po  =°°:  =  M,,.  Let  (a,  a.  ...,  a,,)  be  the  eigenvectors  of 
[20.1.12]: 

ZxxlxylyyDyxa;  =  ,8;, 

[20.1.13} 

normalized  by 

[20.1.14] 
Let  n  be the  smaller  of n,  and  n,,  and  collect  the first  n  vectors  k; and  the first n 
vectors  a; in matrices 

fori  =1,2,...,n. 

aj2xxa;=1 

H  =(k,  ky  --: 

(m, Xn) 

k,] 

A  =  [a,  @,5ii9  SP 

a,,]. 

(m2 xn) 

Assuming  that  r,, A2,  . 

. 

. 

,  A, are  distinct,  then 

(a)  05A,<1  fori  =  1,2,...,m,and0=y,<1  forj  =  | i  See 
(6)  A,  =  pw, fori=1,2,...,n; 
(c)  H'YyyH  =1,  and  A'd,xa4  = 
(d)  A'SyyK = R, 

where  R is a diagonal  matrix  whose  squared  diagonal  elements  correspond  to  the 
632  Chapter 20  | Maximum  Likelihood Analysis  of Cointegrated  Systems 

eigenvalues  of (20.1.8): 

A, 

0O 

OAs 

R? 

0 

0 

o:716 

A, 

If 2 denotes  the variance-covariance  matrix  of the  vector  (y’,  x/)',  then  results 
(c) and  (d) are  the  characterization  of the  canonical  correlations  given  in  [20.1.3] 
through  [20.1.5].  Thus,  the proposition  establishes  that  the  squares  of the  canonical 
.  ,r2) can  be found  from  the first  n eigenvalues  of the  matrix 
correlations  (rj,  r3,.  . 
in  [20.1.8].  Result  (b)  states  that  these  are  the  same  as  the  first  n  eigenvalues  of 
the  matrix  in  (20.1.12].  The  matrices  XH  and  &  that  characterize  the  canonical 
variates  in  [20.1.1]  and  [20.1.2]  can  be  found  from  the  normalized  eigenvectors  of 
these  matrices. 

The  magnitude  a; 2x k; calculated  by the  algorithm  described  in Proposition 
20.1  need  not  be positive—the  proposition  only ensures  that  its square  is equal  to 
the  square  of the  corresponding  canonical  correlation.  If a; 2y,yk,;  <  0 for  some  i, 
one  can  replace  k,  as  calculated  with  —k,,  so  that  the  ith  diagonal  element  of R 
will  correspond  to  the  positive  square  root  of 4,. 

. 

As  an  illustration,  suppose  that  y, consists  of a  single  variable  (n,  =  n  =  1). 
In  this  case,  the  matrix  [20.1.8]  is just  a  scalar,  a  (1  x  1) ‘“‘matrix”’  that  is equal 
to its own  eigenvalue.  Thus,  the  squared  population  canonical  correlation  between 
a  scalar  y, and a set  of n, explanatory  variables  x, is given  by 

Pgh  Zyx2xx2xy 

1 

:  Sey 

. 

To  interpret  this  expression,  recall  from  equation  [4.1.15]  that  the  mean  squared 
error  of a  linear  projection  of y, on  x, is given  by 

and  so 

MSE  =  Zyy  rE  Dyxylux  2 xy> 

bet  =  ee 

-1 

Zyy 

Zyy 

(20.1.15] 

Zyy 

Thus,  for  this  simple  case,  r? is  the  fraction  of  the  population  variance  that  is 
explained  by the  linear  projection;  that  is, r2 is the  population  squared  multiple 
correlation  coefficient,  commonly  denoted  R?. 

Another  interpretation  of canonical  correlations  is also  sometimes  helpful. 
The  first  canonical  variates  7,,  and  é,, can  be  interpreted  as  those  linear  combi- 
nations  of y, and  x,,  respectively,  such  that  the  correlation  between  7,  and 1, is 
as  large  as  possible  (see Exercise  20.1).  The  variates  2,  and  ¢5, give  those  linear 
combinations  of y, and  x, that  are  uncorrelated  with  n,,  and  €,, and  yet  yield  the 
largest  remaining  correlation  between  7),  and  é,,  and  so  on. 

, 

Sample  Canonical  Correlations 
The  canonical  correlations  r, calculated  by the  procedure  just described  are 
population parameters—they  are  functions  of the population  moments  Lyy,  Zyx, 
and 3x. Here we  describe  their  sample  analogs,  to be  denoted  me 

umstyé.  batangei 

20.1.  Canonical  Correlation 

633 

Suppose  we  have  a  sample  of  T observations  on  the  (n,  <  1) vector  y,  and 

the  (n,  <  1) vector  x,,  whose  sample  moments  are  given  by 

Lyy  S(U/T)  D> HY 

I fs 

t=1 

Lyx  =O(1/F)  D>) xxi 

T 

t=1 

T. 

B 

e 

Boss  (1/7)  Dea: 

(20.1.16] 

(20.1.17] 

(20.1.18] 

t=1 

Again,  in many  applications,  y, and  x, would  be measured  in deviations  from  their 
sample  means. 

To  calculate  sample  canonical  correlations,  the  objective  is to  generate a set 
of  T observations  on  a  new  (n  x  1) vector  %,, where  n  is the  smaller  of n,  and  np. 
The  vector  %, is a  linear  combination  of the  observed  value  of y,: 

a, =  X'y,, 

(20.1.19] 

for  X an  (n,  X  n) matrix  to  be estimated  from  the  data.  The  task  will  be to choose 
X so  that  the  ith  generated  series  (7;,)  has  unit  sample  variance  and  is  orthogonal 
to  the jth generated  series: 

T 

(1/T),>,,  RA,  =  shes 

| 

) 

+ 

{20.1.20] 

Similarly,  we  will  generate  an  (n  X  1) vector  E, from  the  elements  of x,: 

| 

é, =  A'x,. 

[20.1.21] 

Each  of  the  variables  é, has  unit  sample  variance  and  is  orthogonal  to  é, for 
L#f: 

eG 
(1/T)  > &€;  =  I,. 

t=1 

. 

(20.1.22] 

Finally,  7, is orthogonal  to é, for i # j, while  the  sample  correlation  between 

Hi, and  &,, is called  the  sample  canonical  correlation  coefficient: 

i 

(1/T)  > & a;  =R 

t=1 

: 

[20.1.23] 

for 

ny 

0 
fan  8  Bing” 
Poon 

. 
ih  ae 

(20.1.24] 

Moma 
Finding  matrices  X, sf, and  R satisfying  (20.1.20},  [20.1.2],  and  [20.1.23] 
involves  exactly the same  calculations  as did finding matrices  X, 4, and R satisfying 
[20.1.3]  through  [20.1.5].  For  example,  [20.1.19]  allows  us  to  write  [20.1.20]  as 

28 

pet 

I, =  (1/T) >> yy  =  K'(1/T)  Dy yy,  H =  H'LyyH, 

T 

T 

a 

A 

a 

a 

a 

—[20.1.25] 

where  the  last  line  follows  from  [20.1. 16]. Expression  [20.1.25]  is  identical  to 
634  Chapter 20  | Maximum  Likelihood Analysis of Cointegrated  Systems 

[20.1.3]  with hats placed  over  the  variables.  Similarly,  substituting  [20.1.21]  into 
(20. 1.22]  oa t'2xx  =  I,  which  corresponds  to  [20.1.4].  Equation  [20.1.23] 
becomes  sf'LyyH  = R,  as  in  [20.1.5].  Again,  we  can  replace  k, with  —k,  if any 
of the  elements  of R should  turn  out  negative. 

Thus, to calculate  the  sample  canonical  correlations,  the  procedure  described 
in  Proposition  20.1  is  simply  applied  to  the  sample  moments  7  i  and 
2xx)  rather  than  to  the  population  moments.  In  particular,  the  square  of the  ith 
sample  canonical  correlation  (7?) is given  by the  ith largest  eigenvalue  of the  matrix 

| 

A 

A 

Srilyx2alay  =  jain) 2 vai} jain ps vxi| 

Ls 

a 

r 

a 

A 

t=] 

t=1 

il 
os {ain) +. xxi| {aun > xsi} 

T 

T 

(20. 1.26] 

The  ith  column  of X  is given  by the  eigenvector  associated  with  this  ith  eigenvalue, 
normalized  so  that 

t=1 

t=1 

kf aT)  bY vai fe By 

TF. 

t=]1 

The  ith column  of  of is  given  by the eigenvector  associated  with  the  eigen- 
value  A,  for  the  matrix  2y{2yy2yyZyx  normalized  by  the  condition  that 
4}2  xa;  =  1. 

For  example,  suppose  that  y,  is a  scalar  (n  =  n,  =  1). Then  [20.1.26]  is a 
scalar equal to its own  eigenvalue.  Hence,  the sample  squared canonical  correlation 
between  the  scalar  y, and  a  set  of n,  explanatory  variables  x, is given  by 

ga  —  (To 2y  K HT  2xx)}  YT  Ex,y)} 

{T-1Zy?} 

_  {2y,x,HEx,x/}"  {Ex,y,} 
{Zy?} 

which  is just  the  squared  sample  multiple  correlation  coefficient  R?. 

20.2.  Maximum  Likelihood  Estimation 

We  are  now  in  a  position  to  describe  Johansen’s  approach  (1988,  1991)  to  full- 
information  maximum  likelihood  estimation  of a  system  characterized  by exactly 
_h cointegrating  relations. 

. 

Let  y, denote  an  (n x  1) vector.  The  maintained  hypothesis  is that  y, follows 
‘a VAR(p)  in levels.  Recall  from  equation  [19.1.39]  that  any pth-order  VAR  can 
be written  in the  form 

Ay,  =  G,Ay,-;  +  GAy,-2  +  °° 

+  O-AX-p4i 

(20.2.1) 

+a  +  Coy,-,+  €,, 

with 

E(e,)  =  0 

E(ee!)  = 
.  (ef, 

ty 
0 

forte 
otherwise. 

20.2.  Maximum  Likelihood  Estimation  635 

Suppose  that  each  individual  variable  y;,  is (1),  although  A linear  combinations  of 
y, are  stationary.  We  saw  in equations  [19.1.35]  and  [19.1.40]  that  this  implies  that 
{, can  be  written  in  the  form 

=  —BA’ 

[20.2.2] 

for  B an  (n  X  h) matrix  and  A’  an  (h  X  n) matrix.  That  is, under  the  hypothesis 
of A cointegrating  relations,  only 4 separate  linear  combinations  of the  level  of y,_, 
(the  h elements  of z,_,  =  A’y,-,)  appear  in  [20.2.1]. 

es: 
Consider  a sample  of T +  p observations  on  y,  denoted  C  aeeeee  Gere 
y,).  If the  disturbances  e,  are  Gaussian,  then  the  log likelihood  of  (y,,  y2,.-., 
yr)  conditional  on  (y_p+1,  Y-p+2»--  >»  Yo) is  given  by 

L(Q,  q,  o,  ee 

Sh-1> Qa, Co) 

=  (— Tn/2) log(2m)  —  (T/2) log|Q| 

r 

—  (1/2)  > c —  tAy,-1  —  GAy:-2  —  + 

t=1 

+ 

+  —  Sp-1A¥—p4i  ~—  &  —  So¥s-1)' 

x  ~  (Ay, Fy CiAy,_1  =  CAy,_2  eS 

ie 

oe  a  ae  a 

ca  tor.) 

[20.2.3] 

The  goal is to chose  (Q, €), &,-  - 
to  the  constraint  that  (, can  be  written  in the  form  of [20.2.2]. 

- 

»  $,-1, &,  Go) So  as  to maximize  [20.2.3]  subject 

We  will  first  summarize  Johansen’s  algorithm,  and  then  verify  that  it indeed 

calculates  the  maximum  likelihood  estimates. 

Step  1:  Calculate  Auxiliary  Regressions 

? 

The  first  step  is to  estimate  a  (p—  1)th-order  VAR  for  Ay,;  that  is, regress 
the  scalar  Ay,, on  a constant  and  ail the  elements  of the  vectors  Ay,_,,  Ay,_2,..., 
Ay,_,+1  by OLS.  Collect  the  i =  1, 2,...,m  OLS  regressions  in vector  form  as 

Ay,  =  To  +  I, Ay,-,  +  Il,Ay,_.  +  a 

aks 

© Agius  +  a,, 

[20.2.4] 

where  II, denotes  an  (nm  X  n) matrix  of OLS  coefficient  estimates  and  i, denotes 
the  (n x  1) vector  of OLS  residuals.  We  also  estimate  a second  battery  of regres- 
sions,  regressing  the  scalar  y,,_,  On  a  constant  and  Ay,_,,  Ay,_2,  ..  -.,  AY;-p41 
fori  =  1,2,...,.n.  Write  this  second  set  of OLS  regressions  as! 

y,-1  =  6 +  Ridy,_,  +  Rody,2  +  +++  +  Ry-Ay,-par  +  %, 

[20.2.5] 

with  ¥, the  (n  x  1) vector  of residuals  from  this  second  battery  of regressions. 

‘Johansen  (1991) described  his procedure  as calculating ¥, in place of #,, where  ¥, is the OLS residual 
from  a  regression  of y,_,  on  a constant  and  Ay,_,,  Ay,.,...  , Ay;-p+1-  Since  y,_,  =  y,-,  —  Ay,  — 
Ay,..  —  ++:  —  Ay,_,,1,  the  residual  ¥, is numerically  identical  to 9, described  in the  text. 

636  Chapter 20  | Maximum  Likelihood  Analysis  of Cointegrated  Systems 

Step 2: Calculate  Canonical  Correlations 

;  bes calculate  the  sample  variance-covariance  matrices  of the  OLS  residuals 

t an 

vw  =  (1/T)  > 9,9! 

’ 

t=1 

Luu  =  (1/T)  > 0,0; 

4 

Luv =  (1/T)  > 4,9; 

T 

t=1 

T 

t=1 

[20.2.6] 

[20.2.7] 

[20.2.8] 

From  these,  find  the  eigenvalues  of the  matrix 

» ae =  > Le 

pe ae)  a 

(20.2.9] 

_  with  the  eigenvalues ordered  A, >  A, >  +--+  >A,.  The  maximum  value  attained 
_  by the log likelihood  function subject to the constraint  that there are h cointegrating 

relations is en bi 
eget 
$AFIRAAP  the consicoe ~  (172) & oat =  Ape’ 

EA =e (THRY lot?) ~  (Ini2) ~  (7?) loxlbual  (202.10) 

Gr... 

3:  Cale  af  2  Me , 

ei 

42 ahs ebab ret ig  ap 

5 

The  MLE  of 

is 

=  (1/T)  > [(@,  —  &9,)(a,  —  60%)’. 

T 

n 

(20.2.15] 

We  now  review  the  logic  behind  each  of these  steps  in  turn. 

Motivation  for Auxiliary  Regressions 

The  first  step  involves  concentrating  the  likelihood  function.*  This  means 
taking  ©  and  (, as  given  and  maximizing  [20.2.3]  with  respect  to  (a,  0, &,..-, 
{,-1)-  This  restricted  maximization  problem  takes  the  form  of seemingly  unrelated 
regressions  of the  elements  of the  (n  x  1) vector  Ay,  —  Coy,-;  On  a  constant  and 
the  explanatory  variables  (Ay,_;,  Ay,_>,-  -  - » AY;-p+1)-  Since  each  of the  n  regres- 
sions  in this  system  has  the  identical  explanatory variables,  the  estimates  of (a, {,, 
.-.,  §-1)  would  come  from  OLS  regressions  of  each  of  the  elements  of 
(, 
»  AY,-p+1)-  Denote  the  values 
Ay,  —  Coy,-;  On  a  constant  and  (Ay,_,,  Ay,_2,  .- 
of (a, f,, &,  . 

,  ¢,-1)  that  maximize  [20.2.3]  for  a  given  value  of , by 
- 
- 

[&*(Co),  EF(Co),  EF (Lo),  - 

»  SF-1(Lo)). 

- 

. 

- 

These  values  are  characterized  by the  condition  that  the  following  residual  vector 
must  have  sample  mean  zero  and  be  orthogonal  to  Ay,_,,  Ay,-2,  - 

,  Ay,-p41: 

. 

. 

[Ay; — Su)  —  {a°(bo) at e (Co)Ay,-1  + Sistas 

| 

/[20.2.16] 

ee  ES. i(Co)Ay,  meets 

But  notice  that  the  OLS  residuals  a, in [20.2.4]  and  ¥, in  [20.2.5]  each  satisfy  this 
orthogonality  requirement,  and therefore  the vector  u,  —  CoV, also  has sample  mean 
zero  and  is orthogonal  to  Ay,_,,  Ay,-2,  . 
,  AY,-p+1-  Moreover,  a, —  CoV, is of 
the  form  of expression  [20.2.16], 

. 

. 

a, —  CoV,  =  (Ay,  —  to  -  II, Ay,_, Si ILAy,_» 

Ue 

Fal  Bi  Api 

ee  O  —  My,  tae, 25 

> AK-ye we 

with 

&*(Co)  =  to  —  C8 
Cio)  =  WH, —  GX; 
Thus,  the  vector  in [20.2.16]  is given  by a, —  [,9,. 

‘for’  =  1,2)...  2, 

p  =  1. 

[20.2.17] 
[20.2.18] 

The  concentrated  ne likelihood  function  (to  be  denoted  M) is found - 

yar (a,  G1,  &,  -- 

:  Sp i) in  [20.2.3]  with  [a&*(Co),  tt (Go),  re (Co), - 

M(M,  Go) =  L{O,  EF(Lo), EF(Lo),  - 

- 

-  »  EF -s(Go),  &*(Lo),  Lo} 

=  —(Tn/2)  log(27)  —  (7/2)  log|Q| 

[20.2.19} 

~  (12)  2 [(@, ~  o8,)'A-"(@,  —  &o8,)]. 

The  idea  behind  concentrating  the  likelihood  function in  this  way  is that  if we  can 
find  the  values  of 
and {> for  which  M  is maximized,  then  these  same  values 
(along  with  &*(£,)  and  {*({,))  will  maximize  [20.2.3]. 

'  *See  Koopmans  and  Hood  (1953,  pp.  156-58)  for more  background  on  concentration  of likelihood 
functions. 

638  Chapter 20  | Maximum  Likelihood  Analysis  of Cointegrated  Systems 

Continuing  the  concentration  one  step  further,  recall  from  the  analysis  of 
i giv ms es the  value  of 2 that maximizes  [20.2.19]  (still  regarding  (> as  fixed) 

ven  by 

2* (£0) =  wns, [(a,—  Co¥,) (4,  —  Co9,)'. 

[20.2.2] 

As  in expression [11.1.32],  the  value  obtained  for  [20.2.19]  when  evaluated  at 
{20.2.20] is then 
. 

N(Eo) =  M{Q* (Co), Co} 

=  ~—(TIn/2)  log(2m)  —  (T/2) log|{2*(£.)|  —  (Tn/2) 
=  —(Tn/2)  log(2m)  —  (Tn/2) 

(20.2.21] 

~  (7/2) log} (1/7) & [(@, —  ot)(G,  ~  £o8.)']}. 

z 

Expression  [20. 2.21} represents  the biggest  value  one  can  achicis for the log 
likelihood for any given value  of Co. Maximizing the likelihood  function  thus comes 
Hows in chosing Bo st'as to  euininize 
at:  ots  rot ieuDiess  sit  ytecd?  giggni  425  ac  f 
gn 9c? magh CLARY >, (8 ~ 69908 - 1) 
aoigeniges 

ibn  evagiol 

inl} TAZ 7A, 

xiviem  ot St  sSttr sey" A  ye surgeon  « 

subject tothe constraint of (20.2.2). 

oc 

TRIE  inaogs ba  30  iblayduelt  ¢ 

je sbsaen sk & ee seu  Gl bavoiniess 938  3  jedi  s2eqque  We 

SIsanw  e233  =  ee 

| 

7 

cians  Raa  an bbe  ge:  mui a 

a 
Ati  pe  ane 

esi 

nS + = + y = aot 

vba he  eee 

hak See  ‘ 

» 

hs 

% 

subject  to  the  constraint  that  Lot, could  make  use  of only  / linear  combinations  of 
E,. If there  were  no  restrictions  on  { (so  that2  =  n),  then  expression  [20.2.27] 
would  be minimized  by OLS  regressions  of 7,,0n  €, fori  =  1,2,..  .  ,n.  Conditions 
([20.2.24]  and  [20.2.25]  establish  that  the  ith  regression  would  have  an  estimated 
coefficient  vector  of 

{any > bé:| jury > in| =  fe, 

where  e, denotes  the  ith  column  of I,,. Thus,  even  if all  n  elements  of €, appeared 
in  the  regression,  only  the  ith  element  &, would  have  a  nonzero  coefficient  in  the 
regression  used  to  explain  7,,.  The  average  squared  residual  for  this  regression 
would  be 

(1/T) . cau} “  {arn ya ado} an > eo} Jain > éaa| 

=  1  -  r-e;'l,-e;7; 
py.  Se  Ps- 

Moreover,  conditions  [20.2.23]  through  [20.2.25]  imply  that  the  residual  for  the  ith 
regression,  is — Fi€_, Would  be orthogonal  to  the  residual  from  the jth regression, 
Ne  —  7; Bes for i # j. Thus,  if (, were  unrestricted,  the  optimal  value  for  the  matrix 
in (20.2.27]  would  be a diagonal  matrix  with  (1 —  r?) in  the  row  i, column i position 
and  zero  elsewhere. 
4 

Now  suppose  that  we  are  restricted  to  use  only  h linear  combinations  of &, 
as  regressors.  From  the  preceding  analysis,  we  might  guess  that  the  best  we  can 
do  is use  the  h elements  of &, that have  the  highest  correlations  with  elements  of 
H,, that is, choose  (é,,, E399., 
»  En) as  regressors.*  When  this  set  of regressors is 
used  to  explain  Nir for  i =  h, “the average  squared  residual  will  be  (1  —  7?),  as  - 
before.  When  this  set  of  regressors  is  used  to  explain  7;,  for  i >  h,  all  of  the 
regressors  are  orthogonal  to  7; and  would  receive  regression  coefficients  of zero. 
The  ch tg  squared  residual  for  the  latter  regression  is simply  (V/T) 27, ing  =  1 
ford  =e  +..1,  f+,  2, 
,  n.  Thus,  if we  are  restricted  to  using  only  A linear 
combinations  of é, the  optimized  value  of [20.2.27]  will  be 

- 

(1/7) > [Ge  —  BENA,  —  58") 

PA 
Oo 

Perg 
TEA 

Herrpeg 

II 

(om) ee 

bs  0  ++. 

0 

(20.2.28] 

S 

0 

— 

i  _—ere 

Co 

ove 

0 

G  ac. 

@ 

h 

I] a  -  #). 

*See  Johansen  (1988)  for a more  formal  demonstration  of this  claim. 

640  Chapter 20  | Maximum  Likelihood  Analysis  of Cointegrated  Systems 

Of  course,  the actual  data  @, and  9, will  not  be  in  exact  canonical  form. 
However,  the  previous  section  described  how  to  find  (n  x  n) matrices  XH and  4 
such  that 

a, =  K'A, 
(20.2.29] 
(20.2.30} 
E, =  of'¥,. 
The columns of 4 are given by the eigenvectors of the matrix  in [20.2.9],  normalized 
by the  condition  ’S\\s¢  =  I,.  The  eigenvalues  of  [20.2.9]  give  the  squares  of 
the  canonical  correlations: 

ON  29 
A, =  7?. 
The columns  of  % correspond  to  the  normalized  eigenvectors  of  the  matrix 
Zw Zuv2vvZvy,  though it turns  out  that  X does  not  actually  have to be calculated 
in order  to  use  the  following  results.  Assuming  that  X  and  of  are  nonsingular, 
[20.2.29] and  [20.2.30]  allow  (20.2.22]  to  be written 

Z 

lam 2 a, o  bo¥,)(G,  -  wy" 

b=  5 

a 

‘ 

;* 

feu  ot 

‘ Sri 

= i  .  pes?  Hee  aah 

HK”  (/T)  & [ta —  H'C(s')=!  aly  ~  Seeley | pid 

wat  xe 

. 

ye 

4  vie 

7 Pl  vetoed  on:  CEFi  I  : 

aie: 

; 

t= 

mes 

rar’ 

>  weet 5  are 

WG  MESES  ATE, 

te 

7 

a 

| 

ae  4 

one 

1 

: 

4 

. 

SVULECT  Ss 
d 
ty  ie 

« 

: 

- 

ia 

Pi 
} 
‘we 
Se  ath  errs 
ke  rt 
7 
hs 

os 

‘ 

; 

c 

1 
!  aa 
a 
a 
teri  de 

—— 
i: 
= 
= 
¥ 

: 
aie 

~ 

nat 

bia 

4 

- 
jen° 

- 
: 

‘ 

ft 
— 
s" 

a 

ma 

f 

ral 

_ 
J 

: 

’ 
rhss 

Pe 
See 

Pad 
eee, 
+o! 
(he 
]  ae 
Se  eae |e 

. 

oe 

hm 
Af 

fi 
elie  Suis 

: 

Baye 

: 

i 

’ 
f 
lah 
ye 

Gi 

; 
rt. 

) 

ie 

a 

5 

: 

Sin 

; 
: 
Pie 
ae 

te 

FJ 

7 

= 
‘ 
cet 
St 

ote 
s  Sten  esd  ’ 

é 
ant 
$k}  SER?  IH 
Siaimiies 

_* 

: 

® 

= 

"  ae  —  i 

4 

» 

4 

* 

fl 

= 
ae 

ef 

A 

1 

a. . 2 

coher  ey 

oo 

; 
,  7 

j 

j 

a 

! 

,= 
> 

7 

* 

Taking  determinants  of  both  sides  of [20.2.25]  establishes 

1G  ..  1K" [Zuul.|% 

or 

INK  =  [Zuul- 
Substituting  this  back  into  [20.2.34],  it appears  that  the  optimized  value  of [20.2.32] 
is equal  to 

h 

[Ein  x  I] (1  —  7?). 

Comparing  [20.2.32]  with  [20.2.21],  it  follows  that  the  maximum  value  achieved 
for  the  log likelihood  function  is given  by 

L*  =  N(&)  =  —(Tn/2)  log(2m)  —  (Tn/2)  —  (T/2) os} (Seu x  I (1 -  at, 

as  claimed  in  [20.2.10]. 

Motivation  for Maximum  Likelihood  Estimates 
of Parameters 

We  have  seen  that  the  concentrated  log likelihood  function  [20.2.21] is  max- 
imized  by  selecting  as  regressors  the  first  h  elements  of  E.. Since  —, = 4'¥V,, 
this  means  using  A’¥, as  regressors,  where  the  (n  x  h) matrix  A denotes  the  first 
h columns  of the  (n  x  n) matrix  &. Thus, 

Gov,  =  —BA'Y, 

[20.2.36] 

for some  (n X  h) matrix  B. This  verifies  the claim  that  A is the  maximum  likelihood 
estimate  of a  basis  for the  space  of cointegrating  vectors. 

Given  that  we  want  to  choose  w, =  A’¥,  as  regressors,  the  value  of  B  for 
which  the  concentrated  likelihood  function  will  be  maximized  is  obtained  from 
OLS  regressions  of 8, on  W,: 

~—  jar ra ai || aur Sy ai  | 

{20.2.37] 

t=1 

But  w, is composed  of A canonical  variates,  meaning  that 

T 

Jam x wi  | =  I,. 

Moreover, 

Jan ae a; | =  jam 2 asia 

fi 

3 

144 

ig 

ay 

[20.2.38] 

[20.2.39] 

Substituting  (20.2.39]  and [20.2.38]  into  [20.2.37], 

=  SwA. 

B _  —LuvAj 

and  so,  from  [20.2.2],  the  maximum  likelihood  estimate  of & is given  by 

as  Claimed  in [20.2.12]. 

° 

| 

4 ar LuvAA’ 

642  Chapter 20  | Maximum  Likelihood Analysis  of Cointegrated  Systems 

Expressions  (20.2.17]  and  [20.2.18]  gave  the  values  of a and  (, that  maximized 
the  likelihood  function  for  any  given  value  of  C,.  Since  the  likelihood  function  1s 
maximized  with  respect  to  Co by choosing  t, according  to  [20.2.12],  it is maximized 
with  respect  to  @  and  (; by substituting  : into  [20.2.17]  and  [20.2.18],  as  claimed 
in  [20.2.14]  and  [20.2.13].  Finally,  substituting  {, into  [20.2.20]  verifies  [20.2.15]. 

Maximum  Likelihood  Estimation  in  the  Absence 
of Deterministic  Time  Trends 

The  preceding  analysis  assumed  that  a,  the  (n  x  1) vector  of constant  terms 
in the  VAR,  was  unrestricted.  The  value  of 
contributes  / constant  terms  for  the 
h cointegrating  relations,  along  with  g =  n  —  h deterministic  time  trends  that  are 
common  to each  of the n elements  of y,.  In some  applications  it might  be of interest 
to  allow  constant  terms  in the  cointegrating  relations  but  to  rule  out  deterministic 
time  trends  for  any  of the  variables.  We  saw  in equation  [19.1.45]  that  this  would 
require 

a  =  But, 

(20.2.40] 

where B is the  (mn  x  A) matrix  appearing  in [20.2.2]  while  pj  is an  (A  x  1) vector 
corresponding  to  the  unconditional  mean  of  z,  =  A’y,.  Thus,  for  this  restricted 
case,  we  want  to  estimate  only  the  A elements  of xf rather  than  all  n  elements  of 
a. 

To  maximize  the  likelihood  function  subject  to  the  restrictions  that  there  are 
h cointegrating  relations  and  no  deterministic  time  trends  in  any  of  the  series, 
Johansen’s  (1991)  first  step  was  to  concentrate  out  ¢,,,...,  and  ¢,_,  (but not 
a).  For  given  a  and  (po, this  is achieved  by OLS  regression  of (Ay,  —  a  —  Coy,-,) 
on  (Ay,_,,  Ay,-2,-  - 
,  Ay,-»41).  The  residuals  from  this  regression  are  related  to 
the  residuals  from  three  separate  regressions: 

. 

(1)  A regression  of Ay, on  (Ay,_,,  Ay,_2,  . 

.  - Ay,—p+1)  with  no  constant  term, 
‘Ay,  =  Wi,Ay,_,  +  MW,Ay,_,  +  ---  +  1,_,Ay,_p41  +  G5  [20.2.41] 

(2)  A  regression  of a constant  term  on  (Ay,_,,  Ay,-2,  - 

- 

- 

»  AY;-p+1), 

1 =  @jAy,_,  +  @3Ay,-.  +  ++ 

+  @,_jAY,-p4it  W,; 

 [20.2.42] 

(3)  A-regression  of y,_,  on  (Ay,_,,  Ay,-z,-  . 
Y,-1  =  RyAy,_1  +  RpAy,-2  +  °° 

. 

,  Ay,-,+1)  with  no  constant  term, 
+  Rp-Ay-par  +  %.  — [20.2.43] 

The  concentrated  log likelihood  function  is then 
MQ, 01,  bo)  = —(Tn/2)  log(2m)  —  (TI2) log|| 

-  (1/2) 5 [(G,—  aw,  —  Cov,)'Q-'(a,  —  aw,  —  Cov,))- 

Further  concentrating  out  2 results  in 

N(x, Go) 

:

=  —(Tn/2)  log(27)  —  (Tn/2)  © 

; 

[20.2.44] 

~,(T/2)  oe| wn}  i  aw,  a  Cov,)(a,  —  aw,  —  wiy'} 

Imposing  the constraints  «  =  Buy  andl,  = 

—  BA’,  the  magnitude  in  [20.2.44] 

sreiayd 

byyasn 

20.2.  Maximum  Likelihood  Estimation 

643 

 
can  be  written 

N(a,  Co)  =  —(Tn/2)  log(2m)  -  (Tn/2) 

~  (T/2)  log|  > (1/T){(a,  +  BA'w,)(a,  +  BA'w,)’}}, 

T 

t=1 

where 

math 

= 

Ww, 
(n+1)x1 

~ 
V, 
A’  ={-pf  A’). 

[20.2.45] 

(20.2.46] 

hx  (n+1) 

But  setting  {)  =  —BA’  in  [20.2.21}  produces  an  expression  of  exactly  the  same 
form  as  [20.2.45],  with  A  in [20. 2.21]  replaced  by A and  #, replaced  by w,.  Thus, 
the  restricted  log likelihood is  maximized  simply  by replacing v, in the  analysis of 
(20.2.21]  with  w,. 

To  summarize,  construct 

M < = 

= eo  Ms = ,  = 

~ "  -_ 

M ro e  | 

=~ — “i a 

— M- iT) oe 

~ Hl  - 

uw  =  (UT) 

iMs £ 
i}  -_ 

and  find  the  eigenvalues  of the  (n  +  1)  X  (nm  +  1) matrix 

pet  RP ee  aad 

- 

[20.2.47} 

ordered  A, >  A, >- - - 
>A,,4,.  The  maximum  value  achieved  for the log likelihood 
function  subject  to ite constraint  that  there  are  h cointeprating  relations  and  no 
deterministic  time  trends is 

L, =  —(Tn/2)  log(2m)  —  (Tni2)  —  (Ti2) loglEuul 

—_—_—{20.2.48} 

~  (T/2)  ) log(1  —  X;). 

Let  a,,4,  ...,  4,4,  denote  the  eigenvectors  of  [20.2. 47)  normalized  by 
4/Dwwa,  =  1. Then  the  maximum  likelihood  estimate  of A is given by the  matrix 
[a,  a,  --- 

4,].  The  maximum  likelihood  estimate  of BA’ is 

BA’  =  ~SywAA', 

[20.2.49] 

Recall  from  [20.2.46]  that 

BA’  =  [—Bp;  BA’) 
—a,. 
=  Gl. 
Thus,  [20.2.49]  implies  that  the  maximum  likelihood  estimates  of a  and  Co are 
given  by 

[20.2.50] 

—— 

The  MLE  of {; is 

[&  Co] =  LuwAA’. 

%  =  M, -  a)  -  G8, 

fori  =  1,2,...,p -1, 

644  Chapter  20  | Maximum  Likelihood  Analysis  of Cointegrated  Systems 

while  the  MLE  of 2 is 

r 

>  (1/T) 2 [(a,  =  aw,  2  Co¥,)(a,  —  aw,  —  Co¥,)'. 

20.3.  Hypothesis  Testing 
We saw  in the previous  chapter that  tests  of the  null  hypothesis  of no  cointegration 
typically  involve  nonstandard  asymptotic  distributions,  while  tests  about  the  value 
of the  cointegrating  vector  under  the  maintained  hypothesis  that  cointegration  is 
present  will  have  asymptotic  y? distributions,  provided  that  suitable  allowance  is 
made  for  the  serial  correlation  in  the  data.  These  results  generalize  to  FIML 
analysis.  The  asymptotic  distribution  of a  test  of the  number-of  cointegrating  re- 
lations  is nonstandard,  but  tests  about  the cointegrating  vector  are  often  y?. 

Testing the  Null  Hypothesis of h Cointegrating  Faalasions 
Suppose  that  an (n x  1) vector  y, can be charattelied by a VAR(p) in  levels, sf 

' 

which we write  in the  form  of [20.2.1]: 

Ay, = >  €, Ay,_ 1  7. ‘GAy,- 2  * 

ea  a OY, —p  +t  ME 

boy,  ie E,. 

(20.3. 1]  ; 

Under the null hypothesis  H, that there  are  exactly / cointegrating  relations among 
the elements of y,, this  VAR is restricted  by the  requirement  that  C, can  be written 
yok  dire 
—BA’, for B an (n  x  h) matrix and  A’ an (h  x  n) matrix.  | 
this restriction  is that only h/ linear combinations  of the 
ssions in (20.3. 1]. The largest value that can ; 
achie /ec  for the log likelihood fanction ii this: tion was sii hs 

y,.  earl be  used in the r 

other 
levels  of 

of  describing 

way 

= 

“9 ; y ae rmiBiy, sets OF 2INVOHTE  <! o  *  ¥ enODaISY 
er 4 “Cnt 

Ri rn) = “i shal  is 

Wiis  ts 

2) al  Gs )  (  pba: SH ae 203.2) 
Oe.  tee Pe oc  seis gi rosin  heysiqns  eob; 

. 

2 

we  ee  Panes 

to  be  asymptotically  distributed  as  y?.  In  the  case  of Ho,  however,  the  hypothesis 
involves  the  coefficient  on  y,_;,  which,  from  the  Stock-Watson  common  trends 
representation,  depends  on  the  value  of g =  (n  —  h) separate  random  walks.  Let 
W(r)  be  g-dimensional  standard  Brownian  motion.  Suppose  that  the  true  value  of 
the  constant  term  e  in  [20.3.1]  is zero,  meaning  that  there  is  no  intercept  in  any 
of the  cointegrating  relations  and  no  deterministic  time  trend  in any  of the  elements 
of y,.  Suppose  further  that  no  constant  term  is included  in the  auxiliary  regressions 
[20.2.4]  and  [20.2.5]  that  were  used  to  construct  a, and  ¥,. Johansen  (1988)  showed 
that  under  these  conditions  the  asymptotic  distribution  of  the  statistic  in  [20.3.4] 
is the  same  as  that  of  the  trace  of the  following  matrix: 

Q=  If W(r)  awe  | If W(r)W(r)'  ir| If W(r)  awory  | [20.3.5] 

-1 

Percentiles  for  the  trace  of the  matrix  in [20.3.5]  are  reported  in the  case  1 portion 
of Table  B.10.  These  are  based  on  Monte  Carlo  simulations. 

If  the  number  of  cointegrating  relations  (h)  is  1  less  than  the  number  of 

variables  (n),  then  g  =  1 and  [20.3.5]  describes  the  following  scalar: 

1 

2 

2 

{f W(r) amo |  ay  (wDP -  | 

—  ee  Sc  SEK  al  oe  oS  ae 

{fmora}  —  {f rmar ar} | 

[20.3.6] 

where  the  second  equality  follows  from  [18.1.15].  Expression  [20.3.6]  will  be  rec- 
ognized  as  the  square  of the  statistic  [17.4.12]  that  described  the  asymptotic  dis- 
tribution  of  the  Dickey-Fuller  test  based  on  the  OLS ¢ statistic.  For  example,  if 
we  are  considering  an  autoregression  involving  a  single  variable  (nm  =  1), the  null 
hypothesis  of no  cointegrating  relations  (4  =  0) amounts  to  the  claim  that  ¢)  =  0 
in [20.3.1]  or  that  Ay, follows  an  AR(p  —  1) process.  Thus,  Johansen’s  procedure 
provides  an  alternative  approach to  testing  for  unit  roots  in  univariate  series,  an 
idea  explored  further  in Exercise  20.4. 

| 

Another  approach  would  be  to  test  the  null  hypothesis  of  / cointegrating 
relations  against  the  alternative  of  h +  1 cointegrating  relations.  Twice  the  log 
likelihood  ratio  for  this  case  is given  by  _ 

AL%  —  L8)  =  =Tlog(i  -  4,1): 

eee 

i) 

Again,  under  the  assumption  that  the  true  value  of a  =  0 and  that  no  constant 
term  is included  in [20.2.4]  or  [20.2.5],  the  asymptotic  distribution  of the  statistic 
in [20.3.7]  is the  same  as  that  of the largest  eigenvalue  of the  matrix  Q defined  in 
[20.3.5].  Monte  Carlo  estimates  of this distribution  are  reported  in the case  1 section 
of Table  B.11. 

Note  that  if  g =  1, thenn  =  h  +  1.  In this  case  the  statistics  [20.3.4]  and 
[20.3.7]  are  identical.  For  this  reason,  the  first  row  in Table  B.10  is the  same  as 
the  first  row  of  Table  B.11. 

Typically,  the  cointegrating  relations  could  include  nonzero  intercepts,  in 
which  case  we  would  want  to include  constants  in the auxiliary  regressions  [20.2.4] 
and  [20.2.5].  As  one  might  guess  from  the  analysis  in Chapter  18, the  asymptotic 
distribution  in  this  case  depends  on  whether  or  not  any  of the  series  exhibit  de- 
terministic  time  trends.  Suppose  that  the  true  value  of @  is such  that  there  are  no 
deterministic  trends  in any  of the series,  so  that  the  true  a  satisfies 
a  =  But  as 
in {20.2.40].  Assuming  that  no  restrictions  are  imposed  on  the constant  term  in the 

646  Chapter 20  | Maximum  Likelihood  Analysis  of Cointegrated  Systems 

estimation  of  the  auxiliary  regressions  {20.2.4}  and  [20.2.5],  then  the  asymptotic 
distribution  of [20.3.4]  is  given  in  the  case  2  section  of  Table  B.10,  while  the 
asymptotic  distribution  of [20.3.7]  is given  in  the  case  2 panel  of  Table  B.11.  By 
contrast,  if  any  of  the  variables  exhibit  deterministic  time  trends  (one  or  more 
elements  of  a  —  By*  are  nonzero),  then  the  asymptotic  distribution  of [20.3.4]  is 
that of the variable  in  the  case  3  section  of  Table  B.10,  while  the  asymptotic 
distribution  of [20.3.7]  is given  in  the  case  3 section  of Table  B.11. 

When §  =  landa@  #  By},  the  single  random  walk  that  is common  to  y,  is 
dominated  by a  deterministic  time  trend.  In  this  situation,  Johansen  and  Juselius 
(1990,  p. 180) noted  that  the  case  3 analog  of [20.3.6]  has  a  y?(1)  distribution,  for 
reasons  similar  to  those  noted  by West  (1988)  and  discussed  in  Chapter  18.  The 
modest  differences  between  the  first  row  of the  case  3 part  of Table  B.10  or  B.11 
and  the  first  row  of  Table  B.2  are  presumably  due  to  sampling  error  implicit  in 
the  Monte  Carlo  procedure  used  to  generate  the  values  in Tables  B.10  and  B.11. 

Application  to  Exchange  Rate  Data 

Consider  for  illustration  the  monthly  data  for  Italy  and  the  United  States 
plotted  in  Figure  19.2.  The  systems  of  equations  in  [20.2.4]  and  [20.2.5]  were 
estimated  by OLS  for  y,  =  (p,, 5,, p;)’,  where  p, is 100  times  the  log of the  U.S. 
price  level,  s, is  100  times  the  log of the  dollar-lira  exchange  rate,  and  p*  is  100 
times  the  log of  the  Italian  price  level.  The  regressions  were  estimated  over  ¢  = 
1974:2  through  1989:10  (so that  the  number  of observations  used  for  estimation 
was  T =  189); p  =  12  lags were  assumed  for  the  VAR in  levels. 

The  sample  variance-covariance  matrices  for  the  residuals  a, and  ¥, were 

calculated  from  [20.2.6]  through  [20.2.8]  to  be 

M =  T 

0.0435114 
—0.0316283 
0.0154297 
427.366 
Evy  =  | —370.699 
805.812 
[  —0.484857 
~1.81401 
~  1.80836 

M =  ] 
; 

 —0.0316283 
4.68650 
0.0319877 

0.0154297 
 0.0319877 
0.179927 

-370.699 
424.083 
—709.036 

0.498758 
-2.95927 
1.46897 

805.812 
709.036 
1525.45 
—0.837701 
—2.46896 
—  3.58991 

The  eigenvalues  of the  matrix  in [20.2.9]  are  then* 

A 
A;  =  0.1105 

A, =  0.05603 
‘A; =  0.03039 

with 

T log(1  —  A,) =  —22.12 
T log(1  —  4) =  -  10.90 
T log(1  —  43) =  —5.83. 
4Calculations were based on  more  significant  digits than  reported,  and  so  the reader  may  find slight 

discrepancies  in trying to reproduce  these  results  from  the  figures  reported. 

rise. ol A  (: 

| 

Eh! 

20.3.  Hypothesis  Testing 

647 

The  likelihood  ratio  test  of  the  null  hypothesis  of  h  =  0 cointegrating  relations 
against  the  alternative  of  h  =  3 cointegrating  relations  is  then  calculated  from 
[20.3.4]  to  be 

2(£4  —  Lo)  =  22.12  +  10.90  +  5.83  =  38.85. 

[20.3.8] 

Here  the  number  of unit  roots  under  the  null  hypothesis  is  g =  n  —  h =  3.  Given 
the  evidence  of deterministic  time  trends,  the  magnitude  in [20.3.8]  is to  be  com- 
pared  with  the  case  3 section  of Table  B.10.  Since  38.85  >  29.5,  the  null  hypothesis 
of no  cointegration  is rejected  at  the  5%  level.  Similarly,  the  likelihood  ratio  test 
[20.3.7]  of  the  null  hypothesis  of  no  cointegrating  relations  (h  =  0) against  the 
alternative  of a  single  cointegrating  relation  (h =  1) is given  by 22.12.  Comparing 
this  with  the  case  3 section  of Table  B.11,  we  see  that  22.12  >  20.8,  so  that  the 
null  hypothesis  of no  cointegration  is also  rejected  by this  test. 

This  differs  from  the  conclusion  ofthe  Phillips-Ouliaris  test  for  no  cointe- 
gration  between  these  series,  on  the  basis  of  which  the  null  hypothesis  of  no 
cointegration  for  these  variables  was  found  to  be  accepted  in Chapter  19. 

Searching  for  evidence  of a  possible  second  cointegrating  relation,  consider 
the  likelihood  ratio  test  of  the  null  hypothesis  ef  h  =  1 cointegrating  relation 
against  the  alternative  of  h =  3 cointegrating  relations: 

A£%  —  £%)  =  10.90  +  5.83  =  16.73. 

For this test,  g =  2. Since  16.73  >  15.2,  the  null  hypothesis  of a single  cointegrating 
relation  is rejected  at  the  5%  level.  The  likelihood  ratio  test  of the  null  hypothesis 
of h  =  1 cointegrating  relation  against  the  alternative  of h  =  2 relations  is 10.90 
_  <  14.0;  hence,  the two  tests  offer conflicting  evidence  as to the presence  of a second 

cointegrating  relation. 

The  eigenvector  4, of the  matrix  in  [20.2.9]  associated  with  A,, normalized 

so  that  4; 2 yya,  =  1, is given  by 

4;  =  [-90.7579  0.02801 

0.4220). 

[20.3.9] 

It is natural  to  renormalize  this  by taking  the  first  element  to  be  unity: 

a;  =  [1.00 

-0.04 

-0.56]. 

This  is virtually  identical  to  the  estimate  of the  cointegrating  vector  based  on  OLS 
from  [19.2.49]. 

Likelihood  Ratio  Tests  About  the  Cointegrating  Vector 
Consider  a  system  of n  variables  that  is assumed  (under  both  the  null  and 
the  alternative)  to  be  characterized  by h cointegrating  relations.  We  might  then 
want  to  test  a restriction  on  these  cointegrating  vectors,  such  as  that  only g of the 
variables  are  involved  in  the  cointegrating  relations.  For  example,  we  might  be 
interested  in whether  the  middle  coefficient  in [20.3.9]  is zero,  that  is, in whether 
the cointegrating  relation  involves  solely  the  U.S.  and  Italian  price  levels.  For this 
example  h  =  1, q  =  2, and  nm  =  3.  In general  it must  be the  case  that  h <  q <n. 
Since  h linear combinations  of the q variables  included  in the cointegrating relations 
are  stationary,  if  gq =  h, then  all  g of  the  included  variables  would  have  to  be 
stationary  in levels.  If  g =  n, then  the null  hypothesis  places no  restrictions  on  the 
cointegrating  relations. 

| 

) 

648  Chapter 20  | Maximum  Likelihood  Analysis  of Cointegrated  Systems  — 

a 

Consider  the general  restriction  that  there is  a known  (q X  n) matrix  D’ such 

that  the  cointegrating  relations  involve  only  D’y,.  For  the  preceding  example, 

D'  = 

10  0 
* 0  7 

; 

20.3.10 

Hence,  the  error-cerrection  term  in [20.3.1]  will  take  the  form 

Coy,-1  =  —BA'D'y,_,, 

where B is now  an  (n x  A) matrix  and A’ is an  (A X  q) matrix.  Maximum  likelihood 
estimation  proceeds  exactly  as  in  the  previous  section,  where  ¥, in  [20.2.5]  is re- 
placed  by the  OLS  residuals  from  regressions  of D'y,_,  on  a  constant  and  Ay,-1, 
AY,-2, 
Ay,_p41.  This  is equivalent  to  replacing  Syq  in  [20.2.6]  and  Syy 
.-., 
in [20.2.8] with 

Sw =  D'SyyD 
» SS, 

Let  A; denote  the  ith largest  eigenvalue  of 

LHEvw2lw. 

‘The maximized  value  for the  reptenetesD log likelihood  is then 
| 

(20.3.11] 
(20.3.12] 

[20.3.13] 

$  =  —(Tn2) log(2n)  —  (Tn!2) —  (T?2) loglZuul  —  (7/2) = log(1 —  i). 

| 

: 
| 

' ae $4) i  = 

iP  VE  ORE  Pee  SOULE  teehee  cs 

yes 

°—  fgililastihogs  ratte of he mal bypothess ht the coinegrating reli PRY. 
sof yr would then be 
’  une  elements  of 

t  the h ue in! 

East  the 

g  relat  ons 

ypou ne: 

q 

g 

a  ages  ats  a} 

with  es  ij  Ke.  >  ES ; 

#3  -  %3) = “a Bgtt 4) 4 7 $ eet ~ Ao.  pos. 

318  mo0bss77 ‘i  30  ed  ae 913  #2883  hats ed 

i i eS only coe ficients on 1(0) 

<ehgeesinie  ii. ee pilgen  a 

top  x 

; sre 

q 

The  likelihood  ratio  statistic  [20.3.14]  is 

™L4  -  Lo)  =  22.12  -  21.15 

0.97. 

The  degrees  of  freedom  for  this  statistic  are 

h-(n  —  q)  =  1:3  -  2)  =  1; 
the  null  hypothesis  imposes  a  single  restriction  on  the  cointegrating  vector.  The 
5%  critical  value  for  a  x?(1)  variable  is  seen  from  Table  B.2  to  be  3.84.  Since 
0.97  <  3.84,  the  null  hypothesis  that  the  exchange  rate  does  not  appear  in  the 
cointegrating  relation  is accepted.  The  restricted  cointegrating  vector  (normalized 
with  the  coefficient  on  the  U.S.  price  level  to  be  unity)  is 

a;  =  [1.00 

0.00 

-0.54]. 

As asecond  example,  consider  the hypothesis  that originally suggested interest 
in  a  possible  cointegrating  relation  between  these  three  variables.  This was  the 
hypothesis  that the  real  exchange  rate  is stationary,  or that  the  cointegrating  vector 
is proportional  to  (1,  —1,  —1)’.  For  this  hypothesis,  D'  =  (1,  —1,  —1)  and 
Sw  =  88.5977 

—0.145914 
Luv =  | 3.61422 
0.312582 

“In  this  case,  the  matrix  [20.3.13]  is the  scalar  0.0424498,  and  so  A,  =  0.0424498 
and  T log(1  —  A,)  =  —8.20.  Thus,  the  likelihood  ratio  test  of the  null  hypothesis 
that  the  cointegrating  vector  is proportional  to  (1,  —1,  —1)’  is 

By.  -—  9)  =  2212  —  Oy 

=  13.92. 

In this  case,  the  degrees  of freedom  are  — 

hin  =  @) =  4-(5.  7.1)  =  2. 

The  5%  critical  value  for  a  y?(2)  variable is 5.99.  Since  13.92  >  5.99,  the  null 
hypothesis  that  the cointegrating  vector  is proportional  to (1,  —1,  —1)'  is rejected. 

Other  Hypothesis  Tests 

A number  of other  hypotheses  can  be tested  in this framework.  For example, 
Johansen  (1991)  showed  that  the  null  hypothesis  that  there  are  no  deterministic 
time  trends  in any of the series  cari  be tested  by taking twice  the difference  between 
[20.2.10]  and  [20.2.48].  Under  the  null  hypothesis,  this  likelihood  ratio  statistic is 
asymptotically  x? with  g =  n  —  h degrees  of freedom.  Johansen  also  discussed 
construction  of Wald-type  tests  of hypotheses  involving the cointegrating  vectors. 
Not  all  hypothesis  tests  about  the  coefficients  in Johansen’s  framework  are 
asymptotically  y?. Consider  an  error-correction  VAR of the form of [20.2.1] where 
Co  =  —BA’.  Suppose  we  are  interested  in the  null  hypothesis  that  the  last  n, 
elements  of y, fail  to  Granger-cause  the  first  n,  elements  of y,.  Toda  and  Phillips 
(forthcoming)  showed  that  a Wald test  of this null hypothesis can  have  a nonstand- 
ard  distribution.  See  Mosconi  and  Giannini  (1992)  for further  discussion. 

650  Chapter 20  | Maximum  Likelihood  Analysis  of Cointegrated  Systems 

Comparison  Between  FIML  and  Other  Approaches 
Johansen’s  F/ML  estimation  represents  the  short-run  dynamics  of  a  system 
in  terms  of a  vector  autoregression  in differences  with  the  error-correction  vector 
z,_,  added.  Short-run  dynamics  can  also  be  modeled  with  what  are  sometimes 
called  nonparametric  methods,  such  as  the  Bartlett  window  used  to  construct  the 
fully modified  Phillips-Hansen  (1990)  estimator  in equation  [19.3.53].  Related  non- 
parametric  estimators  have  been  proposed  by Phillips  (1990,  1991a),  Park  (1992), 
and  Park  and  Ogaki  (1991).  Park  (1990)  established  the  asymptotic  equivalence  of 
the  parametric  and  nonparametric  approaches,  and  Phillips  (1991a)  discussed  the 
sense  in  which  any  F/ML  estimator  is  asymptotically  efficient.  Johansen  (1992) 
provided  a  further  discussion  of the  relation  between  limited-information  and  full- 
information  estimation  strategies. 

In  practice,  the  parametric  and  nonparametric  approaches  differ  not  just  in 
their  treatment  of  short-run  dynamics  but  also  in  the  normalizations  employed. 
The  fact  that  Johansen’s  method  seeks  to  estimate  the  space  of  cointegrating  re- 
lations  rather  than  a particular  set  of coefficients  can  be both  an  asset  and a liability. 
It is an  asset  if the  researcher  has no prior information  about  which  variables  appear 
in  the  cointegrating  relations  and  is concerned  about  inadvertently  normalizing 
a,,  =  1 when  the  true  value  of a,,  =  0.  On  the  other  hand,  Phillips  (1991b)  has 
stressed  that  if the  researcher  wants  to  make  structural  interpretations  of the  sep- 
arate  cointegrating  relations,  this  logically  requires  imposing  further  restrictions  on 
the  matrix  A’. 

. 

| 

For  example,  let  ,,  denote  the  nominal  interest  rate  on  3-month  corporate 
debt,  i, the  nominal  interest  rate  on  3-month  government  debt,  and  7, the  3-month 
inflation  rate.  Suppose  that  these  three  variables  appear  to be /(1) and  exhibit  two 
cointegrating  relations.  A natural  view  is that these  cointegrating  relations  represent 
two  stabilizing  relations.  The  first  reflects  forces  that  keep the  risk  premium  sta- 
tionary,  so  that 

SS 

[20.3.15] 
m=  en  +  nt  +  Zi 
with  z* ~  1(0).  A  second  force  is the  Fisher  effect,  which  tends  to  keep  the  real 
interest  rate  stationary: 

, 

7 

(20.3.16] 
T,  =  By  +  Yale  +  22; 
with  z% ~  1(0).  The  system  of  [20.3.15]  and  [20.3.16]  will  be  recognized  as  an - 
example  of Phillips’s  (1991a)  triangular  representation  [19.1.20]  for  the  vector  y,  = 
(r,, 7,,  i,)’. Thus,  in  this  example  theoretical  considerations  suggest  a  natural  or- 
dering  of variables  for  which  the  normalization  used  by Phillips  would  be  of par- 
ticular  interest  for structural  inference—the  coefficients  xj,  and  y,  tell  us  about 
the  risk  premium,  and the coefficients  43,  and  y, tell  us  about  the  Fisher  effect. 

:

| 

20.4.  Overview  of Unit  Roots—To  Difference 
or  Not  to  Difference? 
The preceding chapters  have  explored  a number  of issues  in the statistical  analysis 
of unit roots.  This section  attempts  to  summarize  what  all  this  means  in practice. 
Consider a vector  of variables  y, whose  dynamics  we  would  like to  describe 
and some of whose  elements  may be nonstationary.  For concreteness,  let us assume 
that the goal is to characterize  these  dynamics  in terms  of a vector  autoregression. 
One  option  is to  ignore  the  nonstationarity  altogether  and  simply  estimate 
the VAR  in levels,  relying  on  standard  ¢ and F distributions  for  testing  any  hy- 
20.4.  Overview of Unit Roots—To  Difference or Not to Difference?  651 

 
potheses.  This  strategy  has  the  following  features  to  recommend  it.  (1)  The  pa- 
rameters  that  describe  the  system’s  dynamics  are  estimated  consistently.  (2) Even 
if the  true  model  is  a  VAR  in  differences,  certain  functions  of the  parameters  and 
hypothesis  tests  based  on  a  VAR  in  levels  have  the  same  asymptotic  distribution 
as  would  estimates  based  on  differenced  data.  (3)  A  Bayesian  motivation  can  be 
given  for  the  usual  ¢ or  F distributions  for  test  statistics  even  when  the  classical 
asymptotic  theory  for  these  statistics  is nonstandard. 

A  second  option  is routinely  to  difference  any  apparently  nonstationary  var- 
iables  before  estimating  the  VAR.  If the  true  process  is  a VAR  in differences,  then 
differencing  should  improve  the  small-sample  performance  of  all  of  the  estimates 
and  eliminate  altogether  the  nonstandad  asymptotic  distributions  associated  with 
certain  hypothesis  tests.  The  drawback  to  this  approach  is  that  the  true  process 
may  not  be  a  VAR  in  differences.  Some  of  the  series  may  in  fact  have  been 
stationary,  or  perhaps  some  linear  combinations  of the  series  are  stationary,  as  in‘ 
a  cointegrated  VAR.  In  such  circumstances  a  VAR  in differenced  form  is misspe- 
cified. 

Yet  a  third  approach  is to  investigate  carefully  the  nature  of the  nonstation- 
arity,  testing  each  series  individually  for  unit  roots  and  then  testing  for  possible 
cointegration  among  the  series.  Once  the  nature  of the  nonstationarity  is under- 
stood,  a  stationary  representation  for  the  system  can  be  estimated.  For  example, 
suppose  that  in  a  four-variable  system  we  determine  that  the  first  variable  y,,  1s 
stationary  while  the  other  variables  (y>,,  y3,,  and  y,4,)  are  each  individually  /(1). 
Suppose  we  further  conclude  that  y,,,  y3,,  and  y,,  are  characterized  by a single 
cointegrating  relation.  For y>,  =  (y2,, ¥3,, Y4,)',  this  implies  a vector  error-correction 
representation  of the  form 

pe] ie )<'| i  a  fll y ik  / 4: | i  ll an " 

Ay>, 

a, 

se  4  Ayo, -1 

ty 

Yo  Ay>,-—2 

call  | i aoe  nel sees  4. Fae : 

ée a 

aad  cS”  LS  ae 

-  a: 

£2, 

(0) 

‘ 

where  the (4 x  3) matrix  [Ei] is restricted  to  be of the  form  ba’  where b is (4  x  1) 
and a’  is (1  Xx  3).  Such  a  system  can  then  be  estimated  by adapting  the  methods 
described  in  Section  20.2,  and  most  hypothesis  tests  on  this  system  should  be 
asymptotically  y?. 

The  disadvantage  of the  third  approach  is that,  despite  the care  one  exercises, 
the  restrictions  imposed  may  still  be  invalid—the  investigator  may  have  accepted 
a null  hypothesis  even  though  it is false,  or  rejected  a null  hypothesis  that  is actually 
true.  Moreover,  alternative  tests  for  unit  roots  and  cointegration  can  produce 
conflicting  results,  and  the  investigator  may  be  unsure  as  to  which  should  be  fol- 
lowed. 
2 
Experts  differ  in the  advice  offered  for  applied  work.  One  practical  solution 
is to  employ  parts  of all  three  approaches.  This  eclectic  strategy  would  begin  by 
estimating  the  VAR  in levels  without  restrictions.  The  next  step is to  make  a quick 
assessment  as  to  which  series  are  likely  nonstationary.  This  assessment  could  be 
based  on  graphs  of  the  data,  prior  information  about  the  series  and  their  likely 
cointegrating  relations,  or  any  of the  more  formal  tests  discussed  in Chapter  17. 
Any  nonstationary  series  can  then  be differenced  or expressed  in error-correction 
form  and a stationary  VAR  could  then  be  estimated.  For  example,  to  estimate  a 
VAR  that includes  the  log of income  (y,) and  the  log of consumption  (c,),  these 
two  variables  might be  included  in a  stationary  VAR  as  Ay, and  (c,  —  y,).  If the 
VAR  for the  data  in  levels  form  yields  similar  inferences  to  those  for the  VAR  in 
652  Chapter 20  | Maximum  Likelihood  Analysis  of Cointegrated  Systems 

* 

Stationary  form,  then  the  researcher  might  be  satisfied  that  the  results  were  not 
governed  by the  assumptions  made  about  unit  roots.  If  the  answers  differ,  then 
some  attempt  to reconcile  the  results  should  be  made.  Careful  efforts  along  the 
lines  of the  third Strategy  described  in  this  section  might  convince  the  investigator 
that  the  stationary  formulation  was  misspecified,  or  alternatively  that  the  levels 
results  can  be explained  by the  appropriate  asymptotic  theory.  A nice  example  of 
how  asymptotic  theory  could  be  used  to  reconcile  conflicting  findings  was  provided 
by Stock and  Watson  (1989).  Alternatively,  Christiano  and  Ljunggqvist  (1988)  pro- 
posed  simulating  data  from  the  estimated  levels  model,  and  seeing  whether  incor- 
rectly  fitting  such  simulated  data  with  the  Stationary  specification  would  spuriously 
produce  the  results  found  when  the  Stationary  specification  was  fitted  to  the  actual 
data.  Similarly,  data  could  be simulated  from  the  stationary  model  to  see  if it could 
account  for  the  finding  of the  levels  specification.  If we  find  that  a  single. specifi- 
cation  can  account  for both  the levels  and the stationary  results,  then  our  confidence 
in that  specification  increases. 

APPENDIX  20.A.  Proof of Chapter  20 Proposition 

@  Proof  of Proposition  20.1. 

(a)  First  we  show  that  A,  <  1 fori  =  1,2,  ...,,.  Any  eigenvalue  A of  [20.1.8] 

satisfies 

; 

Since  Zyy  is positive  definite,  this  will  be  true  if and  only  if 

ply All  Sy 

‘ame  AI,  =  0. 

But  from  the  triangular  -factorization  of 2 in equation  [4.5.26],  the  matrix 

Ade,  a  > A  a  A  =  0. 

[20.A.1} 

p eee  os  pr  eee 

[20.A.2] 

is positive  definite.  Hence,  the  determinant  in  [20.A.1]  could  not  be  zero  at  A.  =  1.  Note 
further  that 

ee  oe  ee  Se  AS) 

If A >  1, then  the right side  of expression  (20.A.3]  would  be the  sum  of two  positive  definite 
matrices  and  so  would  be positive  definite.  The  left  side  of [20.A.3]  would  then  be positive 
definite,  implying  that  the  determinant  in  [20.A.1]  could  not  be  zero  for  A  >  1.  Hence, 
A = 1 is not  consistent  with  [20.A.1]. 

: 

To see  that  A, =  0, notice  that  if A were  less  than  zero,  ther’  AZ yy would  be a negative 
number  times  a positive  definite  matrix  so  that ALyy  —  Lyx2xxZ xy would  also be a negative 
number  times  a  positive  definite  matrix.  Hence,  the  determinant  in  [20.A.1]  could  not  be 
zero  for  any  value  of A <  0. 

Parallel  arguments  establish  that  0 =  uw, <1  forj  =  1,2,...  ,  Nn. 
(b)  Let  k, be an  eigenvector  associated  with  a  nonzero  eigenvalue  A, of (20.1.8): 
Sy lyx2 a2 xyk, =  A‘, 

¥ 

[20.A.4] 

Premultiplying  both sides  of [20.A.4]  by 2x, results  in 

yp 
[20.A.5] 
But  [2,k,]  cannot  be  zero,  for  if [Zxyk,]  did  equal  zero,  then  the  left  side  of (20.A.4] 
would  be zero,  implying  that  A, =  0. Thus,  [20.A.5]  implies  that  A, is also  an  eigenvalue  of 
the  matrix  [LZxy2yi2Zvx2xx)  associated  with  the  eigenvector  [Zxyk,].  Recall  further  that 
eigenvalues  are  unchanged  by transposition  of a matrix: 

vy ZyxZxxI[Zxvki]  =  A[Zxvk,]- 

[ZavZ 

[Zxv2vy2yx2xx)"  =  Lix2xv2yyZyx; 

; 

| 

ich 

is  the matrix  [20.1.12].  This  proves  that if A,  is  a nonzero  eigenvalue  of [20.1.8],  then 
be en an a? of es 1) Exactly  parallel calculations  show  that  if 2, is  a nonzero 
eigenvalue  of [20.1.12],  then  it is also  an  eigenvalue  of [20.1.8]. 

WYS)% 

Appendix  20.A.  Proof of Chapter 20 Proposition 

653 

(c)  Premultiply  (20.1.10]  by k/2yy: 

K/L yx2xx2xyk;  =  AK; Zyyk;. 

 (20.A.6] 

Similarly,  replace  { with  j in  [20.1.10): 

and  premultiply  by k/2yy; 

Lyi 2yxLxgexyk;  =  Ak;, 

Kj Dyx2xx2xyk;  =  A,kjZyyk;. 

Subtracting  [20.A.8]  from  [20.A.6],  we  see  that 

0  =  (A,  —  A,)K;  Zyyk;. 

[20.A.7] 

| 

[20.A.8] 

{20.A.9] 

If i #  j, then  A;  #  A, and  [20.A.9]  establishes  that  k;Zyyk,  =  0 for:  #  j. Fori  =  j, we 
normalized  k/S,yk,  =  1 in  [20.1.11].  Thus  we  have  established  condition  [20.1.3]  for  the 
case  of distinct  eigenvalues. 

Virtually  identical  calculations  show  that  [20.1.13]  and  [20.1.14]  imply  [20.1.4]. 
(d)  Transpose  [20.1.13]  and  postmultiply  by 2,,k;: 

Similarly,  premultiply  [20.A.7]  by a/Z x,y: 

| 

aj DyxyZyyZyx2xx2Zxyk;  =  Aja;  Zxyk,. 

[20.A. 10] 

aj Dyxylyy2yx2xx2xvk;  =  Aaj  Zxyk;. 

[20.A.11] 

Subtracting  [20.A.11]  from  [20.A.10]  results  in 

0  =  (A, —  A,)aj2xyk;: 

This  shows  that  a/2yyk,  =  0 for  A;  #  A,, as  required  by [20.1.5]. 

To  find  the  value  of a/2,yk;  for  i =  j, premultiply  [20.1.13]  by-a/2,,,  making  use 

of {20.1.14]: 

ge  So's,  =n 

[20.A.12] 

Let  us  suppose  for  illustration  that  n,  is the  smaller  of n,  and  n,;  that  is,  nm =  n,.° Then  the 
matrix  of eigenvectors  X is (n  X  n) and  nonsingular.  In  this  case,  [20.1.3]  implies  that 

or,  taking  inverses, 

yy  =  [X']-'H-', 

wa  =  KN. 

Substituting  [20.A.13]  into  (20.A.12],  we  find that 

Qi DxyIod  DyyA;  =.  A;. 

Now, 

a/ZxyH  =  a/Zyy[k, 

k,  --- 

k,] 

[20.A.13] 

[20.A.14] 

=  [a/2xyk, 
=(0  0  -:: 

aj2yyk, 

-°- 

afSyyk, 

--- 

aj2xyk; 
Of. 

--- 

a; 2xyk,,] 

[20.A.15] 

Substituting  [20.A.15]  into  [20.A.14],  it follows  that 
(a/Zxyk,)?  =  A,. 

Thus,  the  ith  canonical  correlation, 

is given by the  square  root  of the eigenvalue  A,, as  claimed: 

r, =  ajZyyk,, 

r=),  @ 

*In the  converse  case  when n  =  n,,  a parallel  argument  can  be constructed  using the fact that 

654  Chapter 20  | Maximum  Likelihood  Analysis of Cointegrated  Systems 

k/Lyx2xe2xvk,  -  A,. 

Chapter  20 Exercises 

a  this — you  are  asked  to  verify  the  claim  in  the  text  that  the  first  canonical 
¢,, represent  the  linear  combinations  of y,  and  x,  with  maximum  possible 
correlation.  Consider  the  following  maximization  problem: 

.  _™y,  and 

max  E(kiy,x/a,) 
{k),@)} 

subject  to 

E(kiyy/k,)  =  1 
E(a}x,x/a,)  = 
1. 
pean that the  maximum  value  achieved  for  this  problem  is given  by the  square  root  of  the 
argest  eigenvalue  of the  matrix  2xx2xyZyy2Lyx,  and  that  a,  is the  associated  eigenvector 
normalized  as  stated.  Show  that  k,  is  the  normalized  eigenvector  of  D7JE,x2Zxq)D xy, as- 
sociated  with  this  same  eigenvalue. 
20.2. 
It  was claimed  in  the text  that  the  maximized  log likelihood  function  under  the  null 
hypothesis  of  h  cointegrating  relations  was  given  by  [20.3.2].  What  is  the  nature  of  the 
restriction  on  the  VAR  in [20.3.1]  when  h  =  0? Show  that  the  value  of [20.3.2]  for this  case 
7 the  same  as  the  log likelihood  for  a  VAR(p  —  1) process  fitted  to  the  differenced  data 
y:- 

20.3.  It was  claimed  in  the  text  that  the  maximized  log  likelihood  function  under  the 
alternative  hypothesis  of n  cointegrating  relations  was  given  by [20.3.3].  This  case  involves 
regressing  Ay, On  a  constant,  y,_,,  and  Ay,_,,  Ay,_,,  ...,  Ay,_,,,  without  restric- 
tions.  Let g,  denote  the  residuals  from  this  unrestricted  regression,  with  L¢g  = 
(1/7)>7_,  g,g;.  Equation  [11.1.32]  would  then  assert  that  the  maximized  log  likelihood 
function  should  be  given  by 

$%  =  —(Tn/2)  log(27)  —  (T/2)  log\Sgcg|  —  (Tn/2). 

Show  that  this  number  is the  same  as  that  given  by formula  [20.3.3]. 
20.4. 
Consider  applying  Johansen’s  likelihood  ratio  test  to  univariate  data  (n =  1). Show 
that  the  test  of the  null  hypothesis  that  y, is nonstationary  (h  =  0) against  the  alternative 
that  y, is stationary  (A  =  1) can  be  written 

T{log(sé)  —  log(é})), 
where  6  is the  average  squared  residual  from  a  regression  of Ay, on  a  constant  and  Ay,_,, 
Ay,_2, 
--.,  Ay,_,4,  while  G? is  the  average  squared  residual  when  y,_,  is added  as  an 
explanatory  variable  to  this  regression. 

| 

Chapter 20 References 

Ahn,  S. K., and  G.  C.  Reinsel.  1990.  ‘“‘Estimation  for  Partially  Nonstationary  Multivariate 
Autoregressive  Models.”’  Journal  of the American  Statistical  Association  85:813-—23. 
Christiano,  Lawrence  J., and  Lars  Ljungqvist.  1988.  “Money  Does  Granger-Cause  Output 
in the  Bivariate  Money-Output  Relation.”  Journal  of Monetary  Economics  22:217-3S. 
Johansen,  Soren.  1988.  ‘Statistical  Analysis of Cointegration  Vectors.”  Journal of Economic 
Dynamics  and  Control  12:231-54. 
——.,  1991.  “Estimation  and  Hypothesis  Testing  of Cointegration  Vectors  in Gaussian 
Vector  Autoregressive  Models.””  Econometrica  59:1551-80. 
——.  1992.  “‘Cointegration  in Partial  Systems  and the Efficiency of Single-Equation  Anal- 
ysis.” Journal  of Econometrics  52:389-402. 
——  and  Katarina  Juselius.  1990.  “Maximum  Likelihood  Estimation  and  Inference  on 
Cointegration—with  Applications  to the Demand  for Money.”  Oxford Bulletin  of Econom- 
ics  and Statistics  52:169-210. 
Simultaneous 
Koopmans ,  TjaHing 
Linear enholaie Relationships,”  in  William  C.  Hood  and  Tjalling  C.  Koopmans,  eds., 
Studies in Econometric  Method.  New  York:  Wiley. 

Serena  Df 

C.,  and  William  C.  Hood. 

eet 

Estimation  of 

.  “The 

tee 

. 

amt,  Soaks 

| 

Chapter 20 References  655 

Mosconi,  Rocco,  and  Carlo  Giannini.  1992.  ‘‘Non-Causality  in  Cointegrated  Systems:  Rep- 
resentation,  Estimation  and  Testing,’’  Oxford  Bulletin  of Economics  and  Statistics.  54:399- 
417. 

Park,  Joon  Y.  1990.  **Maximum  Likelihood  Estimation  of Simultaneous  Cointegrated  Models.” 
University  of  Aarhus.  Mimeo. 

.  1992.  ‘Canonical  Cointegrating  Regressions.’’  Econometrica  60:119=43.  — 
and  Masao  Ogaki.  1991.  “Inference  in  Cointegrated  Models  Using  VAR  Prewhi- 

tening  to  Estimate  Shortrun  Dynamics.”’  University  of Rochester.  Mimeo. 
Phillips,  Peter  C.  B.  1990.  ‘‘Spectral  Regression  for  Cointegrated  Time  Series,”  in  William 
Barnett,  James  Powell,  and  George  Tauchen,  eds.,  Nonparametric  and  Semiparametric 
Methods  in  Economics  and  Statistics.  New  York:  Cambridge  University  Press. 

.  1991la.  “Optimal  Inference  in  Cointegrated  Systems.””  Econometrica  59:283-306. 
.  1991b.  “Unidentified  Components  in  Reduced  Rank  Regression  Estimation  of 

ECM’s.”  Yale  University.  Mimeo. 

and  Bruce  E.  Hansen.  1990.  ‘‘Statistical  Inference  in Instrumental  Variables  Regres- 

sion  with  I(1)  Processes."’  Review  of Economic  Studies  57:99-125. 

and  S.  Ouliaris.  1990.  “‘Asymptotic  Properties  of  Residual  Based  Tests  for  Coin- 

tegration.”  Econometrica  58:165-93. 
Stock,  James  H.,  and  Mark  W.  Watson.  1988.  ‘‘Testing  for  Common  Trends. ”  Journal  of 
the  American  Statistical  Association  83:1097-1107. 

and 

.  1989.  ‘Interpreting  the Evidence  on  Money-Income  Causality.”’  Journal 

of Econometrics  40:161-81. 
Toda,  H.  Y., and Peter  C.  B.  Phillips.  Forthcoming.  “‘Vector  Autoregression  and Causality.” 
Econometrica. 
West,  Kenneth  D.  1988.  ‘Asymptotic  Normality,  When  Regressors  Have  a  Unit  Root.” 
Econometrica  56:1397-1417. 

656  Chapter 20  | Maximum  Likelihood  Analysis  of Cointegrated  Systems 

ei 

Time  Series  Models 
of Heteroskedasticity 

21. 1.-A uloregressi ve  Conditional 
Heteroskedasticity  (ARCH) 

An autoregressive’ process of order  p (denoted  AR(p))  for an observed variable 
¥ taken: fhe form 

Si 
a Roe r+ by, a+°  = +4,y,  228  # 
U1 
__  where  u,  is white noise: 
| 
| 
7  — evolve.  MALY  ‘Bord E(u) ade: we -  otherwise. ie 

  pi.) 
ee 
a 
. 
oresii2  aii  ete) 

E(u)  =  0 

{oe  fone 

—  a 

ast 

om 

" 

; 

:

The 

+ 

proce  s  is covariance-stationary provided that the roots  of 

then él ADpimpae)s  ahaa 

i 

ee $2: =  ae 
“ae 
en: ane 

tide ne  gs ‘saat 

re aie A 

the te ca 

Cyl.  + a i oe!  . 
i 

ve  mapgzP = 000 Wt  &  2i a ron 
i  he* 
At a  linear forecast re peed  2s 
m  {71 

Bob i wig Fae 

eer 

Fy 

; 

: 

feta hall ve :  tne: oo =i - SF bon pu 

). W New hatha smn 

s  the lin lear proje  a  re) Y,  9 mn  |  constant an a 
aarti pe ee... 

mea 

etiens 

hl Sad  Biigilt 3  cm pene  Oe\ce  ris Wisk “i 

=  ie 

i, 

= 

ft... 

‘ 

SS 

$8 

61 

64 

67 

70 

73 

76 

79 

82 

6S 

88 

FIGURE  21.1 
rate),  1955-89. 

U.S.  federal  funds  rate  (monthly  averages  quoted  at  an  annual 

describe  the  square  of u, as  itself  following  an  AR(m)  process: 

u2  =  £ +  a,u?_,  +  au?_,+-°+:  +  a,,u2_,,  +  W,, 

[21.1.5] 

where  w, is a  new  white  noise  process: 

E(w,)  =  0 

E(w,w,) 

fort  =  + 
“— 
(wile)  {,  otherwise. 
Since  u,  is the  error  in  forecasting  y,,  expression  [21.1.5]  implies  that  the  linear. 
projection  of the squared  error  of a forecast  of y, on  the previous m squared  forecast 
errors  is given  by 

d? 

= 

: 

E(u?|u?_,,  u?_2,...)  =  £ +  ayu?_y  +  agu?.y  +  +++  +  GU? m.  [21.1.6] 
A  white  noise  process  u, satisfying  [21.1.5]  is described  as  an  autoregressive  con- 
ditional  heteroskedastic  process  of order  m,  denoted  u, ~  ARCH (m). This  class  of 
processes  was  introduced  by Engle  (1982).} 

Since  u, is random  and  u? cannot  be  negative,  this  can  be  a  sensible  repre- 
sentation  only if [21.1.6]  is positive  and  [21.1.5]  is nonnegative  for all realizations 
of {u,}.  This  can  be ensured  if w, is bounded  from  below  by —¢ with  ¢ >  0 and  if 
a, = Oforj  =  1,2,...,m.  In order  for u? to be covariance-stationary,  we  further 
require  that  the  roots  of 

nis 

_ 

1  —  az  -  az?  =+-+  ~'a,z™  =0 

"A nice survey  of ARCH-related  models  was  provided  by Bollerslev,  Chou,  and Kroner  (1992). 

658  Chapter 21  | Time  Series  Models  of Heteroskedasticity 

lie  outside  the  unit  circle.  If the  a; are  all  nonnegative,  this is  equivalent  to  the 
requirement  that 

{21.1.7] 
@t+ta,t:::+a,<1. 
When  these  conditions  are  satisfied,  the  unconditional  variance  of u, is given  by 
[21.1.8] 

Pim  Sue)  =  Wea  ae 

ee). 

Let  d?,,), denote  an  s-period-ahead  linear  forecast: 
Os,  =  E(u?,.|u?,  ues,  ...)- 

This  can  be calculated  as  in [4.2.27]  by iterating  on 

(GP, jy.  —  o)  a  @; (07, ;- 140  -  -)  +  a, (U7, ;- 24, ~-  g*) 

HPF 

hee  GAROT  chai  7  or) 

for j =  1,2,...,5  where 

ay, =  ur  for  St. 

The s-period-ahead forecast  d?, ,), converges in probability toa? ass—> 
sap w, has finite  variance  and  that [21.1.7] 

is satisfied. 

=» 

It is often  convenient  to use an  alternative representation for an ARCH(n) 
‘ melt Se imposes  slightly  stranger assumptions  about the  serial RRA of 
ie  ARR that 

,  assuming 

{EDA 45  i 

.ausfT 

1} is an iid, sequence ate mean and unit variance: 

+> » 

polation  teixs  san :  E(v,)  = }  res OB  SRIUS| pit 

to  Insmom  baoose 

=  Viiv Y, 

. 

(2. EE 9] 

oinepe  aif! 

Taking  the ARCH (1) specification  for illustration,  we  find with  a little  manipulation 
of  the  formulas  for  the  mean  and  variance  of an  AR(1)  process  that 

E(h?)  =  E(g  +  a,u7_,) 

=  E{(aj-us_,)  +  (2a,f-u?_,)  +  £7} 
ll 

a?-[Var(u2_,)  +  [E(u iP]  +  2a,f-E(u?_,)  +  g*   [21.1.15] 

. 

a?-{|——-  + ————] 

ya 

? 

2a,g7 
+  —  + 

4 

2  Fea az  +4  "—  =|  1  =  a 

¢ 

az? 

_NogFnits’ 

“T-a} 

Gd - ay 

Substituting  (21.1.15]  and [21.1.13]  into  [21.1.14],  we  conclude  that  A?  (the  un- 
conditional.  variance  of  w,)  must  satisfy 

: 

az?  at 

: 

S 

i 

: 

Even  when  |[a,|  <  1, equation  [21.1.16]  may  not  have  any  real  solution  for 
A.  For  example,  if v, ~  N(0;  1), then  E(v?  —  1)? =  2 and  [21.1.16]  requires  that 

(1 —  3a3)A?2_——  2 

Le  ah.  ay 

This  equation  has  no  real  solution  for  A whenever  aj =  3. Thus,  if u, ~  ARCH(1) 
with  the  innovations  v, in  [21.1.9]  coming  from  a  Gaussian  distribution,  then  the 
second  moment  of w, (or the fourth  moment  of \,) does  not  exist  unless  a? <  4. 

" 

Maximum  Likelihood  Estimation  with  Gaussian  v, 

-  Suppose  that  we  are  interested  in  ¢stumating  the  parameters  of a  regression 

_model  with  ARCH  disturbances.  Let  the  regression  equation  be 

y,  =  xB  +  u,. 

{21.1.17] 

Here  x,  denotes  a  vector  of  predetermined  explanatory  vanables,  which  could 
include  lagged  values  of y.  The  disturbance  term  u, is assumed  to satisfy  [21.1.9] 
and  [21.1.10].  It  is  convenient  to  condition  on  the  first  m  observations  (t  = 
,0) and to use  observations t =  1,2,.  . . ,  T for estimation. 

—m+1,-—m+2,... 
Let  Y, denote the  vector of observations  obtained  through  date f 

4 

~~ (Y1,  Y- 19°  > + 9  Vis  Yor 

s+ 

+9  Y-met:  K,,  Ky. as.  * 

+:  Shs  Bop.»  os  »  eageaae  a 

If v, ~  1.1.4.  N(O,  1) with  v, independent  ot  both  x, and Y,_,,  then  the conditional 
distribution  of y, is Gaussian  with  mean  x; 

and  variance  h,: 

| 

fore. y,_,)  = os 

she exp( UB) 

[21.1.18] 

_  where 

ng 

, 

hy =  £ +  (Yea  ~  X18)?  +  an(Ye-2  —  X1-2B)?  + 

+  Om(Yr-m  —  Xi-mB)? 

| 

(21.1.19] 

=  [z,(B)]'5 

660  Chapter 21  | Time  Series  Models  of Heteroskedasticity 

for 

OY! 

er 

ae  a,,)' 

(z,(B))’  =  [1, (1-1  -  x;~:B)’,  (y,-2  -  Ree MN)  ss  em  BiceeT 
Collect  the  unknown  parameters  to  be  estimated  in  an  (a  Xx  1) vector  0: 
6 =  (p’,  5’)’. 

The  sample  log likelihood  conditional  on  the  first  m  observations  is then 

£(8)  It 

F 

p> log f(y,|x,,  Y,_1;  8) 

tl 

~  (7/2)  log(2z)  —  (1/2)  py log(h,) 

[21.1.20] 

~  (2) De = 

x B)?Ih,. 

For a given  numerical  value  for the parameter  vector  0, the sequence  of conditional 
variances  can  be calculated  from  [21.1.19]  and  used  to  evaluate  the  log likelihood 
function  [21.1.20].  This  can  then  be  maximized  numerically  using  the  methods 
described in  Section  5.7.  The  derivative  of the  log of the  conditional  likelihood  of 
the  ‘th observation  with  respect  to the  parameter  vector  8, known  as the  th  score, 
is shown  in Appendix  21.A  to  be given  by 

s,(8)  =  seme 

| 

ext) 

es 

2 

igi 

i  at? 
: 

SERS e 
A$ 

| 
~  2ajYy— jX,—;j 

{(u}  hyW2h i|2 2,(B)  / +  | 0 

j=! 

ta} 

, 

. 

|  ore 
; 

(21.1.23] 

| 

The likelihood friction. ean  be maximized  using the  method  of scoring  as  in Engle 
;  (1982, p. 997) or  using  the  Berndt,  Halli,  Hall,  and  Hausman  (1974)  algorithm  as 
in Bollerslev  (1986, p. 317). Alternatively,  the gradient  of the log likelihood function 
can be calculated  analytically  from  the  sum  of the  scores, 

ve) =  ¥ 5,0), 

T 

or  numerically  by numerical  differentiation  of  the  log  likelihood  [21.1.20].  The 
analytically  or  numerically  evaluated  gradient  could  then  be used  with  any  of the 
numerical  optimization  procedures  described in  Section  5.7. 
: 

Imposing  the  stationarity  condition  (S72 14, <  1) and  the  nonnegativity  con- 
dition  (a; =  0 for  all j) can  be  difficult in practice.  Typically,  either  the  value  of 
m  is very  small  or else  some  ad  hoc  ene. is imposed  on  the sequence  {a als 
as  in aor (1982,  —- cea 

c 

>» 

- 

Maximum Likelihood Estimation  with  Non-Gaussian  Vv, 
The preceding formulation  of the  likelihood  function  assumed  that  v, has  a 
Peon distribution.  However,  the  unconditional  distribution  of many  financial 
time  series  seems to  have  fatter  tails  than  allowed  by the  Gaussian  family. Some 
of this:can  be explained  by the  presence of ARCH;  that i is, even  if v, in [21.1.9] 

‘ 

21.1.  Autoregressive Conditional Heteroskedasticity  (ARCH)  661 

has  a  Gaussian  distribution,  the  unconditional  distribution  of  u,  is  non-Gaussian 
with  heavier  tails  than  a  Gaussian  distribution  (see  Milhdj,  1985,  or  Bollerslev, 
1986,  p.  313.  Even  so,  there  is  a  fair  amount  of  evidence  that  the  conditional 
distribution  of u,  is often  non-Gaussian  as  well. 

The  same  basic  approach  can  be  used  with  non-Gaussian  distributions.  For 
example,  Bollerslev  (1987)  proposed  that  v,  in  [21.1.9]  might  be  drawn  from  a  ¢ 
distribution  with  v  degrees  of  freedom,  where  v  is regarded  as  a  parameter  to  be 
estimated  by  maximum  likelihood.  If conditional  on  M,,  the  term  u,  has  a  t dis- 
tribution  with  v  degrees  of  freedom  and  scale  parameter  M,,  then  its  density  is 
given  by 

a  Diy  se  2). 

sei,  we  24.1,23 

f(u,|M,)  i  (av)'?P(v/2)  M,  :  [ +  Mv 

[ 1.1.  J 

where  I'(-)  is the  gamma  function  described  in  the  discussion  following  equation 
[12.1.18].  If  vy >  2, then  v,  has  mean  zero  and  variance? 

E(u?)  =  M,v/(v  —  2). 

Hence,  a ¢ variable  with  v degrees  of freedom  and  variance  h, is obtained  by taking 
the  scale  parameter  M, to  be 

ee 

M,  =  hv  —  2)/v, 

for  which  the  density  [21.1.22]  becomes 
_  T{(v  +  1)/2] 

ipa 

f(u|h,)  = 

n™T(v2)  \% ~  2)  a  f +i 2) 5 | 

(21.1.23] 

u2 

=(v+ty/2 

This  density  can  be  used in place  of the Gaussian  specification  [21.1.18]  along  with 
the  same  specification  of  the  conditional  mean  and  conditional  variance  used  in 
(21.1.17]  and  [21.1.19].  The  sample  log likelihood  conditional  on  the  first  m  ob- 
servations  then  becomes 
; 
2 log f(y 1x,.,  1:8) 

r 

1 

=  T tog  Le I (v -  2)  0} —  (1/2)  2 log(h,) 

us 

-((v+  1121  5 tog | +  Qe niB| 

hv = 2) 

where 

[21.1.24] 

| 

h, =  ¢+  ai(y,—  -  x;— 1B)?  +  a2(y,-2  -  X; 2B)?  tind  Om  (Ye—m  E  X;— mB)? 

=  [z,(B)]'6. 

| 

The  log likelihood  [21.1.24]  is then  maximized  numerically  with  respect  to  v,  B, 
_  and  8 subject  to the  constraint  »v >  2. 
The  same  approach  can  be used  with  other  distributions  for v,.  Other  distri- 
butions  that  have  been  employed  with  ARCH-related  models  include  a Normal- 
Poisson  mixture distribution  (Jorion,  1988), power exponential  distribution (Baillie 
and Bollerslev,  1989),  Normal—log  normal  mixture  (Hsieh,  1989), generalized  ex- 
ponential  distribution  (Nelson,  1991),  and serially  dependent  mixture  of Normals 
(Cai, 1994) or  ¢ variables  (Hamilton  and  Susmel,  1994), 

. 

: 

*See,  for example,  DeGroot  (1970,  p. 42). 

662  Chapter 21  | Time Series  Models  of Heteroskedasticity 

Quasi-Maximum  Likelihood  Estimation 
Even  if the  assumption  that  v, is 1.i.d.  N(O,  1) is invalid,  we  saw  in [21.1.6] 
that the ARCH  specification  can  still  offer  a reasonable  model  on  which  to  base  a 
linear forecast of the squared value of v,. As shown  in Weiss  (1984,  1986), Bollerslev 
and Wooldridge  (1992),  and  Glosten,  Jagannathan,  and  Runkle  (1989),  maximi- 
zation  of  the  Gaussian  log likelihood  function  (21.1.20]  can  provide  consistent 
estimates  of the  parameters  ¢, a,  Q@,...,  @,  Of this  linear  representation  even 
when  the  distribution  of u, is non-Gaussian,  provided  that  », in [21.1.9]  satisfies 

and 

E(v,|x,,  Y,-;)  ”  0 

E(v?|x,,  ¥,-1)  =  1. 
However,  the  standard  errors  have to be  adjusted.  Let  6; be  the  estimate  that 
maximizes  the  Gaussian  log likelihood  [21.1.20],  and  let  @ be  the  true  value  that 
characterizes  the linear representations  [21.1.9], [21.1.17], and [21.1.19].  Then even 
when  », is actually  non-Gaussian,  under  certain  regularity  conditions 

—VT(6; —  6)  M0,  D-'SD-'), 

a)  =  8s 

eis:  plim T~' > [s,(6)]-[s(®))) 

ee 

oe 

yd 

; 

if 

; 

o-% 

; 

1 

ao  © 
~ 
a 
Ga 
ee 

i 
OS 

os 
$ 

}. 

* 

al 

> 

. 
*. 

oe 
PAr? 

die 

i 

‘ebittion  Stk  gaag  Senewenasiad 
it  +  Anbeay te: pedtingpotsiazoans  26  ot  availed 
atin? 
, 
Pa  Som  O° 
he 
ee 
M 
oy  i 

a 
| a  eee 

ene 

ii 

+ 
ton 
- 

OP 
OC Aaa 

sg 

rf 

. 

& 

ri 

* 

a 

e 

: 

. 

‘ 

i 

: 

4 

: 

* 

e 

‘ 

i 

“an  4 

oe  eg  eee  ae  ak” ae 
= 

_ 

x 

‘ 

a.  5 

e 

7 

Standard  errors  for  6, that  are  robust  to  misspecification  of the  family  of densities 
can  thus  be  obtained  from  the  square  root  of diagonal  elements  of 

T-'D='S$,D;z'. 

Recall  that  if the  model,  is  correctly  specified  so  that  the  data  were  really 
generated  by  a  Gaussian  model,  then  S  =  D,  and  this  simplifies  to  the  usual 
asymptotic  variance  matrix  for  maximum  likelihood  estimation. 

Estimation  by Generalized  Method  of Moments 

The  ARCH  regression  model  of (21.1.17]  and  [21.1.19]  can  be  characterized 
by the  assumptions  that  the  residual  in the  regression  equation  is uncorrelated  with 
the  explanatory  variables, 

E{(y,  —  x,B)x,]  =  0, 
and  that  the  implicit  error  in forecasting  the  squared  residual  is uncorrelated  with 
lagged  squared  residuals, 

E(u;  {. h,)z,|  =  0. 

As  noted  by Bates  and  White  (1988),  Mark  (1988),  Ferson  (1989),  Simon  (1989), 
or  Rich,  Raymond,  and  Butler  (1991),  this  means  that  the  parameters  of an  ARCH 
model  could  be estimated  by generalized  method of moments,*  choosing  @  =  (B’, 
3')’  so  as  to  minimize 

where 

{e(; ¥,)]'S;{8(@;  97), 

fe  py (y,  ony  x; B)x, 

g(0;  Y,)  = 

T 

es 2 {(y,  —  x, B)?  —  [z,(B)]’5}z,(B) 

The  matrix  §,., standard  errors  for parameter  estimates,  and  tests  of the  model  can 
be  constructed  using  the  methods  described  in  Chapter  14.  Any  other  variables 
believed  to  be  uncorrelated  with  u, or  with  (u?  —  h,) could  be  used  as  additional 
instruments. 

: 

— 

| 

Testing for ARCH 

Fortunately,  it is simple  to  test  whether. the residuals  u,  from  a  regression 
model  exhibit  time-varying  heteroskedasticity  without  actually  having  to  estimate 
the  ARCH  parameters.  Engle  (1982,  p.  1000)  derived  the  following  test  based  on 
the  Lagrange  multiplier  principle.  First  the  regression  of [21.1.17]  is estimated  by 
OLS  for  observations  ¢ =  ~m  +  1,  —m  +  2....,  T and  the  OLS  sample 
residuals  ti, are  saved.  Next,  «7 is regressed  on  a constant  and  m of its own  lagged 
values: 

) 

a; =  0 +  a7.)  +  az,  +  +++  +  a@,,82.,,  +, 
[21.1.26] 
fort  =  1,2,...,  7. The  sample  size  T times  the centered  R;, from  the  regression 

/ 

‘As  noted  in Section  14.4,  maximum  likelihood  estimation  can  itself be viewed  as  estimation  by 

GMM  in which  the  orthogonality  condition  is that  the  expected  score  is zero, 

664  Chapter  21  | Time  Series  Models  of Heteroskedasticity 

.  Of [21.1.26] then converges  in distribution  to a x’ variable  with m degrees of freedom 
-under  the  null  hypothesis  that  u, is actually  i.i.d.  N(O,  a). 
Recalling  that  the  ARCH(m)  specification  can  be regarded  as  an  AR(m) 
process  for u?, another  approach  developed  by Bollerslev  (1988)  is to use  the Box- 
_  Jenkins  methods  described  in Section  4.8  to  analyze  the  autocorrelations  of u?. 
Other  tests for ARCH  are  described  in Bollerslev,  Chou,  and  Kroner  (1992,  p. 8). 

21.2.  Extensions 

— 

Generalized  Autoregressive  Conditional 
Heteroskedasticity 

(GARCH 

_  Equations  [21.1.9]  and  [21.1.10]  described  an  ARCH(m)  process  (u,) char- 

acterized  by 

, 

Uu, ve Vhy 
where  », is i.i.d.  with  zero mean  and unit  variance  and  where  h, evolves  according 
to 

aa 

GTi 

) 

i 

| 

- 

py 

aes  h, =  g +  a@,u?_,  +  a,u?_,  + i.  2  +  Onli  we 

>. 

More generally,  we can  imagine  a process  for which  the conditional  variance de-- 
Phones 
pends on  an infinite  number  of lags of u?_,,  _ 
[21.2.1] 
b= f+  g(Lu?, 
Racin 

fo 

2h 

5  :  ie 

where  AJA)  8 

kt 

' 

. 

WG] 

.stamsxs 

104 

6 

vivil 

. 

A 

>  a  4 

pee  ats  DEOCGR.  1 

i 

A natural 
idea is to  parameterize 
a 

atinit 

y 

ba 

, 

ae  SF  4.9)  i 1.6  i 

bj  sof  fain 

m 

7(L) as the ratio of two finite-order  polynomials: 

; 

Fe 

- 

weve?  ay:  (  aL  + a Paks  oe 

a, 

gts 

ed 

ey  ce  2) Bh ST AE  OE  9 5 APACE  He 
re 

i  ec, 
roots  of 1 — 5(z) =  0  are outside 
the unit 
vie L),  the result is 

the a0frte  rotate 
fo: 

Se  yee  © 

Peer 

eens 

CHS 

z 

; 

n 

‘ 

4aNyler 
] 

. 

,.. 

C 

ae 

be 

or 

u;  =  KF  (5,  +. @,)u?_,  so  (6,  +  y)U?_>  9°  8 

‘ 

[21.2.4] 

<  (5,  +  @,  Ur»  TiWe 

5, es 

52W,-2  ie 

waiter  tae 

where  w,  =  u?  —  h, and  p  =  max{m,  r}. ‘We  have  further  defined  6; =  0 forj  >r 
and  a,  =  0 for  j >  m.  Notice  that  h, is the  forecast  of u? based  on  its  own  lagged 
values  and  thus  w,  =  u?  —  h, is the  error  associated  with  this  forecast.  Thus,  w,  is 
a  white  noise  process  that  is fundamental  for  u?.  Expression  [21.2.4]  will  then  be 
recognized  as  an  ARMA(p,  r) process  for  u?,  in  which  the  jth  autoregressive 
coefficient  is the  sum  of 5; plus  a; while  the  jth moving  average  coefficient  is  the 
negative  of  5,. If u,  is described  by  a GARCH(r,  m) process,  then  u? follows  an 
ARMA(p,  r) process,  where p is the  larger  of r and  m. 

The  nonnegativity  requirement  is satisfied  if  x  >  0 and  a;  =  0,  6; =  0 for 
j =  1,2,...,p./From  our  analysis  of ARMA  processes,  it then  follows  that  u? 
is covariance-stationary  provided  that  w,  has  finite  variance  and  that  the  roots  of 

1  ra  (6;  +  a,)z  Ae  (6,  +  a)z?  =‘  sD  (6,  7  a, )z?  “2 

are  outside  the  unit  circle.  Given  the  nonnegativity  restriction,  this  means  that 
u? is covariance-stationary  if 

(6;  +  a,)  +  (6,  +  a)  +++ 

+  (6,  +  a,)  <1. 

Assuming  that  this  condition  holds,  the  unconditional  mean  of u? is 

E(u?)  =  o?  =  k/{1  =  (6,  +  @)  a  (6,  +  @>)  a  pe  ae 

ee  (6, Se  a, )}. 

Nelson  and Cao  (1992) noted  that the conditions  a; = 0 and 6; = 0 are  sufficient 
but  not  necessary  to  ensure  nonnegativity  of h,. For  example,  for  a GARCH(1,  2) 
‘process,  the  7(L)  operator  implied  by [21.2.2]  is given  by 

a(L)  =  (1  —  6,L)~"(a,L  +  aL?) 

(1  +  6,L  a  67 L?  of a  +  8%  ‘\(a,L  +  a,L*) 
aL  +  (6;a,  +  a,)L?  +  5,(6,a,  +  a)L? 

fe  63(5,a,  +  a,)L‘4  . 

The  7; coefficients  are  all  nonnegative  provided  that  0 <  6, <  1, a,  =  0, and 
(5,0;  +  a) =  0.  Hence,  a,  could  be  negative  as  long as  —a,  is less  than  5,0). 

The forecast  of u?,, based  onu?,u?_,,...  ,  denoted  “i?, 5), can  be calculated 

as  in [4.2.45]  by iterating  on 

(6,  +  @1)(A?,5—  111 —  0)  +  (5, +  @2)(A7..  210  ~  a 
=e 

(6, ng Ay  (OF.  pir Pa ao”) oe,  =  5,4:,-; 
es 

ee  i 

ee  eo  ee 

et 

Cn  el 

(6,  -  @,)(07,,~1),  -  a”) +  (6, +  a2)(A?, 5-21,  ae! o°) 
$+:  5°  +  (6+  a, )(83,..5,—  07) 

fi 
fotsert  let  2 

a2,  =  u? 
Wy  =  42, —  04,1  ...forr  ew ht—lss.,t—04 1, 
. 
See  Baillie  and  Bollerslev  (1992)  for further  discussion  of  forecasts  and  mean 
squared  errors  for  GARCH  processes. 

forrst 

Calculation  of  the  sequence  of conditional  variances  {h,}7.,  from  [21.2.3] 
requires  presample  values  for  hipats+  ++,  ho and  u2,,,,...,  ud.  If we  have 
666  Chapter 21  | Time Series Models  of Heteroskedasticity 

observations  on  y, and  x, fort  =  1,2,...,  T, Bollerslev  (1986,  p.  316)  suggested 
setting 

where 

A= up =f  forj= 
"=p +1,.:.,0, 

rT 

9 NR  | 
> 

=  te (y,  —  x/B)?. 

The sequence  {h,}7_,  can  be  used  to  evaluate  the  log  likelihood  from  the 
expression  given  in [21.1.20].  This  can  then  be  maximized  numerically  with  respect 
to  B and  the  parameters  x,  6,,...  ,  5,, a,  . 
»  @»  of the  GARCH  process;  for 
details,  see  Bollerslev  (1986). 

. 

. 

Integrated  GARCH 

Suppose  that  u,  =  Vh,-v,,  where  v, is i.i.d.  with  zero  mean  and  unit  variance 

and  where  h, obeys  the  GARCH(r,  m) specification 

h,  =K«K  +  5,h,_1  +  5,h,_>2  aor  wer  rig  6,h,_, 

+  a,u?_,  +  aju?_,  ++  °°  +'a,,U?_,,. 

We  saw  in  [21.2.4]  that  this  implies  an  ARMA  process  for  u? where  the  jth au- 
toregressive  coefficient  is given  by (6;  +  a;).  This  ARMA  process  for  u? would 
have  a  unit  root  if 

| 

25) 

j=1 

j=1 

a  =  1, 

[21.2.5] 

Engle  and  Bollerslev  (1986)  referred  to  a model  satisfying  [21.2.5]  as  an  integrated 
GARCH  process,  denoted  IGARCH. 

If u,  follows  an  JGARCH  process,  then  the  unconditional  variance  of  u,  is 
infinite,  so  neither  u, nor  u? satisfies  the definition  of a covariance-stationary  proc- 
ess.  However,  it is still  possible  for u, to  come  from a strictly  stationary  process  in 
the sense  that  the unconditional  density of u, is the same  for all t; see  Nelson  (1990). 

The  ARCH-in-Mean  Specification 

Finance  theory  suggests  that  an  asset  with  a higher perceived  risk  would  pay 
a higher  return  on  average.  For  example,  let  r, denote  the  ex  post  rate  of return. 
on  some  asset  minus  the  return  on  a  safe  alternative  asset.  Suppose  that  r, is 
decomposed  into  a component  anticipated  by investors  at date  t —  1 (denoted  u,) 
and  a component  that  was  unanticipated  (denoted  u,): 

n=  ph,  +  uy. 

Then  the theory suggests  that the mean  return  (1,) would  be related  to the variance 
of the return  (h,).  In general,  the ARCH-in-mean,  or ARCH-M,  regression  model 
introduced  by Engle,  Lilien,  and  Robins  (1987)  is characterized  by 

, 

| 

y,  =  xB +  5h,  + 

u,  =  Vh,: uy, 
h, =  € +  au2_,  +  au?_,+++*  + On  Ui  m 
for  v, i.i.d.  with  zero  mean  and  unit  variance.  The  effect  that  higher  perceived 
variability  of u, has on  the  level  of y, is captured by the  parameter  6. 

21.2.  Extensions  667 

Exponential  GARCH 
As before,  let u,  =  Vh,-v, where  v, is i.L.d.  with  zero  mean  and  unit  variance. 
Nelson  (1991)  proposed  the  following  model  for  the  evolution  of  the  conditional 
variance  of u,: 

log h, =  £ +  >, m,;*{|v,-;|  —  Elv,-|  +  Xv,-;}- 

[21.2.6] 

x 

Nelson’s  model  is sometimes  referred  to  as  exponential  GARCH,  or  EGARCH.  If 
1;  >  0, Nelson’s  model  implies  that  a  deviation  of  |v,_;|  from  its  expected  value 
causes  the  variance  of u,  to  be  larger  than  otherwise,  an  effect  similar  to  the  idea 
behind  the  GARCH  specification. 

The  X parameter  allows  this  effect  to  be asymmetric.  If  X =  0, then  a positive 
surprise  (v,_;  >.0)  has  the  same  effect  on  volatility  as  a  negative  surprise  of the  - 
same  magnitude.  If  —1  <  X&  <  0, a  positive  surprise  increases  volatility  less  than 
a  negative  surprise.  If  X <  —1,  a positive  surprise  actually  reduces  volatility  while 
a  negative  surprise  increases  volatility. 
A number  of researchers  have  found  evi- 
dénce  of asymmetry  in stock  price  behavior—negative  surprises  seem  to  increase 
volatility  more  than  positive  surprises.*  Since  a  lower stock  price  reduces  the value 
of equity  relative  to  corporate  debt,  a sharp  decline  in stock  prices  increases  cor- 
porate  leverage  and  could  thus  increase  the  risk  of holding  stocks.  For  this  reason, 
the  apparent  finding  that  X <  0 is sometimes  described  as  the  leverage  effect. 

One  of  the  key  advantages  of  Nelson’s  specification  is  that  since  {21.2.6} 
describes  the  log of h,, the variance  itself  (h,) will  be positive  regardless  of whether 
_  the  7; coefficients  are  positive.  Thus,  in  contrast  to  the  GARCH  model,  no  re- 
strictions  need  to  be  imposed  on  [21.2.6]  for  estimation.  This  makes  numerical 
-  Optimization  simpler  and  allows  a  more  flexible  class  of possible  dynamic  models 
for the  variance.  Nelson  (1991,  p. 351)  showed  that [21.2.6]  implies  that  log A,, h,, 
and  uy, are  all  strictly  stationary  provided  that  27,7?  <  ©, 

| 

A natural  parameterization  is to  model  z(L)  as  the  ratio  of two  finite-order 

[ee as  in the  GARCH(r,  m) specification: 

+ 6, log nig t+ aly, ‘h = “Be. . + Re (21.2.7) 

— 

- 

| 

| 

+  a a; 

A 

Elv,_ “al +  Nv,_ A 

~  an{}v,_ al -  Ely, al +  Rv,  a} ih 

The  EGARCH  model  can  be estimated  by maximum  likelihood by specifying 
a density  for v,.  Nelson  proposed  using the generalized  error  distribution,  normal- 
ized  to  have  zero  mean  and  unit  variance: 

) 

45 

_  vexp[—(1/2)]v,/A]"] 

fo)  =  arama 

(21.2.8) 

Here  ['(-)  is the  gamma  function,  A is a constant  given  by 

eee  [erwin 
['(3/v) 

“See  Pagan  and  Schwert  (1990),  Engle  and  Ng (1991),  and the studies  cited in  mer eid Chou, 

and  Kroner  (1992,  p. 24). 

668  Chapter 21  | Time  Series  Models  of Heteroskedasticity 

and » is a positive  parameter  governing  the  thickness  of the  tails.  For  v  =  2, the 
constant  A  =  1 and  expression  [21.2.8]  is  just  the  standard  Normal  density.  If 
v  < 2,  the  density  has  thicker  tails  than  the  Normal,  whereas  for  v  >  2 it  has 
thinner tails.  The  expected  absolute  value  of a variable  drawn  from  this  distribution 
is 

Ely)  = 

Uv 

X2u"P@y) 
r(1/v) 

For  the  standard  Normal  case  (v =  2), this  becomes  - 

E}v,|  =  Vin. 

As an  illustration  of how  this  model  might  be used,  consider  Nelson’s  analysis 
interest  rate 

of stock  return  data.  For  r, the  daily  return  on  stocks  minus  the  daily 
on  Treasury  bills,  Nelson  estimated  a  regression  model  of the  form 

rT,  =art  br,-1  +  5h,  +  U,. 

The  residual  u, was  modeled  as  Vh,-v,,  where  v, is i.i.d.  with  density  [21.2.8]  and 
where  h, evolves  according  to 

log h, —  g, =  8,(log  h,-.  —  &-1)  +  S{log h,_2  —  £,-2) 

*  a,{|v,_1|  .  E|v,_,|  +  Rv,_ i} 

[21.2.9] 

+  a,{|v,2]  —  E|v,-2|  +  Xv,_2}. 

Nelson  allowed  @,, the  unconditional  mean  of log h,, to  be  a  function. of time: 

g$=¢+t  log(1  +  pN,), 

where  N, denotes  the  number  of nontrading  days  between  dates  ¢  —  1 and  t and 
{ and  p are  parameters  to  be  estimated  by maximum  likelihood.  The  sample  log 
likelihood  is then 

& =  T{log(v/A)  —  (1°+  v~1) log(2) —  log{T'(1/r)]} 

ys 

T 

—  (1/2) >» \(r, — a  —  br,  —  5h, MA:  Vh)|” —  (1/2) >» log(h,). 

The  sequence  {h,}7_,  is obtained  by iterating  on  [21.2.7]  with 

and  with presample  values  of log h, set  to  their unconditional  expectations  ¢,. 

Uys:  (r, oT  Qs  br.  -  5h, )/Vh, 

Other  Nonlinear  ARCH  Specifications 
Asymmetric  consequences  of positive  and  negative  innovations  can  also  be 
captured  with  a  simple  modification  of the  linear  GARCH  framework.  Glosten, 
Jagannathan,  and Runkle (1993) proposed  modeling  u,  =  Vh,-v,, where  0, is i.i.d. 
with  zero  mean  and  unit  variance  and 

h, =K  +  5,h,-;  7  a,ur_,  +  Nu2_,T,.). 

(21.2.10] 

Here, [,., =  1 if u,-;20andL,_,  =  Oif u,-,  <0. Again,  if the leverage  effect 
holds,  we  expect  to find X <  0. The  nonnegativity  condition  is satisfied  provided 
that  6, =  0 anda,  +  X20. 

A variety  of other  nonlinear  functional  forms  relating h, to NS  Se  ER 
have been  proposed.  Geweke  (1986), Pantula  (1986), and Milhaj  (1987) suggested 

; 

| 

21.2.  Extensions 

669 

a specification  in  which  the  log of h, depends  linearly  on  past  logs of the  squared 
residuals.  Higgins  and  Bera  (1992)  proposed  a  power  transformation  of the  form 

h,  =  [6°  +  ay(u?_1)®  +  a(u2_9)®  +  +0 

+  Am(Ur—m) DP; 

with  £ >  0,  5 >  0,  and  a,  =  0 fori  =  1, 2,...,  m.  Gourieroux  and  Monfort 
(1992)  used  a  Markov  chain  to  model  the  conditional  variance  as  a  general  stepwjse 
function  of past  realizations. 

Multivariate  GARCH  Models 
The  preceding  ideas  can  also  be  extended  to  an  (n  x  1) vector  y,.  Consider 

a  system  of n  regression  equations  of the  form 

¥,.  Bellew 

(nx 1) 

(nx k) (k x1) 

u,, 

(nx  1) 

where  x,  is  a  vector  of  explanatory  variables  and  u,  is  a  vector  of  white  noise 
residuals.  Let  H, denote  the  (n  x  n) conditional  variance-covariance  matrix  of the 
residuals: 

H, =  E(u,u;ly,-1,  Yi-29  + 

+ 

+ 

>  Mex  M15  - 

-  -)- 

Engle  and  Kroner  (1995)  proposed  the  following  vector  generalization  of  a. 
GARCH(r,  m) specification: 

age 

cf 

— 

H,  =  K  +  A,H,_,4;  +  4,H,-24;  +  -:-  +  4,H,_,A;  +  A,u,_,u;_,A; 

si A2U,_  2U,_ 2A  ara 

hi 

tas  A,M,—mY:—-mAm- 

Here  K, A,,  and  A, fors  =  1,2,  ..  .  denote  (m  X  n) matrices  of parameters.  An 
advantage  of this  parameterization  is that  H, is guaranteed  to  be  positive  definite 
as long as K is positive  definite,  which  can  be ensured  numerically  by parameterizing 
K as  PP’,  where P is a  lower  triangular  matrix. 

In practice,  for  reasonably  sized n it is necessary  to  restrict  the  specification 
for  H, further  to  obtain  a  numerically  tractable  formulation.  One  useful  special 
case  restricts  A, and  A, to  be diagonal  matrices  fors  =  1,2,...  .  In such  a model, 
the  conditional  covariance  between  u;,  and  u,;,  depends  only  on  past  values  of 
u;,-s'U4j,-s,  and  not  on  the  products  or  squares  of other  residuals. 

Another  popular  approach  introduced  by Bollerslev  (1990)  assumes  that  the 
conditional  correlations  among  the  elements  of  u,  are  constant  over  time.  Let 
h{?  denote  the  row  i, column  i element  of H,. Thus,  A‘ represents  the conditional 
variance  of the  ‘th  element  of u,: 

ny’  =  E(uily,-1,  Jendy  +a»  »  X,,  os 

5 

This conditional  variance  might be modeled  with a univariate  GARCH(1,  1) process 
driven  by the  lagged  innovation  in variable  i: 

AY?  =  «+  &AY-”  +  aju?,_). 
We  might postulate  n  such  GA RCH specifications  (i =  1,2,...  ,  m), one  for each 
element  of u,.  The  conditidnal  covariance  between  u,, and u;,, or  the row  i, column 
j element  of H,, is then  taken  to  be a constant  correlation  p,; times  the  conditional 
standard  deviations  of u,, and  u,,: 

hi)?  =  E(uj,,uj,|¥.-15  Y:-25  o 

+ 

6 

yg  Rey  Xo 15  to  a =  By  VAD  VEO. 

Maximum  likelihood  estimation  of this specification  turns  out  to be quite tractable; 
see  Bollerslev  (1990)  for  details. 
670  Chapter 21  | Time  Series  Models  of Heteroskedasticity 

; 

Other  multivariate  models  include  a  formulation  for  vech(H,)  proposed  by 
Bollerslev,  Engle,  and  Wooldridge  (1988)  and  the  factor  ARCH  specifications  of 
Diebold  and  Nerlove  (1989)  and  Engle,  Ng,  and  Rothschild  (1990). 

Nonparametric  Estimates 
Pagan  and  Hong  (1990)  explored  a  nonparametric  kernel  estimate  of  the 
expected value  of u?. The  estimate  is based  on  an  average  value  of those  u2 whose 
preceding values  of u,_,,uU,_>,..  . 
,  U,_,,  were  “close”  to  the  values  that  preceded 
u?: 

T 

h,  -  x w,(t)-u2. 

T= 

T#! 

The  weights  {w,(t)}7_, 4, are  a  set  of  (T —  1) numbers  that  sum  to  unity.  If the 
values  of u,_,,  U,_2,...  ,  U;_,,  that  preceded  u, were  similar  to  the  values  u,_,, 
U,_>,...,  U,_,,  that  preceded  u,,  then  uv? is viewed  as  giving  useful  information 
about  h, =  E(u?|u,_,,  u,-2,  -  ..  ,  U;-m).  In  this  case,  the  weight  w,(t)  would  be 
large.  If the  values  that  preceded  u, are  quite  different  from  those  that  preceded 
u,,  then  u? is viewed  as  giving  little information  about  h, and  so  w,(t) is small.  One 
popular  specification  for  the  weight  w,(t)  is to  use  a  Gaussian  kernel: 

x(t)  =  [] (227) ~1A;-*  exp[—(u,_;  —  u,-;)7/(2A})]. 

j=l 

The  positive  parameter  A, is known  as  the  bandwidth.  The  bandwidth  calibrates 
the  distance  between  u,_;  and  u,_;—the  smaller  is A;, the  closer  u,_;  must  be  to 
u,_; before  giving  the  value  of u? much  weight in estimating  h,. To ensure  that  the 
weights  w,(t)  sum  to  unity,  we  take 

The  key difficulty  with constructing  this estimate  is in choosing  the bandwidth 
parameter  A,  One  approach  is  known  as  cross-validation.  To  illustrate  this  ap- 
proach,  suppose  that  the  same  bandwidth  is selected  for  each  lag (A; =  A forj  = 
1, 2,...,.m).  Then  the  nonparametric  estimate  of h, is implicitly  a  function  of 
the  bandwidth  parameter  imposed,  and  accordingly  could  be  denoted  h,(A).  We 
might  then  choose  A so  as  to  minimize 

wy 

4 

> [u? oa h,(a)fp. 

Semiparametric  Estimates 
Other approaches  to describing  the conditional  variance  of u, include general 
-  :) as  in Pagan  and Schwert 
series  expansions  for the  function  h, =  h(u,-1;  Uy-2y + 
(1990,  p. 278) or  for the  density f(v,)  itself  as  in  Gallant  and  Tauchen  (1989)  and 
Gallant,  Hsieh,  and Tauchen  (1989).  Engle  and Gonzalez-Rivera  (1991)  combined  | 
a parametric  specification  for h, with  a nonparametric  estimate  of the density  of v, 
in  [21.1.9]. 

7 

21.2.  Extensions 

671 

Comparison  of Alternative  Models 
of Stock  Market  Volatility 
A  number  of approaches  have  been  suggested  for  comparing  alternative  ARCH 
specifications.  One  appealing  meashny  is to  see  how  well  different  models  of  het- 
eroskedasticity  forecast  the  value  of u?. Pagan  and  Schwert  (1990)  fitted  a number 
of different  models  to  monthly  U.S.  stock returns  from  1834  to  1925.  They  found 
that  the  semiparametric  and  nonparametric  methods  did  a  good  job  in  sample, 
though  the  parametric  models  yielded  superior  out-of-sample  forecasts.  Nelson's 
EGARCH  specification  was  one  of the  best  in overall  performance  from  this  com- 
parison.  Pagan  and  Schwert  concluded  that  some  benefits  emerge  from  using  par-, 
‘ametric  and  nonparametric  methods together. 

Another  approach is to calculate  various  specification  tests  of the fitted  model. 
Tests  can  be constructed  from  the  Lagrange  mutiplier  principle  as  in Engle,  Lilien, 
and  Robins  (1987)  or  Higgins  and  Bera  (1992),  on  moment  tests  and  analysis  of 
outliers  as  in  Nelson  (1991),.or-on  the  information  matrix  equality  as  in Bera  and 
Zuo  (1991).  Related  robust  diagnostics  were  developed  by Bollerslev  and  Woold- 
ridge  (1992).  Other  diagnostics  are  illustrated  in Hsieh  (1989).  Engle  and  Ng (1993) 
suggested  some  particularly  simple  tests  of  the  functional  form  of  h, related  to 
Lagrange  multiplier  tests,  from  which  they  concluded  that  Nelson’s  EGARCH 
specification  or  Glosten,  Jagannathan,  and  Runkle’s  modification  of GARCH  de- 
scribed  in  [21.2.10]  best  describes  the  asymmetry  in  the  conditional  volatility  of 
Japanese  stock  returns. 

Engle  and  Mustafa  (1992) proposed  another  approach  to  assessing  the  use- 
fulness  of a  given  specification  of the  conditional  variance  based  on  the  observed 
prices  for  security  options.  These  financial  instruments  give an  investor  the  right 
to  buy or  sell  the  security  at  some  date in  the  future  at  a  price  agreed  upon  today. 
The  value  of such  an  option  increases  with  the perceived  variability  of the  security. 
If the  term  for  which  the  option  applies  is sufficiently  short  that  stock  prices  can  ~ 
be approximated  by Brownian  motion  with constant  variance,  a well-known  formula 
developed  by Black  and  Scholes  (1973)  relates  the  price of the  option  to  investors” 
perception  of the  variance  of the  stock  price.  The  observed  option  prices  can  then 
«be  used  to  construct  the  market’s  implicit  perception  of h,, which  can  be compared 
with  the  specification  implied  by a  given  time  series  model.  The  results  of  such 
comparisons  are  quite  favorable  to  simple  GARCH  and  EGARCH  specifications. 
Studies  by Day  and  Lewis  (1992)  and  Lamoureux and  Lastrapes  (1993)  suggest 
that  GARCH (1, 1) or  EGARCH(1,  1) models can  improve  on  the market's  implicit 
assessment  of h,.  Related  evidence  in  support  of the  GARCH(1,  1) formulation 
was  provided  by Engle,  Hong,  Kane,  and  Noh  (1993)  and  West,  Edison,  and  Cho 
(1993). 

APPENDIX  21.A.  Derivation  of Selected  Equations for Chapter 21 

This  appendix  provides  the  details  behind  several  of the  assertions  in the  text. 

®  Derivation  of (21.1.21]. 

Observe  that 

d log  f(y:|1X,,  Y,-15 8)  _ 
20 

_  1a log h, 
,  ® 

~2{  TA, = BY _ 

2|h, 

8 

LAL 

=  xB) oh, 
h?  0 

672 

Chapter  21  | Time Series  Models  of Heteroskedasticity 

[21.4.2] 

=  0¢/30 +  53 (dar,/80) + u2_,  + > a, (du?_,/0) 

j=1 

0 

: 

iba}  hy  ehss L  a  Mw  ee  aM aioe  ai Ri 01: oe <r  te 
Sobtoing [21.8.2] and (21.4.3) antec uA | Reere cients W  nediet  bus  2shei)  29508 
SS  ee %  oe  SULA  srseals 
PSOne  aa  cae Se  ae p 

WORD?  \e Aatounl..  ESE  Fi 

t ide BALAK  tsi 

eadones  ! 

SL  agi ¥ 

sto  te 

eorti 

nersvinls  ” sagas  RA 
ure leh  et SA aigis x  Ais Bods 4  thee 
1 
izwtouys 2 wondnor  a 

| 

aM 

Noruct 

att lormuat  233 

Thus.  taking  expectations  of  [21.A.5]  conditional  on  x,  and  Y,_,  results  in 

ds,(8) 

_  _1dlogh,  od log  h,  pi  x,X,  | 

e| ae  |  a,.} OY 

OMB 

©  Sage 

h,| 

9 

0 

wine  ghaslatre  a  at 

=e, _ X.-, 

2h? 

é; 

z,(B) 

m 

S -2au,,x/-,  [2,(B)] 
=I 

3 

: 

PL  x 
0 
h,| 

OF 

where  the  last  equality  follows  from  [21.A.3]. 

@ 

Chapter  21  References 

Baillie,  Richard  T.,  and  Tim  Bollerslev.  1989.  ‘‘The  Message  in  Daily  Exchange  Rates:  A 
Conditional  Variance  Tale.”’  Journal  of Business  and  Economic  Statistics  7:297-305. 

and 

.  1992.  “Prediction  in Dynamic  Models  with Time-Dependent  Conditional 

Variances.”  Journal  of Econometrics  52:91-113. 
Bates,  Charles,  and  Halbert  White.  1988.  ‘Efficient  Instrumental  Variables  Estimation  of 
Systems  of  Implicit  Heterogeneous  Nonlinear  Dynamic  Equations  with  Nonspherical  Er- 
rors,”  in  William  A.  Barnett,  Ernst  R.  Berndt,  and  Halbert  White,  eds.,  Dynamic  Econ- 
ometric  Modeling.  Cambridge,  England:  Cambridge  University  Press. 
Bera,  Anil  K.,  and  X.  Zuo.  1991.  ‘Specification  Test  for  a Linear  Regression  Model  with 
ARCH  Process.”  University  of Illinois  at  Champaign-Urbana.  Mimeo. 
Berndt,  E.  K., B. H.  Hall,  R.  E. Hall,  and  J.  A.  Hausman.  1974.  ‘‘Estimation  and  Inference 
in  Nonlinear  Structural  Models.”  Annals  of Economic  and  Social  Measurement  3:653-65. 
Black,  Fischer,  and  Myron  Scholes.  1973.  ‘‘The  Pricing  of Options  and  Corporate  Liabili- 
ties.”’  Journal  of Political  Economy  81:637-54. 
Bollerslev,  Tim.  1986.  ‘‘Generalized  Autoregressive  Conditional  Heteroskedasticity.”’  Jour- 
nal  of Econometrics  31:307-27. 

.  1987.  “A  Conditionally  Heteroskedastic  Time  Series  Model  for  Speculative  Prices 

and  Rates  of Return.”  Review  of Economics  and  Statistics  69:542-47. 

.  1988.  ‘‘On the Correlation  Structure  for the Generalized  Autoregressive  Conditional 

Heteroskedastic  Process.”  Journal  of Time  Series  Analysis  9:121-31. 

.  1990.  “Modelling  the  Coherence  in Short-Run  Nominal  Exchange  Rates:  A  Mul- 

tivariate  Generalized  ARCH  Model.”  Review  of Economics  and  Statistics  72:498-505. 

,  Ray  Y.  Chou,  and  Kenneth  F.  Kroner.  1992.  “ARCH  Modeling  in  Finance:  A 

Review  of the  Theory  and  Empirical  Evidence.’’  Journal  of Econometrics  52:5-59. 

,  Robert  F. Engle,  and Jeffrey M. Wooldridge.  1988.  ‘A Capital  Asset  Pricing Model 

with  Time  Varying  Covariances.”  Journal  of Political  Economy  96:116-31. 

and  Jeffrey  M.  Wooldridge.  1992.  “Quasi-Maximum  Likelihood  Estimation  and  Infer- 
reg in  Dynamic  Models  with  Time  Varying  Covariances.”  Econometric  Reviews  11:143- 

Cai,  Jun.  1994.  “A  Markov  Model  of Switching  Regime  ARCH.”  Journal  of Business  and 
Economic  Statistics  12:309-16, 
Day, Theodore  E., and Craig M. Lewis.  1992.  ‘Stock  Market  Volatility  and the Information 
Content  of Stock  Index  Options.”  Journal  of Econometrics  52:267-87. 
DeGroot,  Morris  H.  1970.  Optimal  Statistical  Decisions.  New  York:  McGraw-Hill. 
Diebold,  Francis  X., and Mark  Nerlove.  1989.  ‘The  Dynamics  of Exchange  Rate Volatility: 
A Multivariate  Latent  Factor  ARCH  Model.”  Journal  of Applied  Econometrics  4:1-21., 
Engle,  Robert  F.  1982.  ‘Autoregressive  Conditional  Heteroscedasticity  with  Estimates  of 
the  Variance  of United  Kingdom  Inflation.’  Econometrica  50:987-1007. 

and  Tim  Bollerslev.  1986.  “Modelling  the  Persistence  of Conditional  Variances.” 

Econometric  Reviews  5:1-50. 

674  Chapter 21  | Time Series Models  of Heteroskedasticity 

Maleal  dad  Ranianuaie  Sterieee:  my  oy  Semiparametric  ARCH  Models.” 

and  Gloria  Gonzalez-Rivera!  1991.  “Semipar: 

: 

Journal  of 

,  Ted  Hong,  Alex  Kane,  and  Jaesun  Noh.  1993.  “Arbitrati 

ion  Valuation  of  Variance 
ence  a Simulated  Options  Markets.”  Advances  in  Futures  and  Options  Research 

7 

‘ 

. 

Econometrie  Theory  11:123-50 

and  Kenneth  F.  Kroner.  1993.  “ 

Multivariate  Simultaneous  Generalized  ARCH. 

ivari 

” 

,  David  M.  Lilien,  and  Russell  P.  Robins. 

; 
Premia  in  the  Term  Structure:  The  ARCH-M  Model.”  Econometrica  55:391—407.  . 

1987.  ‘Estimating 

g Time  Varying 

e 

; 

; 

Ti 

| 

Risk 

and  Chowdhury  Mustafa.  1992.  “Implied  ARCH  Models  from  Options  Prices.” 

Journal  of Econometrics  52:289-311. 

. 

and  Victor  K.  Ng. 1993.  “Measuring  and  Testing  the  Impact  of News  on  Volatility.” 

Journal  of Finance  48:1749-78. 

-  Victor  K.  Ng,  and  Michael  Rothschild.  1990.  ‘‘Asset  Pricing  with 

a  FACTOR- 
ARCH  Covariance  Structure:  Empirical  Estimates  for  Treasury  Bills.”’  Journal  of Econo- 
metrics  45:213-—37. 
Ferson,  Wayne  E.  1989.  ‘Changes  in  Expected  Security  Returns,  Risk,  and  the  Level  of 
Interest  Rates.”  Journal  of Finance  44:1191-1218. 
Gallant,  A.  Ronald,  David  A.  Hsieh,  and  George  Tauchen.  1989.  ‘‘On  Fitting  a  Recalcitrant 
Series:  The  Pound/Dollar  Exchange  Rate  1974-83.”  Duke  University.  Mimeo. 

and  George  Tauchen.  1989.  ‘‘Semi  Non-Parametric  Estimation  of  Conditionally 
pay Heterogeneous  Processes:  Asset  Pricing Applications.”’  Econometrica  57:1091- 

A Comment.” 

Geweke,  John.  1986.  “‘Modeling  the  Persistence  of Conditional  Variances: 
Econometric  Reviews  5:57-61. 
Glosten,  Lawrence  R.,  Ravi  Jagannathan,  and  David  Runkle.  1993.  ‘Relationship  between 
the  Expected  Value  and  the  Volatility  of the  Nominal  Excess  Return  on  Stocks.”  Journal 
of Finance  48:1779-1801. 
Gourieroux,  Christian,  and  Alain  Monfort.  1992.  ‘“‘Qualitative  Threshold  ARCH  Models.”’ 
Journal  of Econometrics  52:159-99. 
Hamilton,  James  D., and Raul  Susmel.  1994.  “Autoregressive  Conditional  Heteroskedasticity 
and Changes  in Regime.” Journal  of Econometrics  64:307-—33. 
Higgins,  M.  L., and  A.  K.  Bera.  1992..‘‘A  Class of Nonlinear  ARCH  Models.”  Jnternational 
Economic  Review  33:137-S8. 
Hsieh,  David  A.  1989.  ‘“‘Modeling  Heteroscedasticity  in  Daily  Foreign-Exchange  Rates.”’ 
Journal  of Business  and  Economic  Statistics  7:307-17. 
_  Jorion,  Philippe.  1988.  “On  Jump  Processes  in the  Foreign  Exchange  and  Stock  Markets.” 

Review  of Financial  Studies  1:427-45. 
Lamoureux,  Christopher  G.,  and  William  D.  Lastrapes.  1993.  “Forecasting  Stock  Return 
Variance:  Toward  an  Understanding  of Stochastic  Implied  Volatilities.””  Review  of Financial 
Studies  5:293-326. 
Mark,  Nelson.  1988.  “Time  Varying Betas and Risk Premia  in the Pricing of Forward Foreign 
Exchange  Contracts."  Journal  of Financial  Economics  22:335-54. 
Milhgj,  Anders.  1985.  ‘The  Moment  Structure  of ARCH  Processes."  Scandinavian  Journal 
of Statistics  12:281-92. 

.  1987.  ‘A  Multiplicative  Parameterization  of ARCH  Models:  Department  of Sta- 

- 
tistics,  University  of Copenhagen.  Mimeo. 
Nelson,  Daniel  B.  1990.  ‘“‘Stationarity  and  Persistence  in the  GARCH(1,  1) Model.”  Econ- 
ometric  Theory  6:318-—34. 

ie 

| 

_  1991.  “*Conditional  Heteroskedasticity  in Asset  Returns: 

A New  Approach.”  Econ- 

|

ometrica  59:347-70. 
——__  and Charles  Q. Cao.  1992.  “Inequality  Constraints  in the Univariate  GARCH  Model.” 
Journal  of Business  and  Economic  Statistics  10:229-35. 
Pagan, Adrian  R.,  and  Y.  S.  Hong.  1990.  ‘‘Non-Parametric  Estimation  and  the  Risk Pre- 
mium,”  in W. Barnett,  J. Powell,  and G. Tauchen,  eds., Semiparametric  and Nonparametric 
Methods  in Econometrics  and Statistics.  Cambridge,  England:  Cambridge  University  Press. 
,  Adrian  R.,  and  G.  William  Schwert.  1990.  “Alternative  Models  for  Conditional 
Volatility.”  Journal  of Econometrics  45:267-90. 

| 

tock 

- 

, 

wh  aries  pir”  , 

Chapter 21  References 

675 

 
Pagan,  Adrian  R., and nee Ullah.  1988.  ‘‘The  Econometric  Analysis of Models  with  Risk 
Terms.” Journal  of Applied  Econometrics  3:87-105. 
Pantula,  Sastry G. 1986.  ‘“‘Modeling  the Persistence  of Conditional  Variances:  A Comment. Pe 
Econometric  Reviews  5:71-74. 
Rich,  Robert  W.,  Jennie  Raymond,  att  J.  9.  Butler.  1991.  “Generalized  Instrumental 
Variables  Estimation  of Autoregressive  Conditional  Heteroskedastic  Models.”  Economics 
Letters  35:179-85. 
Simon,  David P. 1989.  “‘Expectations  and Risk in the Treasury Bill Market:  An Instrumental 
Variables  Approach.”  Journal  of Financial  and  Quantitative  Analysis  24:357-66. 
Weiss,  Andrew  A.  1984.  “ARMA  Models with  ARCH  Errors.”  Journal  of Time  Series 
Analysis  5:129-43. 

.  1986.  ‘“‘Asymptotic  Theory  for  ARCH  Models:  Estimation  and  Testing.” ”  Econo- 

metric Theory 2:107-31. 
West, Kenneth  D., Hali J. Edison,  and Dongchul Cho.  1993.  “A Utility Based Comparison 
of Some  Models of Foreign  Exchange Volatility.”  Journal  of 
International  Economics 
~£} 
3= 23-46. 

ited 

ae 

oH  tourna 
zit 

Shire 

Dao? 

OOS:  SOA  Divelt  Dlanosl  A 

: 

fs  —FTE 

iz 

ronedonst 

OCF 
“St  +h  wtf  $97 a )  DS  --— 

ned  se t 

debs is 209  ott  . 
ost 

bas ms  mvs?  Ae  GOMES  TIES 

i  gees 

eal 

’ 

SYSHRIRA-BOM  tse” 
Pas 

7  ~~ 

; reaises >  SEE  bier  alegre  a yee  Fa22A,  -292252079 

apa Mesias 

Fa  i 

a 

ae  mtr 

1G Soh are  da aailopaly 

vie  eusaticnac 

a  rere  .  aie.  Pelee 

Das 

¥ 

§ 

5 

: 

itni¢e 

‘. 
vel 

b 

—_ 

a 

0 

: 

on  ere?  ae oat 
nas  a 
amore  ren 
} Staal nist A pan amidping a, x Sd  a 
CE -GET Ce sarap a 

erties  et 

eee 

>  te  Oe 

a 

Svtaes  Pics h hag!  Sevasie Ta lae, Ges a. ¥  ap 

’ 
ae 

2 EOE tell - 

Sa 

be 

Modeling  Time  Series 
with  Changes  in Regime 

22.1.  Introduction 

Many  variables  undergo  episodes  in  which  the  behavior  of  the  series  seems  to 
change  quite  dramatically.  A  striking  example  is provided  by Figure  22.1,  which 
is taken  from  Rogers’s  (1992)  study  of the  volume  of dollar-denominated  accounts 
held  in  Mexican  banks.  The  Mexican  government  adopted  various  measures  in 
1982  to  try to discourage  the use  of such  accounts,  and  the effects  are  quite dramatic 
in a  plot  of the  series. 

Similar  dramatic  breaks  will  be seen  if one  follows  almost  any macroeconomic 
or  financial  time  series  for  a sufficiently  long period.  Such  apparent  changes  in the 
time  series  process  can  result  from  events  such  as  wars,  financial  panics,  or  sig- 
nificant  changes  in government  policies. 

_  How  should  we  model  a  change  in the  process  followed  by a particular  time 
series?  For  the  data  plotted  in  Figure  22.1,  one  simple  idea  would  be  that  the 
constant  term  for  the  autoregression  changed  in 1982.  For  data  prior  to  1982  we  | 
might  use  a  model  such  as 

while  data  after  1982  might  be  described  by 

¥  —  Py  L O(y:-1  —  Hi)  +  &, 

|  22.1.1] 

7 

Ye  —  Be  =  $(Y,-1  os 7)  +  €,, 

[22.1.2] 

where  zn <  py. 

The  specification  in [22.1.1]  and  [22.1.2]  seems  a plausible  description  of the 
data  in Figure  22.1,  but  it is not  altogether  satisfactory  as  a time  series  model.  For 
example,  how  are  we  to  forecast  a series  that  is described  by [22.1.1]  and  [22.1.2]? 
If  the process  has  changed  in  the  past,  clearly  it could  also  change  again  in  the 
future,  and  this  prospect  should  be taken  into  account  in forming  a forecast.  More- 
over,  the  change  in  regime  surely  should  not  be  regarded  as  the  outcome  of  a 
perfectly  foreseeable,  deterministic  event.  Rather,  the  change  in regime  is itself  4 
random  variable.  A complete  time  series  model  would  therefore  include  a descrip- 
tion  of the  probability  law  governing  the  change  from  p41,  to  2. 

These observations  suggest  that we  might consider the process  to be influenced 
by an  unobserved  random  variable  s7, which  will  be called  the state  or  regime  that 
the process  was  in  at  date  1.  If s*  =  1, then  the  process  is in  regime  1, while 
s* =  2 means  that  the  process  is in regime  2. Equations  [22.1.1]  and  [22.1.2]  can 
then equivalently  be written  as 
he 
where  p,;  indicates  “, 

Fin,  eee  $(y.-1  7  Ms;  |) +  €,, 
when  s*  =  1 and  indicates  ,  when  s/  =  2. 

[22.1.3] 

. 

ef* 

i, 

4 

677 

78 

79 

80 

81 

82 

83 

84 

8S 

FIGURE  22.1  Log  of  the  ratio  of  the  peso  value  of  dollar-denominated  bank 
accounts  in Mexico  to the peso value  of peso-denominated  bank  accounts  in Mexico, 
monthly,  1978-85.  (Rogers,  1992). 

We  then  need a description  of  the  time  series  process  for  the  unobserved 
variable  s*.  Since  s* takes  on  only  discrete  values  (in this  case,  sj is either  1 or 
2),  this  will  be a  slightly  different  time  series  model  from  those  for  continuous- 
valued  random  variables  considered  elsewhere  in this  book. 

The  simplest  time  series  model  for  a  discrete-valued  random  variable  is  a 
Markov  chain.  The  theory  of Markov  chains  is reviewed  in Section  22.2.  In Section 
22.4  this  theory  will  be  combined  with  a  conventional  time  series  model  such  as 
an  autoregression  that  is assumed  to  characterize  any  given  regime.  Prior  to  doing 
so,  however,  it will  be helpful  to consider  a special  case  of such  processes,  namely, 
that  for which  @ =  0 in [22.1.3]  and s* is an  i.i.d.  discrete-valued  random  variable. 
Such  a specification  describes  y, as  a simple  mixture  of different  distributions,  the 
statistical  theory  for  which  is reviewed  in Section  22.3. 

? 

22.2..  Markov  Chains 

Let s, be a random  variable  that  can  assume  only an  integer value  {1,2,...,  N}. 
Suppose  that  the  probability  that s, equals  some  particular  value  j angen on  the 
past  only through  the  most  recent  value  s,_,: 

P{s,  =  j\s,-1  =  l, §,- 2=k,.  ..}  =  Pfs,  =  js.  =  i}  =  pj.  (2.2.1) 
Such  a process is described  as an N-state  Markov  chain  with transition  probabilities 
{Pishi,j=1,2,....v-  The  transition  probability  Pi; gives  the  oie  that  state  i will 
be followed by State  j. Note  that 

° 

Pig  SEPP 

OAS  Pay 

{22.2.2} 

678  Chapter 22  | Modeling  Time  Series  with  Changes  in Regime 

It is often  convenient  to  collect  the  transition  probabilities  in  an  (N  x  N) 

matrix  P known  as  the  transition  matrix: 

Pu 

Pa 

***  Pe 

Te 

Pp  "Pp 

Pp 
Belge 

ore 

(22.2.3] 

Pin  P2n 

*** 

PNN 

The  row j, column  i element  of P is  the  transition  probability  p;,;  for  example,  the 
row  2, column  1 element  gives  the  probability  that  state  1 will  be  followed  by 
state  2. 

Representing  a  Markov  Chain  with  a  Vector  Autoregression 
A useful  representation  for  a Markov  chain  is obtained  by letting  —, denote 
a random  (N  x  1) vector  whose  jth element  is equal  to unity if s,  =  j and  whose 
jth element  equals  zero  otherwise.  Thus,  when  s, =  1, the  vector  &, is equal  to  the 
first column  of I, (the N  x  N identity  matrix);  when  s,  =  2, the  vector &, is  the 
second  column  of I; and  so on:  | 

“i  Sy  ia! 
inmadboiaicn  ia. 

(1, 6, Oe, ON  when s, = 1 

Ae  ae  apres  1 
c 

when  s, 

Paks 

= fs ‘oes eiilidgd  shai 0, 0,. ee 1)  when 5, sas :  N. 

‘aabata  iS 
:  ithrbee 

hen 

jth jth element  of €,,, is a random variable  die takes on the 
‘Py and takes on the  value zero otherwise.  Such  a 
random variable hs 0 ee Thus, the lade expectation of &, ,, given  — 

the 

siratrix  whore  ‘Guia ; 

Ds  henge Q)832-c.w?  ¢ ea 

ra 
wa 

oe  ber ciewneni:  a  ‘} ake teside  i  3 

Forecasts  for  a Markov  Chain 

Expression  [22.2.6]  implies  that 

ne!  Vi+-m 

+  PV.  elt  oF  PAV  gett  ot 

He  Pm  a  Pe  eee 

where  P”  indicates  the  transition  matrix  multiplied  by itself  m  times.  It  follows 
from  [22.2.8]  that  m-period-ahead  forecasts  for  a Markov  chain  can  be  calculated 
from 

E (Enns  Geini 

2) 

PME 

[22.2.9] 

Again,  since  the jth element  of &,,,,,  will  be  unity  if s,,,,  3 j and zero  otherwise, 
.  -) indicates  the probability 
the jth element  of the (N x  1) vector  E(&,,,,,/&,, 
that  s,,,,  takes  on  the  value  j, conditional  on  the  state  of the  system  at  date  ¢t.  For 
example,  if the  process  is in  state  i at  date  ¢, then  [22.2.9]  asserts  that 

1,  . 

PAS:  a  I|s,  7  i} 

PRR 

2|s, 

i} 

P{Si4m  =  N\s,  =  i} 

=  P”-e, 

(22.2.10] 

where  e; denotes  the  ith  column  of I,,.  Expression  [22.2.10]  indicates  that  the  m- 
period-ahead  transition  probabilities  for  a Markov  chain  can  be calculated  by mul- 
tiplying  the  matrix  P by itself  m  times.  Specifically,  the  probability  that  an  obser- 
vation  from  regime / will  be followed  m  periods  later  by an  observation  from  regime 
J, P{s.4m  =  j|S,  =  i}, is given  by the  row  j, column  i element  of the  matrix  P”. 

Reducible  Markov  Chains 

For  a two-state  Markov  chain,  the  transition  matrix  is 

*  , 7 RE  oe  ‘a 

—  Pu 

P22 

[2.2.11] 

Suppose  that  p,,  =  1, so  that  the  matrix  P is upper  triangular.  Then,  once  the 
process  enters  state  1, there  is no  possibility  of ever  returning  to  state  2.  In such 
a case  we  would  say  that  state  1 is an  absorbing  state  and  that  the  Markov  chain 
is reducible. 

More  generally,  an  N-state  Markov  chain  is said to be reducible  if there exists 
a way  to label  the states  (that is,  a way  to choose  which  state  to call  state  1, which 
to call state  2, and  so on) such  that  the transition  matrix  can  be written  in the form 

|e of 

where  B denotes  a  (K  x  K) matrix  for  some  1 =  K <  N.  If P is upper  block- 
triangular,  then  so  is P”  for  any  m.  Hence,  once  such  a process  enters  a state  j 
such  that  j =  K,  there  is no  possibility  of  ever  returning  to  one  of the  states 
Ke. 1, Ko  Dye  no,  at 

A Markov  chain  that  is not  reducible  is said  to be irreducible.  For example, 

a  two-state  chain  is irreducible  if p,,  <  1 and p..  <  1. 

680  Chapter 22  | Modeling  Time  Series  with  Changes  in Regime 

Ergodic  Markov  Chains 
Equation  [22.2.2]  requires  that  every  column  of  P sum  to  unity,  or 

P'l  =  1. 
(22.2.12] 
where I denotes  an  (N  x  1) vector  of  1s.  Expression  [22.2.12]  implies  that  unity 
is  an  eigenvalue  of the  matrix  P’  and  that 1 is the  associated  eigenvector.  Since  a 
matrix  and  its  transpose  share  the  same  eigenvalues,  it  follows  that  unity  is  an 
eigenvalue  of the  transition  matrix  P for  any  Markov  chain. 

Consider  an  N-state  irreducible  Markov  chain  with  transition  matrix  P.  Sup- 
pose  that  one  of  the  eigenvalues  of  P  is  unity  and  that  all  other  eigenvalues  of 
P  are  inside  the  unit  circle.  Then  the  Markov  chain  is  said  to  be  ergodic.  The 
(N  xX  1) vector  of  ergodic  probabilities  for  an  ergodic  chain  is  denoted  m.  This 
vector  tm  is  defined  as  the  eigenvector  of  P associated  with  the  unit  eigenvalue; 
that  is, the  vector  of ergodic  probabilities  7  satisfies 

Pr  =  %. 

[22.2.13] 

The  eigenvector  w  is normalized  so  that  its  elements  sum  to  unity  (1’a  =  1).  It 
can  be  shown  that  if P is the  transition  matrix  for  an  ergodic  Markov  chain,  then 

lim  P”  =  wl’. 

[22.2.14] 

We  establish  [22.2.14]  here  for  the  case  when  all  the  eigenvalues  of  P are 
distinct;  a  related  argument  based  on  the  Jordan  decomposition  that  is valid  for 
ergodic  chains  with  repeated  eigenvalues  is developed  in  Cox  and  Miller  (1965, 
pp.  120-23).  For  the  case  of distinct  eigenvalues,  we  know  from  [A.4.24]  that  P 
can  always  be  written  in the  form 

| 

| 

P  =  TAT-', 

(22.2.15] 

where  T is an  (N  X  N)  matrix  whose  columns  are  the  eigenvectors  of P while  A 
is a  diagonal  matrix  whose  diagonal  contains  the  corresponding  eigenvalues  of P. 
It follows  as  in [1.2.19]  that 

P”  =  TA" T-!. 

(22.2.16] 

Since  the  (1,  1) element  of A is unity  and  all  other  elements  of A are  inside  the 
unit  circle,  A”  converges  to  a  matrix  with  unity  in  the  (1,  1) position  and  zeros 
elsewhere.  Hence, 

lim  P”  =  x-y’, 

[22.2.17]} 

where x is the  first  column  of T and  y’ is the first  row  of T~'. 

The  first  column  of T is the  eigenvector  of P corresponding  to  the  unit  ei- 

genvalue,  which  eigenvector  was  denoted 

in (22.2.13]: 

| 

x=  7. 
Moreover,  the  first  row  of T~',  when  expressed  as  a column  vector,  corresponds 
to the eigenvector  of P’ associated  with  the  unit eigenvalue,  which  eigenvector  was 
seen  to be proportional  to  the  vector  1 in [22.2.12]: 
:
Te * 
: 
matrix  P is characterized  by 

(22.2.19} 
nny (22.2.19],  note  from  [22.2.15]  that  the  matrix  of eigenvectors  T of the 

y=a'l. 

| 

(22.2.18] 

PT = TA. 

(22.2.20] 

et YS 

22.2.  Markov  Chains 

681 

 
Transposing  [22.2.15]  results  in 

P=  (T-)' AT’, 

and  postmultiplying  by (T~')'  yields 

PoCTAY  ft  (ids 

[22.2.21] 

Comparing  [22.2.21]  with  [22.2.20]  confirms  that  the  columns  of (T~ if correspond 
to  eigenvectors  of P’.  In particular,  then,  the  first  column  of (T~ "7 is proportional 
to  the  eigenvector  of P’  associated  with  the  unit  eigenvalue,  which  eigenvector  was 
seen  to  be given  by 1 in equation  (22.2.12].  Since  y was  defined  as  the  first  column 
of (T~')’,  this  establishes  the  claim  made  in  equation  [22.2.19]. 

Substituting  [22.2.18]  and  [22.2.19]  into  [22.2.17],  it follows  that 

lim  P”  =  tal’. 

Since  P™  can  be  interpreted  as  a  matrix  of  transition  probabilities,  each  column 
must  sum  to unity.  Thus,  since  the vector  of ergodic probabilities  m was  normalized 
by the  condition  that  I’  =  1, it follows  that  the  normahzing  constant  a  must  be 
a 
unity,  establishing  the  claim  made  in [22.2.14]. 

Result  [22.2.14]  implies  that  the  long-run  forecast  for  an  ergodic  Marko 

chain  is independent of the  current  state,  since,  from  [22.2.9], 

E(e  le) = 

.-  y  wa PE,  > mE,  — 

where  the  final  equality  follows  from  the  observation  that  1'€,  =  1 regardless  of 
the  value  of  €,.  The  long-run  forecast  of  &,,,,  is  given  by the  vector  of  ergodic 
probabilities  m  regardless  of the  current  value  of &,. 

The  vector  of ergodic  probabilities  can  also  be  viewed  as  indicating  the  un- 
conditional  probability  of each  of the  N  different  states.  To  see  this,  suppose  that 
we  had  used  the  symbol  2;  to  indicate  the  unconditional  probability  P{s,  =  j}. 
Then  the  vector  m  =  (7,  7%,  ...,  ™y)'  could  be  described  as  the  unconditional 
expectation  of &,: 

a  =  E(é,). 

[22.2.22] 

If one  takes  unconditional  expectations  of [22.2.6],  the  result  is 

E(& 1)  =  P-E(&). 

Assuming  stationarity  and  using  the  definition  [22.2.22],  this  becomes 

: 

m=  Pon, 
which  is identical  to  equation  [22.2.13]  characterizing  m  as  the  eigenvector  of P 
associated  with  the  unit  eigenvalue.  For an  ergodic  Markov  chain,  this eigenvector 
is unique,  and  so  the  vector  of ergodic  probabilities 
can  be  interpreted  as  the 
vector  of unconditional  probabilities. 

An ergodic Markov chain is a covariance-stationary  process.  Yet [22.2.6] takes 
the  form  of a  VAR  with  a  unit  root,  since  one  of the  eigenvalues  of P is unity. 
This  VAR is stationary  despite the unit root  because  the variance-covariance  matrix 
of v, is singular.  In particular,  since  1'—,  =  1 for all  ¢ and  since  1'P  =  1’, equation 
[22.2.6]  implies  that  1’v,  =  0 for  all ¢.  Thus,  from  [22.2.19],  the  first  element  of 
the  (N  x  1) vector  T~'y,  is always  zero,  meaning  that  from  [22.2.16]  the  unit 
eigenvalue  in P”v, always  has  a coefficient  of zero. 

682  Chapter 22  | Modeling  Time  Series  with  Changes  in Regime 

Further  Discussion  of Two-State  Markov  Chains 
The  eigenvalues  of the  transition  matrix  P for any  N-state  Markov  chain  are 
found from  the  solutions  to  |P —  Aly|  =  0. For  the  two-state  Markov  chain,  the 
eigenvalues  satisfy 

0 = 

Pu  —  Abdix  P22 

1—  py  Pp» 

A 
=  (Pir  —  A)(P2  —  A) —  (1 —  pud(1  —  pra) 
=  PuP22  —  (Pir  +  Pr)A  +  A?  —  1 +  Puy  +  P22  —  PirP22 

=?  —  (py  +  py)A-  1+  put  Pa 

=  (A  —.1)(A  +  1  =  pry  —  Pr). 

Thus,  the  eigenvalues  for  a two-state  chain  are  given  by A,  =  1 and  A,  =  —1  + 
Pu  +  P22.  The  second  eigenvalue,  A,, will  be inside  the unit circle as long as  0 < 
Pu  + P22  <  2.  We  saw  earlier  that  this chain is irreducible  as  long  as  py,  <  1 
is ergodic  provided  that p,,  <  1, 

and px, <  1.  Thus,  a  two-state  Markov chain 

Pn  < 1, and  p;,  ag Px  >  0. Thy, 
1s 
_  The eigenvector  associated  with A, for the two-state  chain  turns  out 

(oe  BSisRGds:  “a  en  11,  O28  wieder: 

to  be 

~ 

ES  Ee ee a  eee  ge  8  [‘ in  P2)/(2  Sw  Paks  S  all  ~:  a 
fer  A  ENIOW,  eBoy 

(AE?  SRBGivag 
G sip  Pu:  Paden  Thee 
AO 

wWaiaies  <o  BOGS  ABM  ADT  ¥ 
confirm 

this and the claims that follow 

in Exercis gives 

reese 

ei 

eee 

a 

| 

= 

m  see  Cox  and  Miner  (i9G*: 

afd.  OPH! ; 

eR  ESE”  aa 
gre  Disitipuliors  °° 

& 

VE  OF OC 

ie 
iS 

we, 

ihe 

an  weobserved 
— 

i 

ee, 

ReteanG  asihcox 

£2  Saratae 

e  nc 

eS 
= 

*, 

& 

= 

- 

Before 

<, 

at 

Thus,  for  example  if  the  process  is  currently  in  state  1,  the  probability  that  m 
periods  later  it will  be  in  state  2 is given  by 

Plscam  =  2|s,  =  1}  =  @— Pu) — AC — Pd 

2—~  fa  =  Pa 

where  A>  =  —]  +  Pu  +  P22- 

A  two-state  Markov  chain  can  also  be  represented  by a  simple  scalar  AR(1) 
process,  as  follows.  Let  é,, denote  the  first  element  of the  vector  €,; that  is,  &,, is 
a  random  variable  that  is equal  to  unity  when  s,  =  1 and  equal  to  zero  otherwise. 
For  the  two-state  chain,  the  second  element  of &, is then  1  —  ,,.  Hence,  [22.2.6] 
can  be  written 

Sir+l 

= 

Pir 

i=  P22 

: -  alge  , 7  P22  Il, oo  Rte 

ji,  | +  vise  22.2.23 
Y2441 

The first  row  of [22.2.23]  states  that 

Exner  =  (1  —  Paz)  +  (-1  +  Dar  +  Pardu  +  Viner- 

(22.2.24] 

Expression  [22.2.24]  will  be  recognized  as  an  AR(1)  process  with  constant  term 
(1 —  p>) and  autoregressive  coefficient  equal  to  (—1  +  pi;  +  pz).  Note  that  this 
autoregressive  coefficient  turns  out  to  be  the  second  eigenvalue  A, of P calculated 
previously.  When  p,,;  +  P22  >  1, the  process  is likely  to  persist  in its current  state 
and  the  variable  ¢,, would  be  positively  serially  correlated,  whereas  when  p,,  + 
P22  <  1, the process  is more  likely to switch  out  of a state  than  stay  in it, producing 
negative  serial  correlation.  Recall  further  from  equation  [3.4.3]  that  the  mean  of 
a first-order  autoregression  is given  by c/(1  —  ¢). Hence,  the  representation  [22.2.24] 
implies  that 

E(&)-= 

1  —  Px 
2-  Pu  —  Pr 

which  reproduces  the  earlier  calculation  of  the  value  for  the  ergodic  probabil- 
ity 7. 

Calculating  Ergodic  Probabilities 
for an  N-state-Markov  Chain 

For a general ergodic N-state  process,  the vector  of unconditional  probabilities 
represents  a  vector  m  with  the  properties  that  Pa  =  wm  and  I’m  =  1, where  1 
denotes  an  (N  x  1) vector  of 1s.  We  thus  seek  a  vector  m  satisfying 

Att  =  ey,). 

(22.2.25] 

where  e,,,,  denotes  the  (N +  1)th column  of I,,, ,.and  where 

A 

in  Iv  te  P 

(N+1)xN 

li 

Such a solution  can  be found  by premultiplying  [22.2.25]  by (A’A)-!A': 

m™  =  (A‘A)~'A’en,). 

[22.2.26] 

In other  words,  7  is the  (N +  1)th column  of the  matrix  (A’A)~'A’. 

684  Chapter 22  | Modeling  Time  Series  with  Changes  in Regime 

. Periodic  Markov  Chains 
Ifa  Markov  chain  is irreducible,  then  there  is one  and  only  one  eigenvalue 
equal to unity.  However, there  may  be more  than  one  eigenvalue  on  the  unit  circle, 
meaning  that  not  all  irreducible  Markov  chains  are  ergodic.  For  example,  consider 
a  two-state  Markov  chain  in  which  Pu  =  Px  =  0: 

a  | 
he  k - 

The  eigenvalues  of this  transition  matrix  are  A,  =  1 and  A,  =  —1,  both  of  which 
are  on  the  unit  circle.  Thus,  the  matrix  P”  does  not  converge  to  any  fixed  limit  of 
the  form  7-1'  for  this  case.  Instead,  if the  process  is  in  state  1 at  date  ¢,  then 
it is certain  to  be  there  again  for  dates  ¢  +  2,  +  4,4  +  6,...,  with  no  ten- 
dency  to  converge  as  m  —  ©.  Such  a  Markov  chain  is  said  to  be  periodic  with 
period  2. 

In  general,  it  is  possible  to  show  that  for  any  irreducible  N-state  Markov 
chain,  all the  eigenvalues  of the  transition  matrix  will  be on  or  inside  the  unit  circle. 
If there  are  K eigenvalues  strictly  on  the  unit  circle  with  K >  1, then  the  chain  is 
said  to  be  periodic  with  period  K.  Such  chains  have  the  property  that  the  states 
can be  classified  into  K distinct  classes,  such  that  if the  state  at  date  ¢ is from  class 
a,  then  the  state  at  date  ¢  + 1 is certain  to  be from  class  a  +  1 (where  class  a  +  1 
for a  =  K is interpreted  to be class  1). Thus,  there  is a zero  probability  of returning 
to  the  original  state  s,,  and  indeed  zero  probability  of returning  to  any  member  of 
the original  class  a, except  at horizons  that  are  integer multiples  of the period  (such 
+  K,t  +  2K,  ¢t  +  3K,  and  so  on).  For  further  discussion  of periodic 
as  dates 
Markov  chains,  see  Cox  and  Miller  (1965). 

22.3.  Statistical  Analysts  of i.i.d.  Mixture  Distributions 

In Section  22.4,  we  will  consider  autoregressive  processes  in which  the  parameters 
of the autoregression  can  change  as the result  of a regime-shift  variable.  The  regime 
itself  will  be  described  as  the  outcome  of  an  unobserved  Markov  chain.  Before 
analyzing  such  processes,  it is instructive  first  to  consider  a  special  case  of these 
processes  known  as  i.i.d.  mixture  distributions. 

Let the regime  that  a given process  is in at date  t be indexed  by an  unobserved 
random  variable  s,,  where  there  are  N possible  regimes  ts, et  8798  EY 
ROTO) 
When  the process  is in regime  1, the observed  variable  y, is presumed  to have  been 
drawn  from  a N(,,  07) distribution.  If the  process  is in regime  2, then  y,  is drawn 
from  a N(,,  03) distribution,  and  so  on.  Hence,  the  density  of y, conditional  on 
the  random  variable  s, taking  on  the  value j is 

fOrls,  =  i; 8) =  Tse, vp|  "| 

[22.3.1] 

fos j =  1/2,  .. 
ee  ee es  ee 

a 

,  N.  Here  @ is a  vector  of population  parameters  that  includes 

The  unobserved  regime  {s,}  is presumed  to  have  been  generated  by some 
probability  distribution,  for  which  the  unconditional  probability  that  s, takes  on 
the value j is denoted  77;: 

Pfs, =  j:0}  =m 

forj=1,2,...,N. 

-22)3. Statistical Analysis of i.-d.  Mixture  Distributions 

[22.3.2] 
685 

[he  probabilities  7,  . 

. 

- 

,  ™  are  also  included  in  @;  that  is,  @ is given  by 

O  =  (uy,  --- 

,  Myr  OF,  -- 

«  ye  eee  Tu) 

Recall  that  for  any  events  A  and  B,  the  conditional  probability  of A  given  B 

is  defined  as 

P{A|B}  = 

P{A  and  B} 
P{B} 

assuming  that  the  probability  that  event  B occurs  is not  zero.  This expression  implies 
that  the  joint  probability  of  A and  B occurring  together  can  be  calculated  as 

P{A  and  B}  =  P{A|B}-  P{B}. 

For  example,  if we  were  interested  in  the  probability  of  the  joint  event that Ss =  ] 
and  that  y, falls  within  some  interval  [c, d], this  could  be  found  by integrating 

PX)  5,  =  J; 8)  =  fOrls,  =  45 0)-Pis,  =  j; 6} 
over  all  values  of y, between  c  and  d.  Expression  [22.3.3]  will  be  called the  joint 
density-distribution  function  of  y,  and  s,.  From  [22.3.1]  and  [22.3.2],  this 
function  is given  by 

[22.3.3] 

Pn  5,  =  f3 9)  =  Saas exp{ Oe} 

i 

—(y,  -  p;)? 

[22.3.4] 

j 
The  unconditional  density  of y, can  be  found  by summing  [22.3.4]  over  all 

possible  values  for /: 

N 

fs  8)  =  oY P(Y,  5,  =  J; 9). 

jms 

es 

7)  exp| (y,  ES n| 

V2q0; 
72 
*  Vino, exp|  20? 

20; 

—(y,  —  pe)? 

[22.3.5] 

+ 

TN 
*  V21087  °*P 

=O.  =  Bw)? 

20%  }. 

Since  the  regime  s, is unobserved,  expression  [22.3.5]  is the  relevant  density  de- 
scribing  the  actually  observed  data  y,.  If the  regime  variable  s, is distributed  i.i.d. 
across  different  dates  t, then  the  log likelihood  for  the  observed  data  can  be  cal- 
culated  from  [22.3.5]  as 

7 

£(0)  =  > log f(y;  8). 

[22.3.6] 

The  maximum  likelihood  estimate  of @ is obtained  by maximizing  [22.3.6]  subject 
to  the  constraints that 7,  +  m7,  +  +++  +  my  =  land  a,=O0forj  =  1,2,..., 
N.  This  can  be achieved  using the  numerical  methods  described  in Section  5.7, or 
using  the EM  algorithm  developed  later  in this  section. 

Functions  of the  form  of [22.3.5]  can  be  used  to  represent  a  broad  class  of 
different  densities.  Figure  22.2  gives  an  example  for  N.=  2. The  joint  density- 
distribution  p(y,, s,  =  1; @) is 7,  times  a N(,,  07) density,  while p(y,, s, =  2; ®) 
_  is 77, times  a  N( 12, 73) density.  The unconditional  density for the observed  variable 
f(y,;  9) is the sum  of these  two  magnitudes. 

686  Chapter 22 | Modeling  Time Series with Changes  in Regime 

“oF 

FIGURE 22.2 Density of mixture  of two  Gaussian  distributions  with  y,|s,  = 
penalty Bbiplin BATA,  J) and Fase = = 08. 

wniitdr  Yh  is seI2 V2  RWG) 
+ 
oe oe mixture of two Sari Ey are not have the bimodal ay 
of  Figure re  22.2. Gaussian  mixtures  can also  produce a  unimodal  density, 

ance 
allowing 
Pomerat ar: that of a single  AI  TERE kt in Figure, 

gecpiteebe 

RS 

poooedsr ety 

5 

Feet 
sol  sic 
Paige  iG  = Femei 

ine singe vi y= eax andl  8 

date  ¢ observation  of y,.  Again,  from  the  definition  of  a  conditional  probability,  it 
follows  that 

Tf (yl5,  =  J; 9) 
0  Sy  =  j; 8) 
P{s,  ss ily  0}  m  Cae  =  as  ER | ea 

[22.3.7] 

Given  knowledge  of  the  population  parameters  8,  it  would  be  possible  to use 
[22.3.1]  and  [22.3.5]  to  calculate  the  magnitude  in  [22.3.7] for  each  observation  y, 
in  the  sample.  This  number  represents  the  probability,  given  the observed  data, 
that  the  unobserved  regime  responsible  for observation  f was  regime J. For example, 
for  the  mixture  represented  in Figure  22.2,  if an  observation  y, were  equal  to  zero, 
one  could  be  virtually  certain  that  the  observation  had  come  from  a  N(O,  1) dis- 
tribution  rather  than  a  N(4,  1) distribution,  so  that  P{s,  =  1|y,;  0} for  that  date 
would  be  near  unity.  If  instead  y,  were  around  2.3,  it  is  equally  likely  that  the 
observation  might  have  come  from  either  regime,  so  that  P{s,  =  1|y,;  0} for  such 
an  observation  would  be  close  to  0.5. 

Maximum  Likelihood  Estimates  and  the  EM  Algorithm 
It is instructive  to characterize  analytically  the  maximum  likelihood  estimates 
of  the  population  parameter  @.  Appendix  22.A  demonstrates  that  the  maximum 
likelihood  estimate  6 represents  a  solution  to  the  following  system  of  nonlinear 
equations: 

= 

3 y;*  Pts =  Ilys  0} 
A; =  St  _______ 
2, P{s,  =  jly.; 4} 

for  j=  1,2.2  3258 

[22.3.8] 

iit 

a 

>> (,  _  fi;)?- Pfs, er ily  0} 

= 
2,  Pls,  =  ily.s 8 

forj=1,2,...,N 

[22.3.9] 

a, =  T~'  » P{s,  =  jly,, 9} 

f 

a 

t=1 

forj  =  1,2,...,N. 
‘. 

’ 

[22.3.10] 

Suppose  we  were  virtually  certain  which  observations  came  from  regime  j 

and  which  did  not,  so  that  P{s,  =  j{y,; 0} equaled  unity  for those  observations  that 
came  from  regime j and  equaled  zero  for  those  observations  that  came  from  other 
regimes.  Then  the  estimate  of the  mean  for  regime  j in [22.3.8]  would  simply  be 
the  average  value  of y, for  those  observations  known  to  have  come  from  regime j. 
In  the  more  general  case  where  P{s,  =  j|y,;  0} is  between  0  and  1 for  some 
observations,  the  estimate  ji; is a  weighted  average  of all  the  observations  in the 
sample,  where  the  weight  for observation  y, is proportional  to  the  probability  that 
.  date  #’s observation  was  generated  by regime j. The  more  likely  an  observation  is 
to  have  come  from  regime  j, the  bigger  the  weight  given  that  observation  in esti- 
mating  u,.  Similarly,  &? is a weighted  average  of the  squared  deviations  of y, from 
fi;, while  7, is essentially  the  fraction  of observations  that  appear  to  have  come 
from  regime j. 

Because  equations  [22.3.8]  to  [22.3.10]  are  nonlinear,  it is not  possible  to 
solve  them  analytically  for  6 as  a  function  of {y,, yz,  . 
,  ¥z}-  However,  these 
equations  do  suggest  an  appealing  iterative  algorithm  for  finding  the  maximum 

- 

. 

688  Chapter  22  | Modeling  Time  Series  with  Changes in Regime 

likelihood  estimate.  Starting  from  an  arbitrary  initial  guess  for  the  value  of  9, 
denoted  6’,  one  could  calculate  P{s,  =  j|y,;  @}  from  [22.3.7].  One  could  then 
calculate  the magnitudes  on  the  right  sides  of  [22.3.8]  through  [22.3.10]  with  6’ 
in  place  of  @.  The  left  sides  of  [22.3.8]  through  [22.3.10]  would  then  produce  a 
new  estimate  6°)  This  estimate  6)  could  be  used  to  reevaluate  P{s,  =  jly,;  0} 
and  recalculate  the  expressions  on  the  right sides  of [22.3.8]  through  [22.3.10].  The 
left  sides of [22.3.8]  through  [22.3.10]  then  can  produce  a  new  estimate  0.  One 
continues  iterating  in this fashion  until  the change  between  0”*  ") and  0”  is smaller 
than  some  specified  convergence  criterion. 

This  algorithm  turns  out  to  be a special  case  of  the  EM  principle  developed 
by Dempster,  Laird,  and  Rubin  (1977).  One  can  show  that  each  iteration  on  this 
algorithm  increases  the  value  of the  likelihood  function.  Clearly,  if the  iterations 
reach  a  point  such  that  6%  =  @”"*!),  the  algorithm  has  found  the  maximum 
likelihood  estimate  6. 

Further  Discussion 

The  mixture  density  [22.3.5]  has  the  property  that  a global  maximum  of the 
log  likelihood  [22.3.6]  does  not  exist.  A  singularity  arises  whenever  one  of  the 
distributions  is imputed to have  a  mean  exactly  equal  to  one  of the  observations 
(4,  =  y,,  Say)  with  no  variance  (07  —  0).  At  such  a  point  the  log  likelihood 
becomes  infinite. 

| 

| 

Such  singularities  do  not  pose  a  major  problem  in  practice,  since  numerical 
maximization  procedures  typically  converge  to a reasonable  local  maximum  rather 
than  a  singularity.  The  largest  local  maximum  with o; > 0 for  all j is described  as 
the maximum  likelihood  estimate.  Kiefer  (1978)  showed  that  there  exists  a bounded 
local maximum  of [22.3.6]  that yields a consistent,  asymptotically  Gaussian  estimate 
of ® for which  standard  errors  can  be constructed  using  the  usual  formulas  such  as 
expression  [5.8.3].  Hence,  if a  numerical  maximization  algorithm  becomes  stuck 
at a singularity,  one  satisfactory  solution  is simply  to  ignore  the  singularity  and  try 
again  with  different  starting  values. 

Another  approach  is to: maximize  a slightly  different  objective  function  such 

Q(0)  =  £(0)  —  > (a,/2)  log(a?)  —  > bj/(207) - 

te 

[22.3.11] 

‘a 

—  2 Gln;  —  m)*/(2o7), 

N 

f 

where  £(@)  is the  log likelihood  function  described  in  [22.3.6].  If a,  =  c;,  then 
expression  [22.3.11]  is the form  the  log likelihood  would  take  if, in addition  to  the 
data,  the analyst  had  a, observations  from  regime j whose  sample  mean  was  m, and 
whose  sample variance  was  b;/a;. Thus, m, represents  the analyst’s prior expectation 
of the  value  of y,,  and b,/a, represents  the analyst’s  prior expectation  of the  value 
of «?. The  parameters  a, and  c, represent  the  strength  of these  priors,  measured 
in terms  of the confidence  one  would  have  if the priors were  based  on  a; or c; direct 
‘observations  of data  known  to have  come  from  regime j. See  Hamilton  (1991)  for 
further  discussion  of this  approach. 
Hand (1981) and Titterington,  Smith,  and  Makov  (1985). 

Nice surveys  of i.i.d.  mixture  distributions  have  been  provided  by Everitt  and 

, 

22.3,  Statistical Analysis  of i.i.d.  Mixture  Distributions 

689 

22.4.  Time  Series  Models  of Changes  in  Regime 

Description  of the  Process 
We  now  return  to  the  objective  of  developing  a  model  that  allows  a  given 
variable  to  follow  a  different  time  series  process  over  different  subsamples.  As  an 
illustration,  consider a first-order  autoregression  in  which  both  the  constant  term 
and  the  autoregressive  coefficient  might  be  different  for  different  subsamples: 
Y=  Cy  +  dudes  +  Eo 

[22.4.1] 

where  e,  ~  i.i.d.  N(0,  a”).  The  proposal  will  be  to  model  the  regime  s,  as  the 
outcome  of an  unobserved  -N-state  Markov  chain  with  s, independent  of ¢, for  all 
t and  T. 

Why  might  a  Markov  chain  be a  useful  description  of the  process  generating 
changes  in  regime?  One’s  first  thought  could  be  that  a  change  in  regime  such  as 
that  in Figure  22.1  is a  permanent  event.  Such  a  permanent  regime  change  could 
be  modeled  with  a  two-state  Markov  chain  in which  state  2 is an  absorbing  state. 
The  advantage  of using  a Markov  chain  over  a deterministic  specification  for  such 
a process  is that  it allows  one  to  generate  meaningful  forecasts  prior  to  the  change 
that  take  into  account  the  possibility  of the  change  from  regime  1 to  regime  2. 

We  might  also  want  a time  series  model  of changes  in regime  to  account  for 
unusual  short-lived  events  such  as  World  War  II.  Again,  it is possible  to  choose 
parameters  for a  Markov  chain  such  that,  given  100  years  of data,  it is quite  likely 
that  we  would  have  observed a single episode  of regime  2 lasting  for about  5 years. 
A Markov  chain  specification,  of course,  implies  that  given  another  100  years  we 
could well  see  another  such  event.  One  might  argue  that  this  is a sensible  property 
to build  into  a model.  The  essence  of the  scientific  method  is the  presumption  that 
the  future  will  in some  sense  be  like  the  past. 

_ 

_  While  the  Markov  chain  can  describe  such  examples  of changes  in regime,  a 
further  advantage  is its  flexibility.  There  seems  some  value  in specifying  a  prob- 
ability  law  consistent  with  a  broad  range  of  different  outcomes,  and  choosing 
particular  parameters  within  that  class  on  the  basis  of the  data  alone. 

In  any  case,  the  approach  described  here  readily  generalizes  to  processes  in 
which  the  probability  that s,  =  j depends  not  only on  the  value  of s,_,  but  also  on 
a  vector  of other  observed  variables—see  Filardo  (1994)  and  Diebold,  Lee,  and 
Weinbach  (forthcoming). 

The  general  model  investigated  in  this  section  is as  follows.  Let  y,  be  an 
(n x  1) vector  of observed  endogenous  variables  and x, a (k x  1) vector  of observed 
exogenous  variables.  Let  Y,  =  (y/, y/-1,  --.  5  Yom»  Xi> joa)  ++  +5  X im)’  bea 
vector  containing all observations  obtained  through date ¢. If the process is governed 
regime  s, =  j at date  t, then  the conditional  density  of y, is assumed  to be given 

y 

fly.|s,  =  'j, %5'U,_  13a), 
[22.4.2] 
where  a@  is a vector  of parameters  characterizing  the  conditional  density.  If there 
are  N different  regimes,  then there are  N different  densities  represented  by [22.4.2] 
forj  =  1,2,...  , N. These  densities  will be collected  in an (N x  1) vector  denoted 
Ne 

For the  example  of [22.4.1],  y, is a scalar  (n =  1), the  exogenous  variables 
consist  only of a constant  term  (x, =  1), and the unknown  parameters  in  & consist 
~~,  Oy,  and  o2.  With  N  =  2 regimes  the  two  densities 

Of  Cy,  «1.5  Cy,  Oy, 

690  Chapter 22  | Modeling  Time Series  with  Changes  in Regime 

represented  by [22.4.2]  are 

h  = 

fovls,  =  ctaganed _  | V2me  SP 
(1s,  =  2,¥,-13  @) 

1 

hots  {=u —¢-  fal  | 
20° 

imo 

mo 

—(y,  St,  Ga..7  $2y,-4)? 
20? 

It  is assumed  in  [22.4.2]  that  the  conditional  density  depends  only  on  the 

current  regime  s, and  not  on  past  regimes: 

LOI  Y.-10 

a) -1S,  =f; 

5,  = js) 

| 

- 

(22.4.3] 

ey-" 

=  f(y|x,,Y,-1,5,  rey lit,  Si,  Soy 

hy.  par), 

this  is  not  really  restrictive.  Consider,  for  emaniple; the  specification  in 
(22. 1.3}, where  the conditional  density of y, depends on  both s* and s*_, and  where 
s?  is described by a two-state  Markov  chain.  One can  define  a new  variable  s, that 
characterizes  the regime for  date tin  a way consistent  with  (22.4. 2} as  follows: 

Pe  ifs? =  lands?  =1_ 

ee 

3  =2 

ifs?  =  2ands/_,  =  y  3 

toatlo’)  a  | 
momcis  dij  oad 

Dae 

Pee 
=  lands/_ ,=2 
| 
p= 4e  ifs?  =2ands?  =  2. 

1  be Ista Fi a, then s, foley a  four-state Markov fant with 

CBOE ay  i 

¢ 

wee 

i. 

Optimal  Inference  About  Regimes  and  Evaluation 
of  the  Likelihood  Function 

The  population  parameters  that  describe  a  time  series  governed  by [22.4.2] 
and  [22.4.4]  consist  of a  and  the  various  transition  probabilities  p,.  Collect  these 
parameters  in  a  vector  8.  One  important  objective  will  be  to  estimate  the  value  of 
@ based  on  observation  of Y,.  Let  us  nevertheless  put  this  objective  on  hold  for 
the  moment  and  suppose  that  the  value  of @ is somehow  known  with  certainty  to 
the  analyst.  Even  if we  know  the  value  of 8, we  will  not  know  which  regime  the 
process  was  in  at  every  date  in  the  sample.  Instead  the  best  we  can  do  is to  form 
a  probabilistic  inference  that  is a  generalization  of  [22.3.7].  In  the  i.i.d.  case,  the 
analyst’s  inference  about  the  value  of s, depends  only  on  the  value  of y,.  In  the 
more  general  class  of  time  series  models  described  here  the  inference  typically 
depends  on  all  the  observations  available. 

as 

Let  P{s,  =  j|%,; @} denote  the  enalyst’s  inference  about  the  value  of s, based 
on  data  obtained  through  date  ¢ and  based  on  knowledge  of  the  population  pa- 
rameters  8.  This  inference  takes  the  form  of  a  conditional  probability  that  the 
analyst  assigns  to  the  possibility  that  the  tth  observation  was  generated  by regime 
j.  Collect  these  conditional  probabilities  P{s,  =  AM;  ©} for  j =  1,2,...,N 
in an  (N  X  1) vector  denoted  &,,,. 

One  could  also  imagine  forming  forecasts  of how  likely  the  process  is to  be 
in  regime  j in  period  ¢  +  1 given  observations  obtained  through  date  ¢.  Collect 
these  forecasts  in  an  (N  xX  1) vector  &,,,),,  which  is a  vector  whose  jth  element 
represents  P{s,,,  =  j|%,; 9}. 

} 

The  optimal  inference  and  forecast  for each  date  t in the sample  can  be found 

by iterating  on  the  following  pair of equations: 

bi  a  i 
1(E.-1O 0) 

a‘ 

[22.4.5] 

Feil  =  P-&,,,. 

[22.4.6] 
Here  », represents  the  (N x  1) vector  whose jth element  is the conditional  density 
in  [22.4.2],  P  represents  the  (N  x  N)  transition  matrix  defined  in  [22.2.3],  1 
represents  an  (N x  1) vector  of 1s, and the  symbol  © denotes  element-by-element 
multiplication.  Given  a starting  value  ,),) and  an  assumed  value  for the population 
parameter  vector  @, one  can  iterate on [22.4.5]  and  [22.4.6]  fort  =  1,2,...,T 
to  calculate  the  values  of  &,, and  £,+1),  for  each  date  ¢ in  the  sample.  The  log 
likelihood  function  £(@)  for the  observed  data Y; evaluated at the  value  of @ that 
was  used  to  perform  the  iterations  can  also  be  calculated  as  a  by-product  of this 
algorithm  from 

‘ 

£8) =  D log f(yl%  9-150), 

where 

We  now  explain  why this  algorithm  works. 

F (YN X,%,— 158)  =  1'(E,-,  On). 

, 

| 

[22.4.7] 

[22.4.8] 

_  Derivation  of Equations  [22.4.5]  Through  [22.4.8] 
To see  the basis  for the algorithm  just described,  note  that we  have  assumed 
that  x, is exogenous,  by which we  mean  that  x, contains  no  information  about  s, 

692  Chapter 22  | Modeling Time Series  with  Changes  in Regime 

beyond  that  contained  in  Y,_,.  Hence,  the  jth  element  of  &),,  could  also  be 
described  as  P{s,  =  j|x,, Y,_,;  0}. The  jth element  of 4,  is f(y,|s,  =  j, X,  Y,—15  9). 
The  jth  element  of  the  (N  x  1) vector  (€,,,_;  © m,)  is  the  product  of  these  two 
magnitudes,  whieh  product  can  be  interpreted  as  the  conditional  joint  density- 
distribution  of y, and  s,: 

Pis, =  j|x,,  Y,_15  8} X f(y, s, =  j,x,,9,-13 

8) 

(22.4.9] 

-  PLY,, S,  =  i|x,,  Y,-1;  8). 

The  density  of the  observed  vector  y, conditioned  on  past  observables  is  the  sum 
of the  N magnitudes  in (22.4.9]  for j =  1, 2,...,  N.  This  sum  can  be  written  in 
vector  notation  as 

fbx  Y.-13  6) -  1'(é,),-,  On), 

as  Claimed  in  [22.4.8].  If the  joint  density-distribution  in  [22.4.9]  is divided  by the 
density  of y, in  [22.4.8],  the  result  is the  conditional  distribution  of s,: 

P(y,, 5, =  j|x,,  Y,— 138) 
fly1x,,  9,1; 8) 
fhe  @ecavean  oe  Pi{s, me 

. 
ily 

0  X,, y 

1; 

0} 

;6 

Hence,  from  [22.4.8], 

=  P{s, =  j|%,;  O}. 

P{s, =  j|%,; 6} a  PUY.»  S: - i|x,,Y,—15  9) 

(22.4. 10] 

1 (&,-1  Om) 

But  recall  from  [22.4.9]  that  the  numerator  in  the  expression  on  the  right  side  of 
[22.4.10]  is the  jth element  of the  vector  (€,),_ ; © 9,), while  the left side  of [22.4.10] 
is the jth element  of the  vector  &,, Thus,  collecting  the  equations  in (22.4.10]  for 
j =1,2,...,Ninto  an  (N x  1) vector  produces 
g,  -  frm) 

as  claimed  in [22.4.5]. 

To  see  the  basis  for  [22.4.6],  take  expectations  of  [22.2.6]  conditional  on 

Ay 

1'(E,,-,On) 

y,: 

E(&+1|9,)  = P-E(é|%,)  +  E(v,,,|9%,). 

[22.4.11] 

Note  that  v,,,  is  a  martingale  difference  sequence  with  respect  to  Y,,  so  that 
[22.4.11]  becomes 

as  claimed  in [22.4.6]. 

Easy  i  P-,,, 

Starting  the Algorithm 
Given  a starting  value  E,)0, one  can  use  [22.4.5]  and  [22.4.6]  to  calculate 
é,, for  any  t.  Several  options  are  available  for  choosing  the  starting  value.  One 
approach  is to set E10 equal to the vector  of unconditional  probabilities  m described 
in equation  [22.2.26].  Another  option  is to set 
bie=  er 

| 

| 

2.4. Time Series  Models of Changes in Regime 

-—((22.4.12} 
693 

where  p is a  fixed  (N  x  1) vector  of nonnegative  constants  summing  to unity,  such 
as  p  =  N~!-1.  Alternatively,  p could  be  estimated  by maximum  likelihood  along 
with  @ subject  to  the  constraint  that  I’p  =  1 and  p;  =  Oforj  =1,2,...,N. 

Forecasts  and  Smoothed  Inferences  for the  Regime 
Generalizing  the  earlier  notation,  let  E, represent  the  (N  xX  1) vector  whose 
jth element  is P{s,  =  j|%,;  0}. For ¢ >  7,  this  represents  a forecast  about  the  regime 
for  some  future  period,  whereas  for  ¢  <  7  it  represents  the  smoothed  inference 
about  the  regime  the  process  was  in at  date  ¢ based  on  data  obtained  through  some 
later  date  rT. 

The  optimal  m-period-ahead  forecast  of &,,,,  can  be  found  by taking  expec- 

tations  of both  sides  of [22.2.8]  conditional  on  information  available  at  date  ¢: 

or 

E(E, sm  @,) =  P”-E(é,|Y,) 

Eri  =  P™-£,,, 

where  é,, is calculated  from  [22.4.5]. 

[22.4.13] 

. 

Smoothed  inferences  can  be calculated  using  an  algorithm  developed  by Kim 

(1993).  In  vector  form,  this  algorithm  can  be  written  as 

fiz a  E,1, O {P’ 5  7; (=  Ed}, 

[22.4.14] 

where  the sign  (+)  denotes  element-by-element  division.  The  smoothed  proba- 
bilities  7 are  found  by iterating  on  [22.4.14]  backward  for  ¢ =  T  —  1, T  —  2, 
...,  1.  This  iteration  is  started  with  €;,7,  which  is  obtained  from  [22.4.5]  for 
t  =  T.  This  algorithm  is valid  only  when  s, follows  a  first-order  Markov  chain  as 
in  [22.4.4],  when  the  conditional  density  [22.4.2]  depends  on  s,,  s,_,,  . 
.  only 
through  the  current  state  s,,  and  when  x,,  the  vector  of explanatory  variables  other 
than  the  lagged  values  of y,  is strictly  exogenous,  meaning  that  x, is independent 
of s, for  all  ¢ and  7.  The  basis  for  Kim’s  algorithm  is explained.in  Appendix  22.A 
; 
at  the  end  of the  chapter. 

. 

Forecasts  for the  Observed  Variables 

From  the  conditional  density  [22.4.2]  it  is straightforward  to  forecast  y,,, 
conditional  on  knowing Y,, x,,,,  and s,,,.  For example,  for the AR(1)  specification 
Yeat  =  Cs,,,  +  Bs, Ye  +  €:41,  Such  a  forecast  is given  by 

E(yr41lSe41  =  j, Y,; 0)  =  Cc;  +  yy. 

{22.4.15] 

There  are  N different  conditional  forecasts  associated  with  the  N possible  values 
for s,,,.  Note  that  the  unconditional  forecast  based  on  actual  observable  variables 
is related  to  these  conditional  forecasts  by 

E(¥ 411X415  Y,; 9) 

_  | Yer  F(Yr4  11%  1s  Y,; 8) AY, +1 

N 

=  J vee  {$ PCY  415  Sea  =  AX 415  yy; 0| dy, 41 

694  Chapter 22  | Modeling  Time  Series  with  Changes  in Regime 

N 

-  fe { S 1H  4518, 45 

=  J, X41,  %,; 0)Pi{s,,,  =  1%:  .,  y  0)| dy,. 

N 

2 PAS:  51  "3 AX +1»  Y;; 8} | Yeon  S(Yi4  1 |Sr41  >  }; Mit)  9;  0)  dy,,\ 

N 

p> PAS, 41  *  NY,;  BE(y,  4,  18,4;  -  }, Xi4s  M,;  6). 

Thus, the  forecast  appropriate  for  the jth regime  is simply  multiplied  by the  prob- 
ability  that  the  process  will  be  in  the  jth  regime,  and  the  resulting  N  dif- 
ferent  products  are  added  together.  For  example,  if the;  =  1,2,...  ,  N forecasts 
in  (22.4.15]  are  collected  in a  (1  x  N) vector  h/,  then 

E(y,41|9,;  0)  =  h/E,,1), 

Note  that  although  the  Markov  chain  itself  admits  the  linear  representation 
[22.2.6],  the  optimal  forecast  of y,,,  is  a nonlinear  function  of observables,  since 
the  inference  €,,, in  [22.4.5]  depends  nonlinearly  on  Y,.  Although  one  may  use 
a  linear  model  to  form  forecasts  within  a  given  regime,  if an  observation  seems 
unlikely  to  have  been  generated  by the  same  regime  as  preceding  observations, 
the  appearance  of the  outlier  causes  the  analyst  to switch  to  a  new  rule  for  forming 
future  linear  forecasts. 

The  Markov  chain  is clearly  well  suited  for  forming  multiperiod  forecasts  as 

well.  See  Hamilton  (1989,  1993a,  1993b)  for  further  discussion. 

Maximum  Likelihood  Estimation  of Parameters 

In  the  iteration  on  [22.4.5]  and  [22.4.6],  the  parameter  vector  @ was  taken 
to  be  a  fixed,  known  vector.  Once  the  iteration  has  been  completed  for  ¢  =  1, 2, 
...,  7 for  a  given  fixed  9, the  value  of the  log likelihood  implied  by that  value 
of 8 is then  known  from  [22.4.7].  The  value  of @ that  maximizes  the  log likelihood 
can  be  found  numerically  using  the  methods  described  in  Section  5.7. 

If the  transition  probabilities  are  restricted  only  by the  conditions  that  p,  =  0 
and  (pj;  +  P2  +  °°:  +  Pin)  =  1 for  all  i and  j, and  if the  initial  probability 
E1)0 is taken  to  be  a  fixed  value  p unrelated  to  the  other  parameters,  then  it is 
shown  in Hamilton  (1990)  that  the  maximum  likelihood  estimates  for the transition 
probabilities  satisfy 

a 

Pi  = 

wa 

2 P{s,  =  j, 5,1  =  i|Y7;  6} 

§ 
2 P{s,_,  =  i|Y7;  9} 

; 

’ 

(22.4.16] 

where  6 denotes  the  full  vector  of maximum  likelihood  estimates.  Thus,  the  esti- 
mated  transition  probability  p,, is essentially  the  number  of times  state  i seems  to 
have  been  followed  by state  j divided  by the  number  of times  the  process  was  in 
state  i. These  counts  are  estimated  on  the  basis  of the  smoothed  probabilities. 

If the  vector  of  initial  probabilities  p is  regarded  as  a  separate  vector  of 
parameters constrained  only by 1p =  1 and p = 0, the maximum  likelihood  estimate 
of p turns  out  to  be the smoothed  inference  about  the  initial  state: 

b= 

(22.4.17] 

22.4.  Time  Series  Models  of Changes  in Regime 

695 

The  maximum  likelihood  estimate  of the vector  a  that  governs  the  conditional 

density  [22.4.2]  is characterized  by 

T 
> (? “E) Ej,  =  0. 

a 

t=1 

oa 

(22.4.18] 

Here  », is the  (N x  1) vector  obtained  by vertically  stacking  the  densities in [22.4.2] 
for  j =  1,2,...,  Nand  (0 log y,)/da’  is the  (N  x  k) matrix  of derivatives  of the 
logs  of  these  densities,  where  k represents  the  number  of  parameters  in  a.  For 
example,  consider  a Markov-switching  regression  model  of the  form 

y,  =  z,B;  +  €., 

[22.4.19] 

where  ¢,  ~  i.i.d..N(0,  a?)  and  where  z,  is a  vector  of  explanatory  variables  that 
could  include  lagged  values  of y.  The  coefficient  vector  for  this  regression  is  B, 
when  the  process  is in  regime  1, B, when  the  process  is in  regime  2, and  so  on. 
For  this  example,  the  vector  y,  would  be 

1 

nO  er  zB)" 

V2a0  erp|  20? 

i 

exp ad 6 

ziBa)} 

> 

2a07 

1 
V2a0 

and  for  a  =  (Bj,  B3,...,  By,  o7)’,  condition  [22.4.18]  becomes 

Dd (y,  —  2/B,)e- Pls,  =  i975  6} =  0 

forj  =1,2,...,N  [22.4.20] 

re 

t=1 

6?  =T"  > DY (y  —  2B)  Pls,  =  i197 4H. 

[22.4.21] 

: 

N 

t=1  j=1 

Equation  [22.4.20]  describes  B ; as  satisfying  a weighted  OLS  orthogonality  con- 
dition  where  each  observation  is weighted  by the  probability  that  it came  from 
regime  j. In  particular,  the  estimate  B,; can  be  found  from  an  OLS  regression  of 

¥,(i) on  4): 

6, =  b cI 

I> mn]. 

(22.4.22] 

where  > 

yj)  =  ye"  P{s,  =  i|%r;  6} 

| 

[22.4.23] 

2(j)  =  z\  P{s,  =  N97; 6}. 

The  estimate  of a? in [22.4.21]  is just (1/7) times  the combined  sum  of the squared 
residuals  from  these  N different.  regressions. 

. 

: 

Again,  this  suggests  an  appealing  algorithm  for finding  maximum  likelihood 
estimates.  For  the  case  when p is  fixed  a  priori,  given  an  initial  guess  for  the 
parameter  vector  8  one  could  evaluate  [22.4.16],  [22.4.22],  and  [22.4.21]  to 
generate  a  new  estimate  6“.  One  then  iterates  in the same  fashion  described  in 
equations  [22.3.8]  through  [22.3.10]  to  calculate  6@,  @©),  ..  . 
.  This  again  turns 
out  to be an  application  of the  EM algorithm.  Alternatively,  if p is to be estimated 
by maximum  likelihood,  equation  [22.4.17]  would  be added  to the  equations  that 
are  reevaluated  with  each  iteration.  See  Hamilton  (1990)  for details. 

696  Chapter 22  | Modeling  Time  Series  with  Changes  in Regime 

Illustration:  The  Behavior  of U.S.  Real  GNP 
As an  illustration  of this  method,  consider  the data  on  U.S.  real  GNP growth 
analyzed in  Hamilton  (1989).  These  data  are  plotted in  the bottom  panel of Figure 
22.4.  The  following  switching  model  was  fitted  to  these  data  by maximum  likeli- 
hood: 

Jr"  tage  -#  O\(Y-1  —  Ms,  ,) +  $(y;-2  —  a 

+  $3(y,-3  a  Hs-_,)  Ss b4(Yr-4  =  Ms.)  +  €,, 

[22.4.24] 

with  ¢, ~  i.i.d.  N(0, 0?) and with st presumed  to follow  a two-state  Markov  chain 
with  transition  probabilities  p¥. Maximum  likelihood  estimates  of parameters  are 
reported  i in  Table  22.1.  In the  regime  represented by s*  =  1, the  average  growth 
rate  is 4,  =  1.2%  per  quarter,  while  when  s*  =  2, the  average  growth  rate  is 
Mz  =  —0.4%.  Each  regime  is highly persistent.  The probability  that expansion  will } 
be followed  by another  quarter  of expansion  is pj, =  0.9, so  that  this  regime  will 
persist on  average  for 1/(1  —  Pin) =  10 quarters. The probability that a contraction 
will be followed  by contraction  is p}, = 0.75, which episodes  will al persist 
for 1/(1  —  px) = 4 quarters. 

> 

£2027  Ma) 

: 

= 

— 

= 

,  se of )  Shitrctts 7 oes bThinm Bt Ce 2noizeazet 
Cease  mae 

oni aaa ¢ Bi aro paienesab  pet BD tow 

hack 

de arias, mona. "i  wes 

=  TIES 

“¢  a  ide 7.  Rat 

647,31 

oll 

. 

; 

ma  cate 
| 

z 

banal 

Aad. a ot  ae  Me a ey _.* aA% e. 

lEwe nae & Abbe 

| 

TABLE  22.1 
Maximum  Likelihood  Estimates  of Parameters  for  Markov-Switching  Model 
of U.S.  GNP  (Standard  Errors  in  Parentheses) 

eer 

fix  =  1.16 
(0.07) 
@,  =  0.01 
(0.12) 

fi,  =  —0.36 
(0.26) 
do, =  —0.06 
(0.14) 

p*,  =  0.90 
(0.04) 

d, =  -0.25 
(0.11) 

Pp  =  0.75 
(0.10) 
=  —0.21 
(0.11) 

Go?  =  0,59 
(0.10) 

In  order  to  write  [22.4.24]  in  a  form  where  y,  depends  only  on  the  current 
value  of  the  regime,  a  variable  s,  was  defined  that  takes  on  one  of  32  different 
,  Sf_4-  For  ex- 
values  representing  the  32  possible  combinations  for  s*, 57_,,  . 
ample,  s,  =  1 when  s*,s*_,,...,  and  s*_,  all  equal  1, s,  =  2 when  s? =  2 and 
=  s*_,  =  1, and  so  on.  The  vector  &,, calculated  from  [22.4.5]  is  thus 
s*_,  =  ++: 
a  (32  x  1) vector  that  contains  the  probabilities  of each  of these  32  joint  events 
conditional  on  data  observed  through  date  /. 

. 

. 

The  inference  about  the  value  of s* for  a single  date  ¢ is obtained  by summing 

together  the  relevant  joint  probabilities.  For  example,  the  inference 

P{s?  ="2ys  hn  ee 
2 
ek 

ies 

2 

2 

j=l  wH=ly=li 4 
Voie  = 

iM» 
Ah  e410) 

P{s?  =  2, 54  —  li,  <  aa  l,  S;-3  =  13,  S,-4  ca  isly,, 

. 

— 

. 

x 

—-"e 

+ 

pa  * 

(22.4.25] 

is  obtained  by iterating  on  [22.4.5]  and  [22.4.6]  with  ®  equal  to  the  maximum 
likelihood  estimate  6. Oné  then  sums  together  the  elements  in the  even-numbered 
rows  of &,, to obtain  P{s*  =  2|y,, y,-1,---  »  Ya;  9}. 

A probabilistic  inference  in  the  form  of (22.4.25]  can  be  calculated  for  each 
date  ¢ in the  sample.  The  resulting  series  is plotted  as  a  function  of ¢ in panel  (a) 
of Figure  22.4.  The  vertical  lines  in the  figure  indicate  the  dates  at  which  economic 
recessions  were  determined  to  begin  and  end  according  to  the  National  Bureau  of 
Economic  Research.  These  determinations  are  made  informally  on  the  basis  of a 
large  number  of  time  series  and  are  usually  made  some  time  after  the  event. 
Although  these  business  cycle  dates  were  not  used  in any  way  to  estimate  param- 
eters  or  form  inferences  about  s/*,  it is interesting  that  the  traditional  business  cycle 
dates correspond  fairly closely to the expansion  and contraction  phases as described 
by the  model  in  (22.4.24]. 

Determining  the  Number  of States 

One  of the  most  important  hypotheses  that  one  would  want  to  test  for  such 
models  concerns  the  number  of  different  regimes  N that  characterize  the  data. 
Unfortunately,  this  hypothesis  cannot  be  tested  using  the  usual  likelihood  ratio 
test.  One  of the regularity  conditions  for the likelihood  ratio  test  to have  an  asymp- 
totic y? distribution  is that  the information  matrix  ¥ be nonsingular.  This condition 
fails  to  hold  if the  analyst  tries  to  fit an  N-state  model  when  the  true  process  has 
N —  1 states,  since  under  the  null  hypothesis  the parameters  that  describe  the  Nth 
state  are  unidentified.  Tests  that  get around  the  problems  with  the  regularity  con- 
ditions  have  been  proposed  by Davies  (1977),  Hansen  (1996),  Andrews  and  Plo- 
berger  (1994),  and  Stinchcombe  and  White  (1993).  Another  approach  is to  take 

698  Chapter 22  | Modeling  Time  Series  with  Changes  in Regime 

the  (N —  1)-state  model  as  the  null  and  conduct  a variety  of tests  of the  validity 
of that specification  as  one  way  of seeing  whether  an  N-state  model  is  needed; 
Hamilton  (1996)  proposed  a number  of such  tests.  Studies  that  illustrate  the  use 
ania tests  include  Engel  and  Hamilton  (1990),  Hansen  (1992),  and  Goodwin 

APPENDIX  22.A.  Derivation  of Selected  Equations for Chapter  22 

®  Derivation of [22.3.8] through  (22.3.10].  The maximum  likelihood  estimates  are  obtained 
by forming  the  Lagrangean 

(22.A.1] 
J(®)  =  £(0)  +  AQ  2  ie  St?  Se  Tn) 
and  setting  the  derivative  with  respect  to @ equal  to  zero.  From  (22.3.6),  the  derivative  of 
the  log likelihood  is given  by 

af)  _  S_1_ 

,  ays 0) 

| 

(22.A.2] 

_  Observe  from [22.3.5] that 

~All  ee  oe 

fiemegsb 

eiislimié 

2 

. 

: 

oT, 

= 

210; 

fy ls, =  i; 4), 

; 

207 : 

‘2 

7 

: 

8 

3  [22.4.3] 

ip 

ailarg  st 

POns = 1:0)  \  22.A.4] 

SHpiiott  isis  (i.  buemmggu. 

‘. 

er  = 

~ 

et 
Pe  OP)  Rk  = 
‘ 

1 

f 

Rg 
3 
Stee 

; 

ot 
ee  a  eS) 

= 

. 

WP 

oy 

wer 

2 

.  east)  Vi 

: 
XN 

? 

a 

_ 
a4 

“Se 

; 

7 

Pui 
PTH 
nt 

thr 

Equation  [22.3.8]  follows  immediately  from  this  condition.  Similarly,  the  first-order  con-  - 
ditions  for  maximization  with  respect  to  a? are  found  by setting  [22.A.11]  equal  to  zero: 

T 

Dd {-07  +  (y,  -—  wPPAs,  =  slys OF  =  9, 

from  which  [22.3.9]  follows.  Finally,  from  [22.A.9],  the  derivative  of [22.A.1]  with  respect 
to  7, is given  by 

4 
F 
0)  =  ay)  > Ps,  =  fee) 
oT; 

1=1 

A  =  8, 

from  which 

2 
> Pls,  =  fly); 8} =  Ary. 

’ 

4=1 

(22.A.12] 

Summing  [22.A.12]  over j =  1,2,...  ,  N produces 

r 

> [Pis,  =  1]y,; 0} +  ---  +  Pls,  =  Nly,;  O}] =  AC,  +  m  +--+  +  ty) 

- 

t=1 

or 

bo | 

{1}  =  A-(1), 

implying  that  T =  A.  Replacing  A with  T in  [22.A.12]  produces  (22.3.10].  @ 

® Derivation  of (22.4.14]. 
Recall  first  that  under  the  maintained  assumptions,  the  regime 
s, depends  on  past  observations  Y,_,  only  through  the  value  of s,_,.  Similarly,  s, depends 
on  future  observations  only  through  the  value  of s,, ;: 

[22.A.13] 
The  validity of [22.A.13].is  formally  established  as  follows  (the  implicit  dependence 

P{s,  =  jls,4,  =  i, Yr;  0} =  Pls,  =  j|s,.1  =  i, 9; O}. 

on  6 will  be  suppressed  to  simplify  the  notation).  Observe  that 

Pfs,  =  jlS.41  =  6, Yas} 

=  Pfs,  =  fl$i41  =  1, Ye1  Xo  Vd 
—  Piers  &  =  HS  =  He  B) 
IGF)  =  1 her  B) 

[22.A.14] 

- 

»  fQ.:18,  =  Jy Sens  =  1, Xa,  Y,):  Pls, =  jlSe+1  =  1, X41,  9} 

F (Yio rlSiea  =  b Xoa  BW)  . 

; 

which  simplifies  to 

Pis,  =  flso1  =  6,9,.1}  =  Pls,  =  jls.n1  =  6 Rar  Bs 

[22.A.15] 

provided  that 

Pee tl8 =  ie S41  =  i XY)  =  LY rerlSier  =  i Mare  Ws 
(22-A.16) 
which  is  indeed  the  case,  since  the  specification  assumes  that  y,,,  depends  on  {s,,,, 
fg .  -} only through  the  current  value  s,,,.  Since  x is exogenous,  [22.A.15]  further  implies 

Pls,  =  jlsiny  =  4, Big)  =  Pls,  =yls,..  24,  Oh 

[22.A.17] 

By similar  reasoning,  it must  be the  case  that 

Ps,  =  ilS.41  =  i, Yaa} 

=  Pls,  =  jlsis1  =i, Yi+29  Xi 420  Mai 

= 

(Y,+25  5,  = 

S41  =  i, Xi 425  Wy, 41 

L(Yrs2lSi01  =  ty Xe2s  Vos) 

=  LYr+2|8,  =  jf, S41  =  f, X42»  G41)  Pls,  =  f[S41 
FYis2l8,41  =  i X20  Ved 

=  b, Xo2,  Boab 
sie 

700  Chapter 22  | Modeling  Time  Series  with  Changes in Regime 

y grate sisarect 

provided  that 

=  ils.  -  i, Y,,.}  =  Pis, wal bee  -  i, X42)  9.1}, 

[22.A.18] 

‘ 

fY%+2ls,  i  Is Ses  bn i, X,+2)  Y,.1)  "  L(+ 21841  =  1, Xi+2>  Y,.1)- 

In this case,  [22. A.19] is established  from  the  fact  that 

(22.A.19] 

Prvals, =  j,s yy?  Go  i, X42»  Y,.1) 

-> P(Y.+2,  Si+2  =  k\s, a  iF Si41  =  i, X42,  %,.1) 

2? 7 [f(y.+215,+2  =  ky S,  =  J. S01  =  i, Xi+29  9,41) 

x  Pis,.3  m= k\s,  =  pe Sy41  =  i, X,+2)  y,,,}] 

N 

2, [f(y.+218,+2  ™  k, So  =  i, Xi +2»  Y,.1) 
x  Pi{s,.2  =  kl5i41  =  i, X42  Y,413] 

Again. Focaccia 

2S  18] can be written 

ia L¥s 215141  =  1, Kae  Y,,1). 

>  | ae  =  1, Y,,.}  =  Ps, = bead W501  =i,  Yas} = aad Pts, = Hsie1 =  i, wih 

where “ last  equality  follows  from  (22. A.17]. 

sone:  av  paentins nie the same  argument  can  be used  to establish that 

, 

4 

| 

ee 12.  eae which 8 aes follows. 
IPAs  "Note next that mete 

| 

| 

sate 

ons 

#'ge 

et 

> 

eee 
. 
Pad  ats 

-  ASe+1  =  1,9,,n} = Pls, =  AMsees  * =i, mH  none ial A 

_  Pe =hj  abe ey [ 

Pudldar  TeMtig, Lids  PS.  = iY) A.  AW 

al  mort  boodiisaid 
PBA?  GVi202 Snvilainaw  lava Sai  to  npg 

5 Seca  S3HEMISHA  ars 
A  eee 
ee  (220) 
ils =f  atigon. 
=  _sinigaa~ gaimoodnot  stoacnioW  > asda  bas Pe dice. i Ht  XK diene?  -biodiG 
ee  eee  3 ai 2xiiliqnghitac aan?  ann: tT  sie grits Z 
PpinioS  bun  cigars  ase oa 
,  hat Sei  bu.  H  t#moadT  tns posses 
gael)  ome  lesdiog’) ai 
crewed,  sapenneed 

*  SION  rOuigM- +  PGs batghiesh_» 
=  ila, =  cae Sik ai WS, =  Nar  * —  ‘ 

Pace  a biokeO  “hagage 

Beas 

where  the  (1  x  N) vector  p; denotes  the /th  row  of the  matrix  P’  and  the  sign  (+ ) indicates 
1,  2, 
element-by-element  division.  When  the  equations  represented  by  [22.A.22]  for j = 
...,  Mare  collected  in  an  (N  xX  1) vector,  the  result  is 

&5 -  E.,, ©  {P'(E,. 447  (+)  E 4}, 

as  claimed. 

& 

Chapter  22  Exercise 

Let  s,  be  described  by an  ergodic  two-state  Markov  chain  with  transition  matrix  P 

22.1. 
given  by [22.2.11].  Verify  that  the  matrix  of eigenvectors  of this  matrix  is given  by 

T=  fe —  Pu (2\ =P 

(1  —  pid  .awePis-—  P22) 

Po)  7] 
1 

with  inverse 

1 

1 

2a  Ee —  Pu\(2  —  Pr  —  P22)  Cl  —  Pa2)A2  -  Pu  -  me 

Chapter  22  References 

_  Andrews,  Donald  W.  K.,  and  Werner  Ploberger.  1994.  “‘“Optimal  Tests  When  a  Nuisance 

Parameter  Is Present  Only  under  the  Alternative.”  Econometrica  62: 1383-1414. 
Cox,  D.  R., and  H.  D.  Miller.  1965.  The  Theory  of Stochastic  Processes.  London:  Methuen. 
Davies,  R. B.  1977.  ‘‘Hypothesis  Testing  When  a Nuisance  Parameter  Is Present  Only under 
the  Alternative.”  Biometrika  64:247-54. 
Dempster,  A.  P.,  N.  M.  Laird,  and  D.  B.  Rubin.  1977.  ““Maximum  Likelihood  from  In- 
complete  Data  via  the  EM  Algorithm.”  Journal  of the  Royal  Statistical  Society  Series  B, 
39:1-38. 
Diebold,  Francis  X., Joon-Haeng  Lee,  and  Gretchen  C.  Weinbach.  Forthcoming.  “‘Regime 
Switching  with  Time-Varying  Transition  Probabilities,”  in C. Hargreaves,  ed., Nonstationary 
Time  Series  Analysis  and  Cointegration.  Oxford:  Oxford  University  Press. 
Durland,  J.  Michael,  and  Thomas  H.  McCurdy.  1992.  “‘Modelling  Duration  Dependence 
in Cyclical  Data  Using  a  Restricted  Semi-Markov  Process.’  Queen's  University,  Kingston, 
Ontario.  Mimeo. 
Engel,  Charles,  and  James  D.  Hamilton.  1990.  ‘‘Long  Swings  in  the  Dollar:  Are  They  in 
the  Data  and  Do  Markets  Know  It?”  American  Economic  Review  80:689-713. 
ce B.  S., and  D.  J.  Hand.  1981.  Finite  Mixture  Distributions.  London:  Chapman  and 

all. 

Filardo,  Andrew  J. 1994.  “Business  Cycle Phases  and Their Transitional  Dynamics.” Journal 
of Business  and Economic  Statistics  12:299-308. 
Goodwin,  Thomas  H.  1993.  “Business  Cycle  Analysis  with  a  Markov-Switching-Model.” 
Journal  of Business  and  Economic  Statistics  11:331-39. 
Hamilton,  James  D.  1989.  “A  New  Approach  to  the  Economic  Analysis  of Nonstationary 
Time  Series  and  the  Business  Cycle.”  Econometrica  57:357-—84. 

—.  1990.  ‘Analysis  of Time  Series  Subject  to  Changes  in Regime.”  Journal  of Econ- 

ometrics  45:39-70. 

| 

.  1991.  ‘A  Quasi-Bayesian  Approach  to Estimating  Parameters  for Mixtures  of Nor- 

mal  Distributions.”’  Journal  of Business  and  Economic  Statistics  9:27-39. 

.1993a.  “Estimation,  Inference,  and  Forecasting  of Time  Series  Subject  to Changes 
in Regime,”  in G.  S. Maddala,  C.  R.  Rao,  and  H.  D.  Vinod,  eds.,  Handbook  of Statistics, 
Vol.  11.  New  York:  North-Holland. 

book  of Econometrics,  Vol.  4.  New  York:  North-Holland. 

.1993b.  ‘*State-Space  Models,”’  in Robert  Engle  and Daniel  McFadden,  eds.,  Hand- 
.  1996.  “Specification  Testing in Markov-Switching  Time  Series  Models.” Journal of 

Econometrics  70: 127-57. 

. 

702  Chapter 22  | Modeling  Time  Series  with  Changes  in Regime 

Hansen,  Bruce  E.  1992. “The  Likelihood  Ratio  Test  under  Non-Standard  Conditions:  Test- 
ing  the  Markov  Switching  Model  of GNP.”  Journal  of Applied  Econometrics  7:S61-82. 

.  1996.  “Inference  When  a  Nuisance  Parameter  Is  Not  Identified  under  the  Null 

Hypothesis.”  Econometrica  64:413—30, 
Kiefer,  Nicholas  M.  1978.  ‘Discrete  Parameter  Variation:  Efficient  Satienation of a Switching 
Regression  Model.”  Econometrica  46:427-34. 
Kim, Chang-Jin.  1993.  “Dynamic  Linear  Models  with  Markov-Switching.””  Journal  of Econ- 
ometrics  60:1—22. 
Lam,  Pok-sang.  1990.  “The  Hamilton  Model  with  a General  Autoregressive  Component: 
Estimation  and  Comparison  with  Other  Models  of  Economic  Time  Series.’’  Journal  of 
Monetary  Economics  26:409-32. 
Rogers, John H. 1992.  The Currency Substitution  Hypothesis  and Relative  Money  Demand 
in Mexico  and  Canada.”  Journal  of Money,  Credit,  and  Banking  24:300-18. 
Stinchcombe,  Maxwell,  and Halbert  White.  1993.  “An  Approach  to Consistent  Specification 
Testing  Using  Duality  and  Banach  Limit  Theory.””  University  of  California,  San  Diego. 
Mimeo. 
Titterington,  D.M.,  A. Fr M.  Smith,  and  U. E. Makov. 1985.  Statistical  Analysis  of Finite 
Mixture Distributions.  New York: ——  | 

boli  itt 

if 
. 

| 

ABBRT)  192  0b.  s 
passin  aN: 
tail 992 28dioub—  7) > 

oe 

| 

re 

7 

+4  F 
§ 

';  r 

‘s) 

4 

ye) 
: 

e 

(ATCT) 

’ 

: 

x09; 

anane 
iz 

dood 

br 
zid? 

a 

(4°21)  ash'O 

‘ 

. 

’ 

- 

ee 

C-« 

7 

y 
7  oe 
tae  >  - 

: 

%e 

sez  sinomiéen 
idsgoig tea qoon Godt 
enition  x4  dibetaisans 
se S278 

ott 

4 

i} 

. 

‘. 

i  e TE Se  ee  a 1S: IBIS 

; 

a 

si?  en  2  _  ibd alls. st 

4go!  o2ads Aiow  wiaasinebass 

in  hea bawesb rat  ee ; 

A 

Mathematical  Review 

This  book  assumes  some  familiarity  with  elementary  trigonometry,  complex  num- 
bers,  calculus,  matrix  algebra,  and probability.  Introductions  to the first three  topics 
by Chiang  (1974)  or  Thomas  {1972)  are  adequate;  Marsden  (1974)  treated  these 
issues  in  more  depth.  No  matrix  algebra  is required  beyond  the  level  of standard 
econometrics  texts  such  as  Theil  (1971)  or  Johnston  (1984);  for  more  detailed 
treatments,  see  O’Nan  (1976),  Strang  (1976),  and  Magnus  and  Neudecker  (1988). 
The  concepts  of probability  and  statistics  from  standard  econometrics  texts  are  also 
sufficient  for getting through  this  book;  for  more  complete  introductions,  see  Lind- 
gren  (1976)  or  Hoel,  Port,  and  Stone  (1971). 

This  appendix  reviews  the  necessary  mathematical  concepts  and  results.  The 
reader  familiar  with  these  topics  is  invited  to  skip  this  material,  or  consult  sub- 
headings  for  desired  coverage. 

: 

ALL.  Trigonometry 

Definitions 

Figure  A.1  displays  a  circle  with  unit  radius  centered  at  the  origin  in (x, y)- 
space.  Let  (xp, yo) denote  some  point  on  this  unit  circle,  and  consider  the  angle  @ 
between  this  point  and  the  x-axis.  The  sine  of @ is defined  as  the  y-coordinate  of 
the  point,  and  the  cosine  is the  x-coordinate: 

sin(@)  =  yo 
cos(@)  =  Xo. 

[A.1.1] 
[A.1.2] 

_ 

This  text  always  measures  angles  in radians.  The  radian  measure  of the  angle 
6 is defined  as  the  distance  traveled  counterclockwise  along  the  unit  circle  starting 
at the x-axis  before  reaching  (Xo, yo). The  circumference  of a circle  with  unit  radius 
is 27.  A  rotation  one-quarter  of the  way  around  the  unit  circle  would  therefore 
correspond  to radian  measure  of @ =  i(27)  =  7/2.  An angle whose  radian  measure 
is 7/2  is more  commonly  described  as  a  right  angle  or  a 90°  angle.  A 45° angle has 
radian  measure  of 7/4,  a  180°  angle  has  radian  measure  of 7,  and  so  on. 

Polar  Coordinates 

Consider  a  smaller  triangle—say,  the  triangle  with  vertex  (x,, y,) shown  in 
Figure  A.1—that  shares  the  same  angle  6 as  the  original  triangle  with  vertex 

704, 

/  — 

& 
rie  a 

. 

- 

' 

2:  a  eee  SS ie ae Tete 

7 

/ 

;  This GeeMies  the  fr:  Gu ei  LL  ee  rhe 
ee 
-  9, 28 gpes from  O  to 

tag  Lisa  = 
Fis? 

y  = 

Suction  sinf te)  ~auidale  throu 

22 

4 

| 

a, 

ae  $x  a  *  —- 
33 

Ve  Ve 

a... 

; 

as 

‘\ 

a  bile  So  Vee  foes 

—~ 

is 

= 

* 

¥ 

aE  K  CYETSS  1  TOE  Tee  Hf 
af  curse 

pep  ee 

é 

‘ 

,

es 

 
Properties  of Sine  and  Cosine  Functions 

The  functions  sin(@)  and  cos(@)  are  called  trigonometric  or  sinusoidal  func- 

tions.  Viewed  as  a  function  of  6, the  sine  function  starts  out  at  zero: 

sin(0)  =  0. 

The  sine  function  rises  to  1 as  @ increases  to  7/2  and  then  falls  back  to  zero  as  6 
increases  further  to  7;  see  panel  (a) of Figure  A.2.  The  function  reaches  its min- 
imum  value  of  —1  at  6  =  37/2  and  then  begins  climbing  back  up. 

If we  travel  a distance  of 27 radians  around  the  unit  circle,  we  are  right  back 

where  we  started,  and  the  function  repeats  itself: 

sin(27  +  6)  =  sin(6). 

The  function  would  again  repeat  itself  if we  made  two  full  revolutions  around  the 
unit  circle.  Indeed  for  any  integer /, 

sin(27j  +  0) =  sin(6). 

[A.1.8] 

(a) sin(@) 

FIGURE  A.2_ 

Sine  and cosine  functions. 

706  Appendix  A  | Mathematical  Review 

(b) cos(6) 

The sine function is thus periodic and  is for this  reason  often  useful  for  describing 
a time  series  that  repeats  itself  in a  particular  cycle. 

The  cosine  function  starts  out  at  unity  and  falls  to  zero  as  @ increases  to 
m/2; see  panel  (b) of Figure  A.2.  It turns  out  sim  ly  to  be a horizontal  shift  of the 
sine  function: 

ted 

cos(@)  =  sin( 0 +  zy. 

, 

[A.1.9] 

._  _ The  sine  or  cosine  function  can  also  be  evaluated  for  negative  values  of 6, 
defined  as  a clockwise  rotation  around  the  unit  circle  from  the  x-axis.  Clearly, 

sin(— 6) =  —sin(@) 
cos(—@)  =  cos(4). 

[A.1.10]  - 
[A.1.11] 

| 

For  (xo, Yo) a  point  on  the  unit  circle,  [A.1.7]  implies  that 

_  Or,  squaring  both  sides  and  using  [A.1.1]  and  [A.1.2], 
1 =  [cos(@)}?  +  [sin(6)]?. 

as  Vx  + yes : 

P 

nauuriel A, 1.12} 

31  SH  ISl2GO 

__ Using  Trigonometric  Functions  to  Represent  Cycles 

Suppose 

we construct  the  function  g(6) by first  multiplying 6 by 2 and then 

: 

; 

{S.5.Aj 

ue —<  . g(y=sin20). 

3 

ion  sin(k@)  would go  through k cycles in the time  it takes sin(4) 

The  parameter  R  gives  the  amplitude  of  [A.1.13].  The  variable  y,  will  attain  a 
maximum  value  of  +R  and  a  minimum  value  of  —  R.  The  parameter a is the phase. 
The  phase  determines  where  in  the  cycle  y, would  be  at  ¢  =  0.  The  parameter  w 
governs  how  quickly  the  variable  cycles,  which  can  be  summarized  by either  of 
two  measures.  The  period  is the  length  of time  required  for  the  process  to  repeat 
a  full  cycle.  The  period  of [A.1.13]  is 27/w.  For  example,  if  w  =  1 then  y repeats 
itself  every  27 periods,  whereas  if w  =  2 the  process  repeats  itself  every  m periods. 
The  frequency  summarizes  how  frequently  the  process  cycles  compared  with  the 
simple  function  cos(t);  thus,  it measures  the  number  of cycles  completed  during 
2m  periods.  The  frequency  of  cos(r)  is unity,  and  the  frequency  of  [A.1.13]  is w. 
For  example,  if  w  =  2,  the  cycles  are  completed  twice  as  quickly  as  those  for 
cos(t).  There  is  a  simple  relation  between  these  two  measures  of  the  speed  of 
cycles—the  period  is equal  to  27  divided  by the  frequency. 

A.2.  Complex  Numbers 

Definitions 

Consider  the  following  expression: 

x2  =  1. 

[A.2.1]  | 

There are two  values  of x  that  satisfy  [A.2.1], namely,  x  =  1 andx  =  —1. 

Suppose  instead  that  we  were — the  following  equation: 

=  -1. 

[A.2.2] 

No  real number  satiiiies [A.2.2].  However, let us  ORS,  an  imaginary  number 
(denoted  i) that  does: 

[A.2.3] 
’  We assume  that ican  be multiplied i a real number and manipulated using standard 

=  -1. 

rules  of algebra.  For  example, 

and 

2i +  3i =  Si 

| 

(2i) -(3i)  =  (6)i2  =  -6. 
This  last  property  implies  that  a second  solution  to  [A. 2.2] is given  by x  =  —i: 
(-i?  =  (-17@)?  =  -1. 
Thus, [A.2.1]  has  two  real  roots  (+1  and  —1), whereas [A.2.2]  has two  imaginary 
roots  (i and  —i). 

| 

For  any  real  numbers  a  and  b, we  can  construct  the  expression 

a  +  bi. 
[A.2.4] 
If  b =  0, then  [A.2.4] is  a real  number;  whereas  if  a  =  0 and b is  nonzero,  then 

[A. 2.4] is  an  imaginary  number.  A number  written  in the  general  form  of [A. 2. 4] 
is called  a.complex  number. 

(' 

f 

Rules for Manipulating  Complex Miadlione 
Complex  numbers  are  manipulated  using  standard  rules  of algebra.  Two 

complex  numbers  are  added  as  follows: 

(a,  +  by)  +  (a ‘+  bji)  =  (a,  + “) +  (b,  + b,)i. 

708  Appendix  A  | Mathematical  Review 

Complex  numbers  are  multiplied  this  way:  _ 

(a,  +  byt): (a,  + b,j)  =  @,a,  +  a,b,i  +  b, aI  +  b,b,i? 

. 

. 

(aa,  —  b,b,)  +  (a,b,  +  b,a,)i. 

Note  that  the  resulting expressions  are  always  simplified  by separating  the  real 
component  (such  as  [a,a¢,  —  b,b,})  from  the  imaginary  component  (such  as 
(a,b, +  b,a,)i). 

Graphical  Representation  of Complex  Numbers 
A  complex  number  (a  +  bi) is sometimes  represented  graphically  in  an 
Argand  diagram  as in Figure A.4.  The value  of the  real component  (a) is plotted 
on  the  horizontal  axis,  and  the imaginary  component  (b) is plotted  on  the vertical 
axis.  The  size,  or  modulus,  of  a complex  number  is measured  the same  way  as  the 
from the origin of a real element  in (x, y)-space (see equation  [A.1.7]): 
distance 

[A.2.5] 
The  complex  unit circle is  the set of all complex  numbers whose  modulus is 

|a + bil = Va? + b?. 

7 

1.  For example, tbe. seal amber itd is on the complex unit a  ead 

; 

| 

2.5. Cc  A 

; 

provicers 
F 

Hii-} 

| 

Bt 

23 

imaginary aie. 
| 

. 

ol 

Pee 

oe  Bor soompi  +  b)  io 

ay 

\genes  sfT 

fs sais Bt  a 5 28: Bibodix “a 9 33 fuk  — 

gi  bare fw 

soos 

~  avast B05  zsouborg  sy | 

be  dgays os +s 

the  point  A  in  Figure  A.4).  So  are  the  imaginary  number  —i  (point  B)  and  the 
complex  number  (—0.6  —  0.87)  (point  C). 

We  will  often  be  interested  in  whether  a  complex  number  is  less  than  1 in 
modulus,  in which  case  the  number  is said  to  be inside  the  unit  circle.  For  example, 
(—0.3  +  0.4%)  has  modulus  0.5,  so  it lies  inside  the  unit  circle,  whereas  (3 +  41), 
with  modulus  5, lies  outside  the  unit  circle. 

Polar  Coordinates 
Just  as  a  point  in  (x, y)-space  can  be  represented  by its distance  c  from  the 
origin  and  its angle  6 with  the x-axis,  the  complex  number  a  +  bican  be represented 
by the  distance  of (a, b) from  the  origin  (the  modulus  of the  complex  number), 
R  =  Va? + D?, 
and  by the  angle  @ that  the point (a, b) makes  with  the  real  axis,  characterized  by 

cos(@)  =  a/R 
sin(@)  =  D/R. 

Thus,  the  complex  number  a  +  bi is written  in polar  coordinate  form  as 

[R-cos(@)  +  i-R-sin(@)]  =  R[cos(@)  +  i-sin(@)). 

[A.2.6] 

Complex  Conjugates 

The  complex  conjugate  of (a +  bi) is given  by (a —  bi).  The  numbers  (a + 
bi) and (a —  bi) are  described  as  a conjugate  pair.  Notice  that  adding  a  conjugate 
pair produces a real  result: 

The  product  of a  conjugate  pair is also  real: 

(a +  bi)  +  (a  —  bi)  =  2a. 

(a +  bi)-(a  —  bi)  =  a?  +  b?. 

[A.2.7] 

Comparing  this  with  [A.2.5],  we  see  that  the  modulus  of a complex  number  (a + 
bi)  can  be  thought  of  as  the  square  root  of  the  product  of  the  number  with  its 
complex  conjugate: 

la +  bil  =  V(a  +  bi)(a  —  bi). 

[A.2.8} 

Quadratic  Equations 

P 
A quadratic  equation 

\ 
: 

with  a  #  0 has  two  solutions: 

ax?  +  Bx  +  y=0 

[A.2.9] 

1/2 
x,  =  abt  tay 

4 

+ 

- 

- 

[A.2.10] 

xX,  =  —B = (B* = day)!" 

[A.2.11] 

2a 

When  (8? —  4ay)  =  0, both  these  roots  are  real,  whereas  when  (6?  —  4ay) <0, 
the  roots  are  complex.  Notice  that  when  the  roots  are  complex  they  appear  as  a 

710  Appendix A  | Mathematical  Review 

conjugate pair: 

) 

x,  =  {—B/[2a]}  +  {(142a])(4ay  —  B?)!?hi 
x =  {—Bi/[2a]}  —  {(1/2a])(4ay  —  B?)"}i. 

A.3.  Calculus 

A function  f(x) is said  to be  continuous  at x  =  c if f(c)  is finite  and  if for 
every  e  >  0 there  is a  5 >  0 such  that f(x)  —  f(c)|  <  e whenever  |x —  c| <  6. 

Derivatives of Some  Simple Functions 

The  derivative  of f(-) with  respect to x is defined  by 

|

[>-€.A j 

4 =  gat  te +  A) fa  (x) 

dx 

40 

; /

  ;  +. 

AF 

T 

? 

ei  S¥iti  he  #6 
; 

. 

| 

: 

_  provided 

Sie  ee ys 

that this limit  exists. 
2 
TE f( ) is linear in x, or 

a 

— 

| 

: 
UiRuleve  OF  iqutene  1 

 
The  derivative  df(x)/dx  is itself a function  of x.  Often  we  want  to specify  the 

point  at  which  the  derivative  should  be evaluated,  say,  c.  This  is indicated by 
df (x) 

For  example, 

| 

= 

dx  x=c 

dx  oo  ~~ 2x|,-3  =  6 

Note  that  this notation  refers to taking the  derivative first and then  evaluating  the 
derivative  at  a particular point  such  as  x  =  3. 

/ 

Chain  Rule 

-  The  chain  rule states  that  for composite functions such  a 8  rsuovinst 
swhnewinsh  of 
ed  bonitgh(z)  = ay  al 
“ee 

\ to 

the derivative is 

rh 

. 

aa 

se 

For  example, to  evaluate 

- 

Complex 

Consens 

ere 

ae 

ae 

~ 

4 
[AB] 

deco) = ae a 

dx 

du | 
7 

: 

zeus  Hest  2isks  teilt-babivey  > 
10  AT  moni  2  {  - SE 

ie  wapen 
-& ents asiaktee ae, S13  teat natbeeeactiraaced AF sepals 

we let sw) = =  uk «and u(x) = =  par ‘Bx. Then  ‘is 

Geometric  Series 

Consider  the sum 

Sp=1l+  G+  H+  o>  +---  4+ 97, 

Multiplying  both  sides  of [A.3.6]  by ¢, 

7 
r=  G+  P+  Pt  --+  +  G7  +  GT? 

Subtracting  [A.3.7]  from  [A.3.6]  produces 

[A.3.6] 

[A.3.7] 

(1  -— )sp  =  1 —  o7*?. 
[A.3.8] 
For any ¢ #  1, both  sides  of [A.3.8]  can  be divided  by (1 —  ). Hence,  the  sum 
in [A.3.6] is equal to 

aes  ot) 

$ #1 

ag  et 

pudiptig 

[A.3.9] 

From  [A.3.9], 

soe:  iia % 

=  ery  ict 

and  so 

fit, 

. 

= 

Tits  convention(®  sengitte  g4 0+  P50)  = ry  a -* 

evER  FH  g 

418  DSsvleval  4 

P/~.  37a? 
. 

[as19] 

nror 

iff 

a.  a 

nanberc  080 
eae 

Setting  R,(c,  x)  = 
approximation  to  the  function  f(x)  in  the neighborhood  of x  =  c: 

Oandx  =  c  +  Ain  [A.3.11]  produces  an  rth-order  Taylor series 

f(x)  =  f(c) + rl @29+224]  @-o 

x=C 

. 

x=C 

[A.3.12] 

+ 

d’ 

Barer 

Power  Series 

If the  remainder  R,(c,  x) in  [A.3.11]  converges  to  zero  for  all  x  as  r >  ~,  a 
power  series  can  be  used  to  characterize  the  function  f(x).  To  find  a power  series, 
we  choose  a particular  value  c  around  which  to  center  the  expansion,  such  as  c  = 
0. We  then  use  [A.3.12]  with  r >  ».  For  example,  consider  the  sine  function.  The 
first  two  derivatives  are  given  by [A.3.2]  and  [A.3.5],  with  the  following  higher- 
order  derivatives: 

aaa 

ie 

—  cos(x) 

g  ae =  sin(x) 

d° sin(x)  _ 

a  =  cos(x), 

and  so  on. Evaluated  at  x  =  0, we  have 

f(0)  =  sin(0)  =  0 

d 
~ ea  =  cos(0)  =  1 

d? 
= du  =  —sin(0)  =  0 

’ 

ef a  =  —cos(0)  =  -1 

a =  =  sin(0)  =  0 

d°f 
dx’ 2  =  cos(0)  =  1. 

Substituting  into  [A.3.12]  with  c  =  0 and  letting  r  +  ~  produces  a  power  series 
for  the  sine  function: 

1 
ain(e)  ut.  ges~ ty 

4 

* 

1 
eit  8 

- 

[A.3.13] 

Similar  calculations  give  a power series for  the  cosine  function: 

cos(x)  = PL  a  ean  eat Bee 

1 

a 

1 

[A.3.14] 

Exponential  Functions 

A number y raised  to  the  power  x, 

f(x)  =  7%, 

714  Appendix  A  | Mathematical  Review 

is called  an  exponential function  of x.  The  number y is called  the  base  of  this 
function,  and x is called  the  exponent.  To multiply  two  exponential  functions  that 
share  the same  base,  the  exponents  are  added: 

(7*)-(y) =  yer, 

[A.3.15] 

For example, 

)(Y)  =  rd  (ry)  = 
To raise  an  exponential  function  to the  power  k, the  exponents  are  multiplied: 
ly}  =  y*. 

{[A.3.16] 

For example, 

[FP =  (AI (A 1b]  =  7° 

Exponentiation  is distributive  over  multiplication: 

paamree exponents  denote  reciprocals: 

(a-ByY  = _(a*)-(B*). 

[A.3.17] 

2 
4  Ammen  ot  ve  hn  go ay 

Nsvoet  SS  Gh. 5 

Notinish ashe 4 

: 

.  Si rae “preina IS }5 

8.2.4 

aie eee:  ~ 

ie) 

arnig 

arte  2 9  vk  mer  Tiles 

bee ‘of the number 

f 

t2)4.A] bas [ei. EA]  m0 gaitseRest 

(ies + + + (80 =  iy 

: 
vhetimiz 
ohm  =e  est,  Eph igaitiaion¢ 

. 

=  mg 

Pscapai> oie: ae am - — atid  dowel re ty ao ’ 

To  find  a  power  series  for  the function  f(x)  =  e*,  notice  from  [A.3.20]  that 

and  so,  from  [A.3.18], 

ath  JF Ss 
“’ 
dxr 

d'f 
i 
dx’ x=0 

2 
=e°=1 

. 

A.3.22 

for  all  r.  Substituting  [A.3.22]  into  [A.3.12]  with  c  =  0 yields  a  power  series  for 
the  function  f(x)  =  e*: 

277  a 
Setting  x  =  1 in  [A.3.23]  gives  a  numerical  procedure  for  calculating  the 

ae 

[A.3.23] 

value  of e: 

7 

1 
e=T+1  +  pte  ce  rtmer  2.71828...  . 

1 

1 

Euler  Relations  and  De  Moivre’s  Theorem 

Suppose  we  evaluate  the  power  series  [A.3.23]  at  the  imaginary  number  x  = 

i@,  where  i =  V—1  and 6 is some  real  angle  measured  in radians: 

. 

2 

° 

3 

. 

4 

. 

5 

it  am 1  +  poy 

ce  Le eg Pre  oars ai 

ae  es 
, 
gt 
l-  3  ay  Se  rif  Sa  S—..} 

oF 

e 

= 

[A.3.24] 
oe 

Reflecting  on  [A.3.13]  and  [A.3.14]  gives  another  interpretation  of [A.3.24]: 

e'®  =  cos(6)  +  i-sin(6). 

[A.3.25]} 

Similarly, 

—{6 

e 

Todas  ol}  eee  tate $e  : ag  ae 

( -i6)? 

(-i6)° 

(-i6)° 

(-i6)* 

, 

a  ales ht ty goatee  on a 

eT 

e 

- 

cos(6)  —  i-sin(@). 

To  raise  a complex  number  (a +  bi) to the  kth  power,  the  complex  number 

is written  in polar coordinate  form  as  in [A.2.6]: 

Using  [A.3.25],  this  can  then  be treated  as  an exponential  function  of @: 

a  +  bi  =  R{cos(6)  +  i:sin(@)]. 

a+  bi  =  R-e®. 
[A.3.27] 
Now  raise  both  sides  of [A.3.27]  to the kth power,  recalling  [A.3.17] and [A.3.16]: 
(a +  bi}  =  R* fet  =  Rei, 
[A.3.28] 

Finally,  use  [A.3.25]  in reverse, 

| 

| 

, 

e"()  =  cos(9k)  + i-sin(Ok), 

716  Appendix  A  | Mathematical  Review 

to deduce  that  [A.3.28]  can  be written 

(a +  bi)  =  R*-[cos(@k)  +  i-sin(6k)]. 

[A.3.29] 

Definition  of Natural  Logarithm 
The natural  logarithm  (denoted  throughout  the  text  simply  by ‘“‘log”’)  is the 

inverse  of the function  e*: 

Notice  from  (A.3.18]  that  e®  =  1 and  therefore  log(1)  = 

log(e*)  =  x. 

_ Properties  of Logarithms 
_  For any x > 0, BA  Mindi sheicese thet 

From [A.3.30] 
equal to the sum of the logs: 

and 

+  hed”  Ls)goi  | 

[A.3.30] 
[A.3.15], a  i the’ bp  abet numbers is 
ues!  |} misgneils 

log(a-b) =  log{(e'"#)- (ete) = plete}  =  log(a)  + log(o). 

Also, use  [A.3.16] to write 

hipek 
=, [eloa=)}e = 1%: log(x)  5 

Anes 

com  san  : 

+; 

esas & ritov} ponibis Sr St nei: EGY  30% | lgiuign  99}  O37  noite prixorg 

[a3 Crepe the log of a mmber aed othe 
Apete  fle fc Whe  ao amy 

a 

ae Be  (1 

fe : giz) + 

a 

Now  use  the  chain  rule  to  differentiate: 

But  from  [A.3.21], 

A(x) 
dlog(x) 

of  du 
du  d log(x) 

du 
Ee  | 
d log(x) 

d log(x) 
expllog(x)}  d log(x) 

= 

[A.3.33] 

[A.3.34] 

Substituting  [A.3.34]  into  [A.3.33]  gives 
df(x) 
d log(x) 

_ 

4f 
dx 

It follows  from  [A.3.32]  that 

dlog f(x) _  1 df _  [f( + A) ~ FV) 
dlog(x) 

[(x  +  A)  —  x}/x 

f°  dx 

which  has  the  interpretation  as  the  elasticity  of f with  respect  to  x,  or  the  percent 
change  in f resulting  from  a  1%  increase  in x. 

Logarithms  and  Percent 

An  approximation  to  the  natural  log  function  is obtained  from a first-order 

Taylor  series  around  c  =  1: 

err  rer 

d | 
eet  ry  rs  ieee 

a 

[A.3.35] 

But  log(1)  =  0, and 

d log(x) 
l 
oa  =- 

=  1. 

dx 

x=1 

x  v=l 

Thus,  for  A close  to  zero,  an  excellent  approximation  is provided  by 

log(1  +  A) =A. 

[A.3.36] 

An  implication  of [A.3.36]  is the  following.  Let  r denote  the  net  interest  rate 
measured  as  a  fraction  of  1; for  example,  r  =  0.05  corresponds  to  a  5%  interest: 
rate.  Then  (1  +  r) denotes  the  gross  interest  rate  (principal  plus  net  interest). 
Equation  [A.3.36]  says  that  the  log of the gross  interest  rate  (1  +  r) is essentially 
the  same  number  as  the  net  interest  rate  (r). 

Definition  of Indefinite  Integral 

Integration  (indicated  by f dx)  is the  inverse  operation  from  differentiation. 

For  example, 

[x dx  =  x?/2, 

[A.3.37] 

718  Appendix  A  | Mathematical  Review 

because 

n 

2 ae <a  | 

[A.3.38] 

The  function  x7/2  is not  the  only function  satisfying  [A.3.38];  the  function 

(x7/2)  +  C 
also  works  for any  constant  C.  The  term C is referred  to  as  the  constant  of inte- 

Some | Useful Indefinite  Integrals 
The following integrals  can be confirmed  from  [A.3.1],  [4.3.32], [A.3.2], 

[A.3.3],  and [A.3.21]: 

+C 

k#-1 

[A.3.39] 

1 

Outi  —  log(x) # |  a  x>0°.  CoA  wah  i. 
Jrtae= eee $€  £68, 
‘ 

: 

“as “1 

dione)  pe  ole  tee  ees 
[A.3.41] 

[ee.£.A} 

mi cos(x) dx =  sin(x)  +.C_. 

inn  Xe 

’  garde 

tianlian fee ra ilarer  +c 

[trl  Soe  +c | 
eek  eee 

Se  eu 

barsis 

| 

[A3A2] 

| 

A. 

- 

: 

4 
Ms 

t 
a 
i  ang 
; 
== 
. 

aah 2  of  i  ther  = * = , see 

‘4 

? 
? 

iqat th ns) r ¥  eh  atin”  nts  sf 

e 

- 

+ 
7. 

ay 
as  » ke 
4  3  TOr  Con  1 &  it 

“= 

r, 

at 

> 

TY: 
ve 

4 

ITE  y 

q 

4 

. 

: 

9 

eae 

Pry  “pnd  ak rd 

: 

| teers ne praagh 

12, 

me (xX); Pca 

wh.  | fC xX)  ax  +  Oo 

Sze) 

BF Ampere 

ives A.5  rid gakgis integral as. the a area woes a function. 

ib  f  ? 
* 

2 ia as 

“ 

ap ¢ 

' 

ee that  the  fe eogee Re  > a)i is eB inverse oi diferemtiation: 
ith. -_ 

AG ta +O ew \ | 

ates 

- 
parades  din Perce 
e% 

. 

where 

‘* 
Als pK! 338 ation 

earn sroundic = 

——) 
fo  the + ss Re ee Reed  iron  4  liest-order- 
¥ 

oe 

[A.3.45] 

- 

[ sin(x) dx =  [—cos(x)]|,..,.  —  [—cos(x)]}|,-0 

=  [-—cos(7/2)}  +  [cos(0)] 
=0+.1 
=  1. 

To find  the  area  between  0 and  27,  we  take 

[ ” sin(x) dx  =  [-cos(2m)]  +  cos(0) 
=-1+1 
= 0. 

The  positive  values  for  sin(x)  between  0 and  a exactly  cancel  out  the  negative 
values  between  7m and  27. 

A.4. Matrix  Algebra 

Definitions 
An (m X  n) matrix is an array ofn numbers pe: ‘nina mrowsandn columns: 

SSUE ee Re Hsté £-  Fie?  Si 

| 

GA stains ht  Sistiw  7SvVS  Or 

or,  more  compactly, 

A  + 

(mxn) 

B_ 
 (mxn) 

=  [a,  +  5,]. 

The  product  of an  (m  x  n) matrix  and_an  (n  X  q) matrix  is an  (m  x  q) matrix: 

Ax  B=  C, 
(mq) 

 (nxq) 

(mxn) 

where  the  row  /, column  j element  of C is given  by 27.,4;,5,;.  Notice  that  mul- 
tiplication  requires  that  the  number  of columns  of A be  the  same  as  the  number 
of rows  of B. 

pe 

To  multiply  A by a  scalar  a,  each  element  of A is  multiplied  by a: 

; 

with 

i 
(1x1) 

ae,  ee  eae 
(mxn) 

(mn) 

C  =  [aa,;J. 

It is easy  to  show  that  addition  is commutative: 

whereas  multiplication  is not: 

A+B=B+4; 

AB  #  BA. 

Indeed,  the  product  BA  will  not  exist  unless  m  =  q, and  even  where  it exists,  AB 
would  be  equal  to  BA  only  in rather  special  cases. 

Both  addition  and  multiplication  are  associative: 

(A 

+.B)  +  C=A+(B+C) 

(AB)C  =  A(BC). 

Identity  Matrix 

The  identity  matrix  of order  n  (denoted  I,) is an  (n x  n) matrix  with  1s along- 

the  principal  diagonal  and  Os elsewhere: 

7 

: 

eo 

a  mans 
i aoe | 

For  any  (m  x  n) matrix  A, 

and  also 

AXL=A 

Ll, Xx AeA 

Powers  of Matrices 
For  an  (n  x  n) matrix  A, the  expression  A? denotes  A-A.  The  expression 
A“ indicates  the  matrix  A multiplied  by itself  k times,  with  A° interpreted  as  the 
(n Xn) identity  matrix. 

| 

722  Appendix  A  | Mathematical. Review 

Transposition 
Let a,; denote  the row  i, column j element  of a matrix  A: 

A  =  [q;)). 
The  transpose  of A (denoted  A’) is given  by 

| 

A’  =  [a,]. 

For example,  the  transpose  of 

2  4  6 
a 7 
a 

2  @  ill 
4  #52). 
e  729 

The transpose of a row vector is a column  vector. 

It is easy to verify the following: 

~- 

Giaie 

a  ee 
unipdtcateet; 
PUMPS  Tewbivibni  oi ea B)' = A’ +B’ 
xem  stsingonggs ori? tine Ley  wa oo DiS  WO)  Silt 

Of gd  = i. 

i 

[A.4.1) 
itt  bi va as La 

sof 

€? 

If  Ais  an  (n  X  n)  matrix  and A is a  scalar,  then 

trace(AA)  =  > Aaj,  =  A°  > a;,  =  A-trace(A). 

n 

n 

; 
i=1 

i= 

1 

Partitioned  Matrices 
A  partitioned  matrix  is  a  matrix  whose  individual  elements  are  themselves 

matrices.  For  example,  the  (3  x  4) matrix 

Q;; 

@yz 

413  Ay 
A=  ]4,;  42  43  Gx 
As, 

432 

G33 

43; 

could  be  written  as 

where 

A,  = 

a 

11 
Az, 

a 

412 
422 

A,  = 
. 

a 

13 
Qx3, 

a 

4 
Ar, 

— 

a,  =  [a3,  a3] 

,  — 

a,  =  [a35  a3,). 

Partitioned  matrices  are  added  or  multiplied  as if the individual  elements  were 
scalars,  provided  that the row  and column  dimensions  permit the appropriate  matrix 
operations.  For  example. 

A, 
(m,n) 

A, 
(em,  X12) 

ie 

B, 

B, 

A,  +  B,  A,  +  B, 

(mm)  (rm  Xn2)] 

io 

2; 

(m,  Xn) 

Patt  sey oud oe 

(my,  X n2) 

(m2Xn,) 

(m2 X nz) 

(mzXn,)  (nz X nz) 

(mz Xn) 

(mz  n>) 

Similarly, 

A, 
(m, Xn) 
3 
(m,n) 

A, 
(m, Xn) 

(m2 Xn2) 

B, 
(myX G1) 
3 
(n2Xq,) 

B, 

A,B,  +  A,B,  A,B,  +  A,B, 

(my G2)  ) 
B, 
(m2 * 92) 

(mm, X  q)) 

(mm, X  G2) 

A3B,  +  A,B;  A;B,  +  A,B, 

(m2  q1) 

(m2  q2) 

Definition  of Determinant 

The  determinant  of a  2  x  2 matrix  is given  by the  following  scalar: 

JA|  =  @y1@22  —  G24). 
The  determinant  of an  n  X  n  matrix  can  be  defined  recursively.  Let  A;, denote 
the  (n —  1) x  (n —  1) matrix  formed  by deleting  row  i and column  from  A. The 
determinant  of A is given  by 

[A.4.4] 

|A|  =  >» (-1)/*?a,;|A,)l. 

[A.4.5] 

For  example,  the  determinant  of a 3 x  3 matrix  is 

4,  42  3 
Q@y  Ay  G@y| 
a  32 

=  ay  yu 2 —  a,  ee “ug + 

933 

31 

433 

rg ig , 

31 

432 

724  Appendix A | Mathematical  Review 

Properties  of Determinants 

A square  matrix  is said  to  be  lower  triangular  if all  the  elements  above  the 

principal  diagonal  are  zero  (a,, = 0 for j >  i): 

ay, 

0  Sse  6S 

Barna 

ie 

+e 

Bn,  Gn2  Gn3 

di 

Ban 

The  determinant  of a  lower  triangular  matrix  is simply  the  product  of  the  terms 
along  the  principal  diagonal: 

|A|  =  @\4@o2  **  *  Gn,y,- 

[A.4.6] 

That [A.4.6] holds  for n  =  2 follows  immediately from  [A.4.4]  Given  that  it holds 
for a matrix of order n —  1, equation  [A.4.5]  implies  that  it holds  for n: 
>* 

an  0. 

0 

*O 

: [A] = ay  oe fo a 
oa 

Qe  0  on  | 

fo + O-[Ap|  +  +++  +  O-[Aul- 
“ag 

4 

Ap immediate PPapsetion. of “ 4.6] is that the determinant of eae bee-eyd 

aL  ew 

ma 

“2  [hl = Sh 
_  Another  useful fact about  detegratahotsdts that ifann  xn  mattis A 
PE  denen a, the effect is to multiply the determinant by a” 
si maaintratsb-att icsogolt lycnfuafiayte[As Siw satigsahs vsvoh  PAA 

eae  |  [A.4.7] 
is mul- 

|  i for the n  "I = 2 case from [A.4.4]: Ac = 

se ang ok &  magic  8: 

ebes  €7253? 

second  row,  the  determinant  will  again  be  unchanged: 

ay 

a3) 

ae 
= 

ay 

Q\2 

Qa32_ 

ay, 

a3 

a>2  +  Ca32 

ar3  +  Ca33 

a32 

a33 

12 

ar  +  Ca; 
a 

31 

az3  +  Ca33 
a 

33 

+  a3  21 

a, 

+  ca 

31 

a. 

42 

+  ca 

32 

a3 

a32 

=  ay 

Qx2 
432 

ar 
A433 

a 
—a,|  7 
a3, 

a23 
a 

+  @y3  Pe, 

a,  an 
432 

31 

Tee 

| 

In general,  if any  row  of ann  x  n  matrix  is multiplied  by c and  added to  another 
row,  the  new  matrix  will  have  the  same  determinant  as  the  original.  Similarly, 
multiplying  any  column  by c  and  adding  the  result  to  another  column  will  not 
change  the  determinant. 

This  can  be  viewed  as  a  special  case  of the  following  result.  If A and  B are 

both  n  X  n  matrices,  then 

|AB|  =  |A|-|BI. 
Adding  c times  the  second  column  of a 2  x 2 matrix  A to  the  first  column  can  be 
thought  of as  postmultiplying  A by the  followimg  matrix: 

[A.4.9] 

[i 

Since B is lower  triangular  with  1s along  the  principal  diagonal,  its determinant  is 
unity,  and  so,  from  [A.4.9], 

|AB|  =  Al. 

Thus,  the  fact  that  adding a  multiple  of one  column  to  another  does  not  alter  the 
determinant  can  be viewed  as  an  implication  of [A.4.9]. 

If two rows  of a matrix  are  switched,  the determinant  changes signs. To switch 
'  the  ith  row  with  the jth,  multiply  the  ith  row  by  —  1;this  changes  the  sign of the 
determinant.  Then  subtract  row  i from  row  j, add  the  new j back  to i, and  subtract 
i from  j once  again.  These  last  operations  complete  the  switch  and  do  not  affect 
the  determinant  further.  For  example,  let  A be  a  (4 x  4) matrix  written  in par- 
titioned  form  as 

: 

aj 
a) 
’ a; 
a, 

where  the  (1 x  4) vector  a; represents  the  ith  row  of A.  The  determinant  when 
rows  1 and 4 are  switched  can  be calculated  from 

oe 

a; 
a, 
a; 
’ a, 

U 

ay 
a, 
a; 
a, 

ee  eek 

a, 
' 

—aj 
as 
a3 
a; +  a; 

a3 
ai +a; 

a; 
: 

a; 
a’ 

726  Appendix  A  | Mathematical  Review 

‘This  result  permits  calculation  of the ia  hs  of A in  reference  to  any 

row  of an  (nm  X  n) matrix  A: 

To  derive  [A.4.10],  define  A*  as 

|A|  =  > (-1)'*/a,[A,/. 

j= 

[A.4.10] 

Then,  from  [A.4.5], 

Jat} =  3 (- 1) *ailagl  =  B (-1)/*aylAy. 

Moreover,  A* is obtained from A by (i —  1) dp switches, such as switching i with 
i-  1,i—-  1 withi  —  2,...,  and  2 with 1. Hence, 

AGS 

2654S 

ASH 

{Al = (- “ay-ttact = (= at Da 

as claimed in  [A.4.10} 

Su Aa  mpenion cf 4410  i any tow of» maison , 

_  a  ee 

in she  og, Sor 

ro 

For  example,  for  n  =  2, 

Q\; 

A>, 

4j2 

42 

=  (IMa,,a..  —  @242,})°  ee  Pe  [A.4.13] 
—  ar, 

a); 

A matrix  whose  inverse  exists  is said  to be nonsingular.  A matrix  whose  determinant 
is zero  is singular  and  has  no  inverse. 

When  an  inverse  exists, 

AxA-'=l,. 

(A.4.14] 

Taking  determinants  of  both  sides  of  [A.4.14]  and  using  [A.4.9]  and  [A.4.7}, 

|A]-|A7'|  =  1, 

so 

Alternatively,  taking  the  transpose  of  both  sides  of  [A.4.14]  and  recalling 

|A-'|  =  I/Al. 

[A.4.15] 

[A.4.3], 

which  means  that (A~')’  is the  inverse  of A’: 

(Az !)'A'  =  4h, 

(A),  =  {ay 

For  a  a  nonzero  scalar  and  A a  nonsingular  matrix, 

[aA]~'  =a  'A“'. 

Also,  for  A,  B,  and  C all  nonsingular  (n  x  n) matrices, 

and 

{[AB]~-'  =  B-'A™! 

) 

[ABC]~'  =  C"'B“'A™'. 

Linear  Dependence 

Let  x,,  X.,...,  x,  be  a  set  of k different  (nm  x  1) vectors.  she  vectors  are 
,  c, }, not 

said  to  be linearly  dependent  if there  exists  a  set  of k scalars  (c,, c2,..  . 
all  of which  are  zero,  such  that 

C)X,  +  CoX,  +  °°:  +  O,x,  =  GD. 

If no  such  set  of nonzero  numbers  (c,,  ¢2,  . 
. 
X>,...  4  X,) are  said  to  be linearly  independent. 

. 

,  C,) exists,  then  the  vectors  (x,, 

Suppose  the  vectors  (x,,  x3,  . 

. 

. 

.  X,) are  collected  in an  (m  Xx  k) matrix  T, 

written  in partitioned  form  as 

T  =  [x,  X> 

se 

x, ]. 

If the  number  of vectors  (kK)  is equal  to  the  dimension  of each  vector  (n),  then 
there  is a simple  relation  between  the  notion  of linear  dependence  and  the  deter- 
minant  of  the  (n  x  n) matrix  T;  specifically,  if (x,,  x2,  ...,  x,)  are  linearly 
dependent,  then  |I|  =  0.  To  see  this,  suppose  that  x,  is one  of the  vectors  that 
have  a nonzero  value  of c;.  Then  linear  dependence  means  that 

X,  =  —(Cz,/c,)x.  —  (C,/e,)x,  —  +++  —  (c,/e,)x,. 

728  Appendix  A  | Mathematical  Review 

Then  the  determinant  of T is equal  to 

IT|  iy Il =  (e2/ea)xz  -  (C3/c,)x;  er 

RE  y  (c,/c)x,] 

Basa?  *é 

rae 

But  if we  add  (c,,/c,)  times  the nth column  to the  first  column,  (c,,_,/c,)  times  the 
(n —  1)th column  to  the  first  column,  . 
. 
,  and  (c,/c,)  times  the  second  column 
to the first  column,  the result  is 

. 

[Tl =|0  x, 
=  0. 

--- 

x,,| 

The  converse  can  also  be  shown  to  be  true:  if |T|  =  0, then  (x,,  x.,..., 

x, ) are  linearly  dependent. 

Eigenvalues  and  Eigenvectors 

Suppose  that  an  n  X  n  matrix  A, a nonzero  n  X 1 vector  x,  and a scalar  A 

are  related  by 

Then x is called  an  eigenvector  of A and A the  associated  eigenvalue.  Equation 
[A.4.16]  can  be written 

: 

Ax  =  Ax. 

[A.4.16] 

Pf 

Ax  -  AI,x =0 

| 

or 

7 

Faas! 

number  A such  that 

i 

De  PRR i 

ae 

Premultiplying  both  sides  of  [A.4.19]  by A  produces 

C,AX,;  +  C2AX,  =  C,AyX,  +  CpA2X2  =  9. 

[A.4.20] 

If [A.4.19]  is multiplied  by A, and  subtracted  from  [A.4.20],  the  result  is 
Golde  —  Ar 

Yaormidh 

[A.4.21] 

But  x,  is an  eigenvector  of A,  and  so  it cannot  be  the  zero  vector.  Also,  A,  —  A, 
cannot  be  zero,  since  A,  #  A,.  Equation  [A.4.21]  therefore  implies  that  c,  =  0. 
A parallel  set  of calculations  show  that  c,  =  0.  Thus,  the  only  values  of c,  and  c, 
consistent  with  [A.4.19]  are  c,  =  0 and  c,  =  0, which  means  that  x,  and  x,  are 
linearly  independent.  A  similar  argument  for  n  >  2 can  be  made  by induction. 

A  Useful  Decomposition 

Suppose  an  n  X  n  matrix  A  has  n  distinct  eigenvalues  (A;,  Az,  ...,  A,)- 

Collect  these  in a  diagonal  matrix  A: 

| 
| 
ge  0  A,  *'°  0 

eres 

pe 

OF  iD":  %,  Ae 

Collect  the  eigenvectors  (x,,  X,,...,  X,) in an(n  X  n) matrix  T: 

T=,  sR 

Applying  the  formula  for  multiplying  partitioned  matrices, 

AT  =  [Ax,  Ax,  --: 

Ax,]. 

But  since  (x;, X,,..  . 

,  X,) are  eigenvectors,  equation  [A.4.16]  implies  that 

AT  =  [A,x,  Ax.  -°: 

A,X,]. 

[A.4.22] 

A  second  application  of  the  formula  for  multiplying  partitioned  matrices  shows 
that  the  right side, of [A.4.22]  is in turn  equal  to 

[Aix  A2X2 

oe, 

AnXn] 

‘ 

: 

©  1S  play 

=  TA. 

A,  90 

ce  oe  ed  :  re 
ee 

ee 

0 

° 

Thus,  [A.4.22]  can  be written 

AT  =  TA. 

[A.4.23] 
Now,  since  the  eigenvalues  (A,,  Az,  ...  ,  A,) are  taken  to  be  distinct,  the 
eigenvectors  (x,, X,,.  . 
,  X,) are  known  to be linearly  independent.  Thus,  |T|  # 0 
and T ~’  exists.  Postmultiplying  [A.4.23]  by T~!  reveals  a useful  decomposition-of 
A: 

. 

A  =  TAT™'. 

[A.4.24] 

The Jordan  Decomposition 
The  decomposition  in  [A.4.24]  required  the  (n  x  n) matrix  A  to  have  n 
linearly  independent  eigenvectors.  This  will  be  true  whenever  A has n distinct 
730  Appendix  A  | Mathematical  Review 

eigenvalues,  and  could  still  be  true  even  if A  has  some  repeated  eigenvalues.  In 
the  completely  general  case  when  A  has  s  Sn 
linearly  independent  eigenvec- 
tors,  there  always  exists  a  decomposition  similar  to  [A.4.24],  known  as  the 
Jordan  decomposition.  Specifically,  for  such  a  matrix  A there  exists  a  nonsingular 
(m  X  n) matrix  M such  that 

A  =  MJM™', 

[A.4.25] 

where  the  (m  X  n) matrix  J takes  the  form 

oo 

J;  So. 

tes 

Sa, 
ee  «-.  | 

: 

2 

[A.4.26} 

with 

A;  1  0 
0  A;  1 
K=}0  0.4, 

0 
0 
0 

0  0  0  Set 

A, 

[A.4.27] 

Thus,  J, has  the  eigenvalue  A, repeated  along  the  principal  diagonal  and  has  unity 
repeated  along  the  diagonal  above  the  principal  diagonal.  The  same  eigenvalue  A, 
can  appear  in  two  different  Jordan  blocks  J, and  J, if it corresponds  to  several 
linearly  independent  eigenvectors. 

Some  Further  Results  on  Eigenvalues 

Suppose  that A is an  eigenvalue  of the  (n x  n) matrix  A.  Then A is also  an 
eigenvalue  of SAS ~'  for  any  nonsingular  (n x  n) matrix  S. To  see  this,  note  that 

implies  that 

or 

(A —  Al,)x  =  0 
| 

S(A  —  AI,)S~'Sx  =  0 

(SAS-}  —  Al,)x*  =  0 

| 

7 

[A.4.28] 

the eigenvec- 
| 

‘for  x*  =  Sx.  Thus,  A  is  an  eigenvalue  of  SAS~'  associated  with 
a 
tor  x*. 

_  From  [A.4.25],  this  implies  that  the  determinant  of any  (n  x  n) matrix  A is 
the  same  as the  determinant  of its Jordan  matrix  J defined  in [A.4.26].  Since  J is 
upper  triangular,  its  determinant  is the  product  of terms  along  the  principal  di- 
agonal, which  were  just the eigenvalues  of A. Thus,  the determinant  of any matrix 
A is given  by the  product  of its eigenvalues. 

It is also  clear  that  the  eigenvalues  of A are  the  same  as  those  of A’.  Taking 

the transpose  of [A:4.25], 

A’  an  (M’)~'J'M’, 

we  see  that  the  eigenvalues  of A’  are  the  eigenvalues  of J’.  Since  J’  is lower 
731 

A.4.  Matrix  Algebra 

triangular,  its  eigenvalues  are  the  elements  on  its  principal  diagonal.  But  J’ and  J 
have  the same  principal  diagonal,  meaning  that  A’ and A have  the same  eigenvalues. 

Matrix  Geometric  Series 

The  results  of [A.3.6]  through  [A.3.10]  generalize  readily  to  geometric  series 

involving  square  matrices.  Consider  the  sum 

Sp  =  he+  Ait. A?  +  APotinns  +  AT 

[A.4.29] 

for  A an  (n  X  n) matrix.  Premultiplying  both  sides  of [A.4.29]  by A, we  see  that 

AS,  =  A  +:A2+°A?it+  0°*  +  AT  +  ATT? 

[A.4.30] 

Subtracting  [A.4.30]  from  [A.4.29],  we  find  that 

Ge  MS,  =  1,  —  ATP. 

[A.4.31] 

Notice  from  [A.4.18]  that  if |I,  —  A| =  0, then  A  =  1 would  be  an  eigenvalue  of 
A.  Assuming  that  none  of  the  eigenvalues  of  A  is  equal  to  unity,  the  matrix 
(I,  —  A) is nonsingular  and  [A.4.31]  implies  that 

S, =  (I,  -—  A)"(I,  -  A7*) 

[A.4.32] 

if no  eigenvalue  of A equals  1.  If all  the  eigenvalues  of A are  strictly  less  than  1 
in modulus,  it can  be  shown  that  A’*!  —  0 as  T—  ~&,  implying  that 

(IL,  +  A  +  A*  +  A?  +->--)  =(I,  -—  A)" 

[A.4.33] 

assuming  that  the  eigenvalues  of A are  all  inside  unit  circle. 

Kronecker  Products 

For  A an  (m  X  n) matrix  and  B a  (p  x  q) matrix,  the Kronecker  product  of 

A and B is defined  as  the  following  (mp)  x.  (mq)  matrix: 

ete  i 

a,,B 
a,B  °° 
a,,B  aB  -:-: 

a 

a,,B 
a,,B 

ae 

4,,:B  2B  a  AnnB 

The  following  properties  of the  Kronecker  product  are  readily  verified.  For  any 
matrices  A, B, and  C, 

(A  & B)'  =  A’  @B' 

(A ® B) @®C  =A®@®  (BOC). 

, 

Also,  for  A and  B both  (m  x  n) matrices  and  C any  matrix, 
(A  +  B)@C  =  (A@C)  +  (BOC) 

C@(A  +  B)  =  (C@A)  +  (C@B).  — 

[A.4.34] 

[A.4.35] 

[A.4.36] 

[A.4.37] 

Let A be (m x  n), B be (p x  q), C be (n x  k),  and  D be (q  x r).  Then 
[A.4.38] 

(A ® B)(C  @ D)  =  (AC) ® (BD); 

732  Appendix  A  | Mathematical Review 

that  is, 

4,,B 

a,.B 

:-:- 

a,,B 

c,,D  C.D  ::- 

c,,D 

4,,)B  4n2B  Sr 

Qnnb 

C,,,D  Cn2D 

oh 

* 

c,,D 

> a,;C;,BD  a 4,;C;2BD 

a. 

py a,,C;,  BD 

> a, ;C;,BD  YY a  ;C;>BD 

"rs  > a,,C;,  BD 

» 4,,;C;,BD  >. 4,;C;2BD 

oe 

re  > 4,,; Cj,  BD 

For  A  (”  X  n) and  B  (p x p)  both  nonsingular  matrices  we  can  set  C  = 

A~'  and  D  =  B~'  in  [A.4.38]  to  deduce  that 

(A  ® B)(A~' @ B~') =  (AA~") @ (BB~')  =  1, @L,  =  L,. 

Thus, 

(A @ B)-'  =  (A-!'@B-). 

[A.4.39] 

Eigenvalues  of a  Kronecker  Product 

For  A an  (n X  n) matrix  with  (possibly  nondistinct)  eigenvalues  (A,, A2,..  - 

, 
A,,) and  B (p  X  p) with  eigenvalues  (41,, “2,  ...  ,  “,), then  the  (np)  eigenvalues 
p. To see 
of  A  ® B  are given  by  Aju;  fori  =  1,2,...,mandj  =  1,2,..., 
this,  write  A and  B in Jordan  form  as 

7 

A  =  M,J,M;! 

B  —  M;J2,M;'. 

Then  (M,  ® Ms)  has  inverse  given  by (M;,'  ® Mj').  Moreover,  we  know  from 
[A.4.28]  that  the  eigenvalues  of (A ® B) are  the  same  as  the  eigenvalues  of 

(M4z'  @ Mg')(A  @ B)(M,  ® Mz)  =  (M4'AM,)  © (Mz  'BMs) 

ad Ja ® Jz. 

But J, and J, are  both  upper triangular,  meaning that (J,  ® J,) is upper  triangular 
as well.  The eigenvalues  of (A & B) are  thus just the terms  on  the principal  diagonal 
of (J, ® Jz),  which  are  given  by A;,. 

Positive  Definite  Matrices 

An  (n  xX  n) real  symmetric  matrix  A is said  to  be positive  semidefinite  if for 

any  real  (n  x  1) vector  x, 

x'Ax  =  0. 
We make  the stronger  statement  that  a real  symmetric  matrix  A is positive  definite 
if for  any  real  nonzero  (n 

1) vector  x, 

x'Ax  >  0; 
hence,  any positive definite  matrix  could  also  be said  to be positive  semidefinite. 

A.4.  Matrix  Algebra 

733 

Let  A  be  an  eigenvalue  of A  associated  with  the  eigenvector  x: 

Ax  =  Ax. 

Premultiplying  this  equation  by x’  results  in 

x‘Ax  =  Ax’x. 

Since  an  eigenvector  x  cannot  be  the  zero  vector,  x'x  >  0.  Thus,  for  a  positive 
semidefinite  guatrix  A,  any  eigenvalue  A  of  A  must  be  greater  than  or  equal  to 
zero.  For  A  positive  definite,  all  eigenvalues  are  strictly  greater  than  zero.  Since 
the determinant  of A is the product  of the eigenvalues,  the determinant  of a positive 
definite  matrix  A is strictly  positive. 

Let  A  be  a  positive  definite  (n  x  n) matrix  and  let  B denote  a  nonsingplar 
(n  xX  n) matrix.  Then  B’AB  is positive  definite.  To  see  this,  let  x be  any  nonzero 
vector.  Define 

x =  Bx. 

Then  x cannot  be  the  zero  vector,  for  if it were,  this  equation  would  state  that 
there  exists  a  nonzero  vector  x  such  that 

Bx  =  0-x, 
in which  case  zero  would  be an  eigenvalue  of B associated  with  the  eigenvector  x. 
But  since  B  is  nonsingular,  none  of  its  eigenvalues  can  be  zero.  Thus,  x =  Bx 
cannot  be  the  zero  vector,  and 

x’B'ABx  =  x’Ax  >  0, 

establishing  that  the  matrix  B’AB  is positive  definite. 

A special  case  of this  result  is obtained  by letting  A be  the  identity  matrix. 
Then  the  result  implies  that  any  matrix  that  can  be  written  as  B'B  for  some  non- 
singular matrix  B is positive  definite.  More  generally,  any matrix  that can  be written 
as  B’B  for  an  arbitrary  matrix  B must  be positive  semidefinite: 
x’B'Bx  =  x’x  =  2 +  43 +---  +  x220, 

[A.4.40] 

where  x =  Bx. 

The  converse  propositions  are  also  true:  if A is positive  semidefinite,  then 
there  exists  a  matrix  B such  that  A  =  B’B;  if A  is positive  definite,  then  there 
exists  a  nonsingular  matrix  B such  that  A  =  B’B.  A  proof  of this  claim  and  an 
algorithm  for calculating  B are  provided  in Section  4.4. 

Conjugate  Transposes 
Let  A denote  an (m  x  n) matrix  of (possibly)  complex  numbers: 

a4)  = 5  by,  Pa 

Qin  3  byl 
in  Qn, + Dyi  ++ *  Gay + dogl 

y  ami  +  bat  dN! 

ann  +  brant 

The conjugate transpose of A, denoted  A“,  is formed by transposing A and replacing 
each  element  with  its complex  conjugate: 

i? 
A#  = 

a,  —  bi  ++ 5 Gag ~ Opal 
Ay  — byi  --- 

Om  — Smal  | 

Gy, ~— Oyd  eee  @,,, - Of 

Thus, if A is real,  then  A”  and A'  would  denote  the same  matrix. 

734  Appendix  A  | Mathematical  Review 

Notice  that  if an  (n  x  1) complex  vector  is premultiplied  by its  conjugate 

transpose,  the  result is  a nonnegative  real  scalar: 

uMtz  =  [(a,  —  by)  (0, -  bai) 

--+ 

(a,  -  b,i))|%  * 

a,  +  by 

a,, + b,j 

=  > (a? +  b?) =0. 

For B a real  (m  X  n) matrix  and  x a complex  (n  x  1) vector, 

, 

(Bx)#  =  x4B’. 

More  generally,  if both  B and  x are  complex, 

Notice that if A is positive  semidefinite,  then 

(Bx)?  =  x4B*. 

: 

x4Ax = x"B'Bx = xi, 
with  * = Bx.  Thus, x“Ax  is a nonnegative real scalar for any x when A is positive 
semidefinite.  It is a positive real scales for A positive  definite. 

_ 

Continuity  of Functions  of tocots 
A function  of  more than  one  argument,  such as 

a eer  Seppe 

[a4 4.41] 
to  be  continu  rf at at  (Cn C2, 2  if Flex ey 54 6) is finite and or 

tay  yg  TE  ey) 

8 

s

a

i

d

“ths  Pow 

 
For  example,  suppose  f is a  linear  function: 

fi)  Xan 

0s 

Hq)  Sh  ee 

eee 

[A.4.44] 

Define  a  and x to  be  the  following  (n  x  1) vectors: 

a=  | 7 

|

[A.4.45] 

x=]?  [> 

[A.4.46] 

Then  [A.4.44]  can  be  written 

fay =a 
pe  goign  = f(-) wit respec  Soe ith argument i CT  af  se  de shay 
——- apiictogs sabres tea Svitindg 6 a tf SsitiSbeng 

a...  ts 

Seige 

,  32828 

eu: 

x; 

and  the  Fader is 

: 
Ming 
[Ls Ay 

that  the 
aoe 

aan Ys endatonws4  Yo “hinne > 

Le) 
wi  ls  oe  eines  tf  pia  tits en ft s70nt  to Roig  A 
ot 

Oe at  waa a 

be  the pc ene 

© 

q 

a 

ae  ae a ees eae 43 vs a, | 
a  B'S tor  sm wahibery 

. 

or 

Tee: | 

a 

7 

——  om 

a 

eas  gt  eee 

ase) 

 
Derivatives  of Vector-Valued  Functions 
Suppose  we  have a set  of  m  functions  f,(-),  fo(-),.  . . »  fa(-),  each  of which 
,  x,).  We  can  callect the  m  functions  into 

depends  on  the  n  variables  (x,, x,  ..  . 
a single  vector-valued  function: 

fix) 

ry  =  | 2 
~“ 
fin (*) 

(m1) 

We  sometimes  write 

f:  R” >  R™ 

to  indicate  that  the  function  takes  n  different  real  numbers  (summarized  by the 
vector  x,  an  element  of R”) and  calculates  m  different  new  numbers  (summarized 
by the  value  of f, an  element  of R”).  Suppose  that  each  of the  functions  f,(-), 
Sfx(-),  ---5  fm(-)  has  derivatives  with respect  to  each  of  the  arguments  x,,  x2, 
...,X,-  We can summarize  these  derivatives in  an ie x  n) matrix,  called  the 
Jacobian matrix of f and indicated  by df/dx’: 

oe 

. 

of, /ax, 

of, /Ax2 

cee  epee | 

peal b  ob BS ie 219F 

Tales * 

Here  df/dx'  denotes  the  (1  X  n) vector  that  is the  transpose  of the  gradient,  and 
the  remainder  R,(-)  satisfies 

R,(c,  x)  = 

l 

s y  oe 

(x,  —  c)(Qx;  —  ¢;) 

for  (i,  j) an  (n  xX.  1)  vector,  potentially  different  for  each  i and  j, with  each 
5(i,  /) between  c  and  x,  that  is,  5(i,  j)  =  A(i,  je  +  [1  —  Ai,  j))x  for  some 
A(i;  7) between  0 and  1.  Furthermore, 

lim  Birnie  Je) ae 
ele  —e)\(x-op? 

x} 

=  0 

An  implication  of [A.4.47]  is that  if we  wish  to approximate  the consequences 
for f of simultaneously  changing  x,  by A,, x,  by A,,...  ,  and  x,  by A,,  we  could 
use 

late  ti  eet  ete  An  Tt  Ln)  —  PV idigdes 

os  ee 

on 
Ox, 

+  ay  tere  +  Fn. 

a 

dx 

Ox, 

A.4.48] 

If  f(-) has  continuous  third  derivatives,  a second-order  Taylor  series  expan- 

sion  of f(x) around c is given  by 

if 
rs) 

be  ee  See 

[A.4.49] 

(x —  c)  +  Rc,  x), 

Re) = 7 > > yt 

with  8(7, 7, k) between  c  and  x  and 

ei  c;)(x;  =  C  (Xx  +  £9 

x=  8(ij,4) 

Multiple  Integrals 

The  notation 

: 
se 

R(c,  x) 
oy i  Oh 

bd 
[| fo. y) ay ax 

indicates  the  following  operation:  first  integrate 
[ Fe») a 
with  respect  to y, with  x held  fixed,  and  then  integrate  the  resulting  function  with 
respect  to x.  For  example, 

2 

|  | | xy dy dx  =  [ x*{(27/2)  —  (07/2)] dx  =  2[15/5  -  05/5)  = 2/5. 

| 

00 

738.  Appendix  A  | Mathematical  Review 

Provided  that f(x, y) is continuous,  the  order  of integration  can  be  reversed.  For 
example, 

[Ja dx dy  =  [ (19/5)y  dy  =  (1/5) - (27/2)  =  2/5. 

2 

00 

A.5.  Probability  and  Statistics 

Densities  and  Distributions 
A stochastic  or  random  variable  X is said  to  be  discrete-valued  if it can  take 
on  only  one  of  K particular  values;  call  these  x,,  x.,  ...,  Xx.  Its  probability 
distribution  is a set  of numbers  that  give  the  probability  of each outcome: 

P{X  =  x,} =  probability  that  X takes on  the valuex,, 

k=  1,...,K. 

Rs: probabilities S  sum to  unity: 

nglintsetxe.2i  a + 

Bae  ® airiniz  mds PIX = x4} 45 

1 

S&F  tent 

Assuming  that  the possible  outcomes  are eden P<  32  wig Xx, the 
probability  that  X takes  on  a value less than or equal to the value x, is given by 

tai coat sinctibh5. i 

integrals  all  exist.)  The  population  variance  is 

var(X)  =  |e —  w)?-fela)  de 

The  square  root  of the  variance  is called.the  population  standard  deviation. 

In  general,  the  rth  population  moment  is given  by 

= x"  f(x)  dx. 

The  population  mean  could  thus  be  described  as  the  first  population  moment. 

Expectation 

The  population  mean  vy is also  called  the  expectation  of X, denoted  E(X)  or 
sometimes  simply  EX.  In  general,  the  expectation  of a  function  g(X)  is given  by 

E(e(X))  =  | eG)-fee  dx, 

[A.5.3] 

where  f,(x)  is the  density  of X.  For  example,  the  rth  population  moment  of X is 
) 
the  expectation  of X’. 

| 

Consider  the  random  variable  a  +  bX for  constants  a  and  b.  Its  expectation 

is 

E(a  +  bX)  {6 [a +  bx]-fy(x)  dx 

feo) de  +b] x-fe(s) de 

af 
a  +  b+E(X). 

The  variance  of a  +  bX is 

Var(a  +  bX)  =  7 [(a  +  bx)  —  (a +  by)}*-fy(x)  dx 

=  62  [ (=  w)fy(x) ax 

=  b*+Var(X). 

[A5.4 

Another  useful  result  is 

E(X’)  = E[(X  —  pw  +  p)?] 

E((X  -  pw)? +  2u(X  -  w) +  wy 
E[(X  —  p)?]  +  2u-[E(X)  -  pw] +  pw? 
Var(X)  +  0  +  [E(X)f. 

To  simplify  the  appearance  of  expressions,  we  adopt  the  convention  that 
exponentiation  and  multiplication  are  carried  out  before  the  expectation  op- 
erator.  Thus,  we  will  use  E(X  —  pu  +  p)?  to  indicate  the  same  operation  as 
E[(X  —  »  +  pw)’.  The  square  of E(X  —  uw  +  y) is indicated  by using  additional 
parentheses,  as [E(X  —  uw  +  p))?. 

Sample  Moments 
A sample  moment  is a particular  estimate  of a population  moment  based  on 
an  observed  set  of data,  say,  {x,, x,  ...  ,  xr}. The  first  sample  moment  is the 
740  Appendix  A  | Mathematical  Review 

Sample  mean, 

which i is  a  natural  estimate  of the  population  mean  4.  The  sample variance, 

=  (1/T)- (x4,  +  x2  +  +++  +  x7), 

?  =  (1/T)- > (x, -  2, 

affords  an  estimate  of the  population  variance  o?.  More  generally,  the  rth  sample 
moment  is given  by 

where  x; denotes  x, raised  to  the  rth power. 

(1/7): (x5. +  45  +:  °° 

+ x7), 

Bias  and  Efficiency 

Let  @ be  a  sample  estimate  of  a  vector  of  population  parameters  0. For 
example,  6 could  be the sample mean xX and 6 the population  mean  pw. The estimate 
is said to  be unbiased if E(@) =  0. 

Suppose that 6 is  an  unbiased  estimate  of @. The  estimate  6 is said  to be 
ficient if it is the case that for any  other unbiased : estimate  6*, the following 

2 An a  EO 6): (@  - - oy) - -  E(@ - 8): @ -o1 

Notice  that  this  satisfies  the  requirement  of a  density  [A.5.1]: 

J fnuxtvla) dy = |  Peet) ay 

fy,  y) 

ig 

es 

=  | five, 9) ay 
sales 

fx) 

A further  obvious  implication  of the  definition  in [A.5.7]  is that a joint density 
can  be  written  as  the  product  of the  marginal  density  and  the  conditional  density: 

[A.5.8] 
The  conditional  expectation  of  Y given  that  the  random  variable  X takes  on 

fx.y(x,  y)  =  frix(y|x)  ‘fx (x). 

the  particular  value  x  is 

EWIX  =x)  =  | ¥fnxOle) dy. 

[A59) 

Law  of Iterated  Expectations 

Note  that  the conditional  expectation  is a function  of the  value  of the  random 
variable  X.  For  different  realizations  of X,  the  conditional  expectation  will  be  a 
different  number.  Suppose  we  view  E(Y|X)  as  a  random  variable  and  take  its 
expectation  with  respect  to  the  distribution  of X: 

Ex[Ey\x(¥|X)]  =  te i= Yfrix(y|x)  ‘| fix(x)  dx. 

Results  [A.5.8]  and  [A.5.6]  can  be  used  to  express  this  expectation  as_ 

x 

x 

| | Yfy.x(y,  x) dy dx  =  [»-fro dy. 

_-_x 

—-=x 

Thus, 

Ex{Ey\x(¥|X)]  =  Ey(Y). 
[A.5.10] 
In  words,  the  random  variable  E(Y|X)  has  the  same  expectation  as  the  random 
variable  Y.  This  is known  as  the  law  of iterated  expectations. 

Independence 

The  variables  Y and  X are  said  to  be  independent  if 

fv, ¥) =  fe(®) fr). 
Comparing  [A.5.11]  with  [A.5S.8],  if Y and  X are  independent,  then 
fyix(vlx)  =  fr). 

[A.5.11] 

[A.5.12] 

Covariance 

Let  y denote  E(X’) and py denote  E(Y). The population covariance  between 

X and Y is given  by 

cox.  y=  | f ( -  mely =  my) ferles») dy de. 

[AS.13) 

742  Appendix  A  | Mathematical  Review 

Correlation 
The population  correlation  between  X and Y is given  by 

] 

Word,  ¥) 

” 

Cov(X,  Y) 
VVar(X)- VVar(Y) 

If the  covariance  (or correlation)  between  X and Y is  zero,  then  X  and Y are  said 
to be uncorrelated. 

Relation  Between  Correlation  and  Independence 
Note  that  if X and  Y are  independent,  then  they are  uncorrelated: 

Cov(X,  Y) 

[ [= ud  = rd -fel fy) dy ax 

h. 

’ 

«  -klww 

J -  me) I. (y — Hy) “fr(y)  ‘| ‘fil  dx. 

emhUh 
SS ie lc ere 

Pupinetmore. 

7 - BK or4] - ror ee Gee f(y) dy 

Wane)  =  mt Canty Ore 

. 

Population  Moments  of Sums 
Consider  the  random  variable  aX  +  bY. Its  mean  is given  by 
rex  +ory=  | [iee+orhie nad 

=  a f x‘  fx y(x,  y) dy dx + oh f yf,  y) dy dx 

=  a] x-fu(x) ax + of yf) dy, 

and  so 

e 

+ b-E(Y). 
E(aX  +  bY)  =  a-E(X) 
) 

The  variance  of (ax +  bY) is 

[A.5.14] 

Varo + BY) - Paha by) - ome ' on fav 9) dy dx 

“ 

aoe  ee pk  pri} 

spomasdre7 

: 

Y  | es  " ( p  eet 7) Pt 

x (by ~ bay) ; 

co  om 4 

OM 

<> 

£2. 

ee)  Soe 

J 0 Oak: a mer Cs 
+ (by ~  bay) Sut y) dy dx 

: 

q 

>  4 boned  tag al t  aise  ten  a ASE wrod  ,  TAO  IG oe1S¥a09  sf 

bspnis ae ¢ ud f 5  (ae wa laxtgundsoude s%  % bas K i evdT = 

of 

If the  X’s  are  uncorrelated,  then  [A.5 .17] simplifies  to 

"Warla,X,'+  2X, 4° 

Pa i 
=  aj-Var(X,)  +  a3 es +  +++  +  a2-Var(X,). 

[A.5.18] 

Cauchy-Schwarz  Inequality 
The Cauchy-Schwarz  inequality  states  that  for  any  random  variables  X and 
Y whose  variances  and  covariance  exist,  the  correlation is  no  greater  than  unity  in 
absolute  value: 

—1  <  Corr(X,  Y) <1. 

[A.5.19] 

To establish  the  far  right  inequality  in  [A.5.19],  consider  the  random  variable 

a  a 

Goll 
VVar(X) 

3  Y  —  by 

VVar(Y) 

The  square  of this  variable  cannot  take on negative  values,  so 

sn  ie  | 

izing 

that Var(X)  and  Var(Y)  sain ae moments fo cana to 

pare  —- equation 

eal 

ho 

_ 

meee 

— 

M3 

oA teas Brae deduce  —  Teo 
3) 
E.. 

Ky,  |  EO  = we og 

Rows 

oe  ee 

aS 

- 

- 

Bowes 

TO 

hoes  iB Mz  ar(Y) slosisy  {i  Ay Ai. 

ze  ae 

ng  dA  ah /  fh  = 

ot eee orbise  2 Y nedT 

aie  J 

Centered  odd-ordered  population  moments  for  a  Gaussian  variable  are  zero: 

E(Y,  — 

»)’  =90 

for  7  ei,  3z 5. 

The  centered  fourth  moment  is 

E(Y,  —  p»)*  =  3e%. 

Skew  and  Kurtosis 

The  skewness  of  a  variable  Y, with  mean yp is represented  by 

E(Y, — 4) 
[Var(Y,)]}*? 

A  variable  with  a  negative  skew  is more  likely  to  be  far  below  the  mean  than  it is 
to  be  far  above  the  mean.  The  kurtosis  is 

E(Y,  es yu)" 
[Var(Y,)]?  © 

A distribution  whose  kurtosis  exceeds  3 has  more  mass  in the  tails  than  a Gaussian 
distribution  with  the  same  variance. 

Other  Useful  Univariate  Distributions 

Let  (X;,  X2,  ...,  X,,)  be  independent  and  identically  distributed  (i.i.d.) 

N(O,  1) variables,  and  consider  the  sum  of their  squares: 

Yo =  X2  +: X43 +.2°+4 XZ. 

Then Y is said  to have  a chi-square  distribution  with  n degrees  of freedom,  denoted 

Let  X ~  N(0,  1) and  Y ~  y?(n)  with  X and  Y independent.  Then 

Y ~  x(n). 

X 

2 =  Tih 

is said  to  have  a ¢ distribution  with  n  degrees  of freedom,  denoted 

Z ~  t(n). 

Let  Y, ~  y?(n,)  and  Y, ~.  y(n.)  with  Y, and  Y, independent.  Then 
_  Yuin 

Y,/n, 

is said  to  have  an  F distribution  with  n,  numerator  degrees  of  freedom  and  n, 
denominator  degrees  of freedom,  denoted 

. 

Note  that  if Z ~  t(n),  then  Z*  ~  F(1,  n). 

“A ext  F(n,,  n>). 

Likelihood  Function 

Suppose  we  have  observed  a sample  of size  T on some  random  variable  Y,.. 
»  Yrs  @) denote  the  joint  density  of Y,, Y>,...,  Yr. 

Let fy, ¥.....¥r(Yi»  Yar  « 

+ 

» 

746  Appendix  A  | Mathematical  Review 

The notation emphasizes  that  this joint density  is presumed  to depend  on  a vector 
of population  parameters  @.  If we  view  this joint  density  as  a function  of 6 (given 
the  data  on  Y), the  result  is called  the sample  likelihood  function. 
___For example,  consider  a sample  of T'i.i.d.  variables  drawn  from  a N(,  0) 
distribution.  For this distribution,  @ =  (u, o7)', and from  [A.5.11]  the joint density 
is  the  product  of individual  terms  such  as [A.5.20]: 

fyy, ae  yr (V1 Y22+++>  Yr,  B,  a’) nad I SAG  TE bM, a’). 

T 

The  log of the  joint density  is the  sum  of the  logs of these  terms: 

log Fviys.....¥7Q>  yY2>--+-s,Yrs  Kb,  a”) 

Tr 

=  2 log fr.(6s  #,  0) 

: 

[A.5.21] 

=  (~T/2)  log(2m)  —  (T/2) log(o?)  —  > ae 

Thus,  for  a sample  of T Gaussian  random variables with mean  m and  variance  0”, 
pris mR osctpack  ote  tetas  it 

PS eee  LR  SEEN?  OC  2  Vig  sya,  5 

9\  937  mak Sia  By? 

' 

' 

n 

a Lp, 075 Ya» V2 Sétitieg  Yr) =  5 ; ! (7/2) log(e ie roi 20 dois  ‘at Ricctsiaas 
calculating the sample log likelihood function, any constant term that does not 
yolve  the parameter wu or  om be ignored for most purposes es. In [A.5.22], this 

int  termis 

- 

. 

| 

Sah  BS  RE  Pty 

Be 

SeeUence  1.4,, 

As. 

RE" qh Bais a aise  (8  <  \) sitesiooianen  yas  16}  ned?  (M2  Dui~ 

Rates 

Gi 

(2)  logtem). 

mt,  ke 

4 

¥en 

#@&  totssv 

» 

Saath  Fare lag  og 

AME  VAGIIIL  5 7 yp  Vasyw 
: 

SSsRS  SESE  A oan 
4 

5 

a 

: 

F 

* 
4 

Tr 
Se  WEES 
a  Sf  eae 

‘ 
. 
——  se 

ow  ws 

; 

4 

Thus,  the  sample  mean  is  the  MLE  of  the  population  mean  and  the  sample 
variance  is  the  MLE  of  the  population  variance  for  an  i.1.d.  sample  of  Gaussian 
variables. 

Multivariate  Gaussian  Distribution 

Let 

Y=  (¥1,  Ya--  +5  Vn)’ 

be  a  collection  of n  random  variables.  The  vector  Y has  a  multivariate  Normal,  or 
multivariate  Gaussian,  distribution  if its  density  takes  the  form 

fly)  =  (22)~"7|Q|~'?  exp[(-1/2)(y  —  p)’'A-“y  —  p)j. 

 [A.5.26] 

The  mean  of Y is given  by the  vector  p: 

and  its variance-covariance  matrix  is Q: 

E(Y)  =  p; 

E(Y  —  p)(Y  —  p)'  =  Q. 

Note  that  (Y  —  mw)(Y  —  pm)’  is symmetric  and  positive  semidefinite  for  any 
Y,  meaning  that  any  variance-covarrance  matrix  must  be  symmetric  and  positive 
semidefinite;  the  form  of  the  likelihood  in  [A.5.26]  assumes  that  2  is  positive 
definite. 

Result  [A.4.15]  is sometimes  used  to  write  the  multivariate  Gaussian  density 

in an  equivalent  form: 

fly)  =  (20)~"7| 

27> "|"? exp[(—1/2)(y  —  w)'Q-"(y  —  p)]. 

If  Y ~  N(p,  Q),  then  for  any  nonstochastic  (r x  n) matrix  H’  and  (r x  1) 

vector  b, 

H'Y  +  b ~  N((H’p  +  b), H’OH). 

Correlation  and  Independence  for Multivariate 
Gaussian  Variates 

If Y  has  a  multivariate  Gaussian  distribution,  then  absence  of  correlation 
implies  independence.  To see  this,  note  that if the  elements  of Y are  uncorrelated, 
then  E[(Y;  —  «)(Y;  —  )]  =  0 fori  + j and  the  off-diagonal  elements  of 
are 
zero: 

ot  0 
a=|9  % 
=  0 

0 
2 
o2 

For  such  a diagonal  matrix  2, 

|Q| =  0703  ---  0? 
lio? 
O 
Q-1  nw  | OMe; 
Qe 
UEP 

ce 

+--+ 
er’  ee 

OO  si 

Q 
. 
OG 
tie? 

[A.5.27] 

[A.5.28] 

748  Appendix  A  | Mathematical  Review 

Substituting  [A.5.27]  and  [A.5.28]  into  {A.5.26]  produces 

fly)  =  (22)-"?[o703  + 

+ 

-  2) > 

x  exp[(—1/2){(¥.  —  w,)*/07  +  (y2  —  Mp)?*/03  + 
+  (Yn  —  Mn)?/o7}] 

a  tI (22)  (a?]~ '?  exp[(-1/2){(y,  —  u;)?/0?}], 

which is  the  product  of n  univariate  Gaussian  densities.  Since  the  joint  tee is 
the  product  of the  individual  densities,  the  random  variables  (Y,,  Y>,...,  Y,) 
are  independent. 

-  Probability  Limit 

Let {X,, X2, . 

X7} denote  a sequence  of random  variables.  Often  we  are 
interested in  what  happens to this  sequence  as  T becomes  large. For  example,  X; 
might  denote  the  sample  mean  of T observations: 

.  . , 

My =  GTA,  +  Vg +  >  +  Vz), 

[A.5.29] 

in which  case  we  might  want  to  know  the  properties  of the  sample  mean  as  ee 
size of the sample  T grows  large. 
The sequence  {X,, X2,  . 

: 
every  €  >  0 and  6 > 0 there  exists a value  N  such  that,  for all  T=  N, 

.  . , Xz} is said  to converge  in probability  to c if for 

| 

P{|X;  -—  c| >  5} <e. 

| 

| 

[A.5:30) 

When  [A.5.30]  is ee: the  number  c 

is called the probability  limit,  or 

plim, of the sequence {X,, X2,... 

pa 

—. 

,  X7}.  This is  sometimes indicated as 

P 

Xo. 

any raniedpage  ee  ee 

uae 

-~ 

* 

« 

i 

Z a #761  “Ps  ai rr , 

ett 

er ts 

MSG 

é 

ih . ove  mn 

eer  er ea SEs eels a Let  a Ml 

i  an  a n  ea =" 

: 

Appendix  A  References 

Chiang,  Alpha  C.  1974.  Fundamental  Methods  of Mathematical  Economics,  2d  ed.  New 
York:  McGraw-Hill. 
Hoel,  Paul  G.,  Sidney  C.  Port,  and  Charles  J.  Stone.  1971.  Introduction  to  Probability 
Theory.  Boston:  Houghton  Mifflin. 
Johnston,  J.  1984.  Econometric  Methods,  3d  ed.  New  York:  McGraw-Hill. 
Lindgren,  Bernard  W.  1976.  Statistical  Theory,  3d ed.  New  York:  Macmillan. 
Magnus, Jan R., and  Heinz  Neudecker.  1988.  Matrix  Differential  Calculus  with Applications 
in Statistics  and ‘Econometrics.  New  York:  Wiley. 
Marsden,  Jerrold  E.  1974.  Elementary  Classical  Analysis.  San  Francisco:  Freeman. 
O’Nan,  Michael.  1976.  Linear  Algebra,  2d ed.  New  York:  Harcourt  Brace  Jovanovich. 
Strang,  Gilbert.  1976.  Linear  Algebra  and Its Applications.  New  York:  Academic  Press. 
Theil,  Henri.  1971.  Principles  of Econometrics.  New  York:  Wiley. 
Thomas, George B., Jr.  1972.  Calculus and Analytic Geometry,  alternate ed. Reading, Mass. ; 
iesetinnabdliae Publishing  SRP RI  RSG 

peat 

Bee 

| 

: 

2 

7OF 

S.C 

x 

SES  2581 

33 

; 

f 

ry 

Al 
Seer  pape  AE to =sithSqing  ai  Watt Bs  HOw  tagic FR  ses  ad  ae 

At  Ee  BIE  7  CVRy  sy 
34} 

eS  hy 4laines 

BP  bn 

.  ao 

é 

_ 

2 

¢ 

se 

. 

r 

penne  erer  DEE 

73 

SQmse  st!  sleasb  tisirn 

ey 
SO 

“A 

NEE 

SPS 

eee 

ee 
SRS SERGI  TOR  aE  SSUES 
ne  oe 
5  “$355 

er 

. 
te a3  7 

FGM 

ees 

Xt 

2  ae  aig  “  Shaage  oft  1G  Suge 
cay: Pisr  ot oF 
5  Stel 
1502  Vs 
ia 

a Tk Sy : 
EY  &  ates  saedts 0  <  Scbre  6  < 

Sse  SHE 

fadtoos  ont 

a  SCH * 

353>  ‘A  “>  els 

— 

5  a 

ovr 

wees 

oi 

Re 

i 

42 

“ 

: 

1 

7 
; 

: 

EES. 

3 

Sy 
PFo 

ee 
 F5E 

: 

20  nb  Ghe eens  st 

J 

68 

ne 
Sc 
Oe  3 WE  i  Sli:  st 

Sh 

ne 
Mi 

;  ee A449 CRESS  ne 

. 
Lortertre  z ‘  j AGt.2 > 

f 

28 a ~Stit rior ie ‘ge  anit  tH 

¥  ¥ |  Pian  a 

:  if |  ae  pare 4 

Fy 
my  = 

ae 
: 

es  ty  Pee 

ae 

hots  ee  nolllies 

—- 

re 
a 

et  he  rete, 9 i 

1} 

SO4lr  5 

, 

are  ars &.  a 

; 

a 

eee  p 

as See. ae + a Nts: D oa 0). 

ene 

iw 

¥ 
ae 

° 

7 

; 

ee 

; 

7 

: 

| 

=  = 

ee 

B 

Statistical  Tables 

a 
<p)  leups  ti Oasis  miesry, 

ON  AS  ee  i  Te i 
sold 
Seareaits 4  Cusine  a 

% = soley  we / 

a?  SVS  4ST  Se 

TABLE  B.1 

(continued) 

Second  decimal  place  of 2, 

+2. 

5@p 

03 

.04 

OS 

.06 

.07 

1.5 
1.6 
1.7 
1.8 
1.9 

2.0 
ode 
2 
2.35. 
2.4 

.0668 
.0548 
.0446 
20359 
.0287 

.0228 
0179 
Birsetl3d 
20107 
.0082 

25-2 
.0047 
2.6 
.0035 
2.7 
.0026 
2.8 
_.0019 
2.9. 

.0655 
.0537 
.0436 
.0352 
.0281 

.0222 
.0174 
.0136 
.0104 
.0080 

.0060 
.0045 
.0034 
0025 
.0018 

0643 
.0526 
0427 
.0344 
.0274 

.0217 
.0170 
0132 
.0102 
.0078 
0059 
0044 
0033 
0024 
0017 

.0630 
.0516 
.0418 
.0336 
.0268 

.0212 
.0166 
.0129 
.0099 
.0075 

.0057 
0043 
.0032 
0023 
.0017 

.0618 
.0S05 
.0409 
0329 
.0262 

.0207 
.0162 
0125 
.0096 
.0073 

0055 
0041 
.0031 
.0023 
0016 

0606 
.0495 
0401 
.0322 
0256 

.0202 
0158 
0122 
.0094 
.0071 

.0054 
.0040 
.0030 
0022 
0016 

0594 
0485 
.0392 
.0314 
0250 

.0197 
0154 
.0119 
.0091 
0069 

0052 
.0039 
.0029 
.0021 
0015 

0582 
0475 
.0384 
.0307 
0244 

0192 
0150 
.0116 
0089 
0068 

0051 
.0038 
.0028 
0021 
0015 

3.0 
3:5 
4.0 

.00135 
 _.000:233 
.000  031 
~  4.5  .000  003 

is 
40 
.000  000 287 

5:0 

eee 

Table  entries  give the  probability  that  a N(0,  1) variable  takes  on  a  value  greater  than  or  equal  to  Zo. 
For  example,  if  Z ~  N(0,  1), the  probability  that  Z >  1.96  =  0.0250.  By symmetry,  the  table  entries 
could  also  be  interpreted  as  the  probability  that  a N(0,  1) variable  takes  a value  less  than  or  equal  to 

=  Zo- 

Source:  Thomas  H.  Wonnacott  and  Ronald  J.  Wonnacott,  Introductory  Statistics,  2d  ed.,  p.  480. 
Copyright  © 1972  by John  Wiley  & Sons,  Inc.,  New  York.  Reprinted  by permission  of John  Wiley  & 
Sons,  Inc. 

752  Appendix  B  | Statistical  Tables 

The x? Distribution 

that x*(m) is greater than entry 
freedom  ____Probabiliy 
eee 0a  og  oe  oe 
0.455 
0.004 
0.001 
1.39 
0.103 
0.051 
2.37 
0.352 
0.216 
3.36 
0.711 
0.484 

2x  10-* 
0.020 
0.115 
0.297 

4x10-> 
0.010 
0.072 
0.207 

0.016 
0.211 
0.584 
1.06 

0.102 
0.575 
1.21 
1.92 

1 
2 
3 
4 

0.412 
5 
0.676 
6 
0.989 
7 
°% 
g; 
c&73 
% 
2.16 
10 
112,60 
Shop. 
ag! 
a  ° Ss 
86407 
14 
ei  MN 
*'5-14 
Bigs 
: 
olan  *-570 
he  9-626 
19  (6.84 
743 

) 

0.554 
0.872 
$260 
1.65 
2.09 
2.56 
3.05 
\S-a7 
an  fc: 
4.66 
5.23 
5.81 
6.41 
7.01 
7.6: 
: 

090 

4.35 
2.67 
1.19):  1.61 
0.899%). 
5.35 
3.45 
2.20 
164 
124 
1114.25  © 6.35 
2.17  61 2:83 
1.608 
7.34 
>1i507 
3.49 
2.75.2. 
2.98 
8.34 
5.90 
417 
333 
2.70 
19.34 
4.87  \ 616,74 
3.94.51 
3.5501 
10.3 
2.17.58 
5.58 
4.57.21 
3.9215 
11.3 
844 
630 
5.23 
4.40 
12.3 
Sago  704  1 019.30 
“soe 
565  6.5%  9  02-133 
14.3 
7.26 
6.26 
9373: 
7.96  — 
6.91 
16.3 
7.56  8.67. 
17.3 
19.39.< 
8.230%: 
: 
8.91  10.1 
9.506  “10.9 3 

TABLE  B.2 

(continued) 

reeaom 

lire “2 
(m) 
1 
2 
3 
4 

5 
6 
7 
8 
9 

10 
11 
12 
13 
14 

15 
16 
17 
18 
19 

20 
21 
22 
23 
24 

25 
26 
27 
28 
29 

30 
40 
50 
60 

70 
80 
90 
100 

0250. 
1 8. 
2.77 
4.11 
5.39 
6.63 
7.84 
9.04 
10.2 
11.4 

12.5 
7 
14.8 
16.0 
7.1 

18.2 
19.4 
20.5 
21.6 
> IE  | 

23.8 
24.9 
26.0 
at. 
28.2 

29.3 
30.4 
= 
32.6 
o.7 

34.8 
45.6 
56.3 
67.0 

77.6 
88.1 
98.6 
109 

Probability  that  x?(m)  is greater  than  entry 
0.025 
5.02 
7.38 
9.35 
le 
12.8 
14.4 
16.0 
17.5 
19.0 

0.010 
6.63 
9.21 
11.3 
13.3 
15.1 
16.8 
18.5 
20.1 
Zl 

0.050 
3.84 
5.99 
7.81 
9.49 
17 
12.6 
14.1 
153 
16.9 

0100 
va 
4.61 
6.25 
7.78 
9.24 
10.6 
12.0 
13.4 
14.7 

0.005  _— 0.001 
10.8 
7.88" 
13.8 
10.6 
16.3 
12.8 
18.5 
14.9 
20.5 
16.7 
22.5 
18.5 
24.3 
20.3 
26.1 
22.0 
27.9 
23.6 

16.0 
17.3 
18.5 
-  19.8 
21:1 

22.3 
23.3 
24.8 
26.0 
21.2 

284°". 
29.6 
30.8 
32.0 
33:2 

34.4 
35.6 
36.7 
37.9 
| 

40.3 
51.8 
63.2 
- 74.4 

85.5 
96.6 
108 
118 

= 

18.3 
19.7 
21.0 
22.4 
23.7 

25.0 
26.3 
27.6 
28.9 
30.1 

SL4 
3207 
33.9 
35:2 
36.4 

37.7 
38.9 
40.1 
41.3 
42.6 

43.8 
55.8 
67.5 
79.1 

20.5 
21.9 
Za 
24.7 
26.1 

27.5 
28.8 
30.2 
31.3 
329 

34.2 
35.2 
36.8 
38.1 
39.4 

40.6 
41.9 
43.2 
44.5 
45.7 

47.0 
39.3 
71.4 
83.3 

23.2 
24.7 
26.2 
27.7 
29.1 

30.6 
32.0 
33.4 
34.8 
36.2 

37.6 
38.9 
40.3 
41.6 
43.0 

44.3 
45.6 
47.0 
48.3 
49.6 

50.9 
763.7 
76.2 
88.4 

25.2 
26.8 
28.3 
29.8 
31.3 

32.8 
34.3 
35.7 
a2 
38.6 

40.0 
41.4 
42.8 
44.2 
45.6 

46.9 
48.3 
49.6 
51.0 
aes 

53.7 
66.8 
79.5 
92.0 

90.5 
102 
113 
124 

95.0 
107 
118 
130 

100 
112 
124 
136 

104 
116 
128 
140 

29.6 
31.3 
32.9 
34.5 
36.1 

o/.7 
39.3 
40.8 
42.3 
43.8 

45.3 
46.8 
48.3 
49.7 
51.2 

52.6 
54.1 
DD.9 
56.9 
58.3 

59.7 
73.4 
86.7 
99.6 

112 
125 
137 
149 

The  probability  shown  at  the  head  of the  column  is the  area  in the  right-hand  tail.  For example,  there 
is  a 10%  probability  that  a y? variable  with  2 degrees  of freedom  would  be greater  than  4.61. 

Source:  Adapted  from  Henri  Theil,  Principles  of Econometrics,  pp.  718-19.  Copyright  © 1971  by John 
Wiley  & Sons,  Inc.,  New  York.  Also  Thomas  H.  Wonnacott  and  Ronald  J. Wonnacott,  Introductory 
Statistics,  2d  ed.,  p.  482.  Copyright  ©  1972  by John  Wiley  & Sons,  Inc.,  New  York.  Reprinted  by 
permission  of John  Wiley  & Sons,  Inc. 

754  Appendix  B  | Statistical  Tables 

TABLE  B.3 
The  ¢ Distribution NS ee 
Degrees  of 
freedom 
(m) 

Probability  that  t(m)  is greater  than  entry 

0.25 

0.10 

0.05 

0.025 

0.010 

0.005 

0.001 

l 
2 
3 
4 

5 
6 
7 
8 
9 

10 
11 
12 
13 
14 

15 
16 
17 
18 
19 

20 
pa 
22s 
23 
24 

25 
26 
27 
28 
29 

1000 
816 
-765 
.741 

.727 
718 
711 
.706 
.703 

.700 
.697 
.695 
.694 
.692 

.691 
.690 
.689 
.688 
.688 

.687 
.686 
.686 
.685 
.685 

.684 
.684 
.684 
.683 
.683 

3.078 
1.886 
1.638 
1.533 

6.314 
2.920 
2.353 
2.132 

=1.476  ~—- 2.015 
1.440 
1.943 
1.415 
1.895 
1.397 
1.860 
1.383 
1.833 

1.372 
1.363 
1.356 
1.350 
1.345 

1.341 
¥.35/ 
1.333 
1.330 
1.328 

1.325 
. 225 
1.321 
1.319 
1.318 

1.316 
Kb 
1.314 
1.313 
1.311 

1.812 
1.796 
1.782 
1.771 
1.761 

1.753 
1.746 
1.740 
1.734 
1.729 

1.725 
Litze 
1.717 
1.714 
1.711 

1.708 
1.706 
1.703 
1.701 
1.699 

12.706 
4.303 
3.182 
2.776 

2.571 
2.447 
2.365 
2.306 
2.262 

2.228 
2.201 
2.179 
2.160 
2.145 

2.131 
2.120 
2.110 
2.101 
2.093 

2.086 
2.080 
2.074 
2.069 
©  2.064 

2.060 
2.056 
2.052 
2.048 
2.045 

31.821 
6.965 
4.541 
3.747 

3.365 
3.143 
2.998 
2.896 
2.821 

2.764 
2.718 
2.681 
2.650 
2.624 

2.602 
2.583 
2.567 
2352 
2.539 

2.528 
2.518 
2.508 
2.500 
2.492 

2.485 
2.479 
2.473 
2.467 
2.462 

63.657 
9.925 
5.841 
4.604 

318.31 
22.326 
10.213 
7.173 

4.032 
3.707 
3.499 
3.355 
3.250 

3.169 
3.106 
3.055 
3.012 
2.977 

2.947 
2.921 
2.898 
2.878 
2.861 

2.845 
2.831 
2.819 
2.807 
2.797 

2.787 
2.779 
2471 
2.763 
2.756 

5.893 
5.208 
4.785 
4.501 
4.297 

4.144 
4.025 
3.930 
3.852 
3.787 

3.733 
3.686 
3.646 
3.610 
3.579 

3-552 
3.527 
3.505 
3.485 
3.467 

3.450 
3.435 
3.421 
3.408 
3.396 

Nee ne 

30 
40 
60 
120 
oo 

.683 
.681 
.679 
.677 
.674 

1.697 
1.684 
1.671 
1.658 
1.645 

1.310 
1.303 
1.296 
1.289 
1.282 

2.042 
2.021 
2.000 
1.980 
1.960 

3.385 
3.307 
3.232 
3.160 
3.090 
The  probability  shown  at the head  of the column  is the  area  in the right-hand  tail.  For example,  there 
is  a 10%  probability  that  a  ¢ variable  with  20  degrees  of freedom  would  be  greater  than  1.325.  By 
symmetry,  there  is also  a 10%  probability  that  a  ¢ variable  with  20 degrees  of freedom  would  be  less 
than  —  1.325. 
Source:  Thomas  H.  Wonnacott  and  Ronald  J.  Wonnacott,  Introductory  Statistics,  2d  ed.,  p. 481. 
Copyright  © 1972  by John  Wiley  & Sons,  Inc.,  New  York.  Reprinted  by permission  of John  Wiley & 
Sons,  Inc. 

Va  SY 
2.423 
2.390 
2.358 
2.326 

2.750 
2.704 
2.660 
2.617 
2.576 

Appendix  B | Statistical  Tables 

755 

TABLE  B.4 
The  F  Distribution 

Denominator 
degrees  of 
freedom 
(m2) 
i 

2 

3 

4 

5 

6 

7 

8 

9 

10 

11 

12 

13 

i4 

15 

16 

17 

18 

19 

_ 

Numerator  degrees  of freedom  (m,) 

| 
161 
4052 

2 
200 
4999 

18.51 
98.49 

19.00 
99.00 

10.13--"9:55 
34.12 
30.82 

7.71 
21.20 

6.94 
18.00 

3 
216 
5403 

19.16 
99.17 

9.28 
29.46 

6.59 
16.69 

4 
225 
5625 

19.25 
99.25 

--9:12 
28.71 

639 
15.98 

ki 
230 
5764 

19.30 
99.30 

9.09-~ 
28.24 

626 
15.52 

6 
234 
5859 

7 
237 
5928 

19.33 
99.33 

19.36 
99.34 

8.940.888 
27.67 

27.91 

6.16 
15.21 

6.09 
14.98 

8 
«2239 
5981 

19.37 
99.36 

864. 
27.49 

6.04 
14.80 

7) 
10 
«0241  +~« 242 
6056 
6022 

19.38 
99.38 

881 
27.34 

6.00 
14.66 

19.39 
99.40 

828 
27.23 

5.96 
14.54 

.541 
12.06 

3519 
11.39 

305.495 
10.97 
10.67 

498,..48 
10.45 
10.27 

478. 
10.15 

476 
10.05 

6.61. 
16.26 

5.99 
13.74 

5.79 
13.27 

5.14 
10.92 

3-39:74.14 
12.25 
9.55 

4.76 
9.78 

4.38> 
8.45 

5.32 
4.46 
4.07 
11.26 
7.59 
8.65 
5.12¢6426  — 3.86.5 
10.56 
8.02 
6.99 

4.53 
9.15 

412 
7.85 

3.84 
7.01 
363 
6.42 

4.39 
8.75 

4.28 
8.47 

4.21 
8.26 

397  , 3.87,..3.799 
7.46 
7.19 
7.00 

369 
6.63 

358 
6.37 

3.50 
6.19 

3884  3377:3.) 
6.06 
5.80 
5.62 

322° 
5,39 

3:18 
5.21 

.-309..281 
5.07 
4.88 

300 
4.82 

232 
4.65 

2.92~°2:04. 
4.62 
4.44 

2.85 
4.46 

2.77 
4.28 

2.79 
4.32 

2.74 
4.20 

2.70 
4.14 

2.66 
4.03 

3.33°* 
5.64 

320 
5.32 

3.11 
5.06 

SO2Y 
486 

2.96 
469 

2.90 
4.56 

2.85 
4.44 

281° 
4.34 

4.96... 
10.04 

3:48 
5.99 

$40:° 
7.56 

3.71 
655 
4.842°3.98  © 3.59: 
3:36 
9.65 
7.20 
6.22 
5.67 
953.88  ~ 3:49...  3.26 
6.93 
5.95 
5.41 
340  ~ 341 
6.70 
5.74 

518 
5.20 

4:73 
933 

4:67 
9.07 

4.60 
8.86 

3.74 
651 

3.34 
5.56 

3.11 
5.03 

3.29 
5.42 

3.24 
5.29 

320 
5.18 

3.06 
4.89 

3.01 
4.77 

2.9 
4.67 

4.54 
8.68 

3.68 
6.36 

4.49 
8.53 

3.63 
6.23 
4.45..3.39 
8.40 
6.11 

441° 
8.28 
4.38 
8.18 

3.55 
6.01 
352 
5.93 

316° 
5.09 
3.13 
5.01 
SS  sssesesinseessssnsiesssetsocceeser 

2.93 
4.58 
290 
4.50 

2.77 
4.25 
2774 
4.17 

255°. 
3.79 

259 
25 
3.68 
3.59 
251) 
29%6  - 2am 
3.71 
3.60 
3.51 
2:48 
243 
2.38 
3.63  3.82 
3.43 

270° 
4.10 

92 
3.93 
2.660°948 
4.01 
3.85 
2.63 
2.55 
3.94 
3.77 

415 
8.10 

3.73. 
6.84 

3.44 
6.03 

323, 
5.47 

3257°- 
5.06 

235. 
4.74 

“285 
4.50 

297°. 
4.30 

2.70 
4.14 

2.64 
4:00 

2.59 
3.89 

4.10 
7.98 

3.68 
6.71 

3.39 
5.91 

4.06 
7.87 

3.63 
6.62 

3.34 
5.82 

348  - 333 
5.35 
5.26 

302 
4.95 

250 
4.63 

Zap 
4.39 

272. 
4.19 

2.65 
4.03 

2.59 
3.89 

2.54 
3.78 

257 
4.85 

225 
4.54 

275 
4.30 

258 
4.10 

2.60 
3.94 

2.55 
3.80 

2.49 
3.69 

—— 

(continued  on  page  758) 

756  Appendix  B  | Statistical  Tables 

2.34 
3.42 
2.27 
3.26 

2.32 
3.37 
2.24 
3.21 

2.18 
3.07 
2.19 
2.96 
2.08 
2.86 
2.04 

2.21 
3.12 
Z.16 
3.01 
2.11 
2.92 
2.07 
aS  2.7 
2.02 
2.76 

2.00 
2.70 

2.21 
3.16 
2.13 

2.07 
2.87 
2.01 
2.75 
1.96 
2.65 
1.92 
2.57 
1.88 
2.49 

Appendix B |  Statistical  Tables 

757 

TABLE  B.4 

(continued) 

Denominator 
degrees  of 
freedom 

(m2) 

20 

21 

27 

42 

55 

7.27 
4.06 
7.24 
4.05 
7.21 
4.04 
7.19 

4.03 
7.17 
4.02 
7.12 

Numerator  degrees  of freedom  (m,) 

4 

2.87 
4.43 
2.84 
4.37 
2.82 
4.31 
2.80 
4.26 

2.78 
4.22 

2.76 
4.18 
2.74 
4.14 

3 

3.10 
4.94 
3.07 
4.87 

3.05 
4.82 
3.03 
4.76 

3.01 
4.72 

2.99 
4.68 
2.98 
4.64 
2.96 

4.60 

2.95 
4.57 
‘293 
4.54 

2.92 
4.51 
2.90 
4.46 
2.88 
4.42 
2.86 
4.38 
2.85 

4.34 ; 

2.84 
4.31 
2.83 
4.29 
2.82 

4.26 

2.81 
4.24 
2.80 

4.22 

2.79 
4.20 
2.78 
4.16 

5 

6 

3 

8 

2.71 
4.10 
2.68 
4.04 
2.66 
3.99 
2.64 
3.94 
2.62 
3.90 

2.60 
3.86 
2.59 
3.82 
2.57 
3.79 
2.56 
3.76 
2.54 
3.73 

2.53 
3.70 
2.51 
3.66 
2.49 
3.61 
2.48 
3.58 
2.46 
3.54 

2.45 
3.51 
2.44 
3.49 
2.43 
3.46 
2.42 
3.44 
2.41 
3.42 

2.40 
3.41 
2.38 
3.37 

2.52 
3.71 
2.49 
3.65 
2.47 
3.59 
2.45 
3.54 
2.43 
3.50 

2.41 
3.46 

2.45 
3.56 
2.42 
3.51 
2.40 
3.48 
2.38 
3.41 
2.36 
3.36 

2.34 
3.32 
2.32 
3.29 
2.30 
3.26 
2.29 
3.23 
2.28 
3.20 

2.27 
3.17 
2.25 
3.12 
2.23 
3.08 
2.21 
3.04 
2.19 
3.02 

2.18 

2.99 

2.17 
2.96 
2.16 
2.94 
2.14 
2.92 
2.14 

2.90 

2.13 

2.88 

2.11 
2.85 

3.21 
2.27 
3.17 

3.14 

2.24 
3.11 
2.22 

2.21 

2.19 
3.01 
2.17 
2.97 
2.15 
2.94 
2.14 
2.91 

2.12 
2.88 
2.11 
2.86 
2.10 
2.84 
2.09 
2.82 
2.08 
2.80 

2.07 
2.78 
2.05 
2.75 

(continued  on  page  760) 

758  Appendix  B  | Statistical  Tables 

2.32 
1.78 

1.76 

1.75 

1:73 
2.19 

1972 
2.16 
1.69 
2.12 
1.67 
2.08 
1.65 
2.04 
1.63 
2.00 

1.61 
1.97 
1.60 
1.94 
1.58 
1.92 
Lor 
1.90 
1.56 
1.88 

1.55 
1.86 
1.52 
1.82 

2. 62 
1.95 
2.58 

2.54 

2.51 

2. 32 

2 49 
1.89 

2.42 

2.40 

2.39 

1.83 

1.79 

1.78 

1.76 
2.24 
1.75 
2.22 
1.74 
2.20 

1.74 
2.18 
1,72 
2.15 

1.65 
2.04 

2.02 

1.63 
2.00 
1.61 

Appendix  B | Statistical  Tables  759 

TABLE  B.4 

(continued) 

Denominator 
degrees  of 
freedom 

(m2) 

60 

65 

70 

80 

100 

125 

-  150 

200 

400 

1000 

Numerator  degrees  of freedom  (m,) 

3 

4 

5 

6 

7 

8 

2.76 
4.13 

2.75 
4.10 

2.52 
3.65 
2.51 
3.62 
2.50 
3.60 
2.48 
3.56 
2.46 
3,51 

eat 
3.34 

2.36 
3.31 
2.35 
3.29 

2.33 
3,25 
2.30 
3.20 

2.44 
3.47 
2.43 
3.44 
2.41 
3.41 
2.39 
3.36 
2.38 
3.34 

2.29 
3.17 
2.27 
3.14 
2.26 
3.11 
2.23 
3.06 
2.22 
3.04 

2.25 
3.12 
2.24 
3.09 
2.23 

2.17 
2.95 
2.15 
2.93 
2.14 
2.91 
paz 
2.87 

2.10 
2.82 

2.08 
2.79 
2.07 
2.76 
2.05 
2.73 
2.03 
2.69 
2.02 
2.66 

2.10 
2.82 

2.08 
2.79 
2.07 
2.77 

2.05 
2.74 

2.03 
2.69 

2.01 
2.65 
2.00 
2.62 
1.98 
2.60 
1.96 
2.55 
1.95 
2.53 

237 
3.32 

2.21 
3.02 

3.78 

2.01 
2.64 

1.94 
2.51 

1.89 
2.43 

1.88 
2.41 

10 

1.99 
2.63 

1.98 
2.61 
1.97 
2.59 

1.95 
2.55 

1.92 
2.51 

1.90 
2.47 

1.89 
2.44 

1.87 
2.41 
1.85 
2.37 
1.84 
2.34 

1.83 
2.32 

The  table  describes  the  distribution  of an  F variable  with  m, 
numerator  and  m,  denominator  degrees 
of freedom.  Entries  in the  standard  typeface  give  the  5%  critical  value,  and  boldface  entries  give  the 
1%  critical  value  for the  distribution.  ror example,  there  is a 5%  probability  that  an  F variable  with  2 
numerator  and  50 denominator  degrees  of freedom  would  exceed  3.18;  there  is only a  1% 
probability 
that  it would  exceed  5.06. 

Source:  George  W.  Snedecor  and  William  G.  Cochran,  Statistical 
Iowa  State  University  Press.  Reprinted  by permission  of Iowa  St 

Methods,  8th  ed.  Copyright  1989  by 
ate  University  Press. 

760  Appendix  B  | Statistical  Tables 

1.56 
1.87 
1.54 
1.84 
1.53 
1.82 
1.51 
1.78 

1.50 
1.79 
1.49 
1.76 
1.47 
1.74 
1.45 
1.70 

1.59 
1.93 
1.57 
1.90 
1.56 
1.88 
i54 
1.60 
1.94 
1.84 
1571.51 
1.79 
1.89 

1.55,  i  1.49 

.°  ’ 

1.85 _ 1.75  Lo 

in  167 
2.04 

1.60  154  149  142 138  321 
1.92 
1.5835153 
i ogee yet 1.61 154 14 

as 2.23  2.12 
.70 

1.84  1.74 1.64  1.57 

1487141  .1.%  1. 

1.47 

\  OS -dee a  oe  oe  2  a  =  A 

‘geo  eee 

f» 

ad 

tee” 
: 
=,  Oe  ae  = 

4 
aon. 

ur 

ay 

= 

ee 

—) 

==  vee ~ Ss Genes 

ee 

3 
a  xe  Er  7 bet  oe 
a 

ee  ee  nant 

4  =  | 

-  a 

id 

‘  a  is =  Ss  -  =  “ras  : 

ro 

¥ 

7 

TABLE  B.5 
Critical  Values  for  the  Phillips-Perron  Z, Test  and  for  the  Dickey-Fuller  Test 
Based  on  Estimated  OLS  Autoregressive  Coefficient 

Sample 
size 
T 

Probability  that  T(p  —  1) is less  than  entry 

0.01 

0.025 

0.05 

0.10 

0.90 

0.95 

0.975 

0.99 

25 
50 
100 
250 
500 
0 

=  {1.9 
-12.9 
-33 
—  13.6 
—  13.7 
—  13.8 

Pe 
=—9.9 
=102 
=  10.3 
—  10.4 
—  10.5 

—  1s 
sf wh 
9 
—8.0 
—8.0 
#81 

—~  FF2 
—  18:9 
—  19.8 
~~  A 
23> 
—  20.7 

—  14.6 
mek  ee 3 
10.4 
—  16.6 
—  16.8 
16.9 

12.5 
at  BE 
et  ee 
—  14.0 
—  14.0 
—14.1 

25 
50 
100 
250 
500 
co 

ag. 
=25,7 
—27.4 
—  28.4 
28:9 
=  29-0 

x  9.9 
—  22.4 
=23.6 
—24.4 
—  24.8 
2s 

=17_9 
—19.8 
—20:7 
—~P1.3 
mizkS 
—218 

Case  ] 

—5.3 
—5.5 
—5.6 
—5.7 
—5.7 
—5.7 

Case 2 
—  10.2 
—  10.7 
—  11.0 
—11.2 
—11.2 
—  11.3 

Case  4 
—  15.6 
—  16.8 
—17.5 
—  18.0 
—18.1 
—  18.3 

1.01 
0.97 
0.95 
0.93 
0.93 
0.93 

1.40 
|) 
1.31 
1.28 
1.28 
1.28 

—0.76 
—  0.81 
—  0.83 
—  0.84 
—  0.84 
—  0.85 

0.01 
—  0.07 
—0.10 
—  irae 
a5, 
oi MES 

1.79 
1.70 
1.65 
1.62 
1.61 
1.60 

0.65 
0.53 
0.47 
0.43 
0.42 
0.41 

—  3.66 
—J08 
—3.74 
S379 
$3.70 
pee | 

See 
—  2.60 
2,62 
—2.64 
=2.e 
—2.66 

gk 
—  1.66 
1.73 
—  1.78 
—  1.78 
pec! 

The  probability  shown  at  the  head  of the  column  is the  area  in the  left-hand  tail. 
Source:  Wayne  A.  Fuller,  Introduction  to  Statistical  Time  Series,  Wiley,  New  York,  1976,  p. 371. 

762  Appendix  B | Statistical  Tables 

TABLE  B.6 
Critical  Values  for the Phillips-Perron  Z, Test  and  for  the  Dickey-Fuller  Test 
Based  on  Estimated  OLS ¢ Statistic 
fenpie 

ee  ene  ey — SHOR  MES RRAG ENT) 
0.01 
0.10 
0.975 

Probability  that  (p —  1)/6;, is less than  entry 

0.025 

0.90 

0.05 

0.95 

T 

—2.66 
-—2.62 
—2.60 
—2.58 
—2.58 
 =2.58 

-2.26 
-2.25 
-2.24 
23 
1.23 
23 

-1.95 
-1.95 
-1.95 
-1.95 
-1.95 
-1.95 

Case  1 
—-1.60 
-—1.61 
—-1.61 
1.62 
-1.62 
-1.62 

Case 2 

322 

~293 
-3.00 

-2 

3.17, oe 7 

~286 

—" 

ay 

Muda  ae... 

on na? 
ah  oie 8: 324 os 0.80 Ae 

. 

aos.” 

Ee 

tica |  Time ne  3ei fl 2M re nat 
ee  Deities: a  ae 

Gi  BS 

, 

cs 
iota ete  see ayy — 

. 

1 

as  , be = ea  =< Can  Ee  co 

ma 

= 

va 

7 

: 

TABLE  B.7 
Critical  Values  for  the  Dickey-Fuller  Test  Based  on  the  OLS F Statistic 

samp le 

Probability  that  F test  is greater  than  entry 

T 

0.99 

0.975 

0.95 

0.90 

0.10 

0.05 

0.025 

0.0. 

Case  2 

(F test  of  a  =  0, p  =  1 in  regression  y,  =  a@  +  py,_,  +  U,) 

0.29 
0.29 
0.29 
0.30 
0.30 
0.30 

0.38 
0.39 
0.39 
0.39 
0.39 
0.40 

0.49 
0.50 
0.50 
0.51 
0.51 
0.51 

0.65 
0.66 
0.67 
0.67 
0.67 
0.67 

4.12 
3.94 
3.86 
3.81 
3,79 
3.78 

5.18 
4.86 
4.71 
4.63 
4.61 
4.59 

6.30 
5.80 
5.57 
5.45 
5.41 
5.38 

Case  4 
(F test  of 5 =  0, p  =  1 in regression  y,  =  a  +  dt  +  py,_;  +  U,) 
1.33 
1-37 
1.38 
1.39 
1.39 
1.39 

0.74 
0.76 
0.76 
0.76 
0.76 
0.77 

0.90 
0.93 
0.94 
0.94 
0.94 
0.94 

7.24 
6.73 
6.49 
6.34 
6.30 
6.25 

1.08 
1.11 
Iei2 
1.13 
1.13 
145 

8.65 
7.81 
7.44 
p>) 
7.20 
7.16 

5.91 
5.61 
5.47 
5.39 
5.36 
5.34 

25 
50 
100 
250 
500 
eo 

25 
50 
100 
250 
500 
oo 

7.8 
7.0 
6.71 
6.5: 
6.4; 
6.4: 

10.61 
9.31 
8.73 
8.43 
8.34 
8.27 

The  probability  shown  at  the  head  of the  column  is the  area  in  the  right-hand  tail. 

Source:  David  A.  Dickey  and  Wayne  A.  Fuller,  ‘Likelihood  Ratio  Statistics  for  Autoregressive  Time 
Series  with  a  Unit  Root,”  Econometrica  49  (1981),  p.  1063. 

764  Appendix  B  | Statistical  Tables 

TABLE B.8 
Critical  Values  for the Phillips Z, pa When  Applied  to Residuals 
from Spurious  Cointegrating  Regress 
Number  of 
right-hand 
variables  in 
regression, 
excluding 

(NT) 

0.025 

0.010 

eee 

Probability  that  (T —  1)(p  —  1) is less  than  entry 
0.075 

ain  — 
(m-1) 
0.150 
ere 100)  0.125  0.150 
Case  1 
*=18.9°  15.6  -13.8 
—25.2 
-21.5 
-19.6 
—31.5 
-27.9 
—  37.5  nae. 

0.100. 

0.050 

0.125. 

2 

5 
®” 

-12.5° 
-11.6 
-18.2 
-17.0 
-23.9 
-22.6 
—28.9"-—27.4 
=33.8°-—~  32.3, 

—10.7 
-16.0 
—21.5 
+26.2 
=30.9 

5 

3.1 

peas “10-159 

TABLE  B.9 
Critical  Values  for  the  Phillips  Z, Statistic  or  the  Dickey-Fuller  ¢ Statistic  When 
Applied  to  Residuals  from  Spurious  Cointegrating  Regression 

Number  of 
right-hand 
variables  in 
regression, 
excluding 
trend  or 
constant 
(n — 1) 

Sample 
-  size 
(T) 

500 
500 

Ma & WN = Nk WN 

Ak WN 

Probability  that  (p — 

1)/G;, is less  than  entry 

0.010 

0.025 

0.050 

0.075 

0.100 

0.125 

0.150 

Case  1 

339 
—  3.84 
—  4.30 
—  4.67 
—  4.99 

—3.05 
—3.55 
—3.99 
—4.38 
—467 

-2.76 
 -3.27 
-3.74 
-—4.13 
-—4.40 

2.98 
—3.11 
—3o97, 
—395 
— 4.25 

oo).  ae 
=  24S 
—2.99. 
—2.75 
-—2.88 
—3.44,,.-3-3)  Foe 
=3815~35.11.  ~3L 
—3.94 
-4.04 
-4.14 

Case  2 

—  396 
—  4.31 
—  4.73 
—  507 
O28 

—3.64 
—4.02 
—4,37 
—4.71 
—4.98 

-—3.37 
-3.77 
—4.11 
—4.45 
—-4.71 

=  3.20 
—  3.58 
=  3,06 
—  4.29 
—  4.56 

—3.07 
—2.86 
-—2.96 
=3.44.  3.)  soo 
ag Be ON  ey  ke 
—3.% 
—4.16..—405 
-4.24 
-—4.33 
—-4.43 

Case  3 

—  3.98 
—  4.36 
—  4.65 
—  5.04 
—5.36 

—3.68 
—4.07 
—4.39 
—4.77 
—5.02 

—3.42 
-—3.80 
-—4.16 
 —4.49 
 -4.74 

=3, 
—  3.98 
—  4.32 
—4.58 

= 

— 
—3.13 
—352..3-  342  =e 
—3.66 
-3.74 
—3.84 
—4.00 
-4.08 
—4.20 
—4.28 
-4.36 
-4.46 

The  probability  shown  at  the  head  of the  column  is the  area  in the  left-hand  tail. 

Source:  P.  C.  B.  Phillips  and  S.  Ouliaris,  “Asymptotic  Properties  of Residual  Based  Tests  for  Coin- 
tegration,”  Econometrica  58  (1990),  p.  190.  Also  Wayne  A.  Fuller,  Introduction  to  Statistical  Time 
Series,  Wiley,  New  York,  1976,  p.  373. 

766  Appendix  B  | Statistical  Tables 

TABLE  B.10 
Critical  Values  for  Johansen’s  Likelihdod  Ratio  Test  of the  Null  Hypothesis 
of A Cointegrating  Relations  Against  the  Alternative  of No  Restrictions 

Number  of 
random  walks 
(g  =n  —h) 
(g) 

Sample 
size 
(T) 

Probability  that  2(£,  —  £&,)  is greater  than  entry 

0.500 

0.200 

0.100 

0.050 

0.025 

0.010 

400 
400 
400 
400 
400 

0.58 
5.42 
14.30 
27.10 
43.79 

Case  1 

1.82 
8.45 
18.83 
33.16 
$1.13 

Case  2 

2.86 
10.47 
21.63 
36.58 
55.44 

3.84 
12.53 
24.31 
39.89 
59.46 

4.93 
14.43 
26.64 
42.30 
62.91 

6.5] 
16.31 
29.75 
45.58 
66.52 

2.415 
9.335 
20.188 
34.873 
53.373 

4.905 
13.038 
25.445 
41.623 
61.566 

6.691 
15.583 
28.436 
45.248 
65.956 

8.083 
17.844 
31.256 
48.419 
69.977 

Case  3 

0.447 
7.638 
18.759 
33.672 
52.588 

1.699 
11.164 
23.868 
40.250 
60.215 

2.816 
13.338 
26.791 
43.964 
65.063 

3.962 
15.197 
29.509 
47.181 
68.905 

9.658 
19.611 
34.062 
51.801 
73.031 

5.332 
17.299 
32.313 
50.424 
72.140 

11.576 
21.962 
37.291 
55.551 
77.911 

6.936 
19.310 
35.397 
53.792 
76.955 

Ut wWNme 
A m&WN 

UO & Wh — 

The  probability  shown  at  the  head  of the  column  is the  area  in the  right-hand  tail.  The  number  of 
random  walks  under  the  null  hypothesis  (g)  is given  by the  number  of variables  described  by the 
vector  autoregression  (1) minus  the  number  of cointegrating  relations  under  the  null  hypothesis  (A). 
In each  case  the  alternative  is that  g  =  0. 

Source:  Michael  Osterwald-Lenum,  **A  Note  with  Quantiles  of the  Asymptotic  Distribution  of the 
Maximum  Likelihood  Cointegration  Rank  Test  Statistics,”  Oxford  Bulletin  of Economics  and  Sta- 
tistics  54  (1992),  p.  462;  and  Sgren  Johansen  and  Katarina  Juselius,  ‘Maximum  Likelihood  Esti- 
mation  and  Inference  on  Cointegration—with  Applications  to  the  Demand  for  Money,”  Oxford 
Bulletin  of Economics  and  Statistics  52  (1990),  p.  208. 

Appendix  B  | Statistical  Tables 

767 

TABLE  B.1I1 
Critical  Values  for  Johansen’s  Likelihood  Ratio  Test  of the  Null  Hypothesis 
of h Cointegrating  Relations  Against  the  Alternative  of h  +  1 Relations 

Number  of 

Tatas sate  ane le 

Probability  that  2(L£,  —  &,,)  is greater  than  entry 

(T) 

0.500 

0.200 

0.100 

0.050 

0.025 

0.010 

=a- 
(g) 

, 

] 
2 
3 
4 
5 

a 
2 
3 
4 
5 

1 
Z 
3 
4 
= 

400 
400 
400 
400 
400 

400 
400 
400 
400 
400 

400 
400 
400 
400 
400 

0.58 
4.83 
9.71 
14.94 
20.16 

Case  1 

1.82 
7.58 
13.31 
18.97 
24.83 

Case 2 

2.86 
9.52 
15.59 
21.58 
27.62 

3.84 
11.44 
17.89 
23.80 
30.04 

4.93 
13.27 
20:07..2> 
26.14 
32.51 

6.51 
15.69 
22399 

28.82 
35. 17 

2.415 
7.474 
12.707 
17.875 
23.132 

4.905 
10.666 
16.521 
22.341 
27.953 

6.691 
12.783 
18.959 
24.917 
30.818 

8.083 
14.595 
21.279 
27.341 
33.262 

9.658 
16.403 
23.362 
29.599 
35.700 

11.576 
18.782 
26.154 
32.616 
38.858 

Case  3 

1699.: 
10.125 
16.324 

6.936 
().447 
17.936 
6.852 
25.521 
12.381 
U7  799-—022  NAS.2524  Ji2  2927. 16BAy  29.333"  * 31-945 
38.341 
23.211 

3.962m 
14.036 
20.778 

5.332 
15.810 
23.002 

2.53160 
12.099 
18.697 

27.899 

35.546 

30.774 

33.178 

The  probability  shown  at  the  head  of the  column  is the  area  in  the  right-hand  tail.  The  number  of 
random  walks  under  the  null  hypothesis  (g)  is given  by the  number  of variables  described  by the 
vector  autoregression  (71) minus  the number  of cointegrating  relations  under  the  null  hypothesis  (A). 
In cach  case  the  alternative  is that  there  are  #  +  | cointegrating  relations. 

Source:  Michael  Osterwald-Lenum,  “A  Note  with  Quantiles  of the  Asymptotic  Distribution  of the 
Maximum  Likelihood  Cointegration  Rank  Test  Statistics.”  Oxford  Bulletin  of Economics  and  Sta- 
tistics  54  (1992),  p.  462:  and  Sgren  Johansen  and  Katarina  Juselius.  “Maximum  Likelihood  Esti- 
mation  and  Inference  on  Cointegration—with  Applications  to  the  Demand  for  Moncey.”  Oxford 
Bulletin  of Economics  and  Statistics  52  (1990).  p.  208. 

768  Appendix  B  | Statistical  Tables 

i 

Answers 
to  Selected  Exercises 

Chapter 3.  Stationary  ARMA  Feoteases 
3.1.  Yes,  any MA  process  is covariance-stationary. Autocovariances: 

The  coett 

WA  Fs ths 
jAl = Atal  smaguinoe xolqan Yen = 0.8 

bing ee?  As  =  apiliat' 
Yar = 4.32 — 

hn 

1e= °F i moi?  =0-  for Ul: >a AND  sh 20 wer 

‘a Yes, the Hw process is covariance-stationary, since 

(= te +  049 «  0800  O20: 

a  _ 

il 

StS 

iw  s¢ 

2  Rat iets. 

ant stons;. 

<4 

‘ 

b q 

- 

| 
.* 

as 

= 

that  is 

3.4. 

‘From  [2.1.6], 

yy  =  FY. 

WL)e  =  (H+  Wtr+wWrwW+::*)re. 

But  the  sum  (%  +  %  +  %  +  y,  +  * 
atz  =  1 

+  -) can  be  viewed  as  the  polynomial  #(z)  evaluated 

Moreover,  from  [3.4.19], 

W(L)c  =  (1)-e. 

w(1)  =  1/1  —  $,  —  ). 

Let  A, and  A, satisfy  (1 —  ¢,z  —  $,z”) 

3.5. 
are  both  inside  the  unit  circle  for  a  covariance-stationary  AR(2)  process. 

= (1 —  A,z)(1  —  Azz),  noting  that  A, and  A, 

Consider  first  the  case  where  A, and  A, are  real  and  distinct.  Then  from  (1. 2.29], 

3 ly| =  S Ic,A4  +  c2A4 
j=0 

j=0 

<  > |c,A4|  +  > |c,A4| 

j=0 

j=0 

=  |e,|/  —  |A,|)  +  lel  -  |Az|) 

<  @, 

Consider  next  the  case  where  A, and  A, are  distinct  complex  conjugates.  Let R  =  |A,| 

denote  the  modulus of A, or  A. Then  0 =  R < 1, and  from  [1.2.39], 

2, ly|  =  2, lea,  +  c2A4| 

-> aR? cos(6j)  —°2BR/  sin(@)| 

j=0 

<  |2a] > R'|cos(9j)|  +  |2B| > Rilsin(@)) 

<|2a|  >) R/  +  |2B|  > R! 

j=0 

j=0 
=  2a]  +  |B|)/(1 — R) 
<  ©, 

Finally,  for the  case  of a repeated  real  root  |A| <  1, 

D yl =  D kal  +  kaj  <  fh] D [Al +  [|  S jar]. 

j=0 

j=0 

j=0 

j=0 

But 

and 

lk, D> |Al’  =  [k,I/  —  Jal) <<  © 

j= 

> |ia-"| 
jn 

1  +  2/A|  +  3]Al?  +  4a]?  + 

1 +  ({A|  +  Al)  +  (Jal?  +  Jal?  +  Jal 
wa ike  tag  *  ALE  JAR) +  « 

. 

=  (1 +  [Al +  Jal?  +  [Al? +  +++)  +  Cla}  +  Jal?  +  Jape  4  +) 

+  ([A|?  +  [AJ? + +++) 
(1  =  |Al)  +  Jal  =  Jal) +  [Ala  =  [al)  +  - 

=  (1  =  Jal) 
<  @, 

. 

3.8..  (1 +  2.42  +  0.82)  =  (1 +  0.42)(1  +  22). 

770  Appendix  C | Answers  to Selected  Exercises 

_  The invertible  operator  is 

So the  invertible  representation  is 

(1 +  0.4z)(1  +  0.5z)  =  (1 +  0.92  +  0.2z?), 

=  (1 +  0.9L  +  0.2L2)e, 

sit =  4. 

ead 4.  SPE Be 

4.3. 

I 
|-2  : : 9 ? ; 
3  Pye:  3  : 

= : 

4.4.  No.  The  projection  of Y, on  Y;, Y,, and  Y, can  be calculated  from 
— 
The projection  P(Y,|¥,,  Y,), in turn,  is given by 

POAIYs,  Yas ¥1) =  au¥,  +  dal Ys - P(VAIY,))  +  aul ¥s — P(VSI¥2,  YD): 

The coefficient  on  Y, in P(Y,|Y3,  Y2, Y,) is therefore ave bY Ge — AesAs2. 

rp  AL  Y,)  =  ay,Y,  + Axl ¥, — a (Y. 120) oe 

Chapter 5.  Mdsiowan Likelihood | Estimatic  — 

“re 

i ~o 

52. The negative of the matrix  of second  derivativesis 

Rees =  Fi 

he a me 5 ode 

where  Y, =  Y,  —  #.  But 

(WT  -  KY 

VY  =  MT  -  1) DS (+ whine  +H) 

=  (1(T  ry  k)] sf Y,-.  +  wl  T =  ky] 2 Yi 

T 

+  pl1(T  —  k)]  >  ¥, +  w? 

w=k+i 

+ E(Y,Y,,)+0+0+ 

=  E(Y,  +  z)(Y, ~k  to) 

=  E(Y,Y,_,). 

Chapter  8.  Linear  Regression  Models 

snp  = 

PEON RY 

_  Vy  —  y'[l, —  X(X'X)“'X']y 

y'y 

1 —  [(y’'MxM,y)/(y’y)] 
1 —  [(@’@)(y’y)). 
p2  =  YY  May — 73? 
| 
y¥~  Fy? 
: 
=  1 -  [(@’@)Xy’'y  -  TY?)] 

and 

yy  -  TY?  =  2  —  Ty?  =  2 (  —  yy. 

T 

z. 

8.2.  The  5%  critical  value  for a y2(2)  variable  is 5.99.  An  F(2,  N)  variable  will  thus  have 
a  critical  value  that  approaches  5.99/2  =  3.00  as  N —  ~.  One  needs  N of  around  300 
observations  before  the critical  value  of an  F(2,  N) variable  reaches  3.03,  or  within  1%  of 
the limiting  value. 
8.3. 
Fourth  moments  of x,u,  are  of the  form  E(e4)-E (y,- Ye-jYt-1¥1—m)-  The  first  term  is 
bounded  under  Assumption  8.4,  and  the  second  term  is  bounded  as  in  Example  7.14. 
Moreover,  a typical  element  of (UT) EP ,4u?x,x;  is of the  form 

; 

: 

(VT) > e?y,-1Y.-y  =  (WT) > (67  —  oye)  +  (UT)  & Y.W9s-j 

+0  +  0? E(y,_iy,-)). 

Hence,  the conditions  of Proposition  7.9  are  satisfied. 
Proposition  7.5 and  result  [7.2.14]  establish 
8.4. 
 -:* 
t 
bir}  —  | OT).  © 
+  (UT )Zy  1%»  |  | (UT) Zy,-1y, 

(UTSy.,  9  fF Cite, 

(UT)ay,.. 
(UT )2y71 

1 

tr 

(/T)2y,_,  a  tibet: roti’) 
see 
1 

mM 

CUT)zyi_, 

(1/T)2y,_py, 

4\"  %  +  pat sje) 

PA Py 

+ be 

%  +  ow 

which  equals a?) given  in [4.3.6]. 

772  Appendix  C | Answers  to Selected  Exercises 

Chapter  10. 

Covariance-Stationary  Vector  Processes 
Pte, 

aes 

r=  | h, ¥  {hi(1  +  6?)o2  +  o2} 

10.2. 

(a) 

“ 

h(i  +  ae a 

ig 

0 

coe  Be 4 

F.,  =f, 

r_,=f, 
r,  =0  fork = 2=3;  +4,...,. 

(b)  sy(w)  =  (2n)- (2 sa] 

S,,  =  (1 +  6?)o2  +  O02e-” + Bae 

=  h,002e7  + h,(1  +  0?)o2e  +  h,60? 
=  hfore-™  +  oe + 0*)aze-  + ube, 

:  a SRE 

ele)  = Qa) 
qvx() = nOn)haile sin(2w) + (1 +  6)-sin(w)). 

+ 1+ 69 -ca + 

hc) 

te a 

at 

8, 

a4 nese The variable X, follows an  MA(1) process, mcpercemsttits 
a  Su 8 — tee =  h,-e-.  | Wo  elo S,, in turn  by —_ =  h,- a and 

p 

Sexe) 

Lhe =  mr 3 

.  z od] 

Sim 

odt  ai isimetiflog  gaiwollot off oi (A)A| sneniiensish ost oi 
Andyens  RGR — T= ial 
Mom 

Proposition  7.10.  Hence,  (1/T)2/_,z,  > 0.  Moreover, 

(1/T)  > Vijyt-Vput-h  el E( yj 4-4Ynt-hy)? 

- 

by virtue  of Proposition  10.2(d). 
11.2. 

(a)  No. 

Yes. 

(b) 

113. 

a  =  4, 
B= 

Ag  =  2,,07;' 
A, =  ¥  —  Ay Nya; 
E&, =  6  —  1,07  'B; 

oF  = O11 

oF  =  A,  —  0,07'D,2 

Uy,  =  &y 

(c)  No. 
forjw  1,232.52 
tor  { *  4,2..  oe 

for  fom  1  2e  es  a 
ffi;  2...  P 

Ur, =  Sere  04,07", 
Premultiplying  by A*(L)  results  in 

|A(L)| 

0  1] 3  H -éL) 

n(Z)  | 

0 

|A(L)| 

Yu  ig  Ao  +  ACL)  1  —  ¢(L) 

juz 
-|  (1 —  &(L)Ju,  +  n(L)u, 
~ 

[Ap +  ACL),  +  [1 —  S(L) ue, 

11.4. 

Thus, 

|A(L)|y.  = 

I  - 

|A(L) ly,  =  Vor 

Now  the  determinant  | A(L)|  is the  following  polynomial  in the  lag operator: 

|A(Z)|  =  [1 —  €(L)}[1  —  ¢(L)]  —  [n(Z)]fAo  +  A(Z)). 
The  coefficient  on  L® in  this  polynomial is  unity,  and the  highest  power  of so is  L**,  which 
has  coefficient  (&¢, — 7,A,): 

|A(L)|  =  1 +  a,L  +  a,L?  +  +++  +  a,,L*. 

Furthermore,  v,,  is the  sum  of two  mutually  uncorrelated  MA(p)  processes,  and  so  v,, is 
itself  MA(p).  Hence,  y,, follows  an  ARMA(2p,  p) process;  a similar  argument  shows  that 
Y2, follows  an  ARMA(2p,  p) process  with  the  same  autoregressive  coefficients  but  different 
moving  average  coefficients. 

In general,  consider  an  n-variable  VAR  of the  form 

with 

M(L)y,  =  €, 

nme 

=e  = 

E(e,e,)  {0  otherwise. 

Find  the  triangular  factorization  of  9 =  ADA’  and  premultiply  the system  by A~',  yielding 
A(L)y,  =  U, 

where 

A(L)  =  A~'®(L) 
u,  =  A-'e, 

E(u,u,)  =  D. 

774  Appendix  C | Answers  to Selected  Exercises 

Thus,  the  elements  of  u,  are  mutually  uncorrelated  and  A(0)  has  1s  along  its  principal 
diagonal.  The  adjoint  matrix  A*(L)  has  the  property 

saison the  system  by A*(L), 

A*(L)-A(L)  =  |A(L)|-I, 

|A(L)|-y,  =  A*(L)u,. 

The  determinant  |A(L)|  is a  scalar  polynomial  containing  terms  up  to  order  L”’,  while 
elements  of A*(L) contain  terms  up to order  L“~”.  Hence,  the ith row  of the system  takes 
the form 

|A(L)|- Yi  =  Vir» 
where  v,, is the sum  of m  mutually  uncorrelated  MA[(n  —  1)p] processes  and  is therefore 
itself MAl(n =  1)p}. Hence, Yam  ARMA(np,  (n -  1)p]. 

11. 5. (a)  |L -  ®,z| =  a —  0.3z)(1  -  0.4z) =  (0. 82)(0. ies 

=  1 -  0.7z  —  0.62? 
=  (1 — 1.2z)(1  + 0.52). 
Since z* =  1/1.2 is inside the unit circle, the system is nonstatboniaygennad 

: 
(b)  Wo = -() 9] “, “fee A  =  [0.63 0 a) i

+  y, fe _  “a 

fo.s1  A 

-  &) 

| 

a  aS 

en  €.04]  mod  tt 

|  W, diverges as s —  ©. 

juga  ~ 9 

:

ee 

3 

(c)  Vase 7  EOuralYs Yr:  w=  Fis+2  + 0.3€1441  + O.8e2 41 

+  ors  MSE =1+ 13) + (0.8)(2)  =  2.37. 

oe to  phere ods to : viosqetbe Set cro  valent  sssdt taiercrstitn  OT 

cian 

agg, 218 ot teazong (RJA si abe Lavoe oh ta 

o  y + Gu rex retile  phe a 

 
 
Furthermore,  from  [13.3.19], 

bé,,  =  {6a7/[6?  +  6p )}-{y,  —  w  —  GE -14:-1} 

=  {0-'@07/[o0?  +  6p},  —  w-  Ta 

=  {00?/[o?  +  6?  Ale fy,-  eo  6é,_ ir-  > 
which is  the  same  difference  equation  that  generates  {6é,,,},  with  both  sequences,  of course, 
beginning  with  0é,,.  =  G€o10  =  ©.  With  the  sequences  (H’'P,,,,,H  +  R) and  A’x,,,  + 
H' PP identical  for  the  two  representations,  the  likelihood in  (13. 4.1] to  [13.4.3]  must  be 
identical. 
13.6. 
we  see  that 

The innovation e, in [13.5.22]  will be fundamental  when  |@ —  K| <  1. From  [13.5.25], 

@ —  K  =  goi,/(oy  +  P). 
Since  P is a variance,  it follows  that  P =  0, and  so|¢  —  K| =  |¢|, which  is specified to be 
less  than  unity.  This  arises  as  a  consequence of-the  general  result  in Proposition  13.2  that 
the  eigenvalues  of  F  —  KH’  lie inside  the  unit circle. 

From  [13.5.23]  and  the preceding  expression  for  @ —  K, 

—(¢ — K)E(e7)  =  —(¢ — K)(ow  +  P) = —¢do%, 

as  claimed.  Furthermore, 

[1 +  (@ —  KYJE(e7)  = 

+  P) +  (¢ — K)¢o%, 
(% 
(1 + doy  +  P  —  Kooy. 

But from  [13.5.24]  and  [13.5.25], 

| 
and  so 

=  K¢oy,  +  03, 

(1 +  (@ —  KPJE(e7)  =  (1  +  $*)0%,  +  o%. 

To understand  these  formulas  from  the perspective  of the formulas  in Chapter  4, note 
that  the  model  adds  an  AR(1)  process  to  white  noise,  producing  an  ARMA(1,  1) process: 

(1 aa OL) Ye 41  Vian  te  (1 cZ  oL)w,.1- 
The  first  autocovariance  of  the  MA(1)  process on  the  right  side  of  this  ———- is 
—  be%y, while  the variance is (1 +  ¢7)03,  +  o%. 

Chapter 16. 

Processes  with  Deterministic  Time  Trends 

16.1.  E(w7) > [A, +  A,(t/T)Pe2  —  (1/7)  > o7[A?  +  2A,A,(t/T)  +  awry) 

=  (1/T?)  5 [AZ +  2A,A(t/T)  +  AXt/T)P-E(e2  —  0). 

But 

7 

(1/T)  > [A?  +  2A,A,(t/T)  +  AXt/TYP  >  M <@, 

and  thus 

a3 

: 

re(qr) S (A, +  A,(t/T)Pe2  -  (1/T)  . o7{A?  +  2A,A,(t/T)  +  ary) 

Thus 

—  M-E(e?  —  oa”)? <  @, 

(1/T)  > [A, +  A,(t/T)}?e? 

“S (UT) oA}  + AAMT)  + AUT) 

—~  a’ A'Qr. 

776  Appendix  C | Answers  to Selected  Exercises 

16.2. 

Recall  that the variance  of b, is given by 
E (by ~  B\br ~  By =  o°( ux;) 

fi a  T 

ri7r7+i2 

T(T  +  1/2  T(T  +  1)(2T  +  1)/6] 

|} 
© 

Pre-  and  postmultiplying  by Y; results in 

E[Y,(b;  —  B)(b;  —  B)’Y7] 
Rees 

= 

T(T  +  1)2 

0M req +1)/2  T(T  +  1)(2T  +  1)/6  Y; 

= 

-1 

. 

.  {vs Lr 12  T(r +  127  +  1y6]*? 

T 

1(T +  1)/2 

,- 

a 

vq" 
~  of ql 
SHG. 2 Seeeet.of Cie eae ecgeeiaios Sige Yew 

and  so 

- 

set, - )P > 1202, 

r(3;  -  8)  50.  | 

fess » peeks =  T~{(1/T)y,  FIR: ep  «  +  (T/T)y7] 
x an + Dnt ait +  Dra, 

ail 

from  answers  (a)  and  (b).  This,  in  turn,  can  be  written 

(A?/Y%)'? 

H(a2-[WODP  =  90)  (y2p,,y0a|  MOWODF  —  th, 
af [ wor  ar} 

[ wor  ar}  | [ wor  ar} 

40"  =  6) 

(4)  (T?-62,  +  s3)  =  1(T-*2y2_,)  > u(ae| [wir)?  ir), 

from  Proposition  17.3(h).  Thus, 

T(r  —  1) —  (T?+63,  +  857)(1’  —  Yo) 

+> Tir  -  1) -  ae 
A2- | [Win]?  dr 

402  =  Wo) 

d2-| 

[W(r))?  dr 

4 Ha2-(WDP  =  yo) 
| (WOP  dr 
_  Mw@yp  -  2 
| wor ar 

with  the  next-to-last  line  following  from  answer  (a). 

(©)  (yoIA2)!2+te  —  GO?  —  WA}  x  (TG,  +  Sr} 
4  {Mwap  =) ,  4?) 

i [ wor ar}  al [ over ar} | 

-  {ayer =  yA}  +  (. [ wor ar) \ 

from  answers  (c) and  (b). Adding  these  terms  produces the  desired  result. 

To estimate  y, and  A, one  could  use 

4°73  at,-, 

t=j+l 

i =  je +  2D [1 -  ia  +  Di, 

forj. =  0,1,..  ..,@ 

where  @, is the OLS sample residual  and q is the number  of autocovariances  used to represent 
the  serial  correlation  of #(L)e,.  The  statistic  in (d) can  then  be compared  with  the case  1 
entries  of Table  B.S,  while  the  statistic  in  (e) can  be  compared  with  the  case  1 entries  of 
Table  B.6. 

17.3. 

(a)  | T-273é,., 

1 

T-*YE_,  = T-7Et 
T-?dé2, 
T-7%1  | T-52MME_,  2 T-3EP? 

T-52EE,_, 

1 

A: | W(r)  dr 

1/2 

>|  A: | W(r) dr  A? | [W(r)?  dr  A: | rW(r)  dr  |. 

1/2 

A: | rW(r)  dr 

1/3 

778  Appendix  C | Answers  to Selected  Exercises 

ae Sy, 
(o)  | 7-3¢_,u,} 
T-*73tu, 

A+  W(1) 
5}  G20  WEP  -  w)  | 
A-{W(1)  —  f W(r) dr} 

(c)  This  follows  from expression  [17.4.52]  and  answers  (a) and  (b). 
(d)  The  calculations  are  virtually  identical  to those in  (17.4. 54]. 
(€)  tr =  T(r  —  1) +  {T*-63}2 + T(,  —  1)  =  {(52/a2)-Q}'2. 
(f)  Answer  (c) establishes  that 

T(6;  -  1) 

1 

| W(r) dr 

1/2 

“1 

W(1) 

340  1  9  | W(r) dr  | [W(r)}? dr  | rW(r) dr 

H(W(1)}?  -  1} 

1/2  | rWr)dr 

8s|sd 

(A)  -  | W(r) dr 

5  ae Wr) dr  af  bom 

- 41 —  (y/A?)}[0  1  0) Lo W(r) dr ra [WP dr  | rW(r) dr .  F 

=  V+  1  —  (y/A*)}O. 
Moreover, answer  (d) implies that 

v2 

rW(r) dr 

1/3 

| 

| 

fits 
|  1T-65, = 5) @? = ») yon  Yo) 
wy  HI - 3  WANG: 

| 

| 
at  ens 

RE ear me on ae 

For  Y; the  (kK  x  k) matrix  defined  in  [18.2.18]  and  R  of  the  specified  form,  observe  that 
VT-R  =  RY;.  Thus, 

xt =  [RY;(by  —  B)]'[stRY,(2x,x;)-'Y;R’] 

[RY;(b,  ~  B)] 

=  [RY;(b;  —  B)}'[s}R(V7'2x,x/Y7)"'R’'] 
“(noon (al  oJ)  (Fee) 

-1 

-3 

i 

=" 

; 

[RY7(br —  B)] 

=  (R,V~'h,)'(9,,R,V~'R;)~'(R,V~"h,), 

where  the  indicated  convergence  follows  from  [18.2.25],  [18.2.20],  and  consistency  of s3. 
Since  h, ~  N(0,  a;,,V),  it follows  that 

R,V-'h,  ~  N(0,  o,R,V-'R‘). 

Hence,  from  Proposition  8.1, the  asymptotic  distribution  of v7 is x?(m). 
18.2. 

Here 

(Rby), 
x3 =  (Rb;)'[sR(2x,x;)'R’] 

where  x, is as  defined  in Exercise  18.1  and 

R 
(nzp  x  k) 

= 

(I,-,  & R,) 
{[a(p-1)<n(p-1)) 
a 

0 
[nA p-1)x(1+1)) 

4 

[2x n(p  —  1)) 

{a2 x(n  +  1)] 

ae  bi oe 

(np  xn) 

(nz x 7n);) 

(m2  X  nz) 

R, 

— 

0 

R, 

(2 x(n  +  1)] 

ye.  (ese  Fi 

From  the  results  of Exercise  18.1, 
2  & (»| Vhs  |), 
“ay  (e{3-m']) (-.r| 0  o  |") (R|5-"'|) 

v9  1 | 

fives 

=  ag * inh  7 haat @ R)V-'(1,-1  @ Ri) 

0 

0 

ee 

R.Q-'R | 

R,Q‘h, 

2 

,  ee ® x  3 
R,Q“'h, 

; 

18.3. 
€,, and  u,  =  €,,.  Let  x, =  (€,,,  1, Yis-1»  Yau—1)’  and 

(a)  The  null  hypothesis  is that  ¢ =  1 and  y  =  a  =  7  =  0, in which  case  Ay,,  = 

af  aire 
©,  0-0 
LS chee  PR  al ab 
T, 

Dev 

. 

0 

OQ. 

Then 

; 

Y7'Dx,x/¥>! 

T Ze, 

Atte 

T~'Xe,, 

T~  ZY, 1-1€x  Ey  4-1 
TZ  24-182  T-**Zy.5-\  T~*Xyo4-1ra-1 

T “Yen w-1  | Zeyh) 
T-*3y,,.5 
TB yp, 
T*Zyt,-1 
>>  ee  Pee 

T~*2y3,., 

*[0 a) 

780  Appendix  C | Answers  to Selected  Exercises 

1 

o;" | W,(r)  dr  | 

a,"  W,(r)  dr 

Q=  o,-| W,(r)  dr 

oi-| (W,(r)P  dr 

oe,-| (W,(r)]-(W.(r)]  dr 

ox { War) dr  0301] WSL) dro  [ (WAP ar 

T- 32,2, 

Y='5 

ine 

T-“Se 

T 

tlh 
1 

T~  Sy,  -1€, 

=  A 

T  224-18 

and  where  h, ~  N(0,  ojo7) and  the  second  and  third  elements  of the  (3 x  1) vector  h, 
have  a  nonstandard  distributi bution. Hence, 

—  Yr(b; - aa rt Y Se a ("Eau 

. 

| 

.  pea ” 

i 

ree)  ae  oo 

.te 

selrsipy i “ge  {s3 ei(2x,x;)-'e,} 

ae 

< 

oe) < ~T  send  gerbiteidatis 

tate  - — = he  ala pias  ih 

roi, 

GU 

ode 

1  [re  olf 

7  oe) (T  0  re 

0  T'? 
‘ 

|L¥r - Yo 

¥ 

0 

rm?  Lyx  Lyi, 

0 
Rey's ries  0  ll D(yu  —  YoYo) 
 T-*711  FO. - YoY) 
_ffr-  ofr  sed  |i 0  } 

ie, 

0. 

re 

0 

T -52  Lyx  Ly3, 

0  als 

a  0  || Zu  —  YoY2) 

0 

TH? NL 2yau - YoYo) 
-| 1  ran | Ty  wes 
T-7*3y,,  Tye  T->7Zy2(¥ir  —  YoY) 

But | 

7 

ae Pree  cs oa 
& BSS  = Cie  Ee  5)  Oy) 

- and thus  T-*3y_,% T-*8,: ee Je 
more, 

i 3Yy3>  T-383-3P  —> 83/3. Further- 

anatT  aistness  cach >  &  BS ott  BOmitos  int  af Booed sp 

= 

= 780 - Yea) 

-323(E,, — yo€2,). Similarly, 

: 

} +26, iat Yo§2) » 
2 eae 

- eatin a  8  0  Be YoY2.0  —  Yobx) 

19.3.  Notice  as  in (19.3.13]  that  under  the null  hypothesis, 

xr = {R,-T(9,  -  {310 0  R,) 

| T-'3w! 

T-'Sw,w)  = T-'w,  = T-2w,y3.)7'1  07, 
rr  | 0’ 
[Ri 
T-*2yzy3,} 

T-*Zy,w,  T-*?2y, 

1 

-: 

{R,:T (47  —  y)} 

“imo 

©  R,] 

Q- 

x 

0’ 

0 

1 

re 

ng 

{ i [W.(r)]’ ari 

0  Af Wd  he y= - rmseontmso ari 

7 

0)n-: 

[R,Ai»], 

0’ 
Ri 

tom which [19.2.5] low immediately. 

st  ioe a nak! 

re 
es 
s 
T-'3w,w) 
(r@, —  B) 
bsensal,  « 
)T'%(a@r-a)]_  | Tw, 
|  aa  Heres de do HP Fs oth EG  |  scutien  @¥-  £0 

T-'Sw,  T-*Ewy,  Twit  | 

T-*3yi, 

T7250 

1 

| 

soy 

+ 

|  to  hy 

= 

with  uu,  and  4,  Lagrange  multipliers.  First-order  conditions  are 

(a) 

Lyxa@,  =  24,2yyk, 

(b)  Zxyk,  =  24,2  xa. 

Premultiply  (a) by k, and  (b) by aj to  deduce  that 

Next,  premultiply  (a) by r;'Zy¥  and  substitute  the  result  into  (b): 

2,  =  2p,  =  r;. 

LxyLyyZyx8@,  =  17Zxxa 

or 

2xx2xylyy2Lyxa,  =  778). 
Thus,  r? is an  eigenvalue  of  254) 2yyZy Dyx with  a,  the  associated  eigenvector,  as  claimed. 
Similarly,  premultiplying  (b)  by r7'Z)  and  substituting  the  result  into  (a)  reveals 

that 

Ly 2yx2xx2xvk,  =  ik;. 

20.2.  The  restriction  when  h  =  0 is that  {, =  0.  In  this  case,  [20.3.2]  would  be 

L*  =  —(Tn/2)  log(2m)  —  (Tn/2)  —  (T/2)  log|Zyul, 
where  a  is  the  variance-covariance  matrix  for  the  residuals  of  [20.2.4].  This  will  be 
recognized  from  expression  [11.1.32]  as  the  maximum  value  attained  for  the  log likelihood 
for  the  model 

Ay,  =  %  +  I,Ay,_,  +  HAy,_.  +  --:  +  U1,_,Ay,_,.,  +  uw, 

as  claimed. 

The  residuals  g, are  the  same  as  the  residuals  from  an unrestricted  regression  of & 

20.3. 
on  ¢,. The  MSE  matrix  for  the  latter  regression  is given  by Zyy  —  Lyy2vVLwy.  Thus, 

[Ecal  ez [2uu a  Luly  Evul_  ‘ 

as [2uul- IT,  =  Luv2Zuv2wvul 

=  [Zvul- I] 6;, 

where @, denotes the ith eigenvalue  of I, —  Sar5  Sat.  Recalling  that A; is an  eigenvalue 
of Dol2uw2vwiZvy  associated  with  the  eigenvector  k;, we  have  that 

[1 -  Sad uvSiiSre fk, =  (1 -  Ad 

so  that  6, =  (1 —  A,) is an  eigenvalue  of I, —  Dien ksiies and 

[Zoe a  Zul -T] (1 —  A,). 

Hence,  the  two  expressions  are  equivalent. 

20.4. 

Here,  A, is the  scalar 

and  the  test  statistic  is 

A, =  LAS vdodw 

—T  log(1  —  A,) =  —T logl(2a8)-  Suv yo LWEviSve)l.- 
But @, is the residual  from  a regression  of Ay, on  a constant  and Ay,_,,  Ay,_,,...,  OF, carts 
meaning  that  £,,,  =  62.  Likewise,  9, is the  residual  from  a  regression  of y,_,  on Ay,_,, 
Ay,-2, 
»  Ay,-p41.  The  residual  from  a  regression  of @, on  ¥,, whose  average  squared 
value  is given  by (Zyy  —  LuvZvSvv),  is the  same  as  the  residual  from  a regression  of y, 
on a constant,  y,_,,  and Ay,_,, Ay,_2,.  . 
, AY,-p41,  Whose  average  squared value is denoted 
‘t 

--- 

. 

Hence,  the  test  statistic  is equivalent  to  T[log(dz)  —  log(é)],  as claimed. 

Gov ey Swiviivy)  =  6. 

784  Appendix C | Answers 
to Selected  Exercises 

Chapter 22.  Modeling  Time  Series  with  Changes  in Regime 
sashes. re) x —  Pu)(2  —  pu  —  Pa)  ee 

22.1.  PT  = 

1-  Pu 

(1 —  pu)(2  -  pu  —  Pa) 

ee  a  ae oo 

a  Ween  rt)  Arhatre 

a, 
es 

| 

“A 

le-&RPntnoe  Tht  ayes 
_  [@ -  pxW(2 -  pu  -  Pa)  at 
~  LQ = pudl2 -  Pu — P22) 
=TA. 

A2 

=ry 

Sage  :é 

<2 

9%  24 

; 

~adtee 

S8q)  at  aa  f scitos{org  ise  aolslvqod 
ae oe mana  Lot  iGith@yegy  trsisiidte  noadigr mobsiugoq 
oe cote Cea  Sgney meee 
inviting  weap  of 1X nang Seeds uated to? ebhstee. 

od 
oi tof Wait onsiwevopoms  Taam 
xf 

Sas 

& F 

en 

OFS  vogremion  {pam ath aga)  atLnaV 1s Site’ ere 

_ gan: Laser ET. Sie is is 
tizeate  ie’ e  ogeq) : oidsripy  o2ts ostiwy s 

ait. ecb ieee RAs. al apis  istStatice 

D 

Sate 

~  ‘-an0  pases 

= > 

a 

”, 

D 

go 9 5 3 FX) 

Greek  Letters  and 
Mathematical  Symbols 
Used  in  the  Text 

Greek  letters  and  common  interpretation 

Pre FRR 

population  linear  projection  coefficient  (page  74) 
population  regression  coefficient  (page  200) 
autocovariance  matrix  for  vector  process  (page  261) 
autocovariance  for  scalar  process  (page  45) 
change  in value  of variable  (page  436)  . 
small  number; 
coefficient  on  time  trend  (page  435) 
a white  noise  variable  (page  47) 
constant  term  in ARCH  specification  (page  658) 
 AR(@)  coefficient  (page  79) 

epsilon 
zeta 
eta 
theta 

e¢ 
£ 
7 
© _ matrix  of moving  average  coefficients  (page  262) 
vector  of population  parameters  (page  747) 
® 
scalar  MA(q)  coefficient  (page  50) 
6 
kernel  (page  165) 
kappa 
«__ 
matrix  of eigenvalues  (page  730) 
lambda  A_ 
individual  eigenvalue  (page 729) 
A 
Lagrange  multiplier  (page 135) 
~=population  mean  (page  739) 
# 
v  _ degrees  of freedom  (page  409) 
=  matrix  of derivatives  (page  339) 
€  state  vector  (pages  7 and  372) 
product  (page 747) 
II 
a 
the  number  3.14159.... 
p  _ autocorrelation  (page 49) 

mu 
nu 
xi 

rho 

pi 

, 

autoregressive  coefficient  (page  517) 

sigma 

>,  summation 
2 
@ 
7 

long-run  variance-covariance  matrix  (page  614) 
population  standard  deviation  (page  745) 
time  index 

tau 
upsilon  Y _ scaling  matrix  to calculate  asymptotic  distributions  (page 457) 
phi 

7 

chi 
psi 

omega 

a variable  with  a chi-square  distribution  (page 746) 

®  matrix  of autoregressive  coefficients  (page  257) 
@  scalar  autoregressive  coefficient  (page 53) 
xX 
Y  matrix  of moving  average  coefficients  for vector  MA(~)  process (page 262 
w  moving  average  coefficient  for scalar  MA(*)  process  (page  52) 
{2 
w 

 variance-covariance  matrix  (page  748) 
frequency  (page  153)  © 

/ 

b or  b, 

2 Kh eS = 29 
O,(T) 

a~] 
Ors 

Common  uses  of other  letters 

number  of elements  of  unknown  parameter  vector  (page  135) 
estimated  OLS  regression  coefficient  based  on  sample  of size  T (page  75) 
vector  of constant  terms  for  vector  autoregression  (page  257) 
constant  term  in  univariate  autoregression  (page  53) 
the  ith  column  of the  identity  matrix 
the  base  for  the  natural  logarithms  (page  715) 
the  (nm  x  n) identity  matrix  (page  722) 
the  square  root  of negative  one  (page  708) 
value  of Lagrangean  (page  135) 
the  number  of explanatory  variables  in a regression 
the  lag operator  (page  26) 
value  of log likelihood  function  (page  747) 
number  of variables  observed  at  date  ¢ in a  vector  system  (page  257) 
order  T in probability  (page  460) 
MSE  matrix  for  inference  about  state  vector  (page  378) 
the  order  of an  autoregressive  process  (page  58) 
limiting  value  of (X'X/7)  for X the  (J x  k) matrix  of explanatory  Stee for 
an  OLS  regression  (page  208);  variance-covariance  matrix  of disturbances in 
State  equation  (page  373) 
the  order  of a  moving  average  process  (page  50); number  of autocovariances 
used  in Newey-West  estimate  (page  281) 
variance-covariance  matrix  of disturbances  in observation  equation  (page  373) 
the  set consisting  of all real  n-dimensional  vectors  (page  737) 
number  of variables in  state  equation  (page 372);  index of ps for  a 
continuous-time  process 
unbiased  estimate  of residual  variance  for an  OLS  regression with  sample  of 
size  T (page  203) 
State  at  date  ¢ for  a Markov  chain 

the  number  of dates  included  in a sample 
(T x  k) matrix  of explanatory  variables  for  an  OLS  regression  (page  201) 

history  of observations  through  date  ¢ (page  143) 

argument  of autocovariance  generating  function  (page  61) 

Mathematical  symbols 
aleph  (first  letter  of the  Hebrew  alphabet),  used  for matrix  of regression 
coefficients  (page  636) 
the  number e (the  base  for  natural  logarithms)  raised  to the x power  (page  715) 
natural  logarithm  of x  (page  717) 

x  factorial (page  713) 
annihilation  operator  (page  78) 
greatest  integer  less  than  or  equal  to x 
absolute  value  of a  real  scalar  or  modulus  of a complex  scalar  x (page  709) 
determinant  of a square  matrix  X (page  724) 
transpose  of the  matrix  X (page 723) 
an  (n X  m) matrix  of zeros 
a (1 x  n) row  vector  of zeros 
gradient  vector  (page  735) 
Kronecker  product  (page  732) 

Appendix D | Symbols  Used in the Text  787 

© 
y@x 

element-by-element  multiplication  (page  692) 
y is approximately  equal  to  x 
y is defined  to  be  the  value  represented  by x 
the  value  given  by the  larger  of y or  x 

y=x 
max{ y,x} 
y=  sp f(r)  y is the  smallest  number  such  that  y = f(r)  for  all  r  in  [0,1]  (page  481) 

re(0, 

xGA 

ACB 
P{A} 

fr(y) 

x  is an  element  of A 

A  is a  subset  of B (page  189) 
probability  that  event  A occurs  (page  739) 

probability  density  of the  random  variable  Y (page  739) 

Y ~  N(u,o?) 

Y has  a  N(u,07)  distribution  (page  745) 

Y ~  N(u,o2  ~=Y has a distribution  that  is approximately  N(u,07)  (page  210) 

E(X) 

Var(X) 

Cov(X, Y) 
Corr(X, Y) 
Y|X 
Pcy|x) 
E(Y|X) 

Desi 

xr  y 

Xr>y 
xr > y 
troy 
x7(-)  > x(-) 

x7(-) => x(-) 

expectation  of  X (page  740) 

variance  of  X (page  740) 

covariance  between  X and  Y (page  742) 
correlation  between  X and  Y (page  743) 
Y conditional  on  X (page 741) 
linear  projection  of Y on  X (pages  74-75) 
linear  projection  of Y on  X and  a constant  (pages  74-75) 

linear  projection  of y,,, on  a constant  and a set  of variables  observed  at 
date  t (page 74) 
lim xy  =  y (page  180) 

Xz converges  in probability  to y (pages  181,  749) 
x; converges  in mean  square  to y (pages  182,  749) 
x7 converges  in distribution  to y (page  184) 
the  sequence  of functions  whose  value  at  r is x,(r)  converges  in 
probability  to the  function  whose value  at  r is x(r)  (page  481) 
the  sequence  of functions  whose value  at  r is x7(r) converges  in  - - 
probability  law  to the  function  whose  value  at r is x(r)  (page  481)- 

788  Appendix  D | Symbols Used in the  Text 

Author  Index 

A 
Ahn,  S.  K., 601,  630 
Almon,  Shirley,  360 
Amemiya,  Takeshi,  227,  421 
Anderson,  Brian  D.  O., 47,  373,  403 
Anderson,  T.  W.,  152,  195 
Andrews,  Donald  W.  K.,  190,  191,  197,  284, 
285,  412,  425,  513,  533,  618,  698 

Angrist,  Joshua  D.,  253 
Ashley,  Richard,  109 
B 
Baillie,  Richard  T., 336,  573,  662, 666 
Barro,  Robert  J., 361 
Bates,  Charles,  427,  664 
Bendt,  E.  K.,  142,  661 
Bera,  A.  K., 670,  672 
Bernanke,  Ben,  330,  335 
Betancourt,  Roger,  226 
Beveridge,  Stephen,  504 
Bhargava,  A.,  532 
Billingsley,  Patrick,  481 
Black,  Fischer,  672 
Blanchard,  Olivier,  330,  335 
Bloomfield,  Peter,  152 
Blough,  Stephen  R., 445,  562 
Bollerslev,  Tim,  658,  661,  662,  663,  665,  666, 

667,  668,  670,  671,  672 

Bouissou,  M.  B., 305 
Box,  George  E.  P., 72,  109-10,  111,  126,  132, 

133 
Brock,  W.  A., 479 

Cai, Jun,  662 
Caines,  Peter  E., 387,  388 
Campbell,  John  Y., 444,  516,  573 
Cao,  Charles  Q., 666 

G., 532 

Clark,  Peter  K.,  444 
Cochrane,  John  H.,  444,  445,  531 
Cooley,  Thomas  F.,  335 
Corbae,  Dean,  573 
Cox,  D.  R.,  126,  681,  685 
Cramér,  Harald,  157,  184,  411,  427 
D 
Davidon,  W.  C.,  139-42 
Davidson,  James  E.  H.,  571,  572,  581 
Davies,  R.  B., 698 
Day,  Theodore  E., 672 
DeGroot,  Morris  H.,  355,  362 
DeJong,  David  N., 533 
Dempster,  A.  P., 387,  689 
Dent,  Warren,  305 
Diamond, Peter,  335 
Dickey,  David  A., 475,  483,  493,  506,  516, 

530,  532 

Diebold,  Francis  X., 361,  449,  671,  690,  691 
Doan,  Thomas  A.,  362,  402-3 
Durbin,  James,  226 
Durland,  J. Michael,  691 
Durlauf,  Steven  N., 444,  547,  587 
E 
Edison,  Hali  J., 672 
Eichenbaum,  Martin,  445 
Eicker, F., 218 
Engle,  Robert  F., 227,  387, 571,  575,  596,  661, 
|  - 
664,  667,  668,  670,  671,  672,698 

Evans,  G.  B.  A., 216,  488,  516 
Everitt,  B. S., 689 
F 
Fair,  Ray C., 412,  425,  426  . 
Fama,  Eugene  F., 306,  360,  376 
Feige,  Edgar L., 305 
Ferguson,  T.  S., 411 
Ferson,  Wayne  E., 664 
Filardo,  Andrew  J., 690 
Fisher,  Franklin  M., 246,  247  ~ 
Flavin,  Marjorie  A., 216,  335 
Fletcher,  R.,  139-42 
Friedman,  Milton,  440 
Fuller,  Wayne  A.,  152,  153,  164, 275, 454, 

| 

464,  475, 476, 483, 488, 493, 506,  516 

  665,668 

8

,

Lawrence J., 305, 445, 447,  653 

G 

‘ 

Gagnon,  Joseph  E., 444 
Galbraith,  J. I., 124,  133 

Galbraith,  R.  F.,  124,  133 
Gallant,  A.  Ronald,  283,  412,  421,  427,  431, 

672 

Garber,  Peter  M.,  426 
Gevers,  M.,  388 
Geweke,  John,  305,  313-14,  365,  449,  670 
Ghosh,  Damayanti,  389 
Ghysels,  Eric,  426 
Giannini,  Carlo,  334,  336,  650 
Gibbons,  Michael  R.,  376 
Glosten,  Lawrence  R.,  663,  669,  672 
Goldfeld,  Stephen  M.,  1, 3 
Gonzalez-Rivera,  Gloria, 672 
Goodwin,  Thomas  H.,  698 
Gourieroux,  C.,  431,  670 
Granger,  C.  W.  J.,  126,  291,  302-9,  448,  449, 

‘557,  571,  581-82 
Gregory,  Allan  W.,  214 
Griffiths,  William  E.,  142,  148,  227 
H 
Hall,  Alastair,  426,  427,  530,  532 
Hall,  B.  H.,  142,  661 
Hall,  P., 481,  482 
Hall,  R.  E.,  142,  361,  661 
Hamilton,  James  D.,  307,  388,  397,  444,  662, 

689,  695,  698 
Hammersley,  J. M.,  365 
Hand,  D.  J., 689 
Handscomb,  D.  C.,  365 
Hannan,  E.  J., 47,  133,  388 
Hansen,  Bruce E., 589,  596,  601,  613-18, 651, 

698 

Hansen,  Lars  P., 67, 218,  280,  335,  409,  411, 
412,  414,  415,  419,  421,  422,  424 

Harvey,  A.  C., 226,  386 
Haug,  Alfred  A.,  596 
Haugh,  Larry  D., 305 
Hausman,  J. A.,  142,  247,  661 
Hendry,  David  F., 571,  572,  581 
Heyde,  C.  C., 481,  482 
Higgins,  M.  L., 670, 672 
Hill,  R.  Carter,  142,  148,  227 
Hoel,  Paul  G., 704 
Hoerl,  A.  E., 355 
Hong,  Y.  S., 671,  672 
Hood,  William  C., 638 
Hosking,  J. R.  M.,  448 
Hsieh,  David  A., 662,  672 
I 
Imhof,  J. P., 216 
J 
Jagannathan,  R., 663,  669,  672 
Janacek,  G. J., 127 
Jeffreys,  H., 533 
Jenkin,  Gwilym M., 72,  109-10,  111,  132, 

Johansen,  Sgren,  590,  601,  635-36,  640, 646, 

649, 650,  651 

Johnston,  J., 704 
rn signe D. W., 421 
orion,  Philippe,  662 
Joyeux, Roselyrie,  448 
Judge,  George  G., 142,  148, 227 
K 
Kadane,  Joseph  B., 363-65 
Kalman,  R.  E., 372 

~-- 

790  Author  Index 

Kane,  Alex,  672 
Keating,  John  W.,  335 
Kelejian,  Harry,  226 
Kennard,  R.  W.,  355 
Kiefer,  Nicholas  M.,  689 
Kim,  K.,  513,  516 
Kinderman,  A.  J.,  216 
King,  Robert  G.,  426,  573 
Kloek,  T.,  365 
Kocherlakota,  N.  R., 426 
Koopmans,  T.  C., 638 
Koreisha,  Sergio,  133 
Kremers,  J. J.  M.,  573 
Kroner,  Kenneth  F., 658,  665,  668,  670 
Kwiatkowski,  Denis,  532 
L 
Laffont,  J. J., 305,  421 
Lafontaine,  Francine,  214 
Laird,  N.  M.,  387,  689 
Lam,  Pok-sang,  450,  532,  691 
Lamoureux,  Christopher  G., 672 
Lastrapes,  William  D.,  672 

.  Leadbetter,  M.  R.,  157 

Leamer,  Edward,  335,355 
Lee,  Joon-Haeng,  690,  691 
Lee,  Tsoung-Chao,  142,  148,  227 
LeRoy,  Stephen  F., 335 
Lewis,  Craig  M.,  672 
Li, W.  K.,  127 
Lilien,  David  M.,  667,  672 
Litterman,  Robert  B., 360-62,  402-3 
Ljungaqvist,  Lars,  305,  447,  653 
Lo,  Andrew  W., 449,  531,  532 
Loretan,  Mico,  608,  613 
Lucas,  Robert  E., Jr.,  306 
Litkepohl,  Helmut,  336,  339 
M 
McCurdy,  Thomas  H., 691 
MacKinlay,  A.  Craig,  531,  532 
McLeod,  A.  I.,  127 
Maddala,  G.  S., 250 
Magnus,  Jan  R., 302,  317,  318,  704 
Makov,  U.  E., 689 
Malinvaud,  E., 411 
Malliaris,  A.  G., 479 
Mankiw,  N.  Gregory,  361,  444 
Mark,  Nelson,  664 
Marsden,  Jerrold  E.,  196,  704 
Martin,  R.  D.,  127 
Meese,  Richard,  305 
Milhgj,  Anders,  662,  670 
Miller,  H.  D., 681,  685 
Monahan,  J. Christopher, 284, 285, 618 
Monfort,  A., 431,  670 
Moore,  John B., 47, 373,  403 
Mosconi,  Rocco, 650 
Mustafa,  C., 672 
Muth,  John  F., 440 
N 
Nason,  James  A.,  361 
Nelson,  Charles  R., 109, 253, Ny es 504 
Nelson,  Daniel  B., ‘662, 666, 667, 668, 672 
Nelson, Harold fe 126 
Nerlove,  Mark,  671 

Newey, Whitney K., 220, 281-82, 284, 414: 

Ng,  Victor  K.,  668,  671,  672 
Nicholls,  D.  F.,  218 
Noh,  Jason,  672 
O 
Ogaki,  Masao,  424,  573,  575,  618,  651 
Ohanian,  Lee  E.,  554 
O’Nan,  Michael,  203,  704 
Ouliaris,  Sam,  573,  593,  601,  630 
P 
Pagan,  A.  R.,  218,  389,  668,  671,  672 
Pantula,  S.  G.,  532,  670 
Park,  Joon  Y.,  214,  483,  532,  547,  549,  573, 

575,  601,  618,  651 
Pearce,  Douglas  K.,  305 
Perron,  Pierre,  449,  475,  506-16 
Phillips,  G.  D.  A.,  386 
Phillips,  Peter  C.  B.,  195,  214,  475,  483,  487, 
506-16,  532,  533,  534,  545,  547,  549, 
554,  557,  576-78,  587,  593,  601,  608, 
613-18,  630,  650,  651 

Pierce,  David  A.,  305 
Ploberger,  Werner,  698 
Plosser,  Charles  I., 444,  573 
Port,  Sidney  C.,  749 
Porter-Hudak,  Susan,  449 
Powell,  M.  J.  D.,  139-42 
Pukkila,  Tarmo,  133 

Q 
Quah,  Danny,  335 
Quandt,  Richard  E.,  142 
R 
Ramage,  J. G., 216 
Rao,  C.  R., 52,  183,  184,  204 
Rappoport,  Peter,  449 
Raymond,  Jennie,  664 
Reichlin,  Lucrezia,  449 
Reinsel,  G.  C., 601,  630 
Rich,  Robert  W.,  664 
Rissanen,  J.,  133 
Robins,  Russell  P., 667,  672 
Rogers, John  H., 677 
Rothenberg,  Thomas  J., 247,  250,  334,  362, 

388,  411 

Rothschild,  Michael,  671 
Royden,  H.  L.,  191 
Rubin,  D.  B., 387,  689 
Rudebusch,  Glenn  D., 449 
Runkle,  David  E., 337, 339, 663, 669,  672 
Ruud,  Paul  A., 250 
RY 
Said,  Said  E., 530,  532 

Saikkonen,  Pentti, 608 

Sargan,  J. D.,  532 

,  Thomas  J., 33, 34,  39,  41, 67, 78, 
109, 335,  423 
Savin,  N.  E., 216,  488,  516 

Schmidt,  Peter, $13, 516, 532 
Scholes,  Myron,  672 
Schwert,  G. William, 513, 516, 668, 672 
Selover,  David  D., 573 
Shapiro, 

Shier 

tthew D., 335 

Shumway,  R.  H.,  387 
Sill,  Keith,  426 
Simon,  David  P.,  664 
Sims,  Christopher  A.,  291,  297,  302,  304,  330, 
402-3,  445,  454,  455,  464,  483,  532-34, 
547,  549,  555 

Singleton,  Kenneth  J., 422,  424 
Smith,  A.  F.  M.,  689 
Solo,  Victor,  195,  483,  532,  545,  547 
Sowell,  Fallaw,  449,  532 
Srba,  Frank,  571,  572,  581 
Startz,  Richard,  253,  427 
Stinchcombe,  Maxwell,  482,  698 
Stock,  James  H.,  305,  376,  444,  445,  447,  454, 

455,  464,  483,  532,  533,  547,  549,  555, 
573,  578,  587,  601,  608,  613,  630,  653 

Stoffer,  D.  S., 387 
Stone,  Charles  J., 704 
Strang,  Gilbert,  704 
Susmel,  Raul,  662 
Swift,  A.  L.,  127 
T 
Tauchen,  George,  426,  427,  671 
Taylor,  William  E., 247 
Theil,  Henri,  203,  359,  704 
Thomas,  George  B., Jr.,  157,  166,  704 
Tierney,  Luke,  363-65 
Titterington,  D.  M., 689 
Toda,  H.  Y.,  554,  650 
Trognon,  A., 431 
U 
Uhlig,  Harald,  532-34 
V 
van  Dijk,  H.  K., 365 
Veall,  Michael  R., 214 
Vuong,  Q. H.,  305 
W 
Wall,  Kent  D., 386,  388 
Watson,  Mark  W., 305,  330, 335,  376,  387, 

389,  444,  447, 454,  455,  464,  483,  547, 
549,  555,  573,  578,  601, 608,  613,  630, 
653 

Wei,  C. Z., 483,  532 
Weinbach,  Gretchen  C., 690,  691 
Weiss,  Andrew  A., 663 
Wertz,  V., 388 
West,  Kenneth  D., 220, 281-82,  284,  414, 

555,  647,  672 

White,  Halbert,  126,  144,  145,  185,  189,  193, 
196,  218, 280, 281,  282, 412,  418, 427, 
428,  429, 431, 482,  664,  698 

White,  J. S., 483 
White,  Kenneth  J., 214 
Whiteman,  Charles  H., 2°  533 

‘  Wold,  Herman,  108-9,  1c 

Wooldridge,  Jeffrey  M., 431, 590, 591, 608, 

663, 671, 672 

¥ 

Yeo, Stephen,  571, 572, 581 
Yoo,  Byung  Sam,  575, 596 
L 
Zellner,  Arnold,  315, 362 

Zuo,  X., 672 

Author  Index  791 

Subject  Index 

A 
Absolute  summability,  52, 64 
autocovariances  and,  52 
matrix  sequences  and,  262,  264 

Absorbing  state,  680 
Adaptive  expectations,  440 
Adjoint,  727 
Aliasing,  161 
Amplitude,  708 
Andrews-Monahan  standard  errors,  285 
Annihilation  operator,  78 
AR.  See Autoregression 
ARCH.  See Autoregressive  conditional  — 

heteroskedasticity 

Argand  diagram,  709 
ARIMA.  See Autoregressive  integrated 

moving  average 

ARMA.  See Autoregressive  moving  average 
Asset  prices,  360,  422,  667 
Asymptotic  distribution.  See also  Convergence 

autoregression  and,  215 
GMM  and,  414-15 
limit  theorems  for serially  dependent 

observations,  186-95 

review  of, 180-86 
time  trends  and,  454-60 
of 2SLS  estimator,  241-42 
unit  root  process  and,  475-77,  504-6 
vector  autoregression  and,  298-302 

Autocorrelation: 

of a covariance-stationary  process,  49  . 
GLS  and,  221-22 
partial,  111-12 
sample,  110-11 
Autocovariance,  45 

matrix,  261 
population  spectrum  and,  155 
vector  autoregression  and, 264-66 

Autocovariance-generating  function,  61-64 

factoring 

Autoregression  (AR).  See also  Unit  root 

process;  Vector  autoregression 

first  order,  53-56,  486-504 . 
forecasting, 80-82 
maximum  likelihood  estimation  for 

Gaussian,  118-27 

parameter  estimation,  215-17 

792 

pth order,  58-59 
second  order,  56-58 
sums  of,  107-8 

Autoregressive  conditional  heteroskedasticity 

(ARCH): 
ARCH-M,  667 
comparison  of alternative  models,  672 
EGARCH,  668-69 
GARCH,  665-67 
Gaussian  disturbances,  660-61 
generalized  method  of moments,  664 
IGARCH,  667 
maximum  likelihood,  660-62 
multivariate  models,  670-71 
Nelson’s  model,  668-69 
non-Gaussian  disturbances,  661-62 
nonlinear  specifications,  669-70 
nonparametric  estimates,  671 
quasi-maximum  likelihood,  663-64 
semiparametric  estimates,  672 
testing  for, 664-65 

Autoregressive  integrated  moving  average 

(ARIMA),  437 

Autoregressive  moving  average  (ARMA): 
autocovariance-generating  function,  63 
autoregressive  processes,  53-59 
Tesi stationarity,  and  ergodicity, 

forecasting,  83-84 
invertibility,  64-68 
maximum  likelihood  estimation  for Gaussian 

process,  132-33 

mixed  processes,  59-61 
moving  average  processes,  48-52 
non-Gaussian,  127 
parameter  estimation,  132,  387 
population  spectrum  for,  155 
sums  of, 102-8 
white  noise  and,  47-48 

B 
Bandwidth,  165, 671 
Bartlett  kernel,  167, 276-77 
Basis,  cointegrating  vectors  and, 574  _ 

estimating  regression  model  with  lagged 

dependent  variables,  358 

estimating  regression  model  with  unknown 

variance,  355-58 

introduction  to,  351-60 
mixture  distributions,  689 
Monte  Carlo,  365-66 
numerical  methods,  362-66 
posterior  density,  352 
prior density,  351-52 
regime-switching  models,  689 
unit  roots,  532-34 
vector  autoregression  and,  360-62 

Bayes’s  law,  352 
Beveridge-Nelson  decomposition,  504 
Bias,  741 

simultaneous  equations,  233-38 

Block  exogeneity,  309,  311-13 
Block  triangular  factorization,  98-100 
Bootstrapping,  337 
Box-Cox  transformation,  126 
Box-Jenkins  methods,  109-10 
Brownian  motion,  477-79 

differential,  547 
standard,  478,  544 

'  Bubble,  38 

Business  cycle  frequency,  168-69 

Cc 
Calculus,  711-21 
Canonical  cointegration,  618 
Canonica!  correlation: 
population,  630-33 
sample,  633-35 

. 

Cauchy  convergence,  69-70 
Cauchy-Schwarz  inequality,  49, 745 

-  Central  limit  theorem,  185-86 

Martingale  difference  sequence,  193-95 
stationary  stochastic  process,  195 

Chain  rule, 712 
Chebyshev’s  inequality,  182-83 
Chi-square  distribution,  746,  753 
Cholesky  factorization,  91-92,  147 
.  Cochrane-Orcutt  estimation,  224,  324 
Coefficient  of relative  risk  aversion,  423 
Coherence,  population,  275 
Cointegrating  vector,  574,  648-50 
Cointegration,  571 

basis,  574 
canonical,  618 
cointegrating  vector,  574, 648-50 
common’ trends  representation  (Stock- 

PR  ing 3 ian 82 
error-correction  representation,  580-81 
tation  theorem,  581-82 

re 

' 

motivation  for  canonical  correlations, 

639-42 

motivation  for  parameter  estimates, 

642-43 

parameter  estimates,  637-38 
population  canonical  correlations,  630-33 
sample  canonical  correlations,  633-35 
without  deterministic  time  trends,  643-45 

Complex: 

congugate,  710 
numbers,  708-11 
unit  circle,  709 

Concentrated  likelihood,  638 
Conditional  distributions,  741-42 
Conditional  expectation,  742 
for  Gaussian  variables,  102 

Conditional  likelihood,  vector  autoregression 

.  and,  291-93 
Conjugate  pair,  710 
Conjugate  transposes,  734-35 
Consistent,  181,  749 
Consumption  spending, -361,  572,  600-1, 

610-12,  650 

Continuity,  711 
Continuous  function,  711,  735 
Continuous  mapping  theorem,  482-83 
Continuous  time  process,  478 
Convergence: 

Cauchy  criterion,  69-70 
in distribution,  183-85 
Kalman  filter  and,  389-90 
limits  of deterministic  sequences,  180 
in mean  square,  182-83,  749 
for  numerical  optimization,  134,  137 
in probability,  181-82,  749 
of random  functions,  481 
ordinary,  180 
weak,  183 
Correlation: 

canonical,  630-35 
population,  743 
Cosine,  704,  706-7 
Cospectrum,  271-72 
Covariance: 

population,  742 
triangular  factorization  and,  114-15 
Covariance  restrictions,  identification  and, 

246-47 

Covariance-stationary,  45-46,  258 

law of large numbers  and,  186-89 

Cramér-Wold  theorem,  184 
Cross  spectrum,  270 
Cross  validation,  671 

D 
Davidon-Fletcher-Powell,  139-42 
De Moivre’s  theorem,  153, 716-17 
Density/ies,  739.  See also Distribution 

unconditional,  44 

27. 
Determinant, 
time trends. See Time trends 
"Deterministic 

of block  diagonal  matrix,  101 

Subject Index 

793 

Dickey-Fuller  test,  490,  502,  528-29,  762-64 

augmented,  516,  528 
F test,  494,  524 
Difference  equation: 

dynamic  multipliers,  2-7 
first-order,  1-7,  27-29 
pth-order,  7-20,  33-36 
repeated  eigenvalues,  18-19 
second-order,  17,  29-33 
simulating,  10 
solving  by recursive  substitution,  1-2 

Difference  stationary,  444 
Distributions,  739.  See  also  Asymptotic 

distribution 
chi-square,  746,  753 
conditional,  741-42 
convergence  in,  183-85 
F, 205-7,  357,  746,  756-60 
gamma,  355 
Gaussian,  745-46,  748-49,  751- 52. 
generalized  error,  668 
joint,  741 
joint  density-,  686 
marginal,  741 
mixture,  685-89 
Normal,  745-46,  748-49,  751-52 
posterior,  352 
prior,  351-52 
probability,  739 
t, 205,  356-57,  409-10,  746,  755 

Duplication  matrix,  301 
Dynamic  multipliers,  2-7,  442-44 
calculating  by simulation,  2-3 

E 
Efficient  estimate,  741 
Efficient  markets  hypothesis,  306 
Eigenvalues,  729-32 
Eigenvectors,  729-30 
Elasticity,  logarithms  and,  717-18 
EM  algorithm,  688-89,  696 — 
Endogenous  variables,  225-26 
Ergodicity,  46-47 
Ergodic  Markov  chain,  681- 82 
Error-correction  representation,  580-81 
Euler  equations,  422 
Euler  relations,  716-17 
Exchange  rates,  572,  582-86,  598,  647-48 
Exclusion  restrictions,  244 
Expectation,  740 
adaptive,  440 
conditional,  72-73,  742 
of infinite  sum,  52 
stochastic  processes  and,  43-45 

— 

Exponential  functions,  714-15 
Exponential  smoothing,  440 
F 
F distribution,  205-7,  357,  746, 756-60 
Filters,  63-64,  169-72,  277-79.  See also 

Kalman  filter 
multivariate,  264 

FIML.  See Full-information  maximum 

likelihood 

First-difference  operator,  436 
First-order  autoregressive  process, 53-56 

asymptotic  distribution  and, md, 215, 486-504 

First-order  difference  equations, 1-7 

lag operators  and, 27-29 

First-order  moving  average,  48-49 

794  Subject Index 

Fisher  effect,  651 
Forecasts/forecasting: 

ARMA  processes,  83-84 
AR  process,  80-82 
Box-Jenkins  methods,  109-10 
conditional  expectation  and,  72-73 
finite  number  of observations  and,  85-87 
for  Gaussian  processes,  100-102 
infinite  number  of observations  and,  77-84 
Kalman  filter  and,  381-85 
linear  projection  and,  74-76,  92-100 
macroeconomic,  109 
MA  process,  82-83,  95-98 
Markov  chain  and,  680 
for  noninvertible  MA,  97 
nonlinear,  73,  109 
unit  root  process  and,  439-41 
vectors,  77 

Fractional  integration,  448-49 
Frequency,  708 
Frequency  domain.  See  Spectral  analysts 
Full-information  maximum  likelihood  (FIML), 
247-50,  331-32.  See  also  Cointegration, 
full-information  maximum  likelihood 
and 

Functional  central  limit  theorem,  479-86 
Fundamental  innovation,  67,  97,  260 
G 
Gain,  275 

Kalman,  380 

Gamma  distribution,  355 
Gamma  function,  355 
Gaussian: 

distribution,  745-46,  748-49,  751-52 
forecasting,  100-102 
kernel,  671  _ 
maximum  likelihood  estimation  for Gaussian 

ARMA  process,  132-33 

° 

maximum  likelihood  estimation  for  Gaussian 

AR  process,  118-27 

maximum  likelihood  estimation  for Gaussian 

MA  process,  127-31 

process,  46 
white  noise,  25, 43, 48 

Gauss-Markov  theorem,  203,  222 
Generalized  error  distribution,  668 
Generalized  least  squares  (GLS): 

autocorrelated  disturbances,  221-22 
covariance  matrix  and,  220-21 
estimator,  221 
heteroskedastic  disturbances,  221 
maximum  likelihood  estimation  and,  222 
Generalized  method  of moments  (GMM): 

ARCH  models,  664 
asymptotic  distribution  of, 414-15 
estimation  by, 409-15 
estimation  of dynamic  rational  expectation 

models,  422-24 
examples  of, 415-24 
extensions,  424-27 
identification  (econometric)  and,  426 
information  matrix  equality,  429 
instrumental  variable  estimation,  418-20 
instruments  of choice  for, 426-27 
maximum  likelihood  estimation  and, 427-31 
ge  ~ ae of simultaneous equations, 

Bes 

nonstationary  data,  424 
optimal  weighting  matrix,  412-14 

Chebyshev’s,  182-83 
Hdider,  197 
triangle,  70 

Inequality  constraints,  146-48 
Infinite-order  moving  average,  51-52 
Information  matrix,  143-44 

equality,  429 

Innovation,  fundamental,  67 
Instrumental  variable  (IV)  estimation,  242-43, 

418-20 

Instruments,  238,  253,  426-—27 
Integrals: 

definite,  719-21 
indefinite,  718-19 
multiple,  738-39 

Integrated  of order  d, 437,  448 
Integrated  process,  437.  See  also  Unit  root 

process 
fractional,  448-49 

Integration,  718 

constant  of, 719 

Interest  rates,  376,  501,  511-12,  528,  651 
Invertibility,  64-68 
IV.  See  Instrumental  variable  (IV) estimation 
J 
Jacobian  matrix,  737 
Johansen’s  algorithm,  635-38 
Joint  density,  741 
Joint  density-distribution,  686 
Jordan  decomposition,  730-31 
K 
Kalman  filter: 

; 

autocovariance-generating  function  and, 

391-94 

background  of, 372 
derivation  of, 377-81 
estimating  ARMA  processes,  387 
forecasting  and,  381-85 
gain matrix,  380 
identification,  387-88 
MA(1)  process  and,  381-84 
maximum  likelihood  estimation  and, 

“slp 

parameter  uncertainty, 
quasi-maximum  likelihood  and,  389 
smoothing  and,  394-97 
state-space  representation  of dynamic 

398 

system,  372-77 

statistical  inference  with,  397-98 
steady-state,  389-94 
time-varying  parameters,  399-403 
Wold  representation  and,  391-94 
Kernel  estimates,  165-67.  See also 

Nonparametric  estimation 

ordinary  least  squares  and,  416-18 
orthogonality  conditions,  411 
overidentifying  restrictions,  415 
specification  testing,  415,  424-26 
testing  for  structural  stability,  424-26 
two-stage  least  squares  and,  420-21 

Geometric  series,  713,  732 
Global  identification,  388 
Global  maximum,  134,  137,  226 
GLS.  See  Generalized  least  squares 
GMM.  See  Generalized  method  of moments 
GNP.  See  Gross  national  product 
Gradient,  735-36 
Granger  causality  test,  302-9 
Granger  representation  theorem,  582 
Grid  search,  133-34 
Gross  national  product,  112,  307,  444,  450, 
697-98.  See  also  Business  cycle 
frequency;  Industrial  production; 
Recessions 

. 

H 
Hessian  matrix,  139,  736 
Heteroskedasticity,  217-20,  227.  See also 
Autoregressive  conditional 
heteroskedasticity  (ARCH);  Newey- 
West  estimator 

consistent  standard  error,  219,  282-83 
GLS  and,  221 

HGlder’s  inequality,  197 
: 
Hypothesis  tests: 

cointegration  and,  601-18,  645-50 
efficient  score,  430 
Lagrange  multiplier,  145,  430 
likelihood  ratio,  144-45,  296-98 
linear  restrictions,  205 
nonlinear  restrictions,  214,  429-30 
time  trends  and,  461-63 
Wald,  205,  214,  429-30 

I 
I(d). See  Integrated  of order  d 
Idempotent,  201 
Identification,  110,  243-46 

covariance  restrictions,  246-47 
exclusion  restrictions,  244 
global,  388 
GMM  and,  426 
just identified,  250 
Kalman  filter  and,  387-88 

univariate system,  5 
vector  autore 

and, 318-23 

pth-order  difference  equations  and,  33-36 
purpose  of,  26 
second-order  difference  equations  and, 

29-33 

Lagrange  multiplier,  135,  145,  430 
Law  of iterated  expectations,  742 
Law  of iterated  projections,  81,  100 
Law  of large  numbers,  183,  749 

covariance-stationary  processes,  186-89 
mixingales,  190-92 
Leverage  effect,  668 
Likelihood  function,  746-47.  See  also 

Maximum  likelihood  estimation  (MLE) 

concentrating,  638 
vector  autoregression  and,  291-94,  310-11 
Likelihood  ratio  test,  144-—45,'296-98,  648-50 
Limit.  See  Convergence 
Linear  dependence,  728-29 

Geweke’s  measure  of, 313-14 

Linearly  deterministic,  109 
Linearly  indeterministic,  109 
Linear  projection: 

forecasts  and,  74-76,  92-100 
multivariate,  75 
ordinary  least  squares  regression  and, 

75-76,  113-14 
properties  of, 74-75 
updating,  94 

Linear  regression.  See  also  Generalized  least 

squares  (GLS);  Generalized  method  of 
moments  (GMM);  Ordinary  least 
squares  (OLS) 
algebra  of, 200-202 
review  of OLS  and  i.i.d.,  200-207 

Local  identification,  334,  388 
Local  maximum,  134,  137,  226 
Logarithms,  717-18 
Long-run  effect,  6-7 
Loss  function,  72 

M 
MA.  See  Moving  average 
Markov  chain,  678 

absorbing  state,  680 
ergodic,  681-82 
forecasting,  680 
periodic,  685 
reducible,  680 
transition  matrix,  679 
two-state,  683-84 
vector  autoregressive  representation,  679 

Martingale  difference  sequence,  189-90, 

193-95 

Matrix/matrices: 
adjoint,  727 
conjugate  transposes,  734-35 
determinant,  724-27 
diagonal,  721 
duplication,  301 
gain,  380 
geometric  series,  732 
Hessian,  139,  736 
idempotent,  201 
identity,  722 
information,  143-44,  429 
inverse,  727-28 
Jacobian,  737 
Jordan pn aa 730-31 
lower  triangular, 725 

796  Subject Index 

nonsingular,  728 
optimal  weighting,  412-14 
partitioned,  724 
positive  definite,  733-34 
positive  semidefinite,  733 
power  of, 722 
singular,  728 
square,  721 
symmetric,  723 
trace  of,  723-24 
transition,  679 
transposition,  723 
triangular,  729 
triangular  factorization,  87 
upper  triangular,  727 

Maximum  likelihood  estimation  (MLE),  117, 

747.  See  also  Quasi-maximum  likelihood 

asymptotic  properties  of,  142-45,  429-30 

'  concentrated,  638 

conditional,  122,  125-26 
EM  algorithm  and,  688-89 
full-information  maximum  likelihood, 

247-50 

Gaussian  ARMA  process  and,  132-33 
Gaussian  AR  process  and,  118-27 
Gaussian  MA  process  and,  127-31" 
general  coefficient  constraints  and,  315-18 
global  maximum,  134,  137 
GLS  and,  222 
GMM  and,  427-31 
Kalman  filter  and,  385-89 
local,  134,  137 
prediction  error  decomposition,  122,  129 
regularity  conditions,  427,  698 
standard  errors  for,  143-44,  429-30 
Statistical  inference  with,  142-45 
vector  autoregression  and,  291-302,  309-18 
Wald  test  for,  429-30 

Mean: 

ergodic  for  the,  47 
population,  739 
sample,  186-95,  279-85,  740-41 
unconditional,  44 

Mean  square,  convergence  in, 182-83,  749 
Mean  squared  error  (MSE),  73 

of linear  projection,  74, 75, 77 

Mean-value  theorem,  196 
Mixingales,  190-92 
Mixture  distribution,  685-89 
MLE.  See  Maximum  likelihood  estimation 

(MLE) 
Modulus,  709 
Moments.  See also Generalized  method of 

moments  (G 

population,  739-40,  744-45 
posterior,  363-65 
sample,  740-41 
second,  45, 92-95,  192-93 

Money  demand,  1, 324 
Monte  Carlo  method, 216,  337,  365-66,  398 
weet average  (MA): 

cointegration  and, 574-75 
first  order, 48-49 
forecasting,  82-83,  95-98 
infinite  order,  51-52 
maximum  likelihood  estimation  for 

Gaussian,  127-31,  387 
parameter  estimation,  132, 387 
= pov 

reengh  thape for, 154-55, 276 

sums  of,  102-7 
vector,  262-64 

MSE.  See  Mean  squared  error  (MSE) 
N 
Newey-West  estimator,  220,  281-82 
Newton-Raphson,  138-39 
Nonparametric  estimation.  See  also  Kernel 

bandwidth,  165,  671 
conditional  variance  and,  671 
cross  validation,  671 
population  spectrum,  165-67 

Nonsingular,  728 
Nonstochastic,  739 
Normal  distribution,  745-46,  748-49,  751-52 
Normalization,  cointegration  and,  589 
Numerical  optimization: 

convergence  criterion,  134,  137 
Davidon-Fletcher-Powell,  139-42 
EM  algorithm,  688-89,  696 
grid search,  133-34 
inequality  constraints,  146-48 
Newton-Raphson,  138-39 
numerical  maximization,  133,  146 
numerical  minimization,’  142 
steepest  ascent,  134-37 

O 
Observation  equation,  373 
Oil  prices,  effects  of, 307-8 
OLS.  See  Ordinary  least  squares 
O,. See Order  in prabability 
Operators: 

annihilation,  78 
first-difference,  436 
time series, 25-26 

Option prices,  672 

.  Order in  probability, 460 

Ordinary  least  squares  (OLS).  See also 
Generalized  least  squares  (GLS); 
Hypothesis  tests;  Regression 

algebra  of, 75-76,  200-202 
autocorrelated  disturbances,  217,  282-83 
chi-square  test,  213 
distribution  theory,  209,  432-33 
estimated  coefficient  vector,  202-3 
F test, 205-7 
GMM  and,  416-18 
heteroskedasticity,  217,  282-83 
linear  projection and,  75-76,  113-14 
non-Gaussian  disturbances,  209 
time  trends  and,  454-60 

Phase,  275,  708 
Phillips-Ouliaris-Hansen  tests,  599 
Phillips-Perron  tests,  506-14,  762—63- 
Phillips  triangular  representation,  576-78 
Plim,  181,  749 
Polar  coordinates,  704-5,  710 
Polynomial  in lag operator,  27,  258 
Population: 

canonical  correlations,  630-33 
coherence,  275 
correlation,  743 
covariance,  742 
moments,  739-40,  744-45 
spectrum,  61-62,  152-57,  163-67,  269, 

276-77 
Posterior  density,  352 
Power  series,  714 
Precision,  355 
Predetermined,  238 
Prediction  error  decomposition,  122,  129,  310 
Present  value,  4, 19-20 
Principal  diagonal,  721 
Prior  distribution,  351 
Probability  limit,  181,  749 
pth-order  autoregressive  process,  58-59 
pth-order  difference  equations,  7— 20, 33-36, 
Purchasing  power  parity.  See Exchange  rates 
Q 
qth-order  moving  average,  50-51 
Quadratic  equations,  710-11 
Quadratic  spectral  kernel,  284 
Quadrature  spectrum,  271 
Quasi-maximum  likelihood  oxtimaita 126,  145, 

430-31. 

GMM and,  430-31 
Kalman  filter  and,  389 
standard  errors,  145 

R 
Radians,  704 
Random  variable,  739 
Random  walk,  436.  See also  Unit  root  process 

via 

~ 

OLS  estimation,  486-504 
Rational  expectations,  422 

erry 
efficient  markets  hypothesis,  306. 

Real  interest  rate,  376 
Real  number,  708 
Recessions,  167-68,  307-8, ‘450, 697-98 
Recursive  substitution,  1-2 
Reduced  form, 245-46,  250-52 

VAR,  327,  329 

Reducible  Markov  chain,  680 
Regime-switching models: 
_  Bayesian  estimation,  689 

derivation  of equations,  692-93 
description  of, 690-91 
EM  algorithm,  696 
jn  — 692, 695-96 

univariate,  158-63  ~ 

°~ 

Subject Index  797 

Residual  sum  of squares  (RSS),  200 
Ridge  regression,  355 
RSS.  See  Residual  sum  of squares 
R?,  202 
S 
Sample  autocorrelations,  110-11 
Sample  canonical  correlations,  633-35 
Sample  likelihood  function,  747 
Sample  mean: 

definition  of,  741 
variance  of,  188,  279-81 

Sample  moments,  740-41 
Sample  periodogram,  158-63,  272-75 
Scalar,  721 
Score,  427-28 
Seasonality,  167-69 
Second  moments,  45,  92-95 

consistent  estimation  of,  192-93 

Second-order  autoregressive  process,  56-58 
Second-order  difference  equations,  17, 29-33 
Seemingly  unrelated  regressions,  315 
Serial  correlation,  225-27 
Sims-Stock-Watson: 

scaling  matrix,  457 
transformation,  464,  518 

Simultaneous  equations.  See also  Two-stage 

least  squares 
bias,  233-38,  252-53 
estimation  based  on  the  reduced  form, 

250-52 

full-information  maximum  likelihood 

estimation,  247-50 

identification,  243-47 
instrumental  variables  and  two-stage  least 

squares,  238-43 

nonlinear  systems  of, 421-22 
overview  of, 252-53 

Sine,  704,  706-7 

~  Singular,  728 
_.  Singularity,  689 
Sinusoidal,  706 

-- Skew,  746 

Small-sample  distribution,  216-17,  516 
Smoothing,  Kalman  filter  and,  394-97 
Spectral  analysis: 

population  spectrum,  152-57,  163-67,  269 
sample  periodogram,  158- 63, 272-75 
uses  of,  167-72 

Spectral  representation  theorem,  157 
Spectrum.  See also  Kernel  estimates; 

Periodogram 

coherence,  275 
cospectrum,  271-72 
cross,  270 
estimates  of, 163-67,  276-77,  283-85 
frequency  zero  and,  189, 283 
gain,  275 
low-frequency,  169 
-phase,  275 
population,  61-62,  152-57,  163-67,  269, 

} 

276-77 
quadrature,  271 
sample,  158-63,  272-75 
sums  of processes  and,  172 
transfer  function,  278 
vector-processes  and,  268-78 

Spurious  regression,  557-62 
Square  summable,  52 
Standard  deviation,  population,  740 

798  Subject Index 

State  equation,  372 
State-space  model.  See  Kalman  filter 
State  vector,  372 
Stationary/stationanity: 
covariance,  45-46 
difference,  444 
strictly,  46 
trend-stationary,  435 
vector,  258-59 
weakly,  45—46 

Steepest  ascent,  134-37 
Stochastic  processes: 

central  limit  theorem  for  stationary,  195 
composite,  172 
expectations  and,  43-45 

Stochastic  variable,  739 
Stock  prices,  37-38,  306-7,  422-24,  668-69, 

672 

Structural  econometric  models,  vector 

autoregression  and,  324-36 
Student’s  ¢ distribution.  See  ¢ distribution 
Summable: 

absolute,  52, 64 
square,  52 

Sums  of ARMA  processes,  102-8 

autocovariance  generating  function  of,  106 
AR,  107-8 
MA,  102-7 
spectrum  of,  172 
Superconsistent,  460 
Sup operator,  481 
ti 
Taxes,  361 
Taylor  series,  713-14,  737-38 
Taylor  theorem,  713,  737-38 
t distribution,  205,  213,  356-57,  409-10,  746, 

755 

Theorems  (named  after  authors): 

Cramér-Wold,  184 
De  Moivre,  153,  716-17 
Gauss-Markov,  203,  222 
Granger  representation,  582 
Khinchine’s,  183 
Taylor,  713,  737-38 

Three-stage  least  squares,  250 
Time  domain,  152 
Time  series  operators,  25—26 
Time  series  process,  43 
Time  trends,  25, 435.  See also  Trend-stationary 

approaches  to,  447—S0 
asymptotic  distribution  of, 454-60 
asymptotic  inference  for autoregressive 
"process  around,  463-72 
breaks in, 449-50 
hypothesis  testing  fos 461-63 
linear,  438 
OLS  estimation,  463 

Time-varying  parameters,  Kalman  filter  and, 

398-403 

Trace,  723 
Transition  matrix,  679 
Transposition,  723 
Trends  representation  (Stock- Watson), 

common,  578 

Trend-stationary,  435 

comparison  of unit  root process  and, 438-44 
forecasts  for, 439 
Triangular  factorization: 

block,  98-100 

covariance  matrix  and,  114-15 
description  of, 87-91 
maximum  likelihood  estimation  and,  128-29 
of a second-moment  matrix  and  linear 

projection,  92-95 

Triangular  representation,  576-78 
Trigonometry,  157,  166,  704-8 
f statistic,  204 
2SLS.  See.Two-stage  least  squares _ 
Two-stage  least  squares  (2SLS): 

_ 

asymptotic  distribution  of, 241-42 
coefficient  estimator,  238 
consistency  of,  240-41 
general  description  of,  238-39 
GMM  and,  420-21 
instrumental  variable  estimation,  242-43 

U 
Unconditional  density,  44  — 
Unconditional  mean,  44 
Uncorrelated,  92, 743 
Unidentified,  244 
Uniformly  integrable,  191 
Unimodal,  134 
Unit circle,  32, 709 
Unit  root process,  435-36.  See also 

: Dickey-Fuller  test 

asymptotic distribution, 475-77,  504-6 
Bayesian  analysis, 532-34 
Beveridge-Nelson  decomposition,  504, 

545-46 

comparison  of trend-stationary  and,  438-44 
difference versus  not  to difference,  651-53 
‘dynamic  multipliers,  442-44 
ee 

central Kit theorem and, 483-86 

', 444-47,  515-16 

iptotic  theory, 

544, 547  — 
: te 515-16 — 

mn, 

decomposition,  323-24 
population,  740 
of sample  mean,  188,  279-81 

Variance  ratio  test,  531-32 
Vech  operator,  300-301 
Vec  operator,  265 
Vector  autoregression.  See  also  Cointegration; 

Impulse-response  function 

autocovariances  and  convergence  results  for, 

264-66 

autocovariance  generating  function  and,  267 
Bayesian  analysis  and,  360-62 
cointegration  and,  579-80 
impulse-response  function  and,  318-23 
introduction  to,  257-61 
likelihood  ratio  test,  296-98 
Markov  chain  and,  679 
maximum  likelihood  estimation  and, 

291-302,  309-18  _ 

restricted,  309-18  © 
spectrum  for,  276 
standard  errors,  298,  301,  336-340 
stationarity,  259 | 
structural  econometric  models  and,  324- 36 
time-varying  parameters,  401-3 
unit  roots,  549-57 
univariate  representation,  349 

; 

Vector  martingale  difference  sequence,  189  — 
Vector  processes, asymptotic results. for 

nonstationary,  544-48 

Vectors,  forecasting,  77 

W 
Wald form, 213, 299 

were  | 2  3  oY  oct  Boa 

“ae  mii  GOS  SS  Oo — 

Se le.  YE  9 

oi 

a i  sey 

sf te 

ade te 

. 

$55  NATIONAL  LAW  SCHOOL  OF  INDIA 
i 
AA 
rem  ~~“ This  book must be retumed  by the date stamped  below 

UNIVERSITY  LIBRARY 
NAGARABHAVI,  BANGALORE  -  560 072. 

| 

eT  ee 

mon-~< 

Ne 

asa nnn 
a  ee 

as  (iin  hae 

A  A 

GEE 

CC 

RCT  Ry  te  wee 

Lore  yma 

- 

meen 

“old ‘nd slow developaieate ecessible to 
“first-year ‘graduate’ students and 
“nonspecialists. Moreover, ' the work's 
Z oo and i of ‘coverage will 
Ana we an v  invaluable 
amakie 

Series 

Tine 

a OO wultiap of the field be 
we and researchers alte, this volume 

y  PHO EL. 

Oe: 

"Hasailtian’s book | is ee It provides faad § treatment ge not only basic time series 
subjects,  but  also  topics  of central  interest  to  modern  macroeconomics  and  finance. 
Among  others,  these  include  vector  autoregressions,  generalized  method  of moments, 
seas  autoregressive  conditional  heteroskedasticity  and  unit  roots.  It is excellent  both  as  a 
Bees  text  in  a  graduate  time  series  course,  and  as  a  reference  for  those  already  conversant 

with basic econometrics." 

Kenneth  D.  West,  University  of Wisconsin 

"  This book will serve  as  a text for students  studying time series  econometrics...It  is very 
detailed,  very  weil  organized,  and  very  clear.  The  book  completely  dominates  its 
competitors.  It will be used  at all the top universities  in the U.S....It's  a winner!" 

Mark  Watson,  Northwestern  Untversity 

"This  book  is an  extremely  significant  contribution.  It will  be the only. book  to cover  in 
one  place the time series  tools used by all empirical  macroeconomists.  It is an  extremely 
important  book,  destined  to  become  a classic  and  adopted  by all  the  major  Ph.D. 
programs  in economics." 

John  H.  Cochrane,  University  of Chicago 

‘is Professor  of Economics  at the University  of California,  San Diego.  He 
holds a a Phi D.  fromthe  University  of  California,  Berkeley,  and  has  previously  taught  at  the 
University of Virginia. 

Published  by 

Levant  Books 

ceviny  &Olkata 

Exclusively  distributed  by 

Sarat  Book  Distributors 

SARAT  orders@saratbookhouse.com 

www.saratbookhouse.com 

>)  PRINCETON 

press.princeton.edu 

ISBN:  978-93-80663-43-2 

| | | 

